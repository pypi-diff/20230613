# Comparing `tmp/langchain_wenxin-0.3.0.tar.gz` & `tmp/langchain_wenxin-0.4.0.tar.gz`

## Comparing `langchain_wenxin-0.3.0.tar` & `langchain_wenxin-0.4.0.tar`

### file list

```diff
@@ -1,10 +1,10 @@
--rw-r--r--   0        0        0      124 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/src/langchain_wenxin/__about__.py
--rw-r--r--   0        0        0      102 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/src/langchain_wenxin/__init__.py
--rw-r--r--   0        0        0    13474 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/src/langchain_wenxin/llms.py
--rw-r--r--   0        0        0     3076 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/src/langchain_wenxin/retrievers.py
--rw-r--r--   0        0        0      102 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/tests/__init__.py
--rw-r--r--   0        0        0     3078 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/.gitignore
--rw-r--r--   0        0        0     1086 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/LICENSE.txt
--rw-r--r--   0        0        0     1117 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/README.md
--rw-r--r--   0        0        0     3252 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/pyproject.toml
--rw-r--r--   0        0        0     2116 2020-02-02 00:00:00.000000 langchain_wenxin-0.3.0/PKG-INFO
+-rw-r--r--   0        0        0      124 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/src/langchain_wenxin/__about__.py
+-rw-r--r--   0        0        0      102 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/src/langchain_wenxin/__init__.py
+-rw-r--r--   0        0        0    14827 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/src/langchain_wenxin/llms.py
+-rw-r--r--   0        0        0     3076 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/src/langchain_wenxin/retrievers.py
+-rw-r--r--   0        0        0      102 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/tests/__init__.py
+-rw-r--r--   0        0        0     3078 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/.gitignore
+-rw-r--r--   0        0        0     1086 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/LICENSE.txt
+-rw-r--r--   0        0        0     1117 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/README.md
+-rw-r--r--   0        0        0     3252 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/pyproject.toml
+-rw-r--r--   0        0        0     2116 2020-02-02 00:00:00.000000 langchain_wenxin-0.4.0/PKG-INFO
```

### Comparing `langchain_wenxin-0.3.0/src/langchain_wenxin/llms.py` & `langchain_wenxin-0.4.0/src/langchain_wenxin/llms.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 """Wrapper around Baidu Wenxin APIs."""
 import json
+import logging
 import time
 import warnings
 from typing import Any, Dict, Generator, List, Mapping, Optional, Tuple
 
 import requests
 import sseclient
 from langchain.callbacks.manager import (
@@ -18,14 +19,16 @@
     ChatGeneration,
     ChatResult,
     HumanMessage,
 )
 from langchain.utils import get_from_dict_or_env
 from pydantic import BaseModel, Extra, root_validator
 
+logger = logging.getLogger(__name__)
+
 
 class WenxinClient():
     WENXIN_TOKEN_URL = "https://aip.baidubce.com/oauth/2.0/token"
     WENXIN_CHAT_URL = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/{endpoint}"
 
     def __init__(self, baidu_api_key: str, baidu_secret_key: str,
                  request_timeout: Optional[int] = None):
@@ -79,74 +82,92 @@
 
         Args:
             model: The model to use.
             prompt: The prompt to pass into the model.
             **params: Additional parameters to pass to the API.
 
         Returns:
-            Generator: The string generated by the model.
+            Generator: The response generated by the model.
         """
-        params["access_token"] = self.grant_token()
+        params["messages"] = self.construct_message(prompt, history)
+        params["stream"] = True
+        url = self.completions_url(model)
+        logger.debug(f"call wenxin: url[{url}], params[{params}]")
         r = requests.post(
             url=self.completions_url(model),
-            params=params,
-            json={"messages": self.construct_message(
-                prompt, history), "stream": True},
+            params={"access_token": self.grant_token()},
+            json=params,
             timeout=self.request_timeout,
             stream=True,
         )
         r.raise_for_status()
         if not r.headers.get("Content-Type").startswith("text/event-stream"):
             response = r.json()
             error_code = response.get("error_code", 0)
             if error_code != 0:
                 error_msg = response.get("error_msg", "Unknown error")
                 msg = f"call wenxin failed, error_code: {error_code}, error_msg: {error_msg}"
                 raise Exception(msg)
-            return response["result"]
+            return response
 
         client = sseclient.SSEClient(r)
         for event in client.events():
             data = json.loads(event.data)
-            yield data["result"]
+            yield data
 
-    def completion(self, model: str, prompt: str, history: List[Tuple[str, str]], **params) -> str:
+    def completion(self, model: str, prompt: str, history: List[Tuple[str, str]], **params) -> Any:
         """Call out to Wenxin's generate endpoint.
 
         Args:
             model: The model to use.
             prompt: The prompt to pass into the model.
             **params: Additional parameters to pass to the API.
 
         Returns:
-            The string generated by the model.
+            The response generated by the model.
         """
-        params["access_token"] = self.grant_token()
+        params["messages"] = self.construct_message(prompt, history)
+        params["stream"] = False
+        url = self.completions_url(model)
+        logger.debug(f"call wenxin: url[{url}], params[{params}]")
         r = requests.post(
-            url=self.completions_url(model),
-            params=params,
-            json={"messages": self.construct_message(prompt, history)},
+            url=url,
+            params={"access_token": self.grant_token()},
+            json=params,
             timeout=self.request_timeout,
         )
         r.raise_for_status()
         response = r.json()
         error_code = response.get("error_code", 0)
         if error_code != 0:
             error_msg = response.get("error_msg", "Unknown error")
             msg = f"call wenxin failed, error_code: {error_code}, error_msg: {error_msg}"
             raise Exception(msg)
 
-        return response["result"]
+        return response
 
 
-class _BaiduCommon(BaseModel):
+class BaiduCommon(BaseModel):
     client: Any = None  #: :meta private:
     model: str = "wenxin"
     """Model name to use. supported models: wenxin/eb-instant"""
 
+    temperature: Optional[float] = None
+    """A non-negative float that tunes the degree of randomness in generation.
+    range: [0.0, 1.0]."""
+
+    penalty_score: Optional[float] = None
+    """Repeating punishment involves penalizing already generated tokens to reduce the occurrence of repetition.
+    The larger the value, the greater the punishment. Setting it too high can result in poorer text generation
+    for long texts. range: [1.0, 2.0]."""
+
+    top_p: Optional[float] = None
+    """Diversity influences the diversity of output text.
+    The larger the value, the stronger the diversity of the generated text. range: [0.0, 1.0]."""
+
     streaming: bool = False
     """Whether to stream the results."""
 
     request_timeout: Optional[int] = 600
     """Timeout for requests to Baidu Wenxin Completion API. Default is 600 seconds."""
 
     max_message_length: Optional[int] = 2000
@@ -174,36 +195,42 @@
         )
         return values
 
     @property
     def _default_params(self) -> Mapping[str, Any]:
         """Get the default parameters for calling Anthropic API."""
         d = {}
+        if self.temperature is not None:
+            d["temperature"] = self.temperature
+        if self.penalty_score is not None:
+            d["penalty_score"] = self.penalty_score
+        if self.top_p is not None:
+            d["top_p"] = self.top_p
         return d
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
         return {**{}, **self._default_params}
 
     def get_num_tokens(self, text: str) -> int:
-        """Calculate number of tokens."""
-        return 0
+        """Calculate number of tokens, use text length."""
+        return len(text)
 
 
-class Wenxin(LLM, _BaiduCommon):
+class Wenxin(LLM, BaiduCommon):
     r"""Wrapper around Baidu Wenxin large language models.
 
     To use, you should have the ``requests`` python package installed, and the
     environment variable ``BAIDU_API_KEY`` and ``BAIDU_SECRET_KEY``, or pass
     it as a named parameter to the constructor.
 
     Example:
         .. code-block:: python
-            from langchain.llms import Wenxin
+            from langchain_wenxin.llms import Wenxin
             model = Wenxin(model="wenxin", baidu_api_key="my-api-key",
                            baidu_secret_key="my-secret-key")
 
             # Simplest invocation:
             response = model("What are the biggest risks facing humanity?")
     """
 
@@ -255,24 +282,26 @@
                 model=self.model,
                 prompt=prompt,
                 history=[],
                 **self._default_params,
             )
             current_completion = ""
             for data in stream_resp:
+                result = data["result"]
                 if run_manager:
-                    run_manager.on_llm_new_token(data)
+                    run_manager.on_llm_new_token(result)
+                current_completion += result
             return current_completion
         response = self.client.completion(
             model=self.model,
             prompt=prompt,
             history=[],
             **self._default_params,
         )
-        return response
+        return response["result"]
 
     def stream(self, prompt: str, stop: Optional[List[str]] = None) -> Generator:
         r"""Call Baidu Wenxin completion_stream and return the resulting generator.
 
         BETA: this is a beta feature while we figure out the right abstraction.
         Once that happens, this interface could change.
 
@@ -288,31 +317,31 @@
 
 
                 prompt = "Write a poem about a stream."
                 generator = wenxin.stream(prompt)
                 for token in generator:
                     yield token
         """
-        return self.client.completion_stream(
+        for r in self.client.completion_stream(
             prompt=prompt,
             history=[],
-            **self._default_params,
-        )
+            **self._default_params):
+            yield r["result"]
 
 
-class ChatWenxin(BaseChatModel, _BaiduCommon):
+class ChatWenxin(BaseChatModel, BaiduCommon):
     r"""Wrapper around Baidu Wenxin's large language model.
 
     To use, you should have the ``requests`` python package installed, and the
     environment variable ``BAIDU_API_KEY`` and ``BAIDU_SECRET_KEY``, or pass
     it as a named parameter to the constructor.
 
     Example:
         .. code-block:: python
-            from langchain.llms import ChatWenxin
+            from langchain_wenxin.llms import ChatWenxin
             model = ChatWenxin(model="wenxin", baidu_api_key="my-api-key",
                            baidu_secret_key="my-secret-key")
 
             # Simplest invocation:
             response = model("What are the biggest risks facing humanity?")
     """
 
@@ -369,18 +398,19 @@
         params: Dict[str, Any] = {"prompt": prompt,
                                   "history": history, **self._default_params}
 
         if self.streaming:
             completion = ""
             stream_resp = self.client.completion_stream(**params)
             for delta in stream_resp:
-                completion += delta
+                result = delta["result"]
+                completion += result
                 if run_manager:
                     run_manager.on_llm_new_token(
-                        delta,
+                        result,
                     )
         else:
             response = self.client.completion(**params)
             completion = response
         message = AIMessage(content=completion)
         return ChatResult(generations=[ChatGeneration(message=message)])
```

### Comparing `langchain_wenxin-0.3.0/src/langchain_wenxin/retrievers.py` & `langchain_wenxin-0.4.0/src/langchain_wenxin/retrievers.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 from langchain.docstore.document import Document
 from langchain.schema import BaseRetriever
 
 
 @dataclass
 class BaizhongSearchParams:
     # 查询库索引,与项目ID一致
-    project_id: str
+    project_id: int
     # 返回query相关内容数量
     size: int
     # faiss检索返回数量
     db_top: Optional[int] = None
     # 精排排序结果条数
     rank_top: Optional[int] = None
     # 参与精排的最大条数
```

### Comparing `langchain_wenxin-0.3.0/.gitignore` & `langchain_wenxin-0.4.0/.gitignore`

 * *Files identical despite different names*

### Comparing `langchain_wenxin-0.3.0/LICENSE.txt` & `langchain_wenxin-0.4.0/LICENSE.txt`

 * *Files identical despite different names*

### Comparing `langchain_wenxin-0.3.0/README.md` & `langchain_wenxin-0.4.0/README.md`

 * *Files identical despite different names*

### Comparing `langchain_wenxin-0.3.0/pyproject.toml` & `langchain_wenxin-0.4.0/pyproject.toml`

 * *Files identical despite different names*

### Comparing `langchain_wenxin-0.3.0/PKG-INFO` & `langchain_wenxin-0.4.0/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: langchain-wenxin
-Version: 0.3.0
+Version: 0.4.0
 Project-URL: Documentation, https://github.com/unknown/langchain-wenxin#readme
 Project-URL: Issues, https://github.com/unknown/langchain-wenxin/issues
 Project-URL: Source, https://github.com/unknown/langchain-wenxin
 Author-email: Tao Yang <swulling@gmail.com>
 License-Expression: MIT
 License-File: LICENSE.txt
 Classifier: Development Status :: 4 - Beta
```

