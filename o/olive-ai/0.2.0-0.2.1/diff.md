# Comparing `tmp/olive_ai-0.2.0-py3-none-any.whl.zip` & `tmp/olive_ai-0.2.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,210 +1,217 @@
-Zip file size: 330328 bytes, number of entries: 208
--rw-rw-r--  2.0 unx     1147 b- defN 23-May-17 12:12 olive/__init__.py
--rw-rw-r--  2.0 unx     6064 b- defN 23-May-17 12:12 olive/cache.py
--rw-rw-r--  2.0 unx      989 b- defN 23-May-17 12:12 olive/constants.py
--rw-rw-r--  2.0 unx      469 b- defN 23-May-17 12:12 olive/extra_dependencies.json
--rw-rw-r--  2.0 unx     3758 b- defN 23-May-17 12:12 olive/hf_utils.py
--rw-rw-r--  2.0 unx      674 b- defN 23-May-17 12:12 olive/logging.py
--rw-rw-r--  2.0 unx    37681 b- defN 23-May-17 12:12 olive/model.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/azureml/__init__.py
--rw-rw-r--  2.0 unx     3383 b- defN 23-May-17 12:12 olive/azureml/azureml_client.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/common/__init__.py
--rw-rw-r--  2.0 unx     3453 b- defN 23-May-17 12:12 olive/common/auto_config.py
--rw-rw-r--  2.0 unx     7158 b- defN 23-May-17 12:12 olive/common/config_utils.py
--rw-rw-r--  2.0 unx     1468 b- defN 23-May-17 12:12 olive/common/import_lib.py
--rw-rw-r--  2.0 unx     2809 b- defN 23-May-17 12:12 olive/common/ort_inference.py
--rw-rw-r--  2.0 unx     1783 b- defN 23-May-17 12:12 olive/common/user_module_loader.py
--rw-rw-r--  2.0 unx     5298 b- defN 23-May-17 12:12 olive/common/utils.py
--rw-rw-r--  2.0 unx      345 b- defN 23-May-17 12:12 olive/data/__init__.py
--rw-rw-r--  2.0 unx     7641 b- defN 23-May-17 12:12 olive/data/config.py
--rw-rw-r--  2.0 unx     1452 b- defN 23-May-17 12:12 olive/data/constants.py
--rw-rw-r--  2.0 unx     7887 b- defN 23-May-17 12:12 olive/data/registry.py
--rw-rw-r--  2.0 unx      342 b- defN 23-May-17 12:12 olive/data/component/__init__.py
--rw-rw-r--  2.0 unx     1595 b- defN 23-May-17 12:12 olive/data/component/dataloader.py
--rw-rw-r--  2.0 unx     2398 b- defN 23-May-17 12:12 olive/data/component/load_dataset.py
--rw-rw-r--  2.0 unx      948 b- defN 23-May-17 12:12 olive/data/component/post_process_data.py
--rw-rw-r--  2.0 unx     2103 b- defN 23-May-17 12:12 olive/data/component/pre_process_data.py
--rw-rw-r--  2.0 unx      318 b- defN 23-May-17 12:12 olive/data/container/__init__.py
--rw-rw-r--  2.0 unx     2734 b- defN 23-May-17 12:12 olive/data/container/data_container.py
--rw-rw-r--  2.0 unx     1044 b- defN 23-May-17 12:12 olive/data/container/huggingface_container.py
--rw-rw-r--  2.0 unx      345 b- defN 23-May-17 12:12 olive/engine/__init__.py
--rw-rw-r--  2.0 unx    35473 b- defN 23-May-17 12:12 olive/engine/engine.py
--rw-rw-r--  2.0 unx    14347 b- defN 23-May-17 12:12 olive/engine/footprint.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/engine/packaging/__init__.py
--rw-rw-r--  2.0 unx      598 b- defN 23-May-17 12:12 olive/engine/packaging/packaging_config.py
--rw-rw-r--  2.0 unx    11341 b- defN 23-May-17 12:12 olive/engine/packaging/packaging_generator.py
--rw-rw-r--  2.0 unx     1588 b- defN 23-May-17 12:12 olive/engine/packaging/sample_code/ONNXModel/cpp/README.md
--rw-rw-r--  2.0 unx     4990 b- defN 23-May-17 12:12 olive/engine/packaging/sample_code/ONNXModel/cpp/code_sample.cpp
--rw-rw-r--  2.0 unx     1590 b- defN 23-May-17 12:12 olive/engine/packaging/sample_code/ONNXModel/cs/README.md
--rw-rw-r--  2.0 unx     4214 b- defN 23-May-17 12:12 olive/engine/packaging/sample_code/ONNXModel/cs/code_sample.cs
--rw-rw-r--  2.0 unx     2304 b- defN 23-May-17 12:12 olive/engine/packaging/sample_code/ONNXModel/python/README.md
--rw-rw-r--  2.0 unx     2531 b- defN 23-May-17 12:12 olive/engine/packaging/sample_code/ONNXModel/python/code_sample.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/evaluator/__init__.py
--rw-rw-r--  2.0 unx     3386 b- defN 23-May-17 12:12 olive/evaluator/accuracy.py
--rw-rw-r--  2.0 unx     4829 b- defN 23-May-17 12:12 olive/evaluator/metric.py
--rw-rw-r--  2.0 unx     3255 b- defN 23-May-17 12:12 olive/evaluator/metric_config.py
--rw-rw-r--  2.0 unx    18538 b- defN 23-May-17 12:12 olive/evaluator/olive_evaluator.py
--rw-rw-r--  2.0 unx      575 b- defN 23-May-17 12:12 olive/passes/__init__.py
--rw-rw-r--  2.0 unx    16976 b- defN 23-May-17 12:12 olive/passes/olive_pass.py
--rw-rw-r--  2.0 unx     5315 b- defN 23-May-17 12:12 olive/passes/pass_config.py
--rw-rw-r--  2.0 unx     1506 b- defN 23-May-17 12:12 olive/passes/onnx/__init__.py
--rw-rw-r--  2.0 unx     4512 b- defN 23-May-17 12:12 olive/passes/onnx/append_pre_post_processing_ops.py
--rw-rw-r--  2.0 unx     6086 b- defN 23-May-17 12:12 olive/passes/onnx/common.py
--rw-rw-r--  2.0 unx     4818 b- defN 23-May-17 12:12 olive/passes/onnx/conversion.py
--rw-rw-r--  2.0 unx     2861 b- defN 23-May-17 12:12 olive/passes/onnx/float16_conversion.py
--rw-rw-r--  2.0 unx    11658 b- defN 23-May-17 12:12 olive/passes/onnx/inc_quantization.py
--rw-rw-r--  2.0 unx     6355 b- defN 23-May-17 12:12 olive/passes/onnx/insert_beam_search.py
--rw-rw-r--  2.0 unx     7114 b- defN 23-May-17 12:12 olive/passes/onnx/mixed_precision.py
--rw-rw-r--  2.0 unx     5367 b- defN 23-May-17 12:12 olive/passes/onnx/model_optimizer.py
--rw-rw-r--  2.0 unx    12799 b- defN 23-May-17 12:12 olive/passes/onnx/perf_tuning.py
--rw-rw-r--  2.0 unx    20463 b- defN 23-May-17 12:12 olive/passes/onnx/quantization.py
--rw-rw-r--  2.0 unx    11428 b- defN 23-May-17 12:12 olive/passes/onnx/transformer_optimization.py
--rw-rw-r--  2.0 unx    11387 b- defN 23-May-17 12:12 olive/passes/onnx/vitis_ai_quantization.py
--rw-rw-r--  2.0 unx      449 b- defN 23-May-17 12:12 olive/passes/onnx/vitis_ai/__init__.py
--rw-rw-r--  2.0 unx     7272 b- defN 23-May-17 12:12 olive/passes/onnx/vitis_ai/calibrate.py
--rw-rw-r--  2.0 unx     7724 b- defN 23-May-17 12:12 olive/passes/onnx/vitis_ai/qdq_quantizer.py
--rw-rw-r--  2.0 unx     4044 b- defN 23-May-17 12:12 olive/passes/onnx/vitis_ai/quant_utils.py
--rw-rw-r--  2.0 unx    10774 b- defN 23-May-17 12:12 olive/passes/onnx/vitis_ai/quantize.py
--rw-rw-r--  2.0 unx    18375 b- defN 23-May-17 12:12 olive/passes/onnx/vitis_ai/refine.py
--rw-rw-r--  2.0 unx      448 b- defN 23-May-17 12:12 olive/passes/openvino/__init__.py
--rw-rw-r--  2.0 unx     3901 b- defN 23-May-17 12:12 olive/passes/openvino/conversion.py
--rw-rw-r--  2.0 unx     4206 b- defN 23-May-17 12:12 olive/passes/openvino/quantization.py
--rw-rw-r--  2.0 unx      375 b- defN 23-May-17 12:12 olive/passes/pytorch/__init__.py
--rw-rw-r--  2.0 unx     7068 b- defN 23-May-17 12:12 olive/passes/pytorch/cluster.py
--rw-rw-r--  2.0 unx      974 b- defN 23-May-17 12:12 olive/passes/pytorch/pytorch_lightning_utils.py
--rw-rw-r--  2.0 unx     8272 b- defN 23-May-17 12:12 olive/passes/pytorch/qat_utils.py
--rw-rw-r--  2.0 unx     5827 b- defN 23-May-17 12:12 olive/passes/pytorch/quantization_aware_training.py
--rw-rw-r--  2.0 unx      501 b- defN 23-May-17 12:12 olive/passes/snpe/__init__.py
--rw-rw-r--  2.0 unx     4670 b- defN 23-May-17 12:12 olive/passes/snpe/conversion.py
--rw-rw-r--  2.0 unx     3521 b- defN 23-May-17 12:12 olive/passes/snpe/quantization.py
--rw-rw-r--  2.0 unx     2299 b- defN 23-May-17 12:12 olive/passes/snpe/snpe_to_onnx.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/passes/utils/__init__.py
--rw-rw-r--  2.0 unx    10242 b- defN 23-May-17 12:12 olive/passes/utils/whisper_prepost.py
--rw-rw-r--  2.0 unx      432 b- defN 23-May-17 12:12 olive/snpe/__init__.py
--rw-rw-r--  2.0 unx     2496 b- defN 23-May-17 12:12 olive/snpe/configure.py
--rw-rw-r--  2.0 unx     1079 b- defN 23-May-17 12:12 olive/snpe/constants.py
--rw-rw-r--  2.0 unx     1046 b- defN 23-May-17 12:12 olive/snpe/copy_libcdsprpc.ps1
--rw-rw-r--  2.0 unx     1142 b- defN 23-May-17 12:12 olive/snpe/create_python36_env.sh
--rw-rw-r--  2.0 unx     8306 b- defN 23-May-17 12:12 olive/snpe/data_loader.py
--rw-rw-r--  2.0 unx     3908 b- defN 23-May-17 12:12 olive/snpe/snpe.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/snpe/tools/__init__.py
--rw-rw-r--  2.0 unx    10382 b- defN 23-May-17 12:12 olive/snpe/tools/dev.py
--rw-rw-r--  2.0 unx    17791 b- defN 23-May-17 12:12 olive/snpe/tools/inference.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-17 12:12 olive/snpe/utils/__init__.py
--rw-rw-r--  2.0 unx     6759 b- defN 23-May-17 12:12 olive/snpe/utils/adb.py
--rw-rw-r--  2.0 unx     8643 b- defN 23-May-17 12:12 olive/snpe/utils/input_list.py
--rw-rw-r--  2.0 unx     4867 b- defN 23-May-17 12:12 olive/snpe/utils/local.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/strategy/__init__.py
--rw-rw-r--  2.0 unx    11885 b- defN 23-May-17 12:12 olive/strategy/search_parameter.py
--rw-rw-r--  2.0 unx     5731 b- defN 23-May-17 12:12 olive/strategy/search_results.py
--rw-rw-r--  2.0 unx     4876 b- defN 23-May-17 12:12 olive/strategy/search_space.py
--rw-rw-r--  2.0 unx    10608 b- defN 23-May-17 12:12 olive/strategy/search_strategy.py
--rw-rw-r--  2.0 unx     2911 b- defN 23-May-17 12:12 olive/strategy/utils.py
--rw-rw-r--  2.0 unx      598 b- defN 23-May-17 12:12 olive/strategy/search_algorithm/__init__.py
--rw-rw-r--  2.0 unx     1153 b- defN 23-May-17 12:12 olive/strategy/search_algorithm/exhaustive.py
--rw-rw-r--  2.0 unx     4403 b- defN 23-May-17 12:12 olive/strategy/search_algorithm/optuna_sampler.py
--rw-rw-r--  2.0 unx     2498 b- defN 23-May-17 12:12 olive/strategy/search_algorithm/random_sampler.py
--rw-rw-r--  2.0 unx     2429 b- defN 23-May-17 12:12 olive/strategy/search_algorithm/search_algorithm.py
--rw-rw-r--  2.0 unx     1876 b- defN 23-May-17 12:12 olive/strategy/search_algorithm/tpe_sampler.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/systems/__init__.py
--rw-rw-r--  2.0 unx     2079 b- defN 23-May-17 12:12 olive/systems/common.py
--rw-rw-r--  2.0 unx     1535 b- defN 23-May-17 12:12 olive/systems/local.py
--rw-rw-r--  2.0 unx     1192 b- defN 23-May-17 12:12 olive/systems/olive_system.py
--rw-rw-r--  2.0 unx     3618 b- defN 23-May-17 12:12 olive/systems/system_config.py
--rw-rw-r--  2.0 unx     1507 b- defN 23-May-17 12:12 olive/systems/utils.py
--rw-rw-r--  2.0 unx      411 b- defN 23-May-17 12:12 olive/systems/azureml/__init__.py
--rw-rw-r--  2.0 unx     2076 b- defN 23-May-17 12:12 olive/systems/azureml/aml_evaluation_runner.py
--rw-rw-r--  2.0 unx     4179 b- defN 23-May-17 12:12 olive/systems/azureml/aml_pass_runner.py
--rw-rw-r--  2.0 unx    21637 b- defN 23-May-17 12:12 olive/systems/azureml/aml_system.py
--rw-rw-r--  2.0 unx      697 b- defN 23-May-17 12:12 olive/systems/docker/Dockerfile
--rw-rw-r--  2.0 unx      407 b- defN 23-May-17 12:12 olive/systems/docker/__init__.py
--rw-rw-r--  2.0 unx      651 b- defN 23-May-17 12:12 olive/systems/docker/dev_mount_cleanup.py
--rw-rw-r--  2.0 unx     7426 b- defN 23-May-17 12:12 olive/systems/docker/docker_system.py
--rw-rw-r--  2.0 unx     1776 b- defN 23-May-17 12:12 olive/systems/docker/eval.py
--rw-rw-r--  2.0 unx     5352 b- defN 23-May-17 12:12 olive/systems/docker/utils.py
--rw-rw-r--  2.0 unx      381 b- defN 23-May-17 12:12 olive/systems/python_environment/__init__.py
--rw-rw-r--  2.0 unx      874 b- defN 23-May-17 12:12 olive/systems/python_environment/available_eps.py
--rw-rw-r--  2.0 unx     4050 b- defN 23-May-17 12:12 olive/systems/python_environment/inference_runner.py
--rw-rw-r--  2.0 unx     1485 b- defN 23-May-17 12:12 olive/systems/python_environment/is_valid_ep.py
--rw-rw-r--  2.0 unx    11513 b- defN 23-May-17 12:12 olive/systems/python_environment/python_environment_system.py
--rw-rw-r--  2.0 unx      287 b- defN 23-May-17 12:12 olive/workflows/__init__.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/workflows/run/__init__.py
--rw-rw-r--  2.0 unx      634 b- defN 23-May-17 12:12 olive/workflows/run/__main__.py
--rw-rw-r--  2.0 unx     7204 b- defN 23-May-17 12:12 olive/workflows/run/config.py
--rw-rw-r--  2.0 unx     6319 b- defN 23-May-17 12:12 olive/workflows/run/run.py
--rw-rw-r--  2.0 unx      388 b- defN 23-May-17 12:12 olive/workflows/snpe/__init__.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/workflows/snpe/convertquantize/__init__.py
--rw-rw-r--  2.0 unx     1339 b- defN 23-May-17 12:12 olive/workflows/snpe/convertquantize/__main__.py
--rw-rw-r--  2.0 unx     4322 b- defN 23-May-17 12:12 olive/workflows/snpe/convertquantize/convertquantize.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 olive/workflows/snpe/evaluate/__init__.py
--rw-rw-r--  2.0 unx      955 b- defN 23-May-17 12:12 olive/workflows/snpe/evaluate/__main__.py
--rw-rw-r--  2.0 unx     2879 b- defN 23-May-17 12:12 olive/workflows/snpe/evaluate/evaluate.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/integ_test/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-17 12:12 test/integ_test/evaluator/__init__.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/integ_test/evaluator/azureml_eval/__init__.py
--rw-rw-r--  2.0 unx     1587 b- defN 23-May-17 12:12 test/integ_test/evaluator/azureml_eval/test_aml_evaluation.py
--rw-rw-r--  2.0 unx      555 b- defN 23-May-17 12:12 test/integ_test/evaluator/azureml_eval/user_script.py
--rw-rw-r--  2.0 unx     4649 b- defN 23-May-17 12:12 test/integ_test/evaluator/azureml_eval/utils.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/integ_test/evaluator/docker_eval/__init__.py
--rw-rw-r--  2.0 unx     2141 b- defN 23-May-17 12:12 test/integ_test/evaluator/docker_eval/test_docker_evaluation.py
--rw-rw-r--  2.0 unx     1787 b- defN 23-May-17 12:12 test/integ_test/evaluator/docker_eval/user_script.py
--rw-rw-r--  2.0 unx     4033 b- defN 23-May-17 12:12 test/integ_test/evaluator/docker_eval/utils.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/integ_test/evaluator/local_eval/__init__.py
--rw-rw-r--  2.0 unx     1942 b- defN 23-May-17 12:12 test/integ_test/evaluator/local_eval/test_local_evaluation.py
--rw-rw-r--  2.0 unx     5153 b- defN 23-May-17 12:12 test/integ_test/evaluator/local_eval/utils.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/__init__.py
--rw-rw-r--  2.0 unx      549 b- defN 23-May-17 12:12 test/unit_test/conftest.py
--rw-rw-r--  2.0 unx     4266 b- defN 23-May-17 12:12 test/unit_test/test_cache.py
--rw-rw-r--  2.0 unx     5840 b- defN 23-May-17 12:12 test/unit_test/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-17 12:12 test/unit_test/common/__init__.py
--rw-rw-r--  2.0 unx     3560 b- defN 23-May-17 12:12 test/unit_test/common/test_import_lib.py
--rw-rw-r--  2.0 unx     1247 b- defN 23-May-17 12:12 test/unit_test/common/test_retry.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/data_container/__init__.py
--rw-rw-r--  2.0 unx     1266 b- defN 23-May-17 12:12 test/unit_test/data_container/test_config.py
--rw-rw-r--  2.0 unx     1899 b- defN 23-May-17 12:12 test/unit_test/data_container/test_data_container.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-17 12:12 test/unit_test/engine/__init__.py
--rw-rw-r--  2.0 unx    12138 b- defN 23-May-17 12:12 test/unit_test/engine/test_engine.py
--rw-rw-r--  2.0 unx     2423 b- defN 23-May-17 12:12 test/unit_test/engine/test_footprint.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/engine/packaging/__init__.py
--rw-rw-r--  2.0 unx     4141 b- defN 23-May-17 12:12 test/unit_test/engine/packaging/test_packaging_generator.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/evaluator/__init__.py
--rw-rw-r--  2.0 unx     3348 b- defN 23-May-17 12:12 test/unit_test/evaluator/test_accuracy.py
--rw-rw-r--  2.0 unx     4442 b- defN 23-May-17 12:12 test/unit_test/evaluator/test_olive_evaluator.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/hf_utils/__init__.py
--rw-rw-r--  2.0 unx     1021 b- defN 23-May-17 12:12 test/unit_test/hf_utils/test_hf_utils.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/model/__init__.py
--rw-rw-r--  2.0 unx     3040 b- defN 23-May-17 12:12 test/unit_test/model/test_pytorch_model.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/passes/__init__.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/passes/common/__init__.py
--rw-rw-r--  2.0 unx     1353 b- defN 23-May-17 12:12 test/unit_test/passes/common/test_user_script.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/passes/inc/__init__.py
--rw-rw-r--  2.0 unx     4669 b- defN 23-May-17 12:12 test/unit_test/passes/inc/test_inc_quantization.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/passes/onnx/__init__.py
--rw-rw-r--  2.0 unx      931 b- defN 23-May-17 12:12 test/unit_test/passes/onnx/test_insert_beam_search.py
--rw-rw-r--  2.0 unx      632 b- defN 23-May-17 12:12 test/unit_test/passes/onnx/test_mixed_precision.py
--rw-rw-r--  2.0 unx      866 b- defN 23-May-17 12:12 test/unit_test/passes/onnx/test_model_optimizer.py
--rw-rw-r--  2.0 unx      921 b- defN 23-May-17 12:12 test/unit_test/passes/onnx/test_perf_tuning.py
--rw-rw-r--  2.0 unx     3327 b- defN 23-May-17 12:12 test/unit_test/passes/onnx/test_pre_post_processing_op.py
--rw-rw-r--  2.0 unx     2044 b- defN 23-May-17 12:12 test/unit_test/passes/onnx/test_transformer_optimization.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/passes/openvino/__init__.py
--rw-rw-r--  2.0 unx     2062 b- defN 23-May-17 12:12 test/unit_test/passes/openvino/test_openvino_conversion.py
--rw-rw-r--  2.0 unx     4362 b- defN 23-May-17 12:12 test/unit_test/passes/openvino/test_openvino_quantization.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/passes/pytorch/__init__.py
--rw-rw-r--  2.0 unx      985 b- defN 23-May-17 12:12 test/unit_test/passes/pytorch/test_quantization_aware_training.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/passes/vitis_ai/__init__.py
--rw-rw-r--  2.0 unx     2245 b- defN 23-May-17 12:12 test/unit_test/passes/vitis_ai/test_vitis_ai_quantization.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/systems/__init__.py
--rw-rw-r--  2.0 unx     3331 b- defN 23-May-17 12:12 test/unit_test/systems/test_local.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/systems/azureml/__init__.py
--rw-rw-r--  2.0 unx    11185 b- defN 23-May-17 12:12 test/unit_test/systems/azureml/test_aml_system.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/systems/docker/__init__.py
--rw-rw-r--  2.0 unx     7650 b- defN 23-May-17 12:12 test/unit_test/systems/docker/test_docker_system.py
--rw-rw-r--  2.0 unx      247 b- defN 23-May-17 12:12 test/unit_test/systems/python_environment/__init__.py
--rw-rw-r--  2.0 unx     3625 b- defN 23-May-17 12:12 test/unit_test/systems/python_environment/test_python_environment_system.py
--rw-rw-r--  2.0 unx     1141 b- defN 23-May-17 12:13 olive_ai-0.2.0.dist-info/LICENSE
--rw-rw-r--  2.0 unx     2700 b- defN 23-May-17 12:13 olive_ai-0.2.0.dist-info/METADATA
--rw-rw-r--  2.0 unx   760804 b- defN 23-May-17 12:13 olive_ai-0.2.0.dist-info/NOTICE.txt
--rw-rw-r--  2.0 unx       92 b- defN 23-May-17 12:13 olive_ai-0.2.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       11 b- defN 23-May-17 12:13 olive_ai-0.2.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    19492 b- defN 23-May-17 12:13 olive_ai-0.2.0.dist-info/RECORD
-208 files, 1570783 bytes uncompressed, 299002 bytes compressed:  81.0%
+Zip file size: 343054 bytes, number of entries: 215
+-rw-rw-r--  2.0 unx      608 b- defN 23-May-23 07:12 olive/__init__.py
+-rw-rw-r--  2.0 unx     6064 b- defN 23-May-23 07:12 olive/cache.py
+-rw-rw-r--  2.0 unx     1013 b- defN 23-May-23 07:12 olive/constants.py
+-rw-rw-r--  2.0 unx      511 b- defN 23-May-23 07:12 olive/extra_dependencies.json
+-rw-rw-r--  2.0 unx     3758 b- defN 23-May-23 07:12 olive/hf_utils.py
+-rw-rw-r--  2.0 unx     1210 b- defN 23-May-23 07:12 olive/logging.py
+-rw-rw-r--  2.0 unx    38321 b- defN 23-May-23 07:12 olive/model.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/azureml/__init__.py
+-rw-rw-r--  2.0 unx     3383 b- defN 23-May-23 07:12 olive/azureml/azureml_client.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/common/__init__.py
+-rw-rw-r--  2.0 unx     3453 b- defN 23-May-23 07:12 olive/common/auto_config.py
+-rw-rw-r--  2.0 unx     7878 b- defN 23-May-23 07:12 olive/common/config_utils.py
+-rw-rw-r--  2.0 unx     1468 b- defN 23-May-23 07:12 olive/common/import_lib.py
+-rw-rw-r--  2.0 unx     2809 b- defN 23-May-23 07:12 olive/common/ort_inference.py
+-rw-rw-r--  2.0 unx     1783 b- defN 23-May-23 07:12 olive/common/user_module_loader.py
+-rw-rw-r--  2.0 unx     5298 b- defN 23-May-23 07:12 olive/common/utils.py
+-rw-rw-r--  2.0 unx      345 b- defN 23-May-23 07:12 olive/data/__init__.py
+-rw-rw-r--  2.0 unx     7641 b- defN 23-May-23 07:12 olive/data/config.py
+-rw-rw-r--  2.0 unx     1452 b- defN 23-May-23 07:12 olive/data/constants.py
+-rw-rw-r--  2.0 unx     7887 b- defN 23-May-23 07:12 olive/data/registry.py
+-rw-rw-r--  2.0 unx      342 b- defN 23-May-23 07:12 olive/data/component/__init__.py
+-rw-rw-r--  2.0 unx     1595 b- defN 23-May-23 07:12 olive/data/component/dataloader.py
+-rw-rw-r--  2.0 unx     2398 b- defN 23-May-23 07:12 olive/data/component/load_dataset.py
+-rw-rw-r--  2.0 unx      948 b- defN 23-May-23 07:12 olive/data/component/post_process_data.py
+-rw-rw-r--  2.0 unx     2103 b- defN 23-May-23 07:12 olive/data/component/pre_process_data.py
+-rw-rw-r--  2.0 unx      318 b- defN 23-May-23 07:12 olive/data/container/__init__.py
+-rw-rw-r--  2.0 unx     2734 b- defN 23-May-23 07:12 olive/data/container/data_container.py
+-rw-rw-r--  2.0 unx     1044 b- defN 23-May-23 07:12 olive/data/container/huggingface_container.py
+-rw-rw-r--  2.0 unx      345 b- defN 23-May-23 07:12 olive/engine/__init__.py
+-rw-rw-r--  2.0 unx     1289 b- defN 23-May-23 07:12 olive/engine/config.py
+-rw-rw-r--  2.0 unx    38411 b- defN 23-May-23 07:12 olive/engine/engine.py
+-rw-rw-r--  2.0 unx    14617 b- defN 23-May-23 07:12 olive/engine/footprint.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/engine/packaging/__init__.py
+-rw-rw-r--  2.0 unx      598 b- defN 23-May-23 07:12 olive/engine/packaging/packaging_config.py
+-rw-rw-r--  2.0 unx    11782 b- defN 23-May-23 07:12 olive/engine/packaging/packaging_generator.py
+-rw-rw-r--  2.0 unx     1588 b- defN 23-May-23 07:12 olive/engine/packaging/sample_code/ONNXModel/cpp/README.md
+-rw-rw-r--  2.0 unx     4990 b- defN 23-May-23 07:12 olive/engine/packaging/sample_code/ONNXModel/cpp/code_sample.cpp
+-rw-rw-r--  2.0 unx     1590 b- defN 23-May-23 07:12 olive/engine/packaging/sample_code/ONNXModel/cs/README.md
+-rw-rw-r--  2.0 unx     4214 b- defN 23-May-23 07:12 olive/engine/packaging/sample_code/ONNXModel/cs/code_sample.cs
+-rw-rw-r--  2.0 unx     2304 b- defN 23-May-23 07:12 olive/engine/packaging/sample_code/ONNXModel/python/README.md
+-rw-rw-r--  2.0 unx     2531 b- defN 23-May-23 07:12 olive/engine/packaging/sample_code/ONNXModel/python/code_sample.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/evaluator/__init__.py
+-rw-rw-r--  2.0 unx     3386 b- defN 23-May-23 07:12 olive/evaluator/accuracy.py
+-rw-rw-r--  2.0 unx     6271 b- defN 23-May-23 07:12 olive/evaluator/metric.py
+-rw-rw-r--  2.0 unx     3255 b- defN 23-May-23 07:12 olive/evaluator/metric_config.py
+-rw-rw-r--  2.0 unx    29490 b- defN 23-May-23 07:12 olive/evaluator/olive_evaluator.py
+-rw-rw-r--  2.0 unx      354 b- defN 23-May-23 07:12 olive/hardware/__init__.py
+-rw-rw-r--  2.0 unx     2975 b- defN 23-May-23 07:12 olive/hardware/accelerator.py
+-rw-rw-r--  2.0 unx      575 b- defN 23-May-23 07:12 olive/passes/__init__.py
+-rw-rw-r--  2.0 unx    18125 b- defN 23-May-23 07:12 olive/passes/olive_pass.py
+-rw-rw-r--  2.0 unx     5138 b- defN 23-May-23 07:12 olive/passes/pass_config.py
+-rw-rw-r--  2.0 unx     1681 b- defN 23-May-23 07:12 olive/passes/onnx/__init__.py
+-rw-rw-r--  2.0 unx     4600 b- defN 23-May-23 07:12 olive/passes/onnx/append_pre_post_processing_ops.py
+-rw-rw-r--  2.0 unx     6086 b- defN 23-May-23 07:12 olive/passes/onnx/common.py
+-rw-rw-r--  2.0 unx     4906 b- defN 23-May-23 07:12 olive/passes/onnx/conversion.py
+-rw-rw-r--  2.0 unx     2949 b- defN 23-May-23 07:12 olive/passes/onnx/float16_conversion.py
+-rw-rw-r--  2.0 unx    11812 b- defN 23-May-23 07:12 olive/passes/onnx/inc_quantization.py
+-rw-rw-r--  2.0 unx     6443 b- defN 23-May-23 07:12 olive/passes/onnx/insert_beam_search.py
+-rw-rw-r--  2.0 unx     7202 b- defN 23-May-23 07:12 olive/passes/onnx/mixed_precision.py
+-rw-rw-r--  2.0 unx     5455 b- defN 23-May-23 07:12 olive/passes/onnx/model_optimizer.py
+-rw-rw-r--  2.0 unx     1957 b- defN 23-May-23 07:12 olive/passes/onnx/optimum_conversion.py
+-rw-rw-r--  2.0 unx     3372 b- defN 23-May-23 07:12 olive/passes/onnx/optimum_merging.py
+-rw-rw-r--  2.0 unx    13482 b- defN 23-May-23 07:12 olive/passes/onnx/perf_tuning.py
+-rw-rw-r--  2.0 unx    20617 b- defN 23-May-23 07:12 olive/passes/onnx/quantization.py
+-rw-rw-r--  2.0 unx     5270 b- defN 23-May-23 07:12 olive/passes/onnx/transformer_optimization.py
+-rw-rw-r--  2.0 unx    11463 b- defN 23-May-23 07:12 olive/passes/onnx/vitis_ai_quantization.py
+-rw-rw-r--  2.0 unx      449 b- defN 23-May-23 07:12 olive/passes/onnx/vitis_ai/__init__.py
+-rw-rw-r--  2.0 unx     7272 b- defN 23-May-23 07:12 olive/passes/onnx/vitis_ai/calibrate.py
+-rw-rw-r--  2.0 unx     7755 b- defN 23-May-23 07:12 olive/passes/onnx/vitis_ai/qdq_quantizer.py
+-rw-rw-r--  2.0 unx     4044 b- defN 23-May-23 07:12 olive/passes/onnx/vitis_ai/quant_utils.py
+-rw-rw-r--  2.0 unx    10774 b- defN 23-May-23 07:12 olive/passes/onnx/vitis_ai/quantize.py
+-rw-rw-r--  2.0 unx    18348 b- defN 23-May-23 07:12 olive/passes/onnx/vitis_ai/refine.py
+-rw-rw-r--  2.0 unx      448 b- defN 23-May-23 07:12 olive/passes/openvino/__init__.py
+-rw-rw-r--  2.0 unx     3989 b- defN 23-May-23 07:12 olive/passes/openvino/conversion.py
+-rw-rw-r--  2.0 unx     4294 b- defN 23-May-23 07:12 olive/passes/openvino/quantization.py
+-rw-rw-r--  2.0 unx      375 b- defN 23-May-23 07:12 olive/passes/pytorch/__init__.py
+-rw-rw-r--  2.0 unx     7068 b- defN 23-May-23 07:12 olive/passes/pytorch/cluster.py
+-rw-rw-r--  2.0 unx     1044 b- defN 23-May-23 07:12 olive/passes/pytorch/pytorch_lightning_utils.py
+-rw-rw-r--  2.0 unx     8382 b- defN 23-May-23 07:12 olive/passes/pytorch/qat_utils.py
+-rw-rw-r--  2.0 unx     6035 b- defN 23-May-23 07:12 olive/passes/pytorch/quantization_aware_training.py
+-rw-rw-r--  2.0 unx      501 b- defN 23-May-23 07:12 olive/passes/snpe/__init__.py
+-rw-rw-r--  2.0 unx     4758 b- defN 23-May-23 07:12 olive/passes/snpe/conversion.py
+-rw-rw-r--  2.0 unx     3609 b- defN 23-May-23 07:12 olive/passes/snpe/quantization.py
+-rw-rw-r--  2.0 unx     2387 b- defN 23-May-23 07:12 olive/passes/snpe/snpe_to_onnx.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/passes/utils/__init__.py
+-rw-rw-r--  2.0 unx    10242 b- defN 23-May-23 07:12 olive/passes/utils/whisper_prepost.py
+-rw-rw-r--  2.0 unx      432 b- defN 23-May-23 07:12 olive/snpe/__init__.py
+-rw-rw-r--  2.0 unx     2496 b- defN 23-May-23 07:12 olive/snpe/configure.py
+-rw-rw-r--  2.0 unx     1079 b- defN 23-May-23 07:12 olive/snpe/constants.py
+-rw-rw-r--  2.0 unx     1046 b- defN 23-May-23 07:12 olive/snpe/copy_libcdsprpc.ps1
+-rw-rw-r--  2.0 unx     1142 b- defN 23-May-23 07:12 olive/snpe/create_python36_env.sh
+-rw-rw-r--  2.0 unx     8306 b- defN 23-May-23 07:12 olive/snpe/data_loader.py
+-rw-rw-r--  2.0 unx     3908 b- defN 23-May-23 07:12 olive/snpe/snpe.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/snpe/tools/__init__.py
+-rw-rw-r--  2.0 unx    10382 b- defN 23-May-23 07:12 olive/snpe/tools/dev.py
+-rw-rw-r--  2.0 unx    17791 b- defN 23-May-23 07:12 olive/snpe/tools/inference.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-23 07:12 olive/snpe/utils/__init__.py
+-rw-rw-r--  2.0 unx     6759 b- defN 23-May-23 07:12 olive/snpe/utils/adb.py
+-rw-rw-r--  2.0 unx     8643 b- defN 23-May-23 07:12 olive/snpe/utils/input_list.py
+-rw-rw-r--  2.0 unx     4867 b- defN 23-May-23 07:12 olive/snpe/utils/local.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/strategy/__init__.py
+-rw-rw-r--  2.0 unx    11885 b- defN 23-May-23 07:12 olive/strategy/search_parameter.py
+-rw-rw-r--  2.0 unx     5736 b- defN 23-May-23 07:12 olive/strategy/search_results.py
+-rw-rw-r--  2.0 unx     4876 b- defN 23-May-23 07:12 olive/strategy/search_space.py
+-rw-rw-r--  2.0 unx    10731 b- defN 23-May-23 07:12 olive/strategy/search_strategy.py
+-rw-rw-r--  2.0 unx     2911 b- defN 23-May-23 07:12 olive/strategy/utils.py
+-rw-rw-r--  2.0 unx      598 b- defN 23-May-23 07:12 olive/strategy/search_algorithm/__init__.py
+-rw-rw-r--  2.0 unx     1153 b- defN 23-May-23 07:12 olive/strategy/search_algorithm/exhaustive.py
+-rw-rw-r--  2.0 unx     4420 b- defN 23-May-23 07:12 olive/strategy/search_algorithm/optuna_sampler.py
+-rw-rw-r--  2.0 unx     2498 b- defN 23-May-23 07:12 olive/strategy/search_algorithm/random_sampler.py
+-rw-rw-r--  2.0 unx     2429 b- defN 23-May-23 07:12 olive/strategy/search_algorithm/search_algorithm.py
+-rw-rw-r--  2.0 unx     1876 b- defN 23-May-23 07:12 olive/strategy/search_algorithm/tpe_sampler.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/systems/__init__.py
+-rw-rw-r--  2.0 unx     1970 b- defN 23-May-23 07:12 olive/systems/common.py
+-rw-rw-r--  2.0 unx     1975 b- defN 23-May-23 07:12 olive/systems/local.py
+-rw-rw-r--  2.0 unx     1331 b- defN 23-May-23 07:12 olive/systems/olive_system.py
+-rw-rw-r--  2.0 unx     3581 b- defN 23-May-23 07:12 olive/systems/system_config.py
+-rw-rw-r--  2.0 unx     1507 b- defN 23-May-23 07:12 olive/systems/utils.py
+-rw-rw-r--  2.0 unx      411 b- defN 23-May-23 07:12 olive/systems/azureml/__init__.py
+-rw-rw-r--  2.0 unx     2615 b- defN 23-May-23 07:12 olive/systems/azureml/aml_evaluation_runner.py
+-rw-rw-r--  2.0 unx     4573 b- defN 23-May-23 07:12 olive/systems/azureml/aml_pass_runner.py
+-rw-rw-r--  2.0 unx    22712 b- defN 23-May-23 07:12 olive/systems/azureml/aml_system.py
+-rw-rw-r--  2.0 unx      697 b- defN 23-May-23 07:12 olive/systems/docker/Dockerfile
+-rw-rw-r--  2.0 unx      407 b- defN 23-May-23 07:12 olive/systems/docker/__init__.py
+-rw-rw-r--  2.0 unx      651 b- defN 23-May-23 07:12 olive/systems/docker/dev_mount_cleanup.py
+-rw-rw-r--  2.0 unx     7595 b- defN 23-May-23 07:12 olive/systems/docker/docker_system.py
+-rw-rw-r--  2.0 unx     1778 b- defN 23-May-23 07:12 olive/systems/docker/eval.py
+-rw-rw-r--  2.0 unx     5352 b- defN 23-May-23 07:12 olive/systems/docker/utils.py
+-rw-rw-r--  2.0 unx      381 b- defN 23-May-23 07:12 olive/systems/python_environment/__init__.py
+-rw-rw-r--  2.0 unx      874 b- defN 23-May-23 07:12 olive/systems/python_environment/available_eps.py
+-rw-rw-r--  2.0 unx     4050 b- defN 23-May-23 07:12 olive/systems/python_environment/inference_runner.py
+-rw-rw-r--  2.0 unx     1485 b- defN 23-May-23 07:12 olive/systems/python_environment/is_valid_ep.py
+-rw-rw-r--  2.0 unx    11659 b- defN 23-May-23 07:12 olive/systems/python_environment/python_environment_system.py
+-rw-rw-r--  2.0 unx      287 b- defN 23-May-23 07:12 olive/workflows/__init__.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/workflows/run/__init__.py
+-rw-rw-r--  2.0 unx      634 b- defN 23-May-23 07:12 olive/workflows/run/__main__.py
+-rw-rw-r--  2.0 unx     7178 b- defN 23-May-23 07:12 olive/workflows/run/config.py
+-rw-rw-r--  2.0 unx     6434 b- defN 23-May-23 07:12 olive/workflows/run/run.py
+-rw-rw-r--  2.0 unx      388 b- defN 23-May-23 07:12 olive/workflows/snpe/__init__.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/workflows/snpe/convertquantize/__init__.py
+-rw-rw-r--  2.0 unx     1339 b- defN 23-May-23 07:12 olive/workflows/snpe/convertquantize/__main__.py
+-rw-rw-r--  2.0 unx     4322 b- defN 23-May-23 07:12 olive/workflows/snpe/convertquantize/convertquantize.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 olive/workflows/snpe/evaluate/__init__.py
+-rw-rw-r--  2.0 unx      955 b- defN 23-May-23 07:12 olive/workflows/snpe/evaluate/__main__.py
+-rw-rw-r--  2.0 unx     2873 b- defN 23-May-23 07:12 olive/workflows/snpe/evaluate/evaluate.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/integ_test/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-23 07:12 test/integ_test/evaluator/__init__.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/integ_test/evaluator/azureml_eval/__init__.py
+-rw-rw-r--  2.0 unx     1834 b- defN 23-May-23 07:12 test/integ_test/evaluator/azureml_eval/test_aml_evaluation.py
+-rw-rw-r--  2.0 unx      555 b- defN 23-May-23 07:12 test/integ_test/evaluator/azureml_eval/user_script.py
+-rw-rw-r--  2.0 unx     4675 b- defN 23-May-23 07:12 test/integ_test/evaluator/azureml_eval/utils.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/integ_test/evaluator/docker_eval/__init__.py
+-rw-rw-r--  2.0 unx     2388 b- defN 23-May-23 07:12 test/integ_test/evaluator/docker_eval/test_docker_evaluation.py
+-rw-rw-r--  2.0 unx     1787 b- defN 23-May-23 07:12 test/integ_test/evaluator/docker_eval/user_script.py
+-rw-rw-r--  2.0 unx     4059 b- defN 23-May-23 07:12 test/integ_test/evaluator/docker_eval/utils.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/integ_test/evaluator/local_eval/__init__.py
+-rw-rw-r--  2.0 unx     2189 b- defN 23-May-23 07:12 test/integ_test/evaluator/local_eval/test_local_evaluation.py
+-rw-rw-r--  2.0 unx     5231 b- defN 23-May-23 07:12 test/integ_test/evaluator/local_eval/utils.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/__init__.py
+-rw-rw-r--  2.0 unx      549 b- defN 23-May-23 07:12 test/unit_test/conftest.py
+-rw-rw-r--  2.0 unx     4266 b- defN 23-May-23 07:12 test/unit_test/test_cache.py
+-rw-rw-r--  2.0 unx     6084 b- defN 23-May-23 07:12 test/unit_test/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-23 07:12 test/unit_test/common/__init__.py
+-rw-rw-r--  2.0 unx     3560 b- defN 23-May-23 07:12 test/unit_test/common/test_import_lib.py
+-rw-rw-r--  2.0 unx     1247 b- defN 23-May-23 07:12 test/unit_test/common/test_retry.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/data_container/__init__.py
+-rw-rw-r--  2.0 unx     1266 b- defN 23-May-23 07:12 test/unit_test/data_container/test_config.py
+-rw-rw-r--  2.0 unx     1899 b- defN 23-May-23 07:12 test/unit_test/data_container/test_data_container.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-23 07:12 test/unit_test/engine/__init__.py
+-rw-rw-r--  2.0 unx    13575 b- defN 23-May-23 07:12 test/unit_test/engine/test_engine.py
+-rw-rw-r--  2.0 unx     2637 b- defN 23-May-23 07:12 test/unit_test/engine/test_footprint.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/engine/packaging/__init__.py
+-rw-rw-r--  2.0 unx     4404 b- defN 23-May-23 07:12 test/unit_test/engine/packaging/test_packaging_generator.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/evaluator/__init__.py
+-rw-rw-r--  2.0 unx     3348 b- defN 23-May-23 07:12 test/unit_test/evaluator/test_accuracy.py
+-rw-rw-r--  2.0 unx     1908 b- defN 23-May-23 07:12 test/unit_test/evaluator/test_metric.py
+-rw-rw-r--  2.0 unx     5803 b- defN 23-May-23 07:12 test/unit_test/evaluator/test_olive_evaluator.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/hf_utils/__init__.py
+-rw-rw-r--  2.0 unx     1021 b- defN 23-May-23 07:12 test/unit_test/hf_utils/test_hf_utils.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/model/__init__.py
+-rw-rw-r--  2.0 unx     3403 b- defN 23-May-23 07:12 test/unit_test/model/test_pytorch_model.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/passes/__init__.py
+-rw-rw-r--  2.0 unx      878 b- defN 23-May-23 07:12 test/unit_test/passes/test_pass_serialization.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/passes/common/__init__.py
+-rw-rw-r--  2.0 unx     1411 b- defN 23-May-23 07:12 test/unit_test/passes/common/test_user_script.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/passes/inc/__init__.py
+-rw-rw-r--  2.0 unx     4669 b- defN 23-May-23 07:12 test/unit_test/passes/inc/test_inc_quantization.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/passes/onnx/__init__.py
+-rw-rw-r--  2.0 unx      931 b- defN 23-May-23 07:12 test/unit_test/passes/onnx/test_insert_beam_search.py
+-rw-rw-r--  2.0 unx      632 b- defN 23-May-23 07:12 test/unit_test/passes/onnx/test_mixed_precision.py
+-rw-rw-r--  2.0 unx      866 b- defN 23-May-23 07:12 test/unit_test/passes/onnx/test_model_optimizer.py
+-rw-rw-r--  2.0 unx      921 b- defN 23-May-23 07:12 test/unit_test/passes/onnx/test_perf_tuning.py
+-rw-rw-r--  2.0 unx     3327 b- defN 23-May-23 07:12 test/unit_test/passes/onnx/test_pre_post_processing_op.py
+-rw-rw-r--  2.0 unx     2131 b- defN 23-May-23 07:12 test/unit_test/passes/onnx/test_transformer_optimization.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/passes/openvino/__init__.py
+-rw-rw-r--  2.0 unx     2062 b- defN 23-May-23 07:12 test/unit_test/passes/openvino/test_openvino_conversion.py
+-rw-rw-r--  2.0 unx     4643 b- defN 23-May-23 07:12 test/unit_test/passes/openvino/test_openvino_quantization.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/passes/pytorch/__init__.py
+-rw-rw-r--  2.0 unx     1092 b- defN 23-May-23 07:12 test/unit_test/passes/pytorch/test_quantization_aware_training.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/passes/vitis_ai/__init__.py
+-rw-rw-r--  2.0 unx     2245 b- defN 23-May-23 07:12 test/unit_test/passes/vitis_ai/test_vitis_ai_quantization.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/systems/__init__.py
+-rw-rw-r--  2.0 unx     3914 b- defN 23-May-23 07:12 test/unit_test/systems/test_local.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/systems/azureml/__init__.py
+-rw-rw-r--  2.0 unx    22000 b- defN 23-May-23 07:12 test/unit_test/systems/azureml/test_aml_system.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/systems/docker/__init__.py
+-rw-rw-r--  2.0 unx     7863 b- defN 23-May-23 07:12 test/unit_test/systems/docker/test_docker_system.py
+-rw-rw-r--  2.0 unx      247 b- defN 23-May-23 07:12 test/unit_test/systems/python_environment/__init__.py
+-rw-rw-r--  2.0 unx     5246 b- defN 23-May-23 07:12 test/unit_test/systems/python_environment/test_python_environment_system.py
+-rw-rw-r--  2.0 unx     1141 b- defN 23-May-23 07:13 olive_ai-0.2.1.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     2768 b- defN 23-May-23 07:13 olive_ai-0.2.1.dist-info/METADATA
+-rw-rw-r--  2.0 unx   760804 b- defN 23-May-23 07:13 olive_ai-0.2.1.dist-info/NOTICE.txt
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-23 07:13 olive_ai-0.2.1.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       11 b- defN 23-May-23 07:13 olive_ai-0.2.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    20130 b- defN 23-May-23 07:13 olive_ai-0.2.1.dist-info/RECORD
+215 files, 1620608 bytes uncompressed, 310718 bytes compressed:  80.8%
```

## zipnote {}

```diff
@@ -81,14 +81,17 @@
 
 Filename: olive/data/container/huggingface_container.py
 Comment: 
 
 Filename: olive/engine/__init__.py
 Comment: 
 
+Filename: olive/engine/config.py
+Comment: 
+
 Filename: olive/engine/engine.py
 Comment: 
 
 Filename: olive/engine/footprint.py
 Comment: 
 
 Filename: olive/engine/packaging/__init__.py
@@ -129,14 +132,20 @@
 
 Filename: olive/evaluator/metric_config.py
 Comment: 
 
 Filename: olive/evaluator/olive_evaluator.py
 Comment: 
 
+Filename: olive/hardware/__init__.py
+Comment: 
+
+Filename: olive/hardware/accelerator.py
+Comment: 
+
 Filename: olive/passes/__init__.py
 Comment: 
 
 Filename: olive/passes/olive_pass.py
 Comment: 
 
 Filename: olive/passes/pass_config.py
@@ -165,14 +174,20 @@
 
 Filename: olive/passes/onnx/mixed_precision.py
 Comment: 
 
 Filename: olive/passes/onnx/model_optimizer.py
 Comment: 
 
+Filename: olive/passes/onnx/optimum_conversion.py
+Comment: 
+
+Filename: olive/passes/onnx/optimum_merging.py
+Comment: 
+
 Filename: olive/passes/onnx/perf_tuning.py
 Comment: 
 
 Filename: olive/passes/onnx/quantization.py
 Comment: 
 
 Filename: olive/passes/onnx/transformer_optimization.py
@@ -504,14 +519,17 @@
 
 Filename: test/unit_test/evaluator/__init__.py
 Comment: 
 
 Filename: test/unit_test/evaluator/test_accuracy.py
 Comment: 
 
+Filename: test/unit_test/evaluator/test_metric.py
+Comment: 
+
 Filename: test/unit_test/evaluator/test_olive_evaluator.py
 Comment: 
 
 Filename: test/unit_test/hf_utils/__init__.py
 Comment: 
 
 Filename: test/unit_test/hf_utils/test_hf_utils.py
@@ -522,14 +540,17 @@
 
 Filename: test/unit_test/model/test_pytorch_model.py
 Comment: 
 
 Filename: test/unit_test/passes/__init__.py
 Comment: 
 
+Filename: test/unit_test/passes/test_pass_serialization.py
+Comment: 
+
 Filename: test/unit_test/passes/common/__init__.py
 Comment: 
 
 Filename: test/unit_test/passes/common/test_user_script.py
 Comment: 
 
 Filename: test/unit_test/passes/inc/__init__.py
@@ -600,26 +621,26 @@
 
 Filename: test/unit_test/systems/python_environment/__init__.py
 Comment: 
 
 Filename: test/unit_test/systems/python_environment/test_python_environment_system.py
 Comment: 
 
-Filename: olive_ai-0.2.0.dist-info/LICENSE
+Filename: olive_ai-0.2.1.dist-info/LICENSE
 Comment: 
 
-Filename: olive_ai-0.2.0.dist-info/METADATA
+Filename: olive_ai-0.2.1.dist-info/METADATA
 Comment: 
 
-Filename: olive_ai-0.2.0.dist-info/NOTICE.txt
+Filename: olive_ai-0.2.1.dist-info/NOTICE.txt
 Comment: 
 
-Filename: olive_ai-0.2.0.dist-info/WHEEL
+Filename: olive_ai-0.2.1.dist-info/WHEEL
 Comment: 
 
-Filename: olive_ai-0.2.0.dist-info/top_level.txt
+Filename: olive_ai-0.2.1.dist-info/top_level.txt
 Comment: 
 
-Filename: olive_ai-0.2.0.dist-info/RECORD
+Filename: olive_ai-0.2.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## olive/__init__.py

```diff
@@ -10,25 +10,8 @@
 
 _sc = logging.StreamHandler(stream=sys.stdout)
 _formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] [%(filename)s:%(lineno)d:%(funcName)s] %(message)s")
 _sc.setFormatter(_formatter)
 _logger.addHandler(_sc)
 _logger.propagate = False
 
-__version__ = "0.2.0"
-
-
-def set_default_logger_severity(level):
-    """
-    Set log level for olive package.
-
-    :param level: 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL
-    """
-    # mapping from level to logging level
-    level_map = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING, 3: logging.ERROR, 4: logging.CRITICAL}
-
-    # check if level is valid
-    if level not in level_map:
-        raise ValueError(f"Invalid level {level}, should be one of {list(level_map.keys())}")
-
-    # set logger level
-    _logger.setLevel(level_map[level])
+__version__ = "0.2.1"
```

## olive/constants.py

```diff
@@ -27,7 +27,8 @@
     PYTORCH_STATE_DICT = "PyTorch.StateDict"
     PYTORCH_TORCH_SCRIPT = "PyTorch.TorchScript"
     PYTORCH_MLFLOW_MODEL = "PyTorch.MLflow"
     TENSORFLOW_PROTOBUF = "TensorFlow.Protobuf"
     TENSORFLOW_SAVED_MODEL = "TensorFlow.SavedModel"
     SNPE_DLC = "SNPE.DLC"
     OPENVINO_IR = "OpenVINO.IR"
+    OPTIMUM = "Optimum"
```

## olive/extra_dependencies.json

### Pretty-printed

 * *Similarity: 0.8888888888888888%*

 * *Differences: {"'optimum'": "['optimum']"}*

```diff
@@ -18,11 +18,14 @@
     "inc": [
         "neural-compressor"
     ],
     "openvino": [
         "openvino==2022.3.0",
         "openvino-dev[tensorflow,onnx]==2022.3.0"
     ],
+    "optimum": [
+        "optimum"
+    ],
     "tf": [
         "tensorflow==1.15.0"
     ]
 }
```

## olive/logging.py

```diff
@@ -23,7 +23,24 @@
 
 def set_verbosity_error():
     set_verbosity(logging.ERROR)
 
 
 def set_verbosity_critical():
     set_verbosity(logging.CRITICAL)
+
+
+def set_default_logger_severity(level):
+    """
+    Set log level for olive package.
+
+    :param level: 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR, 4: CRITICAL
+    """
+    # mapping from level to logging level
+    level_map = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING, 3: logging.ERROR, 4: logging.CRITICAL}
+
+    # check if level is valid
+    if level not in level_map:
+        raise ValueError(f"Invalid level {level}, should be one of {list(level_map.keys())}")
+
+    # set logger level
+    set_verbosity(level_map[level])
```

## olive/model.py

```diff
@@ -20,23 +20,23 @@
 
 if TYPE_CHECKING:
     from azure.ai.ml import MLClient
 from olive.common.config_utils import ConfigBase, serialize_to_json, validate_config
 from olive.common.ort_inference import get_ort_inference_session
 from olive.common.user_module_loader import UserModuleLoader
 from olive.constants import Framework, ModelFileFormat
+from olive.hardware import AcceleratorLookup, Device
 from olive.hf_utils import (
     get_hf_model_config,
     huggingface_model_loader,
     load_huggingface_model_from_model_class,
     load_huggingface_model_from_task,
 )
 from olive.snpe import SNPEDevice, SNPEInferenceSession, SNPESessionOptions
 from olive.snpe.tools.dev import get_dlc_metrics
-from olive.systems.common import Device
 
 REGISTRY = {}
 logger = logging.getLogger(__name__)
 
 
 class ModelStorageKind(str, Enum):
     LocalFile = "file"
@@ -96,15 +96,19 @@
         Load model from disk, return in-memory model object
         Derived class should implement its specific logic if needed.
         """
         raise NotImplementedError()
 
     @abstractmethod
     def prepare_session(
-        self, inference_settings: Optional[Dict[str, Any]] = None, device: Device = Device.CPU, rank: int = None
+        self,
+        inference_settings: Optional[Dict[str, Any]] = None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+        rank: Optional[int] = None,
     ):
         """
         Prepare inference session for Olive model, return in-memory inference session.
         Derived class should implement its specific logic if needed.
         """
         raise NotImplementedError()
 
@@ -249,15 +253,15 @@
 class ONNXModelBase(OliveModel):
     """
     Abstract class to manage ONNX models
     """
 
     def __init__(
         self,
-        model_path: str = None,
+        model_path: Optional[Union[Path, str]] = None,
         name: Optional[str] = None,
         version: Optional[int] = None,
         aml_storage_name: Optional[str] = None,
         model_storage_kind: Union[str, ModelStorageKind] = ModelStorageKind.LocalFile,
         inference_settings: Optional[dict] = None,
         use_ort_extensions: bool = False,
     ):
@@ -300,27 +304,14 @@
         """
         Returns a list of supported default execution providers
         """
         return ["CPUExecutionProvider"]
 
 
 class ONNXModel(ONNXModelBase):
-    # device type definition: https://github.com/pytorch/pytorch/blob/master/c10/core/DeviceType.h
-    EXECUTION_PROVIDERS = {
-        "cpu": ["CPUExecutionProvider", "OpenVINOExecutionProvider"],
-        "gpu": [
-            "DmlExecutionProvider",
-            "CUDAExecutionProvider",
-            "OpenVINOExecutionProvider",
-            "TensorrtExecutionProvider",
-            "CPUExecutionProvider",
-        ],
-        "npu": ["QNNExecutionProvider", "CPUExecutionProvider"],
-    }
-
     def __init__(
         self,
         model_path: str = None,
         name: Optional[str] = None,
         version: Optional[int] = None,
         aml_storage_name: Optional[str] = None,
         model_storage_kind: Union[str, ModelStorageKind] = ModelStorageKind.LocalFile,
@@ -364,23 +355,42 @@
                 parent_dir.mkdir(parents=True, exist_ok=True)
         return str(path)
 
     def load_model(self, rank: int = None) -> onnx.ModelProto:
         # HACK: ASSUME no external data
         return onnx.load(self.model_path)
 
-    def prepare_session(self, inference_settings: Dict[str, Any], device: Device, rank: int = None):
+    def prepare_session(
+        self,
+        inference_settings: Dict[str, Any],
+        device: Device,
+        execution_providers: Union[str, List[str]] = None,
+        rank: Optional[int] = None,
+    ):
         # user provided inference_settings > model's inference_settings > default settings
         inference_settings = inference_settings or self.inference_settings or {}
         # deep copy to avoid modifying the original settings
         inference_settings = deepcopy(inference_settings)
 
         # if user doesn't not provide ep list, use default value([ep]). Otherwise, use the user's ep list
-        if not inference_settings.get("execution_provider"):
-            inference_settings["execution_provider"] = self.get_default_execution_providers(device)
+        # user provided ep list > eps given by arguments > default eps
+        execution_providers = inference_settings.get("execution_provider") or execution_providers
+        if not execution_providers:
+            execution_providers = self.get_default_execution_providers(device)
+        elif isinstance(execution_providers, str):
+            execution_providers = [execution_providers]
+        else:
+            # the execution_providers is a list
+            pass
+        inference_settings["execution_provider"] = execution_providers
+
+        if (device == Device.GPU) and (rank is not None) and not inference_settings.get("provider_options"):
+            inference_settings["provider_options"] = [
+                {"device_id": str(rank)} if ep == "CUDAExecutionProvider" else {} for ep in execution_providers
+            ]
 
         return get_ort_inference_session(self.model_path, inference_settings, self.use_ort_extensions)
 
     def nodes(self):
         for graph in self.get_all_graphs():
             for node in graph.node:
                 yield node
@@ -434,35 +444,20 @@
                 "hf_config": self.hf_config,
             }
         )
         return serialize_to_json(config, check_object)
 
     def get_default_execution_providers(self, device: Device):
         # return firstly available ep as ort default ep
-        available_providers = ONNXModel.get_execution_providers(device)
+        available_providers = AcceleratorLookup.get_execution_providers_for_device(device)
         for ep in available_providers:
             if self._is_valid_ep(self.model_path, ep):
                 return [ep]
         return super().get_default_execution_providers(device)
 
-    @staticmethod
-    def get_execution_providers(device: Device):
-        import onnxruntime as ort
-
-        available_providers = ort.get_available_providers()
-        eps_per_device = ONNXModel.EXECUTION_PROVIDERS.get(device)
-
-        eps = []
-        if eps_per_device:
-            for ep in available_providers:
-                if ep in eps_per_device:
-                    eps.append(ep)
-
-        return eps if eps else available_providers
-
     def get_io_config(self):
         """
         Get input/output names, shapes, types of the onnx model without creating an ort session.
         This function loads the onnx model and parses the graph to get the io config.
         """
         if self.io_config:
             return self.io_config
@@ -609,15 +604,21 @@
         loaded_model = model_loader(tmp_dir_path)
         loaded_model.eval()
 
         tmp_dir.cleanup()
 
         return loaded_model
 
-    def prepare_session(self, inference_settings: Dict[str, Any], device: Device, rank: int = None):
+    def prepare_session(
+        self,
+        inference_settings: Dict[str, Any],
+        device: Device,
+        execution_providers: Union[str, List[str]] = None,
+        rank: Optional[int] = None,
+    ):
         return self.load_model().eval()
 
     # TODO: remove this method once we have olive datasets implemented.
     # The dataset should be able to provide the dummy inputs.
     def get_dummy_inputs(self):
         """
         Return a dummy input for the model.
@@ -703,14 +704,39 @@
                 "dummy_inputs_func": self.dummy_inputs_func,
                 "hf_config": self.hf_config,
             }
         )
         return serialize_to_json(config, check_object)
 
 
+class OptimumModel(OliveModel):
+    def __init__(self, model_path: str, name: str, model_components: List[str]):
+        super().__init__(
+            name=name,
+            model_storage_kind=ModelStorageKind.LocalFolder,
+            model_path=model_path,
+            model_file_format=ModelFileFormat.OPTIMUM,
+            framework=Framework.PYTORCH,
+        )
+        self.model_components = model_components
+
+    def load_model(self, rank: int = None):
+        raise NotImplementedError()
+
+    def prepare_session(self, inference_settings: Dict[str, Any], device: Device, rank: int = None):
+        raise NotImplementedError()
+
+    def to_json(self, check_object: bool = False):
+        config = {
+            "type": self.__class__.__name__,
+            "config": {"name": self.name, "model_path": self.model_path, "model_components": self.model_components},
+        }
+        return serialize_to_json(config, check_object)
+
+
 class SNPEModel(OliveModel):
     def __init__(
         self,
         input_names: List[str],
         input_shapes: List[List[int]],
         output_names: List[str],
         output_shapes: List[List[int]],
@@ -734,15 +760,19 @@
             "output_shapes": output_shapes,
         }
 
     def load_model(self, rank: int = None):
         raise NotImplementedError()
 
     def prepare_session(
-        self, inference_settings: Dict[str, Any], device: Device, rank: int = None
+        self,
+        inference_settings: Dict[str, Any],
+        device: Device,
+        execution_providers: Union[str, List[str]] = None,
+        rank: Optional[int] = None,
     ) -> SNPEInferenceSession:
         session_options = SNPESessionOptions(**inference_settings) if inference_settings else None
         if device == Device.NPU:
             device = SNPEDevice.DSP
         session_options.device = device
         return SNPEInferenceSession(self.model_path, self.io_config, session_options)
 
@@ -770,15 +800,21 @@
             name=name,
             model_storage_kind=model_storage_kind,
         )
 
     def load_model(self, rank: int = None):
         raise NotImplementedError()
 
-    def prepare_session(self, inference_settings: Dict[str, Any], device: Device, rank: int = None):
+    def prepare_session(
+        self,
+        inference_settings: Dict[str, Any],
+        device: Device,
+        execution_providers: Union[str, List[str]] = None,
+        rank: Optional[int] = None,
+    ):
         raise NotImplementedError()
 
 
 class OpenVINOModel(OliveModel):
     def __init__(
         self,
         model_path: str,
@@ -816,15 +852,21 @@
     def load_model(self, rank: int = None):
         try:
             from openvino.tools.pot import load_model
         except ImportError:
             raise ImportError("Please install olive-ai[openvino] to use OpenVINO model")
         return load_model(self.model_config)
 
-    def prepare_session(self, inference_settings: Dict[str, Any], device: Device, rank: int = None):
+    def prepare_session(
+        self,
+        inference_settings: Dict[str, Any],
+        device: Device,
+        execution_providers: Union[str, List[str]] = None,
+        rank: Optional[int] = None,
+    ):
         try:
             from openvino.runtime import Core
         except ImportError:
             raise ImportError("Please install olive-ai[openvino] to use OpenVINO model")
         ie = Core()
         model_pot = ie.read_model(model=self.model_config["model"])
         if device == Device.INTEL_MYRIAD:
@@ -837,15 +879,15 @@
     EXECUTION_PROVIDERS = {
         "cpu": ["CPUExecutionProvider"],
         "gpu": ["CUDAExecutionProvider", "CPUExecutionProvider"],
     }
 
     def __init__(
         self,
-        model_filepaths: List[str],
+        model_filepaths: List[Union[Path, str]] = [],
         name: Optional[str] = None,
         version: Optional[int] = None,
         aml_storage_name: Optional[str] = None,
         inference_settings: Optional[dict] = None,
         use_ort_extensions: bool = False,
     ):
         super().__init__(
@@ -859,61 +901,45 @@
         )
         self.model_filepaths = model_filepaths
 
     @property
     def ranks(self):
         return len(self.model_filepaths)
 
+    def ranked_model_path(self, rank: int) -> Union[Path, str]:
+        return self.model_filepaths[rank]
+
     def load_model(self, rank: int) -> ONNXModel:
         return ONNXModel(self.model_filepaths[rank], inference_settings=self.inference_settings)
 
     def prepare_session(
-        self, inference_settings: Optional[Dict[str, Any]] = None, device: Device = Device.GPU, rank: int = 0
+        self,
+        inference_settings: Optional[Dict[str, Any]] = None,
+        device: Device = Device.GPU,
+        execution_providers: Union[str, List[str]] = None,
+        rank: Optional[int] = 0,
     ):
-        # user provided inference_settings > model's inference_settings > default settings
-        inference_settings = inference_settings or self.inference_settings or {}
-        # deep copy to avoid modifying the original settings
-        inference_settings = deepcopy(inference_settings)
-
-        # if user doesn't not provide ep list, use default value([ep]). Otherwise, use the user's ep list
-        execution_providers = inference_settings.get("execution_provider")
-        if not execution_providers:
-            execution_providers = self.get_default_execution_providers(device)
-            inference_settings["execution_provider"] = execution_providers
-
-        if not inference_settings.get("provider_options"):
-            inference_settings["provider_options"] = [
-                {"device_id": str(rank)} if ep == "CUDAExecutionProvider" else {} for ep in execution_providers
-            ]
-
-        return get_ort_inference_session(self.model_filepaths[rank], inference_settings)
+        raise RuntimeError("DistributedOnnxModel doesn't have a session of its own")
 
     def get_default_execution_providers(self, filepath: str, device: Device):
         # return firstly available ep as ort default ep
         available_providers = DistributedOnnxModel.get_execution_providers(device)
         for ep in available_providers:
             if self._is_valid_ep(filepath, ep):
                 return [ep]
 
         return ["CUDAExecutionProvider", "CPUExecutionProvider"]
 
     @staticmethod
     def get_execution_providers(device: Device):
         import onnxruntime as ort
 
-        available_providers = ort.get_available_providers()
         eps_per_device = DistributedOnnxModel.EXECUTION_PROVIDERS.get(device)
-
-        eps = []
-        if eps_per_device:
-            for ep in available_providers:
-                if ep in eps_per_device:
-                    eps.append(ep)
-
-        return eps if eps else available_providers
+        available_providers = ort.get_available_providers()
+        return AcceleratorLookup.get_execution_providers(eps_per_device, available_providers)
 
     def to_json(self, check_object: bool = False):
         config = {
             "type": self.__class__.__name__,
             "config": {
                 "model_filepaths": self.model_filepaths,
                 "name": self.name,
@@ -961,15 +987,21 @@
             m.set_composite_parent(self)
 
         self.hf_config = validate_config(hf_config, HFConfig) if hf_config else None
 
     def load_model(self, rank: int = None):
         raise NotImplementedError()
 
-    def prepare_session(self, inference_settings: Dict[str, Any], device: Device, rank: int = None):
+    def prepare_session(
+        self,
+        inference_settings: Dict[str, Any],
+        device: Device,
+        execution_providers: Union[str, List[str]] = None,
+        rank: Optional[int] = None,
+    ):
         raise NotImplementedError()
 
     def get_default_execution_providers(self, device: Device):
         raise NotImplementedError()
 
     def get_model_components(self):
         return self.model_components
```

## olive/common/config_utils.py

```diff
@@ -4,15 +4,15 @@
 # --------------------------------------------------------------------------
 import inspect
 import json
 import logging
 from functools import partial
 from pathlib import Path
 from types import FunctionType, MethodType
-from typing import Any, Callable, Dict, Optional, Union
+from typing import Any, Callable, Dict, List, Optional, Union
 
 from pydantic import BaseModel, create_model, validator
 
 from olive.common.utils import hash_function, hash_object
 
 logger = logging.getLogger(__name__)
 
@@ -111,14 +111,49 @@
         return serialize_to_json(self, check_objects)
 
     @classmethod
     def from_json(cls, json_dict: dict) -> "ConfigBase":
         return cls.parse_raw(json.dumps(json_dict))
 
 
+class ConfigListBase(ConfigBase):
+    __root__: List[Any] = None
+
+    def __iter__(self):
+        return iter(self.__root__)
+
+    def __getitem__(self, item):
+        return self.__root__[item]
+
+    def __len__(self):
+        return len(self.__root__)
+
+
+class ConfigDictBase(ConfigBase):
+    __root__: Dict[str, Any] = None
+
+    def __iter__(self):
+        return iter(self.__root__)
+
+    def keys(self):
+        return self.__root__.keys()
+
+    def values(self):
+        return self.__root__.values()
+
+    def items(self):
+        return self.__root__.items()
+
+    def __getitem__(self, item):
+        return self.__root__[item]
+
+    def __len__(self):
+        return len(self.__root__) if self.__root__ else 0
+
+
 class ConfigParam(ConfigBase):
     """
     Dataclass for pass configuration parameters.
     """
 
     type_: Any
     required: bool = False
```

## olive/engine/engine.py

```diff
@@ -6,62 +6,47 @@
 import logging
 import time
 from collections import OrderedDict, defaultdict
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple, Type, Union
 
 import olive.cache as cache_utils
-from olive.azureml.azureml_client import AzureMLClientConfig
 from olive.common.config_utils import ConfigBase, validate_config
 from olive.common.utils import hash_dict
+from olive.engine.config import PRUNED_CONFIG, EngineConfig
 from olive.engine.footprint import Footprint, FootprintNode, FootprintNodeMetric
 from olive.engine.packaging.packaging_config import PackagingConfig
 from olive.engine.packaging.packaging_generator import generate_output_artifacts
-from olive.evaluator.metric import Metric
+from olive.evaluator.metric import Metric, MetricResult, joint_metric_key
 from olive.evaluator.olive_evaluator import OliveEvaluatorConfig
+from olive.hardware.accelerator import AcceleratorLookup, AcceleratorSpec, Device
 from olive.model import ModelConfig, ModelStorageKind, OliveModel
 from olive.passes.olive_pass import Pass
-from olive.strategy.search_strategy import SearchStrategy, SearchStrategyConfig
+from olive.strategy.search_strategy import SearchStrategy
 from olive.systems.common import SystemType
 from olive.systems.local import LocalSystem
 from olive.systems.olive_system import OliveSystem
-from olive.systems.system_config import SystemConfig
 
 logger = logging.getLogger(__name__)
 
-# pass search-point/config was pruned due invalid config or failed run
-PRUNED_CONFIG = "pruned-config"
-
-
-class EngineConfig(ConfigBase):
-    search_strategy: Union[SearchStrategyConfig, bool] = None
-    host: SystemConfig = None
-    target: SystemConfig = None
-    evaluator: OliveEvaluatorConfig = None
-    azureml_client_config: Optional[AzureMLClientConfig] = None
-    packaging_config: PackagingConfig = None
-    cache_dir: Union[Path, str] = ".olive-cache"
-    clean_cache: bool = False
-    clean_evaluation_cache: bool = False
-    plot_pareto_frontier: bool = False
-
 
 class Engine:
     """
     The engine executes the registered Olive Steps, facilitate evaluation of the output models using
     provided evaluation criteria and produces output model(s).
     """
 
     def __init__(
         self,
         config: Union[Dict[str, Any], EngineConfig] = None,
         search_strategy: Optional[SearchStrategy] = None,
         host: Optional[OliveSystem] = None,
         target: Optional[OliveSystem] = None,
         evaluator_config: Optional[OliveEvaluatorConfig] = None,
+        execution_providers: Optional[List[str]] = None,
     ):
         self._config = validate_config(config, EngineConfig)
 
         self.no_search = False
         # default search strategy
         self.search_strategy = SearchStrategy({"execution_order": "joint", "search_algorithm": "exhaustive"})
         if search_strategy is not None:
@@ -86,14 +71,54 @@
         if target is not None:
             self.target = target
         elif self._config.target is not None:
             self.target = self._config.target.create_system()
         else:
             self.target = LocalSystem()
 
+        if execution_providers is None:
+            execution_providers = self._config.execution_providers
+
+        # verfiy the AzureML system have specified the execution providers
+        # Please note we could not use isinstance(target, AzureMLSystem) since it would import AzureML packages.
+        if self.target.system_type == SystemType.AzureML and execution_providers is None:
+            raise ValueError("AzureMLSystem requires execution providers to be specified.")
+        elif execution_providers is None:
+            if self.target.system_type in (SystemType.Local, SystemType.PythonEnvironment):
+                execution_providers = self.target.get_supported_execution_providers()
+            else:
+                # for docker system and python system, we default use CPUExecutionProvider
+                execution_providers = ["CPUExecutionProvider"]
+
+        self.execution_providers = execution_providers
+
+        # Flatten the accelerators to list of AcceleratorSpec
+        accelerators: List[str] = self.target.accelerators
+        if accelerators is None:
+            logger.warning("No accelerators specified for target system. Using CPU.")
+            accelerators = ["CPU"]
+
+        not_supported_ep = []
+        self.accelerator_specs: List[AcceleratorSpec] = []
+        for accelerator in accelerators:
+            device = Device(accelerator.lower())
+            supported_eps = AcceleratorLookup.get_execution_providers_for_device(device)
+            device_eps = list(set(supported_eps).intersection(self.execution_providers))
+            for ep in set(self.execution_providers).difference(supported_eps):
+                not_supported_ep.append(ep)
+            for ep in device_eps:
+                self.accelerator_specs.append(AcceleratorSpec(device, ep))
+
+        assert self.accelerator_specs, "No valid accelerator specified for target system."
+        if not_supported_ep:
+            logger.warning(
+                f"The following execution provider is not supported: {','.join(not_supported_ep)}. "
+                "Please consider install the onnxruntime contains the appropriated execution providers. "
+            )
+
         # default evaluator
         self.evaluator_config = None
         if evaluator_config is not None:
             self.evaluator_config = evaluator_config
         elif self._config.evaluator is not None:
             self.evaluator_config = self._config.evaluator
 
@@ -208,15 +233,14 @@
             "evaluator": evaluator_config,
         }
 
     def run(
         self,
         input_model: OliveModel,
         packaging_config: Optional[PackagingConfig] = None,
-        verbose: bool = False,
         output_dir: str = None,
         output_name: str = None,
         evaluation_only: bool = False,
     ):
         """
         Run all the registered Olive passes on the input model and produce one or more candidate models.
 
@@ -232,87 +256,111 @@
         """
         if not self._initialized:
             self.initialize()
 
         output_dir: Path = Path(output_dir) if output_dir else Path.cwd()
         output_dir.mkdir(parents=True, exist_ok=True)
 
-        # TODO by myguo: replace the following for loop using the accelerator and excution provider list when adding
-        # accelerator support
         outputs = {}
-        for i in range(1):
-            # generate search space and intialize the passes for each hardware accelerator
-            self.setup_passes()
+        pf_footprints = {}
+        for accelerator_spec in self.accelerator_specs:
+            # generate search space and initialize the passes for each hardware accelerator
+            self.setup_passes(accelerator_spec)
 
             # hash the input model
             input_model_id = self._init_input_model(input_model)
-            self.footprints[i].record(model_id=input_model_id)
+            self.footprints[accelerator_spec].record(model_id=input_model_id)
 
-            if evaluation_only:
-                prefix_output_name = f"{output_name}_{i}_" if output_name is not None else f"{i}_"
-                assert self.evaluator_config is not None, "'evaluation_only' is True but no evaluator provided"
-                results = self._evaluate_model(input_model, input_model_id, self.evaluator_config, i, verbose)
-                result_name = f"{prefix_output_name}metrics"
-                results_path = output_dir / f"{result_name}.json"
-                with open(results_path, "w") as f:
-                    json.dump(results, f, indent=4)
-                outputs[i] = results
-            elif self.no_search:
-                output = self.run_no_search(
-                    input_model, input_model_id, i, packaging_config, verbose, output_dir, output_name
-                )
-                outputs[i] = output
-            else:
-                footprint = self.run_search(
-                    input_model, input_model_id, i, packaging_config, verbose, output_dir, output_name
-                )
-                outputs[i] = footprint
+            try:
+                if evaluation_only:
+                    prefix_output_name = (
+                        f"{output_name}_{accelerator_spec}_" if output_name is not None else f"{accelerator_spec}_"
+                    )
+                    assert self.evaluator_config is not None, "Evaluation only is True but no evaluator provided"
+                    results = self._evaluate_model(input_model, input_model_id, self.evaluator_config, accelerator_spec)
+                    result_name = f"{prefix_output_name}metrics"
+                    results_path = output_dir / f"{result_name}.json"
+                    with open(results_path, "w") as f:
+                        f.write(results.json())
+                    outputs[accelerator_spec] = results
+                elif self.no_search:
+                    output = self.run_no_search(
+                        input_model,
+                        input_model_id,
+                        accelerator_spec,
+                        output_dir,
+                        output_name,
+                    )
+                    outputs[accelerator_spec] = output
+                    pf_footprints[accelerator_spec] = self.footprints[accelerator_spec].get_last_node()
+                else:
+                    footprint = self.run_search(
+                        input_model,
+                        input_model_id,
+                        accelerator_spec,
+                        output_dir,
+                        output_name,
+                    )
+                    outputs[accelerator_spec] = footprint
+                    pf_footprints[accelerator_spec] = footprint
+
+            except Exception as e:
+                logger.warning(f"Failed to run Olive on {accelerator_spec}: {e}")
+
+        if packaging_config:
+            logger.info(f"Package top ranked {sum([len(f.nodes) for f in pf_footprints.values()])} models as artifacts")
+            generate_output_artifacts(
+                packaging_config,
+                self.footprints,
+                pf_footprints,
+                output_dir,
+            )
+        else:
+            logger.info("No packaging config provided, skip packaging artifacts")
 
         return outputs
 
-    def setup_passes(self):
+    def setup_passes(self, accelerator_spec: AcceleratorSpec):
         # TODO: add the hardware spec later
         # clean the passes
         self.passes.clear()
         for config in self.pass_config.values():
             pass_cls: Type[Pass] = config["type"]
             pass_cfg = config["config"]
-            config_class, pass_cfg = pass_cls.generate_search_space(pass_cfg, config["disable_search"])
-            p = pass_cls(config_class, pass_cfg)
+            pass_cfg = pass_cls.generate_search_space(accelerator_spec, pass_cfg, config["disable_search"])
+            p = pass_cls(accelerator_spec, pass_cfg, config["disable_search"])
             self.register_pass(p, host=config["host"], evaluator_config=config["evaluator"])
 
         # list of passes starting from the first pass with non-empty search space
         # These passes will be added to the search space
         self.pass_search_spaces = []
         for pass_name in self.passes.keys():
             p: Pass = self.passes[pass_name]["pass"]
             self.pass_search_spaces.append((pass_name, p.search_space()))
 
     def run_no_search(
         self,
         input_model: OliveModel,
         input_model_id: str,
-        accelerator_spec: Any,
-        packaging_config: Optional[PackagingConfig] = None,
-        verbose: bool = False,
+        accelerator_spec: AcceleratorSpec,
         output_dir: str = None,
         output_name: str = None,
     ):
         for pass_item in self.passes.values():
             if len(pass_item["pass"].search_space()) > 0:
                 pass_name = pass_item["name"]
                 raise ValueError(f"Pass {pass_name} has search space but search strategy is None")
 
         evaluator_config = self.evaluator_for_pass(list(self.passes.keys())[-1])
         if evaluator_config is None:
             # provide dummy objective
             objective_dict = {"dummy": {"higher_is_better": True, "goal": 0}}
         else:
             objective_dict = self.resolve_objectives(
-                input_model, input_model_id, evaluator_config.metrics, accelerator_spec, verbose
+                input_model, input_model_id, evaluator_config.metrics, accelerator_spec
             )
 
         # initialize the search strategy
         self.search_strategy.initialize(self.pass_search_spaces, input_model_id, objective_dict)
 
         # get the next step
         next_step = self.search_strategy.next_step()
@@ -321,60 +369,47 @@
         # get the model id of the first input model
         model_id = next_step["model_id"]
         if model_id == input_model_id:
             model = input_model
         else:
             model = self._load_model(model_id)
 
-        if verbose:
-            logger.info(f"Step no search with search point {next_step['search_point']} ...")
+        logger.debug(f"Step no search with search point {next_step['search_point']} ...")
 
         # run all the passes in the step
         (
             _,
             signal,
             model_ids,
-        ) = self._run_passes(next_step["passes"], model, model_id, accelerator_spec, verbose)
+        ) = self._run_passes(next_step["passes"], model, model_id, accelerator_spec)
         model_id = model_ids[-1]
 
         prefix_output_name = f"{output_name}_{accelerator_spec}_" if output_name is not None else f"{accelerator_spec}_"
         # save the model to output_dir
         output_model_name = f"{prefix_output_name}model"
         output_model_json = cache_utils.save_model(model_id, output_dir, output_model_name, self._config.cache_dir)
 
         # save the evaluation results to output_dir
         result_name = f"{prefix_output_name}metrics"
         results_path = output_dir / f"{result_name}.json"
         if signal is not None:
             with open(results_path, "w") as f:
-                json.dump(signal, f, indent=4)
+                f.write(signal.json())
 
         output = {"model": output_model_json}
         if signal is not None:
             output["metrics"] = signal
 
-        # Package output model as artifacts if no search
-        if packaging_config:
-            logger.info("Package output model as artifacts")
-            generate_output_artifacts(
-                packaging_config,
-                self.footprints[accelerator_spec],
-                self.footprints[accelerator_spec].get_last_node(),
-                output_dir,
-            )
-
         return output
 
     def run_search(
         self,
         input_model: OliveModel,
         input_model_id: str,
-        accelerator_spec: Any,
-        packaging_config: Optional[PackagingConfig] = None,
-        verbose: bool = False,
+        accelerator_spec: AcceleratorSpec,
         output_dir: str = None,
         output_name: str = None,
     ):
         """
         Run all the registered Olive passes on the input model and produce one or more candidate models.
 
         if search strategy is None, all passes are run in the order they were registered.
@@ -391,15 +426,15 @@
         # get objective_dict
         evaluator_config = self.evaluator_for_pass(list(self.passes.keys())[-1])
 
         if evaluator_config is None:
             raise ValueError("No evaluator provided for the last pass")
         else:
             objective_dict = self.resolve_objectives(
-                input_model, input_model_id, evaluator_config.metrics, accelerator_spec, verbose
+                input_model, input_model_id, evaluator_config.metrics, accelerator_spec
             )
 
         # initialize the search strategy
         self.search_strategy.initialize(self.pass_search_spaces, input_model_id, objective_dict)
         output_model_num = self.search_strategy.get_output_model_num()
 
         # record start time
@@ -418,125 +453,140 @@
             # get the model id of the first input model
             model_id = next_step["model_id"]
             if model_id == input_model_id:
                 model = input_model
             else:
                 model = self._load_model(model_id)
 
-            if verbose:
-                logger.info(f"Step {iter_num} with search point {next_step['search_point']} ...")
+            logger.debug(f"Step {iter_num} with search point {next_step['search_point']} ...")
 
             # run all the passes in the step
-            should_prune, signal, model_ids = self._run_passes(
-                next_step["passes"], model, model_id, accelerator_spec, verbose
-            )
+            should_prune, signal, model_ids = self._run_passes(next_step["passes"], model, model_id, accelerator_spec)
 
             # record feedback signal
             self.search_strategy.record_feedback_signal(next_step["search_point"], signal, model_ids, should_prune)
 
             time_diff = time.time() - start_time
             self.search_strategy.check_exit_criteria(iter_num, time_diff, signal)
 
         self.footprints[accelerator_spec].to_file(output_dir / f"{prefix_output_name}footprints.json")
 
         pf_footprints = self.footprints[accelerator_spec].get_pareto_frontier()
         if output_model_num is None or len(pf_footprints.nodes) <= output_model_num:
             logger.info(f"Output all {len(pf_footprints.nodes)} models")
         else:
-            metrics = evaluator_config.metrics if evaluator_config else []
-            top_ranked_nodes = self._get_top_ranked_nodes(metrics, pf_footprints, output_model_num)
+            top_ranked_nodes = self._get_top_ranked_nodes(objective_dict, pf_footprints, output_model_num)
             logger.info(f"Output top ranked {len(top_ranked_nodes)} models based on metric priorities")
             pf_footprints.update_nodes(top_ranked_nodes)
 
         pf_footprints.to_file(output_dir / f"{prefix_output_name}pareto_frontier_footprints.json")
 
         if self._config.plot_pareto_frontier:
             pf_footprints.plot_pareto_frontier_to_html(
                 save_path=output_dir / f"{prefix_output_name}pareto_frontier_footprints_chart.html"
             )
 
-        if packaging_config:
-            logger.info(f"Package top ranked {len(pf_footprints.nodes)} models as artifacts")
-            generate_output_artifacts(packaging_config, self.footprints[accelerator_spec], pf_footprints, output_dir)
-        else:
-            logger.info("No packaging config provided, skip packaging artifacts")
-
         return pf_footprints
 
     def resolve_objectives(
         self,
         input_model: OliveModel,
         input_model_id: str,
         metrics: List[Metric],
-        accelerator_spec: Any,
-        verbose: bool = False,
+        accelerator_spec: AcceleratorSpec,
     ) -> Dict[str, Dict[str, Any]]:
         """
         Return a dictionary of objectives and their higher_is_better and goal values.
 
         {objective_name: {"higher_is_better": bool, "goal": float}}
         """
-        goals = self.resolve_goals(input_model, input_model_id, metrics, accelerator_spec, verbose)
-        objective_dict = {
-            metric.name: {"higher_is_better": metric.higher_is_better, "goal": goals.get(metric.name)}
-            for metric in metrics
-        }
+        goals = self.resolve_goals(input_model, input_model_id, metrics, accelerator_spec)
+        objective_dict = {}
+        for metric in metrics:
+            for sub_type in metric.sub_types:
+                if sub_type.priority <= 0:
+                    continue
+                metric_key = joint_metric_key(metric.name, sub_type.name)
+                objective_dict[metric_key] = {
+                    "higher_is_better": sub_type.higher_is_better,
+                    "goal": goals.get(metric_key),
+                    "priority": sub_type.priority,
+                }
         self.footprints[accelerator_spec].record_objective_dict(objective_dict)
-        return objective_dict
+        ranked_objective_dict = dict(sorted(objective_dict.items(), key=lambda x: x[1]["priority"]))
+        return ranked_objective_dict
 
     def resolve_goals(
         self,
         input_model: OliveModel,
         input_model_id: str,
         metrics: List[Metric],
-        accelerator_spec: Any,
-        verbose: bool = False,
+        accelerator_spec: AcceleratorSpec,
     ) -> Dict[str, float]:
         """
         Resolve the goals of the given metrics into thresholds for the given model.
         """
         goals = {}
         multipliers = {}
         for metric in metrics:
-            if metric.goal is not None:
-                goals[metric.name] = metric.goal
-                multipliers[metric.name] = 1 if metric.higher_is_better else -1
-        if verbose and len(goals) > 0:
-            logger.info(f"Resolving goals: {goals}")
-
-        # compute baseline for input model if needed
-        baseline = {}
-        for _, goal in goals.items():
-            if goal.type != "threshold":
-                assert self.evaluator_config is not None, "Default evaluator must be provided to resolve goals"
-                if verbose:
-                    logger.info("Computing baseline for metrics ...")
-                baseline = self._evaluate_model(
-                    input_model, input_model_id, self.evaluator_config, accelerator_spec, verbose=False
-                )
+            # only resolve sub metrics whose priority > 0
+            goals[metric.name] = metric.get_sub_type_info("goal")
+            multipliers[metric.name] = metric.get_sub_type_info(
+                info_name="higher_is_better",
+                callback=lambda x: 1 if x else -1,
+            )
+
+        if goals:
+            logger.debug(f"Resolving goals: {goals}")
+
+        baseline = MetricResult()
+        for goal in goals.values():
+            _evaluated = False
+            for sub_goal in goal.values():
+                if not sub_goal:
+                    break
+                if sub_goal.type != "threshold":
+                    assert self.evaluator_config is not None, "Default evaluator must be provided to resolve goals"
+                    logger.debug("Computing baseline for metrics ...")
+                    baseline = self._evaluate_model(
+                        input_model, input_model_id, self.evaluator_config, accelerator_spec
+                    )
+                    _evaluated = True
+                    break
+            if _evaluated:
                 break
-        if verbose and len(baseline) > 0:
-            logger.info(f"Baseline: {baseline}")
+        if not baseline:
+            logger.debug("No baseline got as no goal is provided the the goal is threshold")
+            return {}
+
+        if baseline:
+            logger.debug(f"Baseline: {baseline}")
 
         # resolve goals to thresholds
         resolved_goals = {}
-        for name, goal in goals.items():
-            # TODO: make the logic cleaner
-            if goal.type == "threshold":
-                resolved_goals[name] = goal.value
-            elif goal.type == "max-degradation":
-                resolved_goals[name] = baseline[name] - multipliers[name] * goal.value
-            elif goal.type == "min-improvement":
-                resolved_goals[name] = baseline[name] + multipliers[name] * goal.value
-            elif goal.type == "percent-max-degradation":
-                resolved_goals[name] = baseline[name] * (1 - multipliers[name] * goal.value / 100)
-            elif goal.type == "percent-min-improvement":
-                resolved_goals[name] = baseline[name] * (1 + multipliers[name] * goal.value / 100)
-        if verbose and len(resolved_goals) > 0:
-            logger.info(f"Resolved goals: {resolved_goals}")
+        for metric_name, sub_type_goals in goals.items():
+            for sub_type_name, goal in sub_type_goals.items():
+                # TODO: make the logic cleaner
+                resolved_goal_value = None
+                baseline_sub_type = baseline.get_value(metric_name, sub_type_name)
+                multiplier = multipliers[metric_name][sub_type_name]
+                if goal.type == "threshold":
+                    resolved_goal_value = goal.value
+                elif goal.type == "max-degradation":
+                    resolved_goal_value = baseline_sub_type - multiplier * goal.value
+                elif goal.type == "min-improvement":
+                    resolved_goal_value = baseline_sub_type + multiplier * goal.value
+                elif goal.type == "percent-max-degradation":
+                    resolved_goal_value = baseline_sub_type * (1 - multiplier * goal.value / 100)
+                elif goal.type == "percent-min-improvement":
+                    resolved_goal_value = baseline_sub_type * (1 + multiplier * goal.value / 100)
+
+                resolved_goals[joint_metric_key(metric_name, sub_type_name)] = resolved_goal_value
+        if len(resolved_goals) > 0:
+            logger.debug(f"Resolved goals: {resolved_goals}")
 
         return resolved_goals
 
     def host_for_pass(self, pass_id: str):
         host = self.passes[pass_id]["host"]
         if host is None:
             return self.host
@@ -658,28 +708,25 @@
             return None
 
     def _run_passes(
         self,
         passes: List[Tuple[str, Dict[str, Any]]],
         model: OliveModel,
         model_id: str,
-        accelerator_spec: Any,
-        verbose: bool = False,
+        accelerator_spec: AcceleratorSpec,
     ):
         """
         Run all the passes in the order they were registered.
         the passes is the list of (pass_name, pass_search_point) tuples
         """
         should_prune = False
         # run all the passes in the step
         model_ids = []
         for pass_id, pass_search_point in passes:
-            if verbose:
-                message = f"Running pass {pass_id}"
-                logger.info(message)
+            logger.debug(f"Running pass {pass_id}")
 
             if (
                 model.model_storage_kind == ModelStorageKind.AzureMLModel
                 and not self.host_for_pass(pass_id).system_type == SystemType.AzureML
             ):
                 if not self.azureml_client_config:
                     raise ValueError("AzureML client config is required to download the model from AzureML storage")
@@ -691,77 +738,82 @@
                 if model_path.is_dir():
                     model.model_storage_kind = ModelStorageKind.LocalFolder
                 elif model_path.is_file():
                     model.model_storage_kind = ModelStorageKind.LocalFile
                 else:
                     raise ValueError(f"Invalid model path {model_path}")
 
-            model, model_id = self._run_pass(pass_id, pass_search_point, model, model_id, accelerator_spec, verbose)
+            model, model_id = self._run_pass(pass_id, pass_search_point, model, model_id, accelerator_spec)
             if model == PRUNED_CONFIG:
                 should_prune = True
-                logger.info("Pruned")
+                logger.debug("Pruned")
                 break
             model_ids.append(model_id)
 
         signal = {}
         if not should_prune:
             # evaluate the model
             try:
                 evaluator_config = self.evaluator_for_pass(pass_id)
                 if self.no_search and evaluator_config is None:
                     # skip evaluation if no search and no evaluator
                     signal = None
                 else:
-                    signal = self._evaluate_model(model, model_id, evaluator_config, accelerator_spec, verbose)
+                    signal = self._evaluate_model(model, model_id, evaluator_config, accelerator_spec)
             except Exception as e:
                 logger.error(f"Evaluation failed: {e}")
                 raise e
-            if verbose:
-                logger.info(f"Signal: {signal}")
+            logger.debug(f"Signal: {signal}")
 
         return should_prune, signal, model_ids
 
     def _run_pass(
         self,
         pass_id: str,
         pass_search_point: Dict[str, Any],
         input_model: OliveModel,
         input_model_id: str,
-        accelerator_spec: Any,
-        verbose: bool,
+        accelerator_spec: AcceleratorSpec,
     ):
         """
         Run a pass on the input model.
         """
         # pass
         p: Pass = self.passes[pass_id]["pass"]
         pass_name = p.__class__.__name__
         pass_config = p.config_at_search_point(pass_search_point)
         pass_config = p.serialize_config(pass_config)
 
         # load run from cache if it exists
         output_model_id = self._load_run(input_model_id, pass_name, pass_config)
         if output_model_id is not None:
-            if verbose:
-                logger.info("Loading model from cache ...")
+            logger.debug("Loading model from cache ...")
             output_model = self._load_model(output_model_id)
             if output_model is not None:
                 # footprint model and run
                 self.footprints[accelerator_spec].record(
                     model_id=output_model_id,
                     model_config=output_model.to_json() if output_model != PRUNED_CONFIG else {"is_pruned": True},
                     parent_model_id=input_model_id,
                     from_pass=pass_name,
                     pass_run_config=pass_config,
                 )
                 return output_model, output_model_id
 
         # new model id
         input_model_number = input_model_id.split("_")[0]
-        output_model_id = f"{self._get_new_model_number()}_{pass_name}-{input_model_number}-{hash_dict(pass_config)}"
+        # Note: the output model id need contains the accelerator information.
+        # TODO: consider how to reuse the run which is indepedent with accelerator and EP.
+        output_model_id_parts = [
+            f"{self._get_new_model_number()}_{pass_name}",
+            input_model_number,
+            hash_dict(pass_config),
+            accelerator_spec,
+        ]
+        output_model_id = "-".join(map(str, output_model_id_parts))
         output_model_path = str(self._model_cache_path / f"{output_model_id}")
 
         # prune if invalid search_point
         if not p.validate_search_point(pass_search_point) and not self.no_search:
             output_model = PRUNED_CONFIG
         else:
             # run pass
@@ -796,21 +848,21 @@
     def get_evaluation_json_path(self, model_id: str):
         """
         Get the path to the evaluation json.
         """
         evaluation_json_path = self._evaluation_cache_path / f"{model_id}.json"
         return evaluation_json_path
 
-    def _cache_evaluation(self, model_id: str, signal: dict):
+    def _cache_evaluation(self, model_id: str, signal: MetricResult):
         """
         Cache the evaluation in the cache directory.
         """
         evaluation_json = {
             "model_id": model_id,
-            "signal": signal,
+            "signal": signal.dict(),
         }
         evaluation_json_path = self.get_evaluation_json_path(model_id)
         try:
             with open(evaluation_json_path, "w") as f:
                 json.dump(evaluation_json, f, indent=4)
         except Exception as e:
             logger.error(f"Failed to cache evaluation: {e}")
@@ -821,73 +873,73 @@
         """
         evaluation_json_path = self.get_evaluation_json_path(model_id)
         if evaluation_json_path.exists():
             try:
                 with open(evaluation_json_path, "r") as f:
                     evaluation_json = json.load(f)
                 signal = evaluation_json["signal"]
+                signal = MetricResult(**signal)
             except Exception as e:
                 logger.error(f"Failed to load evaluation: {e}")
                 signal = None
             return signal
         else:
             return None
 
     def _evaluate_model(
         self,
         model: OliveModel,
         model_id: str,
         evaluator_config: OliveEvaluatorConfig,
-        accelerator_spec: Any,
-        verbose: bool,
+        accelerator_spec: AcceleratorSpec,
     ):
         """
         Evaluate a model.
         """
-        if verbose:
-            logger.info("Evaluating model ...")
+        logger.debug("Evaluating model ...")
         # load evaluation from cache if it exists
         signal = self._load_evaluation(model_id)
         if signal is not None:
-            if verbose:
-                logger.info("Loading evaluation from cache ...")
+            logger.debug("Loading evaluation from cache ...")
             # footprint evaluation
             self.footprints[accelerator_spec].record(
                 model_id=model_id,
                 metrics=FootprintNodeMetric(
                     value=signal,
                     is_goals_met=False,
                 ),
             )
             return signal
 
-        # TODO: add the accelerator spec to the evaluate
         # evaluate model
         metrics = evaluator_config.metrics if evaluator_config else []
-        signal = self.target.evaluate_model(model, metrics)
+        signal = self.target.evaluate_model(model, metrics, accelerator_spec)
 
         # cache evaluation
         self._cache_evaluation(model_id, signal)
 
         # footprint evaluation
         self.footprints[accelerator_spec].record(
             model_id=model_id,
             metrics=FootprintNodeMetric(
                 value=signal,
                 is_goals_met=False,
             ),
         )
         return signal
 
-    def _get_top_ranked_nodes(self, metrics: List[Metric], footprint: Footprint, k: int) -> List[FootprintNode]:
-        metric_priority = [metric.name for metric in sorted(metrics, key=lambda x: x.priority_rank)]
+    def _get_top_ranked_nodes(
+        self, objective_dict: Dict[str, Any], footprint: Footprint, k: int
+    ) -> List[FootprintNode]:
         footprint_node_list = footprint.nodes.values()
         sorted_footprint_node_list = sorted(
             footprint_node_list,
             key=lambda x: tuple(
-                x.metrics.value[metric] if x.metrics.cmp_direction[metric] == 1 else -x.metrics.value[metric]
-                for metric in metric_priority
+                x.metrics.value[metric].value
+                if x.metrics.cmp_direction[metric] == 1
+                else -x.metrics.value[metric].value
+                for metric in objective_dict.keys()
             ),
             reverse=True,
         )
         selected_footprint_nodes = sorted_footprint_node_list[:k]
         return selected_footprint_nodes
```

## olive/engine/footprint.py

```diff
@@ -3,44 +3,45 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import logging
 from collections import OrderedDict, defaultdict
 from datetime import datetime
 from pathlib import Path
-from typing import Dict
+from typing import DefaultDict, Dict
 
 from olive.common.config_utils import ConfigBase, config_json_dumps, config_json_loads
+from olive.evaluator.metric import MetricResult
 
 logger = logging.getLogger(__name__)
 
 
 class FootprintNodeMetric(ConfigBase):
     """
     value: {"metric_name": metrics_value, ...}
     cmp_direction: will be auto suggested. The format will be like: {"metric_name": 1, ...},
         1: higher is better, -1: lower is better
     is_goals_met: if the goals set by users is met
     """
 
-    value: Dict = None
-    cmp_direction: Dict = None
+    value: MetricResult = None
+    cmp_direction: DefaultDict[str, int] = None
     is_goals_met: bool = False
 
 
 class FootprintNode(ConfigBase):
     # None for no parent which means current model is the input model
     parent_model_id: str = None
     model_id: str
     model_config: Dict = None
     from_pass: str = None
     pass_run_config: Dict = None
     is_pareto_frontier: bool = False
     # TODO add EP/accelerators for same_model_id metrics
-    metrics: FootprintNodeMetric = FootprintNodeMetric()
+    metrics: FootprintNodeMetric = None
 
     date_time: float = datetime.now().timestamp()
 
     def update(self, **kwargs):
         for k, v in kwargs.items():
             setattr(self, k, v)
 
@@ -70,37 +71,37 @@
                 continue
             return len(metric.metrics.value)
 
     def record_objective_dict(self, objective_dict):
         self.objective_dict = objective_dict
 
     def _is_empty_metric(self, metric: FootprintNodeMetric):
-        return not metric or not metric.value
+        return not metric
 
     def resolve_metrics(self):
         for k, v in self.nodes.items():
-            if v.metrics is None:
+            if self._is_empty_metric(v.metrics):
                 continue
             if self.nodes[k].metrics.cmp_direction is None:
                 self.nodes[k].metrics.cmp_direction = {}
-            if self.nodes[k].metrics.value is None:
-                self.nodes[k].metrics.value = {}
 
             is_goals_met = []
             for metric_name in v.metrics.value:
-                if metric_name in self.objective_dict:
-                    cmp_direction = 1 if self.objective_dict[metric_name]["higher_is_better"] else -1
-                    self.nodes[k].metrics.cmp_direction[metric_name] = cmp_direction
-                    _goal = self.objective_dict[metric_name]["goal"]
-                    if _goal is None:
-                        is_goals_met.append(True)
-                    else:
-                        is_goals_met.append(v.metrics.value[metric_name] * cmp_direction >= _goal)
+                if metric_name not in self.objective_dict:
+                    logger.debug("There is no goal set for metric: {metric_name}.")
+                    continue
+                higher_is_better = self.objective_dict[metric_name]["higher_is_better"]
+                cmp_direction = 1 if higher_is_better else -1
+                self.nodes[k].metrics.cmp_direction[metric_name] = cmp_direction
+
+                _goal = self.objective_dict[metric_name]["goal"]
+                if _goal is None:
+                    is_goals_met.append(True)
                 else:
-                    logger.warning(f"Metric {metric_name} is not in the objective dict")
+                    is_goals_met.append(v.metrics.value[metric_name].value * cmp_direction >= _goal)
             self.nodes[k].metrics.is_goals_met = all(is_goals_met)
 
     def record(self, foot_print_node: FootprintNode = None, **kwargs):
         _model_id = kwargs.get("model_id", None)
         if foot_print_node is not None:
             _model_id = foot_print_node.model_id
             self.nodes[_model_id] = foot_print_node
@@ -136,16 +137,19 @@
                 # other point's metrics is [2, 3, 4], then current point is not pareto frontier
                 # but if current point's metrics is [3, 2, 3], other point's metrics is [2, 3, 4],
                 # then current point is pareto frontier
                 # Note: equal points don't dominate one another
                 equal = True  # two points are equal
                 dominated = True  # current point is dominated by other point
                 for metric_name in v.metrics.value:
-                    other_point_metrics = _v.metrics.value[metric_name] * _v.metrics.cmp_direction[metric_name]
-                    current_point_metrics = v.metrics.value[metric_name] * v.metrics.cmp_direction[metric_name]
+                    if metric_name not in _v.metrics.cmp_direction:
+                        logger.debug(f"Metric {metric_name} is not in cmp_direction, will not be compared.")
+                        continue
+                    other_point_metrics = _v.metrics.value[metric_name].value * _v.metrics.cmp_direction[metric_name]
+                    current_point_metrics = v.metrics.value[metric_name].value * v.metrics.cmp_direction[metric_name]
                     dominated &= current_point_metrics <= other_point_metrics
                     equal &= current_point_metrics == other_point_metrics
                 # point is not on pareto frontier if dominated and not equal
                 _against_pareto_frontier_check = dominated and not equal
                 cmp_flag &= not _against_pareto_frontier_check
             self.nodes[k].is_pareto_frontier = cmp_flag
         self.is_marked_pareto_frontier = True
@@ -174,46 +178,47 @@
 
     def _get_metrics_name_by_indices(self, indices):
         rls = list()
         for _, v in self.nodes.items():
             if not self._is_empty_metric(v.metrics):
                 for index in indices:
                     if isinstance(index, str):
-                        if index in v.metrics.value:
+                        if index in self.objective_dict:
                             rls.append(index)
                         else:
                             logger.error(f"the metric {index} is not in the metrics")
                     if isinstance(index, int):
-                        if index < len(v.metrics.value):
-                            rls.append(list(v.metrics.value.keys())[index])
+                        if index < len(self.objective_dict):
+                            rls.append(list(self.objective_dict.keys())[index])
                         else:
                             logger.error(f"the index {index} is out of range")
                 return rls
         return rls
 
     def plot_pareto_frontier_to_html(self, index=None, save_path=None, is_show=False):
         self.plot_pareto_frontier(index, save_path, is_show, "html")
 
     def plot_pareto_frontier_to_image(self, index=None, save_path=None, is_show=False):
         self.plot_pareto_frontier(index, save_path, is_show, "image")
 
-    def plot_pareto_frontier(self, index=None, save_path=None, is_show=True, save_format="html"):
+    def plot_pareto_frontier(self, ranks=None, save_path=None, is_show=True, save_format="html"):
         """
         plot pareto frontier with plotly
-        :param index: the index of the metrics to be shown in the pareto frontier chart
+        :param ranks: the rank list of the metrics to be shown in the pareto frontier chart
         :param save_path: the path to save the pareto frontier chart
         :param is_show: whether to show the pareto frontier chart
         :param save_format: the format of the pareto frontier chart, can be "html" or "image"
         """
         assert save_path is not None or is_show, "you must specify the save path or set is_show to True"
         if self.metric_numbers() <= 1:
             logger.warning("There is no need to plot pareto frontier with only one metric")
             return
 
-        index = index or [0, 1]
+        ranks = ranks or [1, 2]
+        index = [i - 1 for i in ranks]
         self.mark_pareto_frontier()
         nodes_to_be_plotted = self.get_candidates()
 
         if not nodes_to_be_plotted:
             logger.warning("there is no candidate to be plotted.")
             return
         # plot pareto frontier
@@ -236,16 +241,16 @@
         for k, v in nodes_to_be_plotted.items():
             dict_data["model_id"].append(k)
             dict_data["is_pareto_frontier"].append(v.is_pareto_frontier)
             dict_data["marker_color"].append("red" if v.is_pareto_frontier else "blue")
             dict_data["marker_size"].append(12 if v.is_pareto_frontier else 8)
             show_list = [k]
             for metric_name in metric_column:
-                dict_data[metric_name].append(v.metrics.value[metric_name])
-                show_list.append(f"{metric_name}: {v.metrics.value[metric_name]}")
+                dict_data[metric_name].append(v.metrics.value[metric_name].value)
+                show_list.append(f"{metric_name}: {v.metrics.value}")
             dict_data["show_text"].append("<br>".join(show_list))
         data = pd.DataFrame(dict_data)
 
         fig = go.Figure()
         fig.add_trace(
             go.Scatter(
                 x=dict_data[metric_column[index[0]]],
```

## olive/engine/packaging/packaging_generator.py

```diff
@@ -7,59 +7,70 @@
 import platform
 import shutil
 import tempfile
 import urllib.request
 from collections import OrderedDict
 from pathlib import Path
 from string import Template
+from typing import Dict
 
 import pkg_resources
 
 from olive.common.utils import run_subprocess
 from olive.engine.footprint import Footprint
 from olive.engine.packaging.packaging_config import PackagingConfig, PackagingType
+from olive.hardware import AcceleratorSpec
 
 logger = logging.getLogger(__name__)
 
 
 def generate_output_artifacts(
-    packaging_config: PackagingConfig, foot_print: Footprint, pf_footprint: Footprint, output_dir: Path
+    packaging_config: PackagingConfig,
+    footprints: Dict[AcceleratorSpec, Footprint],
+    pf_footprints: Dict[AcceleratorSpec, Footprint],
+    output_dir: Path,
 ):
-    if pf_footprint.nodes is None or len(pf_footprint.nodes) == 0:
+    if sum([len(f.nodes) if f.nodes else 0 for f in pf_footprints.values()]) == 0:
         logger.warning("No model is selected. Skip packaging output artifacts.")
         return
     if packaging_config.type == PackagingType.Zipfile:
-        _generate_zipfile_output(packaging_config, foot_print, pf_footprint, output_dir)
+        _generate_zipfile_output(packaging_config, footprints, pf_footprints, output_dir)
 
 
 def _generate_zipfile_output(
-    packaging_config: PackagingConfig, footprint: Footprint, pf_footprint: Footprint, output_dir: Path
+    packaging_config: PackagingConfig,
+    footprints: Dict[AcceleratorSpec, Footprint],
+    pf_footprints: Dict[AcceleratorSpec, Footprint],
+    output_dir: Path,
 ) -> None:
     logger.info("Packaging Zipfile output artifacts")
     cur_path = Path(__file__).parent
     with tempfile.TemporaryDirectory() as tempdir:
         tempdir = Path(tempdir)
-        _package_sample_code(cur_path, tempdir, pf_footprint)
-        _package_candidate_models(tempdir, footprint, pf_footprint)
-        _package_onnxruntime_packages(tempdir, pf_footprint)
+        _package_sample_code(cur_path, tempdir)
+        for accelerator_spec, pf_footprint in pf_footprints.items():
+            if pf_footprint.nodes and footprints[accelerator_spec].nodes:
+                _package_candidate_models(tempdir, footprints[accelerator_spec], pf_footprint, accelerator_spec)
+        _package_onnxruntime_packages(tempdir, next(iter(pf_footprints.values())))
         shutil.make_archive(packaging_config.name, "zip", tempdir)
         shutil.move(f"{packaging_config.name}.zip", output_dir / f"{packaging_config.name}.zip")
 
 
-def _package_sample_code(cur_path, tempdir, pf_footprint: Footprint):
+def _package_sample_code(cur_path, tempdir):
     shutil.copytree(cur_path / "sample_code", tempdir / "SampleCode")
 
 
-def _package_candidate_models(tempdir, footprint: Footprint, pf_footprint: Footprint) -> None:
+def _package_candidate_models(
+    tempdir, footprint: Footprint, pf_footprint: Footprint, accelerator_spec: AcceleratorSpec
+) -> None:
     candidate_models_dir = tempdir / "CandidateModels"
-    candidate_models_dir.mkdir()
     model_rank = 1
     for model_id, node in pf_footprint.nodes.items():
-        model_dir = candidate_models_dir / f"BestCandidateModel_{model_rank}"
-        model_dir.mkdir()
+        model_dir = candidate_models_dir / f"{accelerator_spec}" / f"BestCandidateModel_{model_rank}"
+        model_dir.mkdir(parents=True, exist_ok=True)
         model_rank += 1
         # Copy model file
         model_path = pf_footprint.get_model_path(model_id)
         model_type = pf_footprint.get_model_type(model_id)
         if model_type == "ONNXModel":
             shutil.copy2(model_path, model_dir / "model.onnx")
         elif model_type == "OpenVINOModel":
@@ -85,15 +96,15 @@
         with open(configuration_path, "w") as f:
             json.dump(OrderedDict(reversed(footprint.trace_back_run_history(model_id).items())), f)
 
         # Copy metrics
         # TODO: Add target info to metrics file
         metric_path = str(model_dir / "metrics.json")
         with open(metric_path, "w") as f:
-            json.dump(node.metrics.value, f)
+            f.write(node.json())
 
 
 def _package_onnxruntime_packages(tempdir, pf_footprint: Footprint):
     NIGHTLY_PYTHON_CPU_COMMAND = Template(
         "python -m pip download -i "
         "https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ "
         "ort-nightly==$ort_version --no-deps -d $python_download_path"
@@ -107,16 +118,16 @@
         "ort-nightly-gpu==$ort_version --no-deps -d $python_download_path"
     )
     STABLE_PYTHON_GPU_COMMAND = Template(
         "python -m pip download onnxruntime-gpu==$ort_version --no-deps -d $python_download_path"
     )
 
     installed_packages = pkg_resources.working_set
-    onnxruntime_pkg = [i for i in installed_packages if i.key == "onnxruntime"]
-    ort_nightly_pkg = [i for i in installed_packages if i.key == "ort-nightly"]
+    onnxruntime_pkg = [i for i in installed_packages if i.key.startswith("onnxruntime")]
+    ort_nightly_pkg = [i for i in installed_packages if i.key.startswith("ort-nightly")]
     is_nightly = True if ort_nightly_pkg else False
     is_stable = True if onnxruntime_pkg else False
 
     if not is_nightly and not is_stable:
         logger.warning("ONNXRuntime package is not installed. Skip packaging ONNXRuntime package.")
         return
```

## olive/evaluator/metric.py

```diff
@@ -1,21 +1,24 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import logging
 from enum import Enum
-from typing import Union
+from typing import ClassVar, Dict, List, Union
 
 from pydantic import validator
 
-from olive.common.config_utils import ConfigBase, validate_config
+from olive.common.config_utils import ConfigBase, ConfigDictBase, validate_config
 from olive.data.config import DataConfig
 from olive.evaluator.accuracy import AccuracyBase
 from olive.evaluator.metric_config import LatencyMetricConfig, MetricGoal, get_user_config_class
 
+logger = logging.getLogger(__name__)
+
 
 class MetricType(str, Enum):
     ACCURACY = "accuracy"
     LATENCY = "latency"
     CUSTOM = "custom"
 
 
@@ -35,104 +38,144 @@
     P75 = "p75"
     P90 = "p90"
     P95 = "p95"
     P99 = "p99"
     P999 = "p999"
 
 
-# TODO: support multiple subtypes at the same type for the same type
-# Otherwise it's a waste of compute and time if we have to evaluate a model for different subtypes
-# names, subtypes: Union[str, List[str]]
-# However accuracy metric poses a slight problem since AUC has a different config. Need to resolve this
-# so that we get a single metric config for a single type
-# This way, the user can return multiple metrics at once
+class SubMetric(ConfigBase):
+    name: Union[AccuracySubType, LatencyMetricConfig, str]
+    metric_config: ConfigBase = None
+    # -1 means no priority which will be evaluated only
+    priority: int = -1
+    higher_is_better: bool = False
+    goal: MetricGoal = None
+
+    @validator("goal")
+    def validate_goal(cls, v, values):
+        if v is None:
+            return v
+        if v.type not in ["percent-min-improvement", "percent-max-degradation"]:
+            return v
+
+        if "higher_is_better" not in values:
+            raise ValueError("Invalid higher_is_better")
+        higher_is_better = values["higher_is_better"]
+
+        ranges = {
+            ("percent-min-improvement", True): (0, float("inf")),
+            ("percent-min-improvement", False): (0, 100),
+            ("percent-max-degradation", True): (0, 100),
+            ("percent-max-degradation", False): (0, float("inf")),
+        }
+        valid_range = ranges[(v.type, higher_is_better)]
+        if not valid_range[0] < v.value < valid_range[1]:
+            raise ValueError(
+                f"Invalid goal value {v.value} for {v.type} and higher_is_better={higher_is_better}. Valid range is"
+                f" {valid_range}"
+            )
+        return v
+
+
 class Metric(ConfigBase):
     name: str
     type: MetricType
-    sub_type: Union[AccuracySubType, LatencySubType] = None
-    higher_is_better: bool = True
-    priority_rank: int = 1
-    goal: MetricGoal = None
-    metric_config: ConfigBase = None
+    sub_types: List[SubMetric]
     user_config: ConfigBase = None
     data_config: DataConfig = DataConfig()
 
-    @validator("sub_type", always=True, pre=True)
-    def validate_sub_type(cls, v, values):
+    def get_sub_type_info(self, info_name, no_priority_filter=True, callback=lambda x: x):
+        sub_type_info = {}
+        for sub_type in self.sub_types:
+            if no_priority_filter and sub_type.priority <= 0:
+                continue
+            sub_type_info[sub_type.name] = callback(getattr(sub_type, info_name))
+        return sub_type_info
+
+    @validator("sub_types", always=True, pre=True, each_item=True)
+    def validate_sub_types(cls, v, values):
         if "type" not in values:
             raise ValueError("Invalid type")
 
         if values["type"] == MetricType.CUSTOM:
-            return None
+            if v.get("priority", -1) != -1 and v.get("higher_is_better", None) is None:
+                raise ValueError(f"higher_is_better must be specified for ranked custom metric: {v['name']}")
+            return v
+        # name
         sub_type_enum = AccuracySubType if values["type"] == MetricType.ACCURACY else LatencySubType
         try:
-            v = sub_type_enum(v)
+            v["name"] = sub_type_enum(v["name"])
         except ValueError:
             raise ValueError(
-                f"sub_type must be one of {list(sub_type_enum.__members__.keys())} for {values['type']} metric"
+                f"sub_type {v['name']} is not in {list(sub_type_enum.__members__.keys())} for {values['type']} metric"
             )
-        return v
 
-    @validator("higher_is_better", always=True, pre=True)
-    def validate_higher_is_better(cls, v, values):
-        if "type" not in values:
-            raise ValueError("Invalid type")
+        # metric_config
+        metric_config_cls = None
+        if sub_type_enum is AccuracySubType:
+            v["higher_is_better"] = True
+            metric_config_cls = AccuracyBase.registry[v["name"]].get_config_class()
+        elif sub_type_enum is LatencySubType:
+            v["higher_is_better"] = False
+            metric_config_cls = LatencyMetricConfig
+        v["metric_config"] = validate_config(v.get("metric_config", {}), ConfigBase, metric_config_cls)
 
-        if values["type"] == MetricType.ACCURACY:
-            return True
-        if values["type"] == MetricType.LATENCY:
-            return False
-        if v is None:
-            raise ValueError("higher_is_better must be specified for custom metric")
         return v
 
-    @validator("metric_config", always=True, pre=True)
-    def validate_metric_config(cls, v, values):
+    @validator("user_config", pre=True)
+    def validate_user_config(cls, v, values):
         if "type" not in values:
             raise ValueError("Invalid type")
-        if "sub_type" not in values:
-            raise ValueError("Invalid sub_type")
 
-        if values["type"] == MetricType.CUSTOM:
+        user_config_class = get_user_config_class(values["type"])
+        return validate_config(v, ConfigBase, user_config_class)
+
+
+class SubMetricResult(ConfigBase):
+    value: Union[float, int]
+    priority: int
+    higher_is_better: bool
+
+
+class MetricResult(ConfigDictBase):
+    __root__: Dict[str, SubMetricResult] = None
+    delimiter: ClassVar[str] = "-"
+
+    def get_value(self, metric_name, sub_type_name):
+        if not self.__root__:
             return None
+        return self.__root__[joint_metric_key(metric_name, sub_type_name)].value
 
-        # metric config class
-        if values["type"] == MetricType.LATENCY:
-            metric_config_class = LatencyMetricConfig
-        elif values["type"] == MetricType.ACCURACY:
-            metric_config_class = AccuracyBase.registry[values["sub_type"]].get_config_class()
+    def get_all_sub_type_metric_value(self, metric_name):
+        return {k.split(self.delimiter)[-1]: v.value for k, v in self.__root__.items() if k.startswith(metric_name)}
 
-        # validate metric config
-        return validate_config(v, ConfigBase, metric_config_class)
+    def __str__(self) -> str:
+        repr_obj = {k: v.value for k, v in self.__root__.items()}
+        return f"{repr_obj}"
 
-    @validator("user_config", pre=True)
-    def validate_user_config(cls, v, values):
-        if "type" not in values:
-            raise ValueError("Invalid type")
 
-        user_config_class = get_user_config_class(values["type"])
-        return validate_config(v, ConfigBase, user_config_class)
+def joint_metric_key(metric_name, sub_type_name):
+    return f"{metric_name}{MetricResult.delimiter}{sub_type_name}"
 
-    @validator("goal")
-    def validate_goal(cls, v, values):
-        if v is None:
-            return v
-        if v.type not in ["percent-min-improvement", "percent-max-degradation"]:
-            return v
 
-        if "higher_is_better" not in values:
-            raise ValueError("Invalid higher_is_better")
-        higher_is_better = values["higher_is_better"]
+def flatten_metric_sub_type(metric_dict: Dict[str, Dict]):
+    flatten_results = {}
+    for metric_name, metric_res in metric_dict.items():
+        for sub_type_name, sub_type_res in metric_res.items():
+            key = joint_metric_key(metric_name, sub_type_name)
+            flatten_results[key] = sub_type_res
+    return flatten_results
 
-        ranges = {
-            ("percent-min-improvement", True): (0, float("inf")),
-            ("percent-min-improvement", False): (0, 100),
-            ("percent-max-degradation", True): (0, 100),
-            ("percent-max-degradation", False): (0, float("inf")),
-        }
-        valid_range = ranges[(v.type, higher_is_better)]
-        if not valid_range[0] < v.value < valid_range[1]:
-            raise ValueError(
-                f"Invalid goal value {v.value} for {v.type} and higher_is_better={higher_is_better}. Valid range is"
-                f" {valid_range}"
-            )
-        return v
+
+def flatten_metric_result(dict_results: Dict[str, MetricResult]):
+    return MetricResult.parse_obj(flatten_metric_sub_type(dict_results))
+
+
+def get_latency_config_from_metric(metric: Metric):
+    warmup_num, repeat_test_num, sleep_num = None, None, None
+    for sub_type in metric.sub_types:
+        if sub_type.metric_config:
+            warmup_num = sub_type.metric_config.warmup_num
+            repeat_test_num = sub_type.metric_config.repeat_test_num
+            sleep_num = sub_type.metric_config.sleep_num
+            break
+    return warmup_num, repeat_test_num, sleep_num
```

## olive/evaluator/olive_evaluator.py

```diff
@@ -1,29 +1,40 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 import time
 from abc import ABC, abstractmethod
-from typing import Any, Dict, List
+from numbers import Number
+from typing import Any, Dict, List, Tuple, Union
 
 import numpy as np
 import torch
 from pydantic import validator
 from torch.utils.data import Dataset
 
 from olive.common.config_utils import ConfigBase
 from olive.common.user_module_loader import UserModuleLoader
 from olive.common.utils import tensor_data_to_device
 from olive.constants import Framework
 from olive.evaluator.accuracy import AUC, AccuracyScore, F1Score, Precision, Recall
-from olive.evaluator.metric import AccuracySubType, LatencySubType, Metric, MetricType
-from olive.model import OliveModel, ONNXModel, OpenVINOModel, PyTorchModel, SNPEModel
-from olive.systems.common import Device
+from olive.evaluator.metric import (
+    AccuracySubType,
+    LatencySubType,
+    Metric,
+    MetricResult,
+    MetricType,
+    SubMetricResult,
+    flatten_metric_result,
+    get_latency_config_from_metric,
+    joint_metric_key,
+)
+from olive.hardware import Device
+from olive.model import DistributedOnnxModel, OliveModel, ONNXModel, OpenVINOModel, PyTorchModel, SNPEModel
 
 logger = logging.getLogger(__name__)
 
 
 class DummyDataloader(Dataset):
     def __init__(self, input_names, input_shapes, input_types):
         self.input_names = input_names
@@ -72,54 +83,94 @@
             metric.user_config.inference_settings.get(self.framework.lower())
             if metric.user_config.inference_settings
             else None
         )
 
     @abstractmethod
     def _evaluate_accuracy(
-        self, model: OliveModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
+        self,
+        model: OliveModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
         raise NotImplementedError()
 
     @abstractmethod
     def _evaluate_latency(
-        self, model: OliveModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
+        self,
+        model: OliveModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
         raise NotImplementedError()
 
     def _evaluate_custom(
         self,
         model: OliveModel,
         metric: Metric,
         dataloader: Dataset,
         eval_func,
-        device: Device = Device.CPU,
         post_func=None,
-    ) -> Dict[str, Any]:
+        device: Device = Device.CPU,
+        execution_providers=None,
+    ) -> MetricResult:
         # TODO: Change the evaluate function to accept the metric rather than
         # breaking it into multiple arguments
         # return eval_func(model, metric, dataloader, device, post_func)
-        return eval_func(model, metric.user_config.data_dir, metric.user_config.batch_size, device)
+        raw_res = eval_func(
+            model, metric.user_config.data_dir, metric.user_config.batch_size, device, execution_providers
+        )
+        metric_res = {}
+        for sub_type in metric.sub_types:
+            if isinstance(raw_res, Number):
+                assert len(metric.sub_types) == 1, "Only one sub type is allowed for single value custom metric"
+                metric_res[sub_type.name] = SubMetricResult(
+                    value=raw_res, priority=sub_type.priority, higher_is_better=sub_type.higher_is_better
+                )
+            elif isinstance(raw_res, dict):
+                assert sub_type.name in raw_res, f"Custom metric {sub_type.name} is not in the result"
+                metric_res[sub_type.name] = SubMetricResult(
+                    value=raw_res[sub_type.name],
+                    priority=sub_type.priority,
+                    higher_is_better=sub_type.higher_is_better,
+                )
+        return MetricResult.parse_obj(metric_res)
 
-    def evaluate(self, model: OliveModel, metrics: List[Metric], device: Device = Device.CPU) -> Dict[str, Any]:
+    def evaluate(
+        self,
+        model: OliveModel,
+        metrics: List[Metric],
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
         metrics_res = {}
         for metric in metrics:
             dataloader, eval_func, post_func = OliveEvaluator.get_user_config(metric)
 
             if metric.type == MetricType.ACCURACY:
-                metrics_res[metric.name] = self._evaluate_accuracy(model, metric, dataloader, device, post_func)
+                metrics_res[metric.name] = self._evaluate_accuracy(
+                    model, metric, dataloader, post_func, device, execution_providers
+                )
             elif metric.type == MetricType.LATENCY:
-                metrics_res[metric.name] = self._evaluate_latency(model, metric, dataloader, device, post_func)
+                metrics_res[metric.name] = self._evaluate_latency(
+                    model, metric, dataloader, post_func, device, execution_providers
+                )
             elif metric.type == MetricType.CUSTOM:
                 metrics_res[metric.name] = self._evaluate_custom(
-                    model, metric, dataloader, eval_func, device, post_func
+                    model, metric, dataloader, eval_func, post_func, device, execution_providers
                 )
             else:
                 raise TypeError(f"{metric.type} is not a supported metric type")
-        return metrics_res
+        return flatten_metric_result(metrics_res)
 
     @staticmethod
     def get_user_config(metric: Metric):
         user_module = UserModuleLoader(metric.user_config.user_script, metric.user_config.script_dir)
 
         post_processing_func = getattr(metric.user_config, "post_processing_func", None)
         post_func = user_module.load_object(post_processing_func)
@@ -144,56 +195,66 @@
             # dataloder to meet back compatibility for time being.
             dataloader = dataloader or dc.create_dataloader()
             post_func = post_func or dc.config.post_process
 
         return dataloader, eval_func, post_func
 
     @staticmethod
-    def compute_accuracy(metric: Metric, preds: Any, targets: Any) -> Dict[str, Any]:
+    def compute_accuracy(metric: Metric, preds: Any, targets: Any) -> MetricResult:
         """
         Compute accuracy metrics
         """
-        if metric.sub_type == AccuracySubType.ACCURACY_SCORE:
-            return AccuracyScore(metric.metric_config).measure(preds, targets)
-        elif metric.sub_type == AccuracySubType.F1_SCORE:
-            return F1Score(metric.metric_config).measure(preds, targets)
-        elif metric.sub_type == AccuracySubType.PRECISION:
-            return Precision(metric.metric_config).measure(preds, targets)
-        elif metric.sub_type == AccuracySubType.RECALL:
-            return Recall(metric.metric_config).measure(preds, targets)
-        elif metric.sub_type == AccuracySubType.AUC:
-            return AUC(metric.metric_config).measure(preds, targets)
-        else:
-            raise TypeError(f"{metric.sub_type} is not a supported accuracy metric")
+        metric_res = {}
+        sub_type_metric_value = None
+        sub_types = metric.sub_types
+        for sub_type in sub_types:
+            metric_config = sub_type.metric_config
+            if sub_type.name == AccuracySubType.ACCURACY_SCORE:
+                sub_type_metric_value = AccuracyScore(metric_config).measure(preds, targets)
+            elif sub_type.name == AccuracySubType.F1_SCORE:
+                sub_type_metric_value = F1Score(metric_config).measure(preds, targets)
+            elif sub_type.name == AccuracySubType.PRECISION:
+                sub_type_metric_value = Precision(metric_config).measure(preds, targets)
+            elif sub_type.name == AccuracySubType.RECALL:
+                sub_type_metric_value = Recall(metric_config).measure(preds, targets)
+            elif sub_type.name == AccuracySubType.AUC:
+                sub_type_metric_value = AUC(metric_config).measure(preds, targets)
+            else:
+                raise TypeError(f"{sub_type} is not a accuracy metric supported")
+            metric_res[sub_type.name] = SubMetricResult(
+                value=sub_type_metric_value,
+                priority=sub_type.priority,
+                higher_is_better=sub_type.higher_is_better,
+            )
+        return MetricResult.parse_obj(metric_res)
 
     @staticmethod
-    def compute_latency(metric: Metric, latencies: Any) -> float:
+    def compute_latency(metric: Metric, latencies: Any) -> MetricResult:
         """
         Compute latency metrics
         """
-        if metric.sub_type == LatencySubType.AVG:
-            return round(sum(latencies) / len(latencies) * 1000, 5)
-        elif metric.sub_type == LatencySubType.MAX:
-            return round(max(latencies) * 1000, 5)
-        elif metric.sub_type == LatencySubType.MIN:
-            return round(min(latencies) * 1000, 5)
-        elif metric.sub_type == LatencySubType.P50:
-            return round(np.percentile(latencies, 50) * 1000, 5)
-        elif metric.sub_type == LatencySubType.P75:
-            return round(np.percentile(latencies, 75) * 1000, 5)
-        elif metric.sub_type == LatencySubType.P90:
-            return round(np.percentile(latencies, 90) * 1000, 5)
-        elif metric.sub_type == LatencySubType.P95:
-            return round(np.percentile(latencies, 95) * 1000, 5)
-        elif metric.sub_type == LatencySubType.P99:
-            return round(np.percentile(latencies, 99) * 1000, 5)
-        elif metric.sub_type == LatencySubType.P999:
-            return round(np.percentile(latencies, 99.9) * 1000, 5)
-        else:
-            raise TypeError(f"{metric.sub_type} is not a supported latency metric")
+        latency_metrics = {
+            LatencySubType.AVG: round(sum(latencies) / len(latencies) * 1000, 5),
+            LatencySubType.MAX: round(max(latencies) * 1000, 5),
+            LatencySubType.MIN: round(min(latencies) * 1000, 5),
+            LatencySubType.P50: round(np.percentile(latencies, 50) * 1000, 5),
+            LatencySubType.P75: round(np.percentile(latencies, 75) * 1000, 5),
+            LatencySubType.P90: round(np.percentile(latencies, 90) * 1000, 5),
+            LatencySubType.P95: round(np.percentile(latencies, 95) * 1000, 5),
+            LatencySubType.P99: round(np.percentile(latencies, 99) * 1000, 5),
+            LatencySubType.P999: round(np.percentile(latencies, 99.9) * 1000, 5),
+        }
+        metric_res = {}
+        for sub_type in metric.sub_types:
+            metric_res[sub_type.name] = SubMetricResult(
+                value=latency_metrics[sub_type.name],
+                priority=sub_type.priority,
+                higher_is_better=sub_type.higher_is_better,
+            )
+        return MetricResult.parse_obj(metric_res)
 
 
 class OnnxEvaluator(OliveEvaluator, framework=Framework.ONNX):
     def __init__(self):
         super().__init__()
 
     @staticmethod
@@ -211,18 +272,28 @@
                 dtype=name_to_type[k],
             )
             for k in input_data.keys()
             if k in input_names
         }
         return input_dict
 
-    def _evaluate_accuracy(
-        self, model: ONNXModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
-        session = model.prepare_session(inference_settings=self.get_inference_settings(metric), device=device)
+    def _evaluate_onnx_accuracy(
+        self,
+        model: ONNXModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
+        session = model.prepare_session(
+            inference_settings=self.get_inference_settings(metric),
+            device=device,
+            execution_providers=execution_providers,
+        )
         io_config = model.get_io_config()
 
         preds = []
         targets = []
         output_names = io_config["output_names"]
         for input_data, labels in dataloader:
             input_dict = OnnxEvaluator.format_input(input_data, io_config)
@@ -230,22 +301,30 @@
             result = torch.Tensor(res[0]) if len(output_names) == 1 else torch.Tensor(res)
             outputs = post_func(result) if post_func else result
             preds.extend(outputs.tolist())
             targets.extend(labels.data.tolist())
 
         return OliveEvaluator.compute_accuracy(metric, preds, targets)
 
-    def _evaluate_latency(
-        self, model: OliveModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
-        warmup_num = metric.metric_config.warmup_num
-        repeat_test_num = metric.metric_config.repeat_test_num
-        sleep_num = metric.metric_config.sleep_num
-
-        session = model.prepare_session(inference_settings=self.get_inference_settings(metric), device=device)
+    def _evaluate_onnx_latency(
+        self,
+        model: OliveModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
+        warmup_num, repeat_test_num, sleep_num = get_latency_config_from_metric(metric)
+
+        session = model.prepare_session(
+            inference_settings=self.get_inference_settings(metric),
+            device=device,
+            execution_providers=execution_providers,
+        )
         io_config = model.get_io_config()
 
         input_data, _ = next(iter(dataloader))
         input_dict = OnnxEvaluator.format_input(input_data, io_config)
 
         if metric.user_config.io_bind:
             io_bind_op = session.io_binding()
@@ -271,26 +350,209 @@
                 t = time.perf_counter()
                 session.run(input_feed=input_dict, output_names=None)
                 latencies.append(time.perf_counter() - t)
             time.sleep(sleep_num)
 
         return OliveEvaluator.compute_latency(metric, latencies)
 
+    @staticmethod
+    def _evaluate_distributed_accuracy_worker(config) -> Tuple[List[Any], List[Any]]:
+        model_path = config["model_path"]
+        local_rank = config["local_rank"]
+        world_size = config["world_size"]
+        inference_settings = config.get("inference_settings", {}) or {}
+        metric = Metric.from_json(config["metric"])
+        dataloader, _, post_func = OnnxEvaluator.get_user_config(metric)
+
+        import os
+
+        os.environ["OMPI_COMM_WORLD_RANK"] = str(local_rank)
+        os.environ["OMPI_COMM_WORLD_SIZE"] = str(world_size)
+
+        from mpi4py import MPI
+
+        local_rank = MPI.COMM_WORLD.Get_rank()
+
+        # TODO: EPs should be selected based on accelerator_spec param passed down from the engine
+        inference_settings["execution_provider"] = ["CUDAExecutionProvider", "CPUExecutionProvider"]
+        inference_settings["provider_options"] = [{"device_id": str(local_rank)}, {}]
+
+        model = ONNXModel(model_path, inference_settings=inference_settings)
+        session = model.prepare_session(inference_settings=inference_settings, device=Device.GPU, rank=int(local_rank))
+        io_config = model.get_io_config()
+
+        preds = []
+        targets = []
+        output_names = io_config["output_names"]
+        for _, (input_data, labels) in enumerate(dataloader):
+            input_dict = OnnxEvaluator.format_input(input_data, io_config)
+            MPI.COMM_WORLD.barrier()  # Synchronize before starting each run
+            output = session.run(input_feed=input_dict, output_names=None)
+            output = torch.Tensor(output[0]) if len(output_names) == 1 else torch.Tensor(output)
+            output = post_func(output) if post_func else output
+            preds.extend(output.tolist())
+            targets.extend(labels.data.tolist())
+
+        return preds, targets
+
+    def _evaluate_distributed_accuracy(self, model: DistributedOnnxModel, metric: Metric) -> MetricResult:
+        from copy import deepcopy
+
+        from mpi4py.futures import MPIPoolExecutor
+
+        config = {
+            "model_path": None,
+            "local_rank": None,
+            "world_size": model.ranks,
+            "inference_settings": self.get_inference_settings(metric),
+            "metric": metric.to_json(),
+        }
+
+        args = []
+        for rank in range(model.ranks):
+            cfg = deepcopy(config)
+            cfg["local_rank"] = rank
+            cfg["model_path"] = model.ranked_model_path(rank)
+            args.append(cfg)
+
+        with MPIPoolExecutor(max_workers=model.ranks) as executor:
+            results = executor.map(OnnxEvaluator._evaluate_distributed_accuracy_worker, args)
+            executor.shutdown()
+
+        preds = [x for p, _ in results for x in p]
+        targets = [x for _, t in results for x in t]
+        return OliveEvaluator.compute_accuracy(metric, preds, targets)
+
+    @staticmethod
+    def _evaluate_distributed_latency_worker(config) -> List[float]:
+        model_path = config["model_path"]
+        local_rank = config["local_rank"]
+        world_size = config["world_size"]
+        inference_settings = config.get("inference_settings", {}) or {}
+        metric = Metric.from_json(config["metric"])
+        dataloader, _, _ = OnnxEvaluator.get_user_config(metric)
+
+        import os
+
+        os.environ["OMPI_COMM_WORLD_RANK"] = str(local_rank)
+        os.environ["OMPI_COMM_WORLD_SIZE"] = str(world_size)
+
+        from mpi4py import MPI
+
+        local_rank = MPI.COMM_WORLD.Get_rank()
+        warmup_num, repeat_test_num, sleep_num = get_latency_config_from_metric(metric)
+        # TODO: EPs should be selected based on accelerator_spec param passed down from the engine
+        inference_settings["execution_provider"] = ["CUDAExecutionProvider", "CPUExecutionProvider"]
+        inference_settings["provider_options"] = [{"device_id": str(local_rank)}, {}]
+
+        model = ONNXModel(model_path, inference_settings=inference_settings)
+        session = model.prepare_session(inference_settings=inference_settings, device=Device.GPU, rank=int(local_rank))
+        io_config = model.get_io_config()
+
+        input_feed, _ = next(iter(dataloader))
+        input_feed = OnnxEvaluator.format_input(input_feed, io_config)
+
+        if metric.user_config.io_bind:
+            io_bind_op = session.io_binding()
+            for k, v in input_feed.items():
+                io_bind_op.bind_cpu_input(k, v)
+            for item in session.get_outputs():
+                io_bind_op.bind_output(item.name, "cuda")
+
+        latencies = []
+        for i in range(warmup_num + repeat_test_num):
+            MPI.COMM_WORLD.barrier()  # Synchronize before starting each run
+            start_time = time.perf_counter()
+            if metric.user_config.io_bind:
+                session.run_with_iobinding(io_bind_op)
+            else:
+                session.run(input_feed=input_feed, output_names=None)
+            if i > warmup_num:
+                latencies.append(time.perf_counter() - start_time)
+            time.sleep(sleep_num)
+
+        return latencies
+
+    def _evaluate_distributed_latency(self, model: DistributedOnnxModel, metric: Metric) -> MetricResult:
+        from copy import deepcopy
+
+        from mpi4py.futures import MPIPoolExecutor
+
+        config = {
+            "model_path": None,
+            "local_rank": None,
+            "world_size": model.ranks,
+            "inference_settings": self.get_inference_settings(metric),
+            "metric": metric.to_json(),
+        }
+
+        args = []
+        for rank in range(model.ranks):
+            cfg = deepcopy(config)
+            cfg["local_rank"] = rank
+            cfg["model_path"] = model.ranked_model_path(rank)
+            args.append(cfg)
+
+        with MPIPoolExecutor(max_workers=model.ranks) as executor:
+            results = executor.map(OnnxEvaluator._evaluate_distributed_latency_worker, args)
+            executor.shutdown()
+
+        latencies = [x for r in results for x in r]
+        return OliveEvaluator.compute_latency(metric, latencies)
+
+    def _evaluate_accuracy(
+        self,
+        model: ONNXModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
+        if isinstance(model, ONNXModel):
+            return self._evaluate_onnx_accuracy(model, metric, dataloader, post_func, device, execution_providers)
+        elif isinstance(model, DistributedOnnxModel):
+            return self._evaluate_distributed_accuracy(model, metric)
+        else:
+            raise TypeError(f"Cannot evaluate accuracy for model of type: {type(model)}")
+
+    def _evaluate_latency(
+        self,
+        model: OliveModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
+        if isinstance(model, ONNXModel):
+            return self._evaluate_onnx_latency(model, metric, dataloader, post_func, device, execution_providers)
+        elif isinstance(model, DistributedOnnxModel):
+            return self._evaluate_distributed_latency(model, metric)
+        else:
+            raise TypeError(f"Cannot evaluate latency for model of type: {type(model)}")
+
 
 class PyTorchEvaluator(OliveEvaluator, framework=Framework.PYTORCH):
     def __init__(self):
         super().__init__()
 
     @staticmethod
     def _device_string_to_torch_device(device: Device):
         return torch.device("cuda") if device == Device.GPU else torch.device(device)
 
     def _evaluate_accuracy(
-        self, model: PyTorchModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
+        self,
+        model: PyTorchModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
         session = model.prepare_session(inference_settings=self.get_inference_settings(metric), device=device)
 
         preds = []
         targets = []
         device = PyTorchEvaluator._device_string_to_torch_device(device)
         if device:
             session.to(device)
@@ -304,18 +566,23 @@
             #  ValueError: expected sequence of length 128 at dim 1 (got 3)
             preds.extend(outputs.tolist())
             targets.extend(labels.data.tolist())
 
         return OliveEvaluator.compute_accuracy(metric, preds, targets)
 
     def _evaluate_latency(
-        self, model: PyTorchModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
-        warmup_num = metric.metric_config.warmup_num
-        repeat_test_num = metric.metric_config.repeat_test_num
+        self,
+        model: PyTorchModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
+        warmup_num, repeat_test_num, _ = get_latency_config_from_metric(metric)
         session = model.prepare_session(inference_settings=self.get_inference_settings(metric), device=device)
 
         input_data, _ = next(iter(dataloader))
         device = PyTorchEvaluator._device_string_to_torch_device(device)
         if device:
             session.to(device)
             input_data = tensor_data_to_device(input_data, device)
@@ -340,16 +607,22 @@
 
 
 class SNPEEvaluator(OliveEvaluator, framework=Framework.SNPE):
     def __init__(self):
         super().__init__()
 
     def _evaluate_accuracy(
-        self, model: SNPEModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
+        self,
+        model: SNPEModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
         session = model.prepare_session(inference_settings=self.get_inference_settings(metric), device=device)
 
         preds = []
         targets = []
         for data_dir, input_list, labels in dataloader:
             result = session(input_list, data_dir)
             if post_func:
@@ -358,33 +631,46 @@
                 raise ValueError("Post processing function is required for SNPE model")
             preds.extend(outputs.tolist())
             targets.extend(labels.tolist())
 
         return OliveEvaluator.compute_accuracy(metric, preds, targets)
 
     def _evaluate_latency(
-        self, model: SNPEModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
+        self,
+        model: SNPEModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
+        warmup_num, repeat_test_num, sleep_num = get_latency_config_from_metric(metric)
         session = model.prepare_session(inference_settings=self.get_inference_settings(metric), device=device)
 
         data_dir, input_data, _ = next(iter(dataloader))
-        total_runs = metric.metric_config.warmup_num + metric.metric_config.repeat_test_num
-        results = session(input_data, data_dir, runs=total_runs, sleep=metric.metric_config.sleep_num)
-        latencies = results["latencies"]["total_inference_time"][metric.metric_config.warmup_num:]  # fmt: skip
+        total_runs = warmup_num + repeat_test_num
+        results = session(input_data, data_dir, runs=total_runs, sleep=sleep_num)
+        latencies = results["latencies"]["total_inference_time"][warmup_num]
 
         return OliveEvaluator.compute_latency(metric, latencies)
 
 
 class OpenVINOEvaluator(OliveEvaluator, framework=Framework.OPENVINO):
     def __init__(self):
         super().__init__()
 
     def _evaluate_accuracy(
-        self, model: OpenVINOModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
+        self,
+        model: OpenVINOModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
         session = model.prepare_session(inference_settings=self.get_inference_settings(metric), device=device)
 
         preds = []
         targets = []
         for input_data, labels in dataloader:
             result = session.infer_new_request({0: input_data})
             outputs = post_func(result) if post_func else result
@@ -392,16 +678,22 @@
                 labels = [labels]
             preds.extend(outputs)
             targets.extend(labels)
 
         return OliveEvaluator.compute_accuracy(metric, preds, targets)
 
     def _evaluate_latency(
-        self, model: OpenVINOModel, metric: Metric, dataloader: Dataset, device: Device = Device.CPU, post_func=None
-    ) -> Dict[str, Any]:
+        self,
+        model: OpenVINOModel,
+        metric: Metric,
+        dataloader: Dataset,
+        post_func=None,
+        device: Device = Device.CPU,
+        execution_providers: Union[str, List[str]] = None,
+    ) -> MetricResult:
         session = model.prepare_session(inference_settings=self.get_inference_settings(metric), device=device)
 
         latencies = []
         for input_data, _ in dataloader:
             t = time.perf_counter()
             session(input_data)
             latencies.append(time.perf_counter() - t)
@@ -418,20 +710,36 @@
 
 class OliveEvaluatorConfig(ConfigBase):
     metrics: List[Metric] = []
 
     @validator("metrics")
     def validate_metrics(cls, v):
         metric_len = len(v)
-        if metric_len == 1:
-            return v
 
         metric_names = set([metric.name for metric in v])
         assert len(metric_names) == metric_len, "Metric names must be unique"
 
-        rank_set = set([metric.priority_rank for metric in v])
-        expected_rank_set = set(range(1, metric_len + 1))
+        sub_type_names = set()
+        sub_type_with_rank = set()
+        rank_set = set()
+        for metric in v:
+            for sub_type in metric.sub_types:
+                sub_type_names.add(joint_metric_key(metric.name, sub_type.name))
+                if sub_type.priority != -1:
+                    sub_type_with_rank.add(sub_type.name)
+                    rank_set.add(sub_type.priority)
+
+        if not rank_set and len(sub_type_names) == 1:
+            logger.debug(
+                "No priority is specified, but only one sub type "
+                " metric is specified. Use rank 1 for single for this metric."
+            )
+            v[0].sub_types[0].priority = 1
+        elif not rank_set and len(sub_type_names) > 1:
+            raise ValueError("Priority must be specified for multiple sub type metrics")
+
+        expected_rank_set = set(range(1, len(sub_type_with_rank) + 1))
         # Check if all ranks are present
         if rank_set != expected_rank_set:
-            raise ValueError(f"Priority ranks must be unique and in the range 1 to {metric_len}")
+            raise ValueError(f"Priorities must be unique and in the range 1 to {metric_len}")
 
         return v
```

## olive/passes/olive_pass.py

```diff
@@ -9,14 +9,15 @@
 from typing import Any, Callable, Dict, Optional, Tuple, Type, Union
 
 from pydantic import validator
 
 from olive.common.config_utils import ConfigBase, validate_config
 from olive.common.user_module_loader import UserModuleLoader
 from olive.data.config import DataConfig
+from olive.hardware import DEFAULT_CPU_ACCELERATOR, AcceleratorSpec
 from olive.model import CompositeOnnxModel, DistributedOnnxModel, OliveModel
 from olive.passes.pass_config import (
     PassConfigBase,
     PassConfigParam,
     PassParamDefault,
     create_config_class,
     get_data_config,
@@ -53,22 +54,30 @@
     @classmethod
     def __init_subclass__(cls, **kwargs) -> None:
         """Register the Pass."""
         super().__init_subclass__(**kwargs)
         if not inspect.isabstract(cls):
             cls.registry[cls.__name__.lower()] = cls
 
-    def __init__(self, config_class: Type[PassConfigBase], config: Dict[str, Any]):
+    def __init__(
+        self, accelerator_spec: AcceleratorSpec, config: Dict[str, Any], disable_search: Optional[bool] = False
+    ):
         """Initialize the pass.
 
         :param config_class: the PassConfig class with the default value or default search values.
         :type config_class: Type[PassConfigBase]
         :param config: the configuration representing search space.
         :type config: Dict[str, Any]
         """
+        assert accelerator_spec is not None, "Please specify the accelerator spec for the pass."
+        assert config is not None, "Please specify the configuration for the pass."
+
+        config_class, default_config = self.get_config_class(accelerator_spec, disable_search)
+
+        self._accelerator_spec = accelerator_spec
         self._config_class = config_class
         self._config = config
         if self._requires_user_script:
             self._user_module_loader = UserModuleLoader(self._config["user_script"], self._config["script_dir"])
         if self._requires_data_config:
             data_config = self._config["data_config"] or {}
             self._data_config = DataConfig(**data_config)
@@ -79,70 +88,78 @@
             if isinstance(v, SearchParameter):
                 self._search_space[k] = v
             else:
                 self._fixed_params[k] = v
 
         # Params that are paths [(param_name, required)]
         self.path_params = []
-        for param, param_config in self._config_class._default_config.items():
+        for param, param_config in default_config.items():
             if param_config.is_path:
                 self.path_params.append((param, param_config.required))
 
         self._initialized = False
 
     @classmethod
     def requires_data_config(cls):
         return cls._requires_data_config
 
     @classmethod
     def generate_search_space(
-        cls, config: Optional[Union[Dict[str, Any], PassConfigBase]] = None, disable_search: Optional[bool] = False
+        cls,
+        accelerator_spec: AcceleratorSpec,
+        config: Optional[Union[Dict[str, Any], PassConfigBase]] = None,
+        disable_search: Optional[bool] = False,
     ) -> Tuple[Type[PassConfigBase], Dict[str, Any]]:
         """
         Generate search space for the pass.
         """
-        default_config = cls.default_config()
+        assert accelerator_spec is not None, "Please specify the accelerator spec for the pass"
+
         # Get the config class with default value or default search value
-        config_class = cls.get_config_class(default_config, disable_search)
+        config_class, default_config = cls.get_config_class(accelerator_spec, disable_search)
         # Generate the search space by using both default value and default search value and user provided config
-        config = cls._resolve_config(config_class, config, default_config)
+        config = validate_config(config, PassConfigBase, config_class)
+
+        config = cls._resolve_config(config, default_config)
         config = cls._init_fixed_and_search_params(config, default_config)
-        return config_class, config
+        return config
 
     @classmethod
     def get_config_class(
-        cls, default_config: Dict[str, PassConfigParam], disable_search: Optional[bool] = False
+        cls, accelerator_spec: AcceleratorSpec, disable_search: Optional[bool] = False
     ) -> Type[PassConfigBase]:
         """
         Get the configuration class for the pass.
         """
-        return create_config_class(cls.__name__, default_config, disable_search, cls._validators())
+        default_config = cls.default_config(accelerator_spec)
+        config_class = create_config_class(cls.__name__, default_config, disable_search, cls._validators())
+        return config_class, default_config
 
     @classmethod
-    def default_config(cls) -> Dict[str, PassConfigParam]:
+    def default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         """
         Get the default configuration for the pass.
         """
         config = {}
         if cls._requires_user_script:
             config.update(get_user_script_config())
         if cls.requires_data_config():
             config.update(get_data_config())
-        return {**config, **cls._default_config()}
+        return {**config, **cls._default_config(accelerator_spec)}
 
     @staticmethod
     def _validators() -> Dict[str, Callable]:
         """
         pydantic validators for config params
         """
         return {}
 
     @staticmethod
     @abstractmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         """
         Get the default configuration for the pass. Doesn't include user_script and script_dir.
 
         Example:
             return {
                 # required parameter
                 "param1": PassConfigParam(type_=int, required=True, description="param1 description"),
@@ -271,23 +288,21 @@
             # if there is only one choice, use that choice
             param = param.get_support()[0]
         return param
 
     @classmethod
     def _resolve_config(
         cls,
-        config_class: Type[PassConfigBase],
         input_config: Union[Dict[str, Any], PassConfigBase],
         default_config: Dict[str, PassConfigParam],
     ) -> Dict[str, Any]:
         """
         Resolve config to PassConfigBase.
         """
-        config = validate_config(input_config, PassConfigBase, config_class)
-        config = config.dict()
+        config = input_config.dict()
         config = cls._resolve_defaults(config, default_config)
         if cls._requires_user_script:
             user_module_loader = UserModuleLoader(config["user_script"], config["script_dir"])
             config = cls._validate_user_script(config, user_module_loader, default_config)
         return config
 
     def _initialize(self):
@@ -355,50 +370,60 @@
             )
         elif isinstance(model, CompositeOnnxModel) and not self._accepts_composite_model:
             components = []
             for cidx, child in enumerate(model.get_model_components()):
                 component_output_path = Path(output_model_path).with_suffix("") / str(cidx)
                 components.append(self._run_for_config(child, config, str(component_output_path)))
             return CompositeOnnxModel(components, model.name, hf_config=model.hf_config)
-
-        return self._run_for_config(model, config, output_model_path)
+        else:
+            return self._run_for_config(model, config, output_model_path)
 
     def serialize_config(self, config: Dict[str, Any], check_objects: bool = False) -> str:
         """
         Serialize the configuration.
         """
         return self._config_class(**config).to_json(check_objects)
 
     def to_json(self, check_objects: bool = False) -> Dict[str, Any]:
         """
         Convert the pass to json.
         """
         return {
             "type": self.__class__.__name__,
             "disable_search": True,
+            "accelerator": self._accelerator_spec.to_json(),
             "config": self.serialize_config(self._config, check_objects),
         }
 
 
 # TODO rename. We are using FullPassConfig since PassConfigBase already refers to inner config
 class FullPassConfig(ConfigBase):
     type: str
     disable_search: bool = False
+    accelerator: Dict[str, str] = None
     config: Dict[str, Any] = None
 
     @validator("type")
     def validate_type(cls, v):
         if v.lower() not in Pass.registry:
             raise ValueError(f"Unknown pass type {v}")
         return v
 
     def create_pass(self):
         pass_cls = Pass.registry[self.type.lower()]
-        return create_pass_from_dict(pass_cls, self.config, self.disable_search)
+        accelerator_spec = AcceleratorSpec(**self.accelerator)
+        return pass_cls(accelerator_spec, self.config, self.disable_search)
 
 
-def create_pass_from_dict(pass_cls: Type[Pass], config: Dict[str, Any] = None, disable_search=False) -> Pass:
+# TODO: deprecate or remove this method by explicitly specify the accelerator_spec in the arguement instead of using
+# the default argument.
+def create_pass_from_dict(
+    pass_cls: Type[Pass], config: Dict[str, Any] = None, disable_search=False, accelerator_spec: AcceleratorSpec = None
+) -> Pass:
     """
     Create a pass from a dictionary.
     """
-    config_class, config = pass_cls.generate_search_space(config, disable_search)
-    return pass_cls(config_class, config)
+    if accelerator_spec is None:
+        accelerator_spec = DEFAULT_CPU_ACCELERATOR
+
+    config = pass_cls.generate_search_space(accelerator_spec, config, disable_search)
+    return pass_cls(accelerator_spec, config, disable_search)
```

## olive/passes/pass_config.py

```diff
@@ -133,11 +133,8 @@
 
         type_ = Optional[Union[type_, SearchParameter, PassParamDefault]]
         if not disable_search and param_config.searchable_values is not None:
             config[param] = (type_, param_config.searchable_values)
         else:
             config[param] = (type_, param_config.default_value)
 
-    class PassConfigBaseDefaultConfig(PassConfigBase):
-        _default_config = default_config  # store default config as a class attribute, not included in json
-
-    return create_model(f"{pass_type}Config", **config, __base__=PassConfigBaseDefaultConfig, __validators__=validators)
+    return create_model(f"{pass_type}Config", **config, __base__=PassConfigBase, __validators__=validators)
```

## olive/passes/onnx/__init__.py

```diff
@@ -5,14 +5,16 @@
 from olive.passes.onnx.append_pre_post_processing_ops import AppendPrePostProcessingOps
 from olive.passes.onnx.conversion import OnnxConversion
 from olive.passes.onnx.float16_conversion import OnnxFloatToFloat16
 from olive.passes.onnx.inc_quantization import IncDynamicQuantization, IncQuantization, IncStaticQuantization
 from olive.passes.onnx.insert_beam_search import InsertBeamSearch
 from olive.passes.onnx.mixed_precision import OrtMixedPrecision
 from olive.passes.onnx.model_optimizer import OnnxModelOptimizer
+from olive.passes.onnx.optimum_conversion import OptimumConversion
+from olive.passes.onnx.optimum_merging import OptimumMerging
 from olive.passes.onnx.perf_tuning import OrtPerfTuning
 from olive.passes.onnx.quantization import OnnxDynamicQuantization, OnnxQuantization, OnnxStaticQuantization
 from olive.passes.onnx.transformer_optimization import OrtTransformersOptimization
 from olive.passes.onnx.vitis_ai_quantization import VitisAIQuantization
 
 __all__ = [
     "AppendPrePostProcessingOps",
@@ -26,8 +28,10 @@
     "OrtPerfTuning",
     "OrtTransformersOptimization",
     "OnnxModelOptimizer",
     "OnnxFloatToFloat16",
     "InsertBeamSearch",
     "OrtMixedPrecision",
     "VitisAIQuantization",
+    "OptimumConversion",
+    "OptimumMerging",
 ]
```

## olive/passes/onnx/append_pre_post_processing_ops.py

```diff
@@ -5,27 +5,28 @@
 import tempfile
 from pathlib import Path
 from typing import Any, Callable, Dict, List
 
 import onnx
 from onnxruntime import __version__ as OrtVersion
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 
 
 class AppendPrePostProcessingOps(Pass):
     """
     Add Pre/Post nodes to the input model
     """
 
     @staticmethod
-    def _default_config() -> Dict[str, Dict[str, Any]]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, Dict[str, Any]]:
         config = {
             "pre": PassConfigParam(
                 type_=List[str],
                 default_value=None,
                 description="List of pre-processing commands to add.",
             ),
             "post": PassConfigParam(
```

## olive/passes/onnx/conversion.py

```diff
@@ -6,14 +6,15 @@
 from pathlib import Path
 from typing import Any, Dict, Union
 
 import onnx
 import torch
 
 from olive.common.utils import tensor_data_to_device
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import CompositeOnnxModel, ONNXModel, PyTorchModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 
 
 class TraceModelWrapper(torch.nn.Module):
@@ -29,15 +30,15 @@
 
 class OnnxConversion(Pass):
     """Convert a PyTorch model to ONNX model using torch.onnx.export."""
 
     _requires_user_script = True
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "target_opset": PassConfigParam(
                 type_=int, default_value=14, description="The version of the default (ai.onnx) opset to target."
             )
         }
         config.update(get_external_data_config())
         return config
```

## olive/passes/onnx/float16_conversion.py

```diff
@@ -2,28 +2,29 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from typing import Any, Dict, List
 
 import onnx
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 
 
 class OnnxFloatToFloat16(Pass):
     """Converts a model to float16.
     It is based on onnxconverter-common.convert_float_to_float16.
     See https://onnxruntime.ai/docs/performance/model-optimizations/float16.html#float16-conversion
     """
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "min_positive_val": PassConfigParam(
                 type_=float, default_value=1e-7, description="Constant values will be clipped against this value"
             ),
             "max_finite_val": PassConfigParam(
                 type_=float, default_value=1e4, description="Constant values will be clipped against this value"
             ),
```

## olive/passes/onnx/inc_quantization.py

```diff
@@ -3,14 +3,15 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 from copy import deepcopy
 from pathlib import Path
 from typing import Any, Callable, Dict, Union
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 from olive.strategy.search_parameter import Boolean, Categorical, Conditional
 
 logger = logging.getLogger(__name__)
@@ -157,15 +158,15 @@
     _requires_user_script = True
     _requires_data_config = True
 
     def _initialize(self):
         super()._initialize()
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "approach": PassConfigParam(
                 type_=str,
                 default_value="static",
                 searchable_values=Categorical(["dynamic", "static"]),
                 description="""
                 Intel Neural Compressor Quantization mode. 'dynamic' for dynamic quantization,
@@ -250,15 +251,15 @@
 
 class IncDynamicQuantization(IncQuantization):
     """Intel Neural Compressor Dynamic Quantization Pass"""
 
     _requires_user_script = False
 
     @staticmethod
-    def _default_config() -> Dict[str, Any]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, Any]:
         config = {
             "approach": PassConfigParam(type_=str, default_value="dynamic", description="dynamic quantization mode")
         }
         # common quantization config
         config.update(deepcopy(_inc_quantization_config))
         # external data config
         config.update(get_external_data_config())
@@ -267,15 +268,15 @@
 
 class IncStaticQuantization(IncQuantization):
     """Intel Neural Compressor Static Quantization Pass"""
 
     _requires_user_script = True
 
     @staticmethod
-    def _default_config() -> Dict[str, Any]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, Any]:
         config = {
             "approach": PassConfigParam(type_=str, default_value="static", description="static quantization mode")
         }
         # common quantization config
         config.update(deepcopy(_inc_quantization_config))
         # static quantization specific config
         config.update(deepcopy(_inc_static_dataloader_config))
```

## olive/passes/onnx/insert_beam_search.py

```diff
@@ -3,27 +3,28 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from typing import Any, Dict
 
 from onnx import ModelProto, TensorProto, helper
 from onnxruntime.transformers.convert_generation import get_shared_initializers
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import CompositeOnnxModel, OliveModel, ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 
 
 class InsertBeamSearch(Pass):
     """Insert Beam Search Op."""
 
     _accepts_composite_model = True
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "no_repeat_ngram_size": PassConfigParam(
                 type_=int,
                 default_value=3,
                 description=" If set to int > 0, all ngrams of that size can only occur once.",
             ),
         }
```

## olive/passes/onnx/mixed_precision.py

```diff
@@ -1,27 +1,28 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 from typing import Any, Dict, List
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 
 logger = logging.getLogger(__name__)
 
 
 class OrtMixedPrecision(Pass):
     """Convert model to mixed precision."""
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "op_block_list": PassConfigParam(
                 type_=List[str],
                 default_value=["SimplifiedLayerNormalization", "SkipSimplifiedLayerNormalization", "Relu", "Add"],
                 description="List of op types to leave as float32",
             ),
         }
```

## olive/passes/onnx/model_optimizer.py

```diff
@@ -3,14 +3,15 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from typing import Any, Dict
 
 import numpy as np
 import onnx
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 
 
 class ModelOptimizer:
@@ -114,15 +115,15 @@
             graph.node.insert(node_index, node)
 
 
 class OnnxModelOptimizer(Pass):
     """Optimize ONNX model by fusing nodes."""
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return get_external_data_config()
 
     def _run_for_config(self, model: ONNXModel, config: Dict[str, Any], output_model_path: str) -> ONNXModel:
         output_model_path = ONNXModel.resolve_path(output_model_path)
 
         # optimize model
         model_optimizer = ModelOptimizer(model.model_path)
```

## olive/passes/onnx/perf_tuning.py

```diff
@@ -4,27 +4,32 @@
 # --------------------------------------------------------------------------
 import copy
 import itertools
 import logging
 from pathlib import Path
 from typing import Any, Callable, Dict, Union
 
-from olive.evaluator.metric import LatencySubType, Metric, MetricType
+from olive.evaluator.metric import LatencySubType, Metric, MetricType, joint_metric_key
 from olive.evaluator.metric_config import get_properties_from_metric_type
+from olive.hardware.accelerator import AcceleratorLookup, AcceleratorSpec
 from olive.model import ONNXModel
 from olive.passes import Pass
 from olive.passes.pass_config import PassConfigParam
 
 logger = logging.getLogger(__name__)
 
 
 def generate_tuning_combos(model, config):
     import onnxruntime as ort
 
-    providers_list = config.providers_list if config.providers_list else model.get_execution_providers(config.device)
+    providers_list = (
+        config.providers_list
+        if config.providers_list
+        else AcceleratorLookup.get_execution_providers_for_device(config.device)
+    )
     execution_mode_list = (
         config.execution_mode_list
         if config.execution_mode_list
         else [ort.ExecutionMode.ORT_SEQUENTIAL.value, ort.ExecutionMode.ORT_PARALLEL.value]
     )
     opt_level_list = config.opt_level_list if config.opt_level_list else [99]
 
@@ -50,16 +55,17 @@
 def tune_onnx_model(model, config):
     latency_user_config = {}
     # which should be the same as the config in the metric
     config_dict = config.dict()
     for eval_config in get_properties_from_metric_type(MetricType.LATENCY):
         if eval_config in config_dict:
             latency_user_config[eval_config] = config_dict.get(eval_config)
+    latency_sub_types = [{"name": LatencySubType.AVG}]
     latency_metric = Metric(
-        name="latency", type=MetricType.LATENCY, sub_type=LatencySubType.AVG, user_config=latency_user_config
+        name="latency", type=MetricType.LATENCY, sub_types=latency_sub_types, user_config=latency_user_config
     )
 
     pretuning_inference_result = get_benchmark(model, latency_metric, config)
 
     tuning_results = []
     for tuning_combo in generate_tuning_combos(model, config):
         tuning_item = ["provider", "execution_mode", "ort_opt_level", "io_bind"]
@@ -207,15 +213,18 @@
         test_result["execution_provider"] = test_params.get("execution_provider")
         test_result["session_options"] = test_params.get("session_options").copy()
         test_result["io_bind"] = io_bind
 
         # add the io_bind back to test_params
         test_params["_io_bind"] = io_bind
     evaluator = OliveEvaluatorFactory.create_evaluator_for_model(model)
-    test_result["latency_ms"] = evaluator.evaluate(model, [latency_metric], config.device)[latency_metric.name]
+    joint_key = joint_metric_key(latency_metric.name, latency_metric.sub_types[0].name)
+    test_result["latency_ms"] = evaluator.evaluate(model, [latency_metric], config.device, config.providers_list)[
+        joint_key
+    ].value
     return test_result
 
 
 def parse_tuning_result(*tuning_results):
     best_result = min(tuning_results, key=lambda x: x["latency_ms"])
     return best_result
 
@@ -228,15 +237,15 @@
 class OrtPerfTuning(Pass):
     """Optimize ONNX Runtime inference settings."""
 
     _requires_user_script = True
     _requires_data_config = True
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "data_dir": PassConfigParam(
                 type_=Union[Path, str], is_path=True, description="Directory of sample inference data."
             ),
             "dataloader_func": PassConfigParam(
                 type_=Union[Callable, str],
                 is_object=True,
@@ -287,12 +296,19 @@
                 type_=Dict[str, Any],
                 default_value=None,
                 description="Extra customized session options during tuning process.",
             ),
         }
 
     def _run_for_config(self, model: ONNXModel, config: Dict[str, Any], output_model_path: str) -> ONNXModel:
+        if "providers_list" not in config:
+            # add the provider to the config if user doesn't provide the execution providers
+            config["providers_list"] = [self._accelerator_spec.execution_provider]
+
+        if "device" not in config:
+            config["device"] = self._accelerator_spec.accelerator_type
+
         config = self._config_class(**config)
         # TODO: decide on whether to ignore the output_model_path
         # if we want to ignore it, we can just return the model
         # otherwise save or symlink the original model to the output_model_path
         return tune_onnx_model(model, config)
```

## olive/passes/onnx/quantization.py

```diff
@@ -7,14 +7,15 @@
 from copy import deepcopy
 from pathlib import Path
 from typing import Any, Callable, Dict, Union
 
 import onnx
 
 from olive.common.utils import hash_string
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ModelStorageKind, ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_file, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 from olive.strategy.search_parameter import Boolean, Categorical, Conditional, ConditionalDefault
 
 logger = logging.getLogger(__name__)
@@ -218,15 +219,15 @@
     _requires_data_config = True
 
     def _initialize(self):
         super()._initialize()
         self.tmp_dir = tempfile.TemporaryDirectory(prefix="olive_tmp")
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "quant_mode": PassConfigParam(
                 type_=str,
                 default_value="static",
                 searchable_values=Categorical(["dynamic", "static"]),
                 description="""
                     Onnx Quantization mode. 'dynamic' for dynamic quantization,
@@ -439,15 +440,15 @@
 
 class OnnxDynamicQuantization(OnnxQuantization):
     """ONNX Dynamic Quantization Pass"""
 
     _requires_user_script = False
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "quant_mode": PassConfigParam(type_=str, default_value="dynamic", description="dynamic quantization mode")
         }
         # common quantization config
         config.update(deepcopy(_onnx_quantization_config))
         # exposed extra options config
         config.update(deepcopy(_exposed_extra_options_config))
@@ -459,15 +460,15 @@
 
 class OnnxStaticQuantization(OnnxQuantization):
     """ONNX Static Quantization Pass"""
 
     _requires_user_script = True
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "quant_mode": PassConfigParam(type_=str, default_value="static", description="static quantization mode")
         }
         # common quantization config
         config.update(deepcopy(_onnx_quantization_config))
         # static quantization specific config
         config.update(deepcopy(_static_dataloader_config))
```

## olive/passes/onnx/transformer_optimization.py

```diff
@@ -1,28 +1,28 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import os
 from copy import deepcopy
 from typing import Any, Dict, List, Union
 
-from packaging import version
-
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 
 
 class OrtTransformersOptimization(Pass):
     """Optimize transformer based models in scenarios where ONNX Runtime does not apply the optimization at load time.
     It is based on onnxruntime.transformers.optimizer."""
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         # TODO: add default search if supported
         from onnxruntime.transformers.fusion_options import FusionOptions
 
         config = {
             "model_type": PassConfigParam(
                 type_=str,
                 required=True,
@@ -64,176 +64,52 @@
                 type_=bool,
                 default_value=True,
                 description="Keep input and output tensors in their original data type",
             ),
             "force_fp32_ops": PassConfigParam(
                 type_=List[str], default_value=None, description="Operators that are forced to run in float32"
             ),
-            "target_provider": PassConfigParam(
-                type_=str,
-                default_value=None,
-                description=(
-                    "Target execution provider. This parameter will be removed when "
-                    "accelerators/targets are visible to passes."
-                ),
-            ),
         }
         config.update(get_external_data_config())
         return config
 
     @staticmethod
     def _set_fusion_options(run_config: Dict[str, Any]):
         from onnxruntime.transformers.fusion_options import FusionOptions
 
         fusion_options = FusionOptions(run_config["model_type"])
         fusion_options.__dict__.update(run_config["optimization_options"])
         run_config["optimization_options"] = fusion_options
 
-    @staticmethod
-    def sd_model_types():
-        """Returns model types in the stable diffusion pipeline recognized by the ORT transformer optimizer"""
-        return ("unet", "vae", "clip")
-
-    @staticmethod
-    def _set_sd_fusion_options(run_config: Dict[str, Any], pass_config: Dict[str, Any]):
-        """Configures fusion options for stable diffusion models"""
-        import onnxruntime as ort
-
-        ort_version = version.parse(ort.__version__)
-        is_ort_1_13_or_older = ort_version < version.parse("1.14.0")
-        # is_ort_1_14 = ort_version >= version.parse("1.14.0") and ort_version < version.parse("1.15.0")
-
-        # default to no specific fusion options in earlier releases of ORT
-        if is_ort_1_13_or_older:
-            return
-
-        is_ort_1_15_0_or_newer = ort_version >= version.parse("1.15.0")
-        is_ort_1_15_1_or_newer = ort_version >= version.parse("1.15.1")
-
-        input_model_type = run_config["model_type"]
-        if not is_ort_1_15_0_or_newer and input_model_type != "unet":
-            # 'vae' and 'clip' are only recognized in ORT v1.15+. earlier versions of ORT stable diffusion
-            # optimization simply use "unet" for these model types.
-            run_config["model_type"] = "unet"
-
-        from onnxruntime.transformers.fusion_options import FusionOptions
-
-        fusion_options = FusionOptions(run_config["model_type"])
-
-        # TODO: remove dml_future when ORT 1.15.1 with future version of DML is released
-        # This "provider" value is simply a way to test in-development changes without having
-        # to bump the ORT version.
-        dml_future = pass_config["target_provider"] == "directml_future"
-
-        if pass_config["target_provider"] == "directml" or dml_future:
-            # Some of these fusions are disabled because they provide no performance advantage,
-            # and it's preferable to limit ops outside the ONNX domain.
-            fusion_options.enable_gelu = is_ort_1_15_0_or_newer
-            fusion_options.enable_layer_norm = is_ort_1_15_0_or_newer
-            fusion_options.enable_attention = is_ort_1_15_1_or_newer or dml_future
-            fusion_options.use_multi_head_attention = is_ort_1_15_1_or_newer or dml_future
-            fusion_options.enable_skip_layer_norm = False
-            fusion_options.enable_embed_layer_norm = is_ort_1_15_0_or_newer
-            fusion_options.enable_bias_skip_layer_norm = False
-            fusion_options.enable_bias_gelu = is_ort_1_15_0_or_newer
-            fusion_options.enable_gelu_approximation = False
-            fusion_options.enable_qordered_matmul = False
-            fusion_options.enable_shape_inference = is_ort_1_15_0_or_newer
-            fusion_options.enable_gemm_fast_gelu = False
-            fusion_options.enable_nhwc_conv = False
-            fusion_options.enable_group_norm = is_ort_1_15_1_or_newer or dml_future
-            fusion_options.enable_bias_splitgelu = False
-            fusion_options.enable_packed_qkv = pass_config["float16"] and is_ort_1_15_0_or_newer
-            fusion_options.enable_packed_kv = pass_config["float16"] and is_ort_1_15_0_or_newer
-            fusion_options.enable_bias_add = False
-        else:
-            if input_model_type == "unet":
-                fusion_options.enable_packed_kv = pass_config["float16"]
-                fusion_options.enable_packed_qkv = pass_config["float16"] and is_ort_1_15_0_or_newer
-                fusion_options.enable_bias_add = is_ort_1_15_0_or_newer
-
-        run_config["optimization_options"] = fusion_options
-
-    @staticmethod
-    def _get_op_block_list(config: Dict[str, Any]):
-        import onnxruntime as ort
-
-        op_block_list = []
-
-        if config["float16"]:
-            if config["model_type"] in OrtTransformersOptimization.sd_model_types():
-                if version.parse(ort.__version__) < version.parse("1.15.0"):
-                    op_block_list += ["RandomNormalLike", "Resize", "GroupNorm"]
-                else:
-                    op_block_list += ["RandomNormalLike"]
-            if config["force_fp32_ops"]:
-                op_block_list += config["force_fp32_ops"]
-
-        return op_block_list if len(op_block_list) > 0 else None
-
-    @staticmethod
-    def _run_without_optimization(model: ONNXModel, config: Dict[str, Any], output_model_path: str) -> ONNXModel:
-        """Fallback/pass-through path that skips onnxruntime.transformers.optimizer and simply copies the model
-        or converts it to FP16 without performing transformer-specific optimizations. This fallback exists so
-        that a pass configuration can be used with multiple ORT versions, some of which may not support optimization.
-        """
-
-        if not config["float16"]:
-            return model
-
-        import onnx
-        from onnxconverter_common import float16
-
-        # stable diffusion unet is too large for the converter to handle with shape inference
-        disable_shape_infer = config["model_type"] == "unet"
-
-        op_block_list = OrtTransformersOptimization._get_op_block_list(config)
-
-        model_fp32 = onnx.load(str(model.model_path))
-        model_fp16 = float16.convert_float_to_float16(
-            model_fp32,
-            keep_io_types=config["keep_io_types"],
-            op_block_list=op_block_list,
-            disable_shape_infer=disable_shape_infer,
-        )
-        # save the model to the output path and return the model
-        return model_proto_to_olive_model(model_fp16, output_model_path, config, model.name)
-
     def _run_for_config(self, model: ONNXModel, config: Dict[str, Any], output_model_path: str) -> ONNXModel:
-        import onnxruntime as ort
         from onnxruntime.transformers import optimizer as transformers_optimizer
 
         # start with a copy of the config
         run_config = deepcopy(config)
         del (
             run_config["float16"],
             run_config["input_int32"],
             run_config["keep_io_types"],
             run_config["force_fp32_ops"],
-            run_config["target_provider"],
         )
         for key in get_external_data_config():
             del run_config[key]
 
-        output_model_path = ONNXModel.resolve_path(output_model_path)
+        output_model_path = ONNXModel.resolve_path(os.path.join(output_model_path, os.path.basename(model.model_path)))
 
-        if config["model_type"] in OrtTransformersOptimization.sd_model_types():
-            # stable diffusion optimization only applies to the CUDA EP in ORT 1.14 and earlier.
-            if config["target_provider"] != "cuda" and version.parse(ort.__version__) < version.parse("1.15.0"):
-                return self._run_without_optimization(model, config, output_model_path)
-
-            if config["optimization_options"] is None:
-                self._set_sd_fusion_options(run_config, config)
-        elif config["optimization_options"]:
+        if config["optimization_options"]:
             self._set_fusion_options(run_config)
 
         optimizer = transformers_optimizer.optimize_model(input=model.model_path, **run_config)
 
         if config["float16"]:
-            op_block_list = self._get_op_block_list(config)
+            op_block_list = config["force_fp32_ops"]
             optimizer.convert_float_to_float16(keep_io_types=config["keep_io_types"], op_block_list=op_block_list)
 
         if config["input_int32"]:
             optimizer.change_graph_inputs_to_int32()
 
+        # Topologically sort the graph at the end since previous optimizations may have broken it
+        optimizer.topological_sort()
+
         # save the model to the output path and return the model
         return model_proto_to_olive_model(optimizer.model, output_model_path, config, model.name)
```

## olive/passes/onnx/vitis_ai_quantization.py

```diff
@@ -8,14 +8,15 @@
 from pathlib import Path
 from shutil import copyfile
 from typing import Any, Callable, Dict, Union
 
 from onnxruntime.quantization.preprocess import quant_pre_process
 from onnxruntime.quantization.quant_utils import QuantFormat, QuantType
 
+from olive.hardware import AcceleratorSpec
 from olive.model import ONNXModel
 from olive.passes import Pass
 from olive.passes.onnx.vitis_ai import quantize_static
 from olive.passes.onnx.vitis_ai.quant_utils import PowerOfTwoMethod
 from olive.passes.pass_config import PassConfigParam
 from olive.strategy.search_parameter import Boolean, Categorical, Conditional
 
@@ -187,15 +188,15 @@
     _requires_user_script = True
 
     def _initialize(self):
         super()._initialize()
         self.tmp_dir = tempfile.TemporaryDirectory()
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "quant_mode": PassConfigParam(
                 type_=str,
                 default_value="static",
                 searchable_values=Categorical(["static"]),
                 description="""
                     Onnx Quantization mode. ,
```

## olive/passes/onnx/vitis_ai/qdq_quantizer.py

```diff
@@ -27,14 +27,16 @@
     find_by_name,
 )
 from onnxruntime.quantization.registry import CreateQDQQuantizer
 
 from olive.passes.onnx.vitis_ai.quant_utils import vitis_quantize_data
 from olive.passes.onnx.vitis_ai.refine import adjust_quantize_info
 
+logger = logging.getLogger(__name__)
+
 
 class VitisQuantizer(QDQQuantizer):
     def __init__(
         self,
         model,
         per_channel,
         reduce_range,
@@ -64,34 +66,33 @@
             op_types_to_quantize,
             extra_options,
         )
         self.tensors_to_quantize = {}
         self.calibrate_method = calibrate_method
 
         if per_channel:
-            logging.error("per_channel is not supported in PowerOfTwoMethod calibrate_method.")
+            logger.error("per_channel is not supported in PowerOfTwoMethod calibrate_method.")
 
         # In PowerOfTwoMethod calibrate_method, QDQ should always appear as a pair.
         # Therefore, we need to add qdq pair to weight.
         if "AddQDQPairToWeight" in self.extra_options and not self.extra_options["AddQDQPairToWeight"]:
-            logging.error("AddQDQPairToWeight should be True in PowerOfTwoMethod calibrate_method.")
+            logger.error("AddQDQPairToWeight should be True in PowerOfTwoMethod calibrate_method.")
         self.add_qdq_pair_to_weight = True
 
         # In PowerOfTwoMethod calibrate_method, QDQ should always set WeightSymmetric as True.
         if "WeightSymmetric" in self.extra_options and not self.extra_options["WeightSymmetric"]:
-            logging.error("WeightSymmetric should be True in PowerOfTwoMethod calibrate_method.")
+            logger.error("WeightSymmetric should be True in PowerOfTwoMethod calibrate_method.")
         self.is_weight_symmetric = True
 
         # In PowerOfTwoMethod calibrate_method, QDQ should always always set ActivationSymmetric as True.
         if "ActivationSymmetric" in self.extra_options and not self.extra_options["ActivationSymmetric"]:
-            logging.error("ActivationSymmetric should be True in PowerOfTwoMethod calibrate_method.")
+            logger.error("ActivationSymmetric should be True in PowerOfTwoMethod calibrate_method.")
         self.is_activation_symmetric = True
 
     def vitis_quantize_initializer(self, weight, bit_width=8, keep_float_weight=False):
-
         # Find if this input is already quantized
         if weight.name in self.quantized_value_map:
             quantized_value = self.quantized_value_map[weight.name]
             return (
                 quantized_value.q_name,
                 quantized_value.zp_name,
                 quantized_value.scale_name,
@@ -120,15 +121,14 @@
             None,
         )
         self.quantized_value_map[weight.name] = quantized_value
 
         return q_weight_name, zp_name, scale_name
 
     def quantize_model(self):
-
         self.tensor_info = {}
 
         for node in self.model.nodes():
             if self.should_quantize_node(node):
                 op_quantizer = CreateQDQQuantizer(self, node)
                 op_quantizer.quantize()
 
@@ -188,15 +188,15 @@
     def quantize_bias_tensor(self, bias_name, input_name, weight_name, beta=1.0):
         weight = find_by_name(bias_name, self.model.initializer())
         if weight is not None:
             if weight.data_type == onnx_proto.TensorProto.FLOAT:
                 # Use int8 quantization for bias as well as weights.
                 self.tensors_to_quantize[bias_name] = QDQTensorQuantInfo()
         else:
-            logging.warning("Expected {} to be a weight".format(bias_name))
+            logger.warning("Expected {} to be a weight".format(bias_name))
 
     def _quantize_refine(self):
         self.model = adjust_quantize_info(
             self.model,
             adjust_vitis_sigmoid=True,
             adjust_shift_cut=True,
             adjust_shift_bias=True,
```

## olive/passes/onnx/vitis_ai/refine.py

```diff
@@ -4,17 +4,18 @@
 #
 import logging
 
 import numpy as np
 
 from olive.passes.onnx.vitis_ai.quant_utils import pos2scale, scale2pos
 
+logger = logging.getLogger(__name__)
+
 refine_op_type = ["DequantizeLinear", "QuantizeLinear"]
 postfix = "_Output"
-logging.basicConfig(level=logging.INFO)
 
 
 class QuantPosManager(object):
     def __init__(self, model):
         self.model = model
 
     def get_scale(self, node):
@@ -139,31 +140,30 @@
             wpos_name = self.get_wpos_name(node)
             wpos, wpos_node = self.get_pos_by_name(wpos_name)
 
             # Adjust shift_cut
             min_sc = 0
             max_sc = 16
             if wpos is None or ipos is None or opos is None:
-                logging.warning(
+                logger.warning(
                     "Found a pos that is None. Shift cut of layer {} has not taken effect.".format(node.name)
                 )
                 continue
             sc = wpos + ipos - opos
             new_sc = None
             if sc < min_sc:
                 new_sc = min_sc
             elif sc > max_sc:
                 new_sc = max_sc
 
             if new_sc is not None:
                 new_wpos = new_sc + opos - ipos
                 self.set_pos(wpos_node, new_wpos)
-                logging.info(
-                    "Shift cut of layer {} is {}. It exceeds range [{}, {}]. "
-                    "Modify wpos from {} to {}.".format(
+                logger.info(
+                    "Shift cut of layer {} is {}. It exceeds range [{}, {}]. Modify wpos from {} to {}.".format(
                         node.input[1], int(sc), int(min_sc), int(max_sc), int(wpos), int(new_wpos)
                     )
                 )
 
     def adjust_shift_bias(self):
         """Adjust the shift bias of node.
 
@@ -183,15 +183,15 @@
             wpos_name = self.get_wpos_name(node)
             wpos, wpos_node = self.get_pos_by_name(wpos_name)
             bpos_name = self.get_bpos_name(node)
             if bpos_name:
                 bpos, bpos_node = self.get_pos_by_name(bpos_name)
                 # Adjust shift_bias
                 if wpos is None or ipos is None or opos is None or bpos is None:
-                    logging.warning(
+                    logger.warning(
                         "Found a pos that is None. Shift bias of layer {} has not taken effect.".format(node.name)
                     )
                     continue
                 shift_cut = wpos + ipos - opos
                 min_sb = min(0, -(24 - (8 + shift_cut)))
                 max_sb = 16
                 shift_bias = wpos + ipos - bpos
@@ -201,93 +201,92 @@
                     new_sb = min_sb
                 elif shift_bias > max_sb:
                     new_sb = max_sb
 
                 if new_sb is not None:
                     new_bpos = wpos + ipos - new_sb
                     self.set_pos(self.get_node_by_name(node.input[2].strip(postfix)), new_bpos)
-                    logging.info(
-                        "Shift bias of layer {} is {}. It exceeds range [{}, {}]. "
-                        "Modify bpos from {} to {}.".format(
+                    logger.info(
+                        "Shift bias of layer {} is {}. It exceeds range [{}, {}]. Modify bpos from {} to {}.".format(
                             node.input[2], int(shift_bias), int(min_sb), int(max_sb), int(bpos), int(new_bpos)
                         )
                     )
 
     def adjust_vitis_sigmoid(self):
         """Adjust quantize info of VitisSigmoid nodes.
 
         DPU compiler constraints for VitisSigmoid:
         1. input pos of VitisSigmoid >= 0 && <= 15
         2. output pos of VitisSigmoid >= 7
         3. shift_sigmoid >= 0 && shift_sigmoid <= 31 where
             shift_sigmoid = 14 + 'input pos' - ' output pos'
         """
         for i, node in enumerate(self.model.model.graph.node):
-
             if node.op_type not in ["Sigmoid"]:
                 continue
             ipos_name = self.get_ipos_name(node)
             ipos, _ = self.get_pos_by_name(ipos_name)
 
             opos_name = self.get_opos_name(node)
             opos, _ = self.get_pos_by_name(opos_name)
 
             if ipos is None or opos is None:
-                logging.warning(
+                logger.warning(
                     "Found a pos that is None. Adjust quantize info of VitisSigmoid "
                     "nodes of layer {} has not taken effect.".format(node.name)
                 )
                 continue
 
             new_ipos = ipos if ipos > 0 else 0
             new_ipos = new_ipos if new_ipos <= 15 else 15
 
             new_opos = opos if opos > 7 else 7
             shift_sigmoid = 14 + new_ipos - new_opos  # will not bigger than 31 now
             new_opos = new_opos if shift_sigmoid > 0 else 14 + new_ipos
 
             if new_ipos != ipos:
                 self.set_pos(self.get_node_by_name(node.inpus[0].strip(postfix)), new_ipos)
-            logging.info(
-                "Input quantize pos of VitisSimoid layer {} is {}, modify it to {} "
-                "to meet the DPU constraints.".format(node.input[0], int(ipos), int(new_ipos))
+            logger.info(
+                "Input quantize pos of VitisSimoid layer {} is {}, modify it to {} to meet the DPU constraints.".format(
+                    node.input[0], int(ipos), int(new_ipos)
+                )
             )
 
             if new_opos != opos:
                 self.set_pos(self.get_node_by_name(self.find_o_name(node.output[0])), new_opos)
-            logging.info(
+            logger.info(
                 "Output quantize pos of VitisSimoid layer {} is {}, modify it to {} "
                 "to meet the DPU constraints.".format(node.output[0], int(opos), int(new_opos))
             )
 
     def adjust_shift_read(self):
         """Adjust the shift bias of node.
 
         shift_read = max(ipos) - min(ipos)
 
         DPU compiler constraints of shift_bias:
         1. 0 <= shift_read <= 15
         """
         for i, node in enumerate(self.model.model.graph.node):
-
             if node.op_type not in ["Add"] or node.op_type not in ["Mul"]:
                 continue
             ipos_layers = []
             iposes = []
             skip = False
 
             for i in len(node.input):
                 ipos_name = self.get_ipos_name_by_id(node, i)
                 ipos_layers.append(ipos_name)
             for i in ipos_layers:
                 ipos, _ = self.get_pos_by_name(i)
                 if ipos is None:
-                    logging.info(
-                        "Fail to get quantize position for layer {}, "
-                        "skip adjust_shift_read for it.".format(ipos_layers[i])
+                    logger.info(
+                        "Fail to get quantize position for layer {}, skip adjust_shift_read for it.".format(
+                            ipos_layers[i]
+                        )
                     )
                     skip = True
                 iposes.append(ipos)
 
             if skip:
                 continue
 
@@ -299,17 +298,16 @@
             new_sr = None
             if sr > max_sr:
                 new_sr = max_sr
 
             if new_sr is not None:
                 new_ipos_max = iposes[id_min] + new_sr
                 self.set_pos(self.get_node_by_name(ipos_layers[id_max].strip(postfix)), new_ipos_max)
-                logging.info(
-                    "Shift read of layer {} is {}({}-{}). It exceeds range [{}, {}]. "
-                    "Modify ipos from {} to {}.".format(
+                logger.info(
+                    "Shift read of layer {} is {}({}-{}). It exceeds range [{}, {}]. Modify ipos from {} to {}.".format(
                         node.name,
                         int(sr),
                         int(iposes[id_max]),
                         int(iposes[id_min]),
                         int(min_sr),
                         int(max_sr),
                         int(iposes[id_max]),
@@ -322,39 +320,39 @@
 
         shift_write = min(ipos) - opos
 
         DPU compiler constraints of shift_write:
         1. -15 <= shift_write <= 15
         """
         for i, node in enumerate(self.model.model.graph.node):
-
             if node.op_type not in ["Add"] or node.op_type not in ["Mul"]:
                 continue
             ipos_layers = []
             iposes = []
             skip = False
 
             for i in len(node.input):
                 ipos_name = self.get_ipos_name_by_id(node, i)
                 ipos_layers.append(ipos_name)
             for i in ipos_layers:
                 ipos, _ = self.get_pos_by_name(i)
                 if ipos is None:
-                    logging.info(
+                    logger.info(
                         "Fail to get quantize position for layer {}(input:{}) (output of layer {}), "
                         "skip adjust_shift_read for it.".format(ipos_layers[i], i, ipos_layers[i])
                     )
                     skip = True
                 iposes.append(ipos)
             opos_name = self.get_opos_name(node)
             opos, _ = self.get_pos_by_name(opos_name)
             if opos is None:
-                logging.info(
-                    "Fail to get quantize position for layer {}(output:0), "
-                    "skip adjust_shift_write for it.".format(node.name)
+                logger.info(
+                    "Fail to get quantize position for layer {}(output:0), skip adjust_shift_write for it.".format(
+                        node.name
+                    )
                 )
             skip = True
             if skip:
                 continue
 
             id_min = np.argmin(iposes)
             sw = iposes[id_min] - opos
@@ -365,15 +363,15 @@
                 new_sw = max_sw
             elif sw < min_sw:
                 new_sw = min_sw
 
             if new_sw is not None:
                 new_opos = iposes[id_min] - new_sw
                 self.set_pos(self.get_node_by_name(self.find_o_name(node.output[0])), new_opos)
-                logging.info(
+                logger.info(
                     "Shift write of layer {} is {}({}-{}). It exceeds range [{}, {}]. "
                     "Modify opos from {} to {}.".format(
                         node.name,
                         int(sw),
                         int(iposes[id_min]),
                         int(opos),
                         int(min_sw),
@@ -399,27 +397,27 @@
                 ipos_layers.append(ipos_name)
             for name in ipos_layers:
                 ipos, _ = self.get_pos_by_name(name)
                 if ipos is not None:
                     min_pos = min(ipos, min_pos)
             if opos != min_pos:
                 self.set_pos(self.get_node_by_name(self.find_o_name(node.output[0])), min_pos)
-                logging.info(
-                    (
-                        "Output pos of concat node {} is {}, min_pos is {}. "
-                        "Modify opos from {} to {}.".format(node.name, int(opos), int(min_pos), int(opos), int(min_pos))
+                logger.info(
+                    "Output pos of concat node {} is {}, min_pos is {}. Modify opos from {} to {}.".format(
+                        node.name, int(opos), int(min_pos), int(opos), int(min_pos)
                     )
                 )
             for name in ipos_layers:
                 ipos, ipos_node = self.get_pos_by_name(name)
                 if ipos is not None and ipos != min_pos:
                     self.set_pos(ipos_node, min_pos)
-                    logging.info(
-                        "Input pos of concat node {} is {}, min_pos is {}. "
-                        "Modify ipos from {} to {}.".format(node.name, int(ipos), int(min_pos), int(ipos), int(min_pos))
+                    logger.info(
+                        "Input pos of concat node {} is {}, min_pos is {}. Modify ipos from {} to {}.".format(
+                            node.name, int(ipos), int(min_pos), int(ipos), int(min_pos)
+                        )
                     )
 
     def align_pool(self):
         """Align max/avg pooling input and output pos."""
         for i, node in enumerate(self.model.model.graph.node):
             if (
                 node.op_type not in ["MaxPool"]
@@ -430,23 +428,23 @@
             ipos_name = self.get_ipos_name(node)
             ipos, ipos_layer = self.get_pos_by_name(ipos_name)
 
             opos_name = self.get_opos_name(node)
             opos, opos_layer = self.get_pos_by_name(opos_name)
             if ipos is not None and opos is not None and opos > ipos:
                 self.set_pos(opos_layer, ipos)
-                logging.info(
+                logger.info(
                     "Input pos of pooling layer {} is {}. Output pos of pooling layer {} is {}."
                     "Modify opos from {} to {}.".format(
                         node.name, int(ipos), node.name, int(opos), int(opos), int(ipos)
                     )
                 )
             elif ipos is not None and opos is not None and opos < ipos:
                 self.set_pos(ipos_layer, opos)
-                logging.info(
+                logger.info(
                     "Input pos of pooling layer {} is {}. Output pos of pooling layer {} is {}."
                     "Modify ipos from {} to {}.".format(
                         node.name, int(ipos), node.name, int(opos), int(ipos), int(opos)
                     )
                 )
 
     def check_scale(self):
```

## olive/passes/openvino/conversion.py

```diff
@@ -2,26 +2,27 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from pathlib import Path
 from typing import Any, Dict, List, Tuple, Union
 
 from olive.constants import Framework
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ModelStorageKind, ONNXModel, OpenVINOModel, PyTorchModel
 from olive.passes import Pass
 from olive.passes.pass_config import PassConfigParam
 
 
 class OpenVINOConversion(Pass):
     """
     Converts PyTorch, ONNX or TensorFlow Model to OpenVino Model.
     """
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "input": PassConfigParam(
                 type_=List[Tuple],
                 required=False,
                 description=(
                     "Input can be set by passing a list of tuples. "
                     "Each tuple should contain input name and optionally input type or input shape."
```

## olive/passes/openvino/quantization.py

```diff
@@ -2,29 +2,30 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from pathlib import Path
 from typing import Any, Callable, Dict, List, Union
 
 from olive.common.user_module_loader import UserModuleLoader
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import OpenVINOModel
 from olive.passes import Pass
 from olive.passes.pass_config import PassConfigParam
 
 
 class OpenVINOQuantization(Pass):
     """
     Post-training quantization for OpenVINO model.
     Please refer to https://docs.openvino.ai/latest/pot_introduction.html for more details.
     """
 
     _requires_user_script = True
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "engine_config": PassConfigParam(
                 type_=Dict,
                 required=True,
                 description=(
                     "Specific config for openvino.tools.pot.IEEngine. 'engine_config' can be set"
                     " by passing a dictonary, for example engine_config: {'device': 'CPU'}"
```

## olive/passes/pytorch/pytorch_lightning_utils.py

```diff
@@ -14,21 +14,23 @@
     logger,
     callbacks=None,
     max_epochs=None,
     max_steps=None,
     val_check_interval=None,
     log_every_n_steps=50,
     precision=32,
+    default_root_dir=None,
     **kwargs,
 ):
     trainer = pl.Trainer(
         logger=logger,
         callbacks=callbacks,
         max_epochs=max_epochs,
         max_steps=max_steps,
         val_check_interval=val_check_interval,
         log_every_n_steps=log_every_n_steps,
         precision=precision,
+        default_root_dir=default_root_dir,
         **kwargs,
     )
 
     return trainer
```

## olive/passes/pytorch/qat_utils.py

```diff
@@ -84,15 +84,19 @@
                 kwargs["devices"] = num_gpus
                 if version.parse(pytorch_lightning.__version__) >= version.parse("1.9.0"):
                     kwargs["use_distributed_sampler"] = False
                 else:
                     kwargs["replace_sampler_ddp"] = False
 
             trainer = create_trainer(
-                max_epochs=self.config.num_epochs, max_steps=self.config.num_steps, logger=self.config.logger, **kwargs
+                max_epochs=self.config.num_epochs,
+                max_steps=self.config.num_steps,
+                logger=self.config.logger,
+                default_root_dir=self.config.checkpoint_path,
+                **kwargs
             )
 
             trainer.fit(ptl_module, datamodule=ptl_data_module)
 
             if self.config.do_validate:
                 trainer.validate(ptl_module, datamodule=ptl_data_module)
             quantized_model = copy.deepcopy(ptl_module.model)
```

## olive/passes/pytorch/quantization_aware_training.py

```diff
@@ -1,26 +1,27 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from pathlib import Path
 from typing import Any, Callable, Dict, Iterable, List, Union
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import PyTorchModel
 from olive.passes import Pass
 from olive.passes.olive_pass import PassConfigParam
 
 
 class QuantizationAwareTraining(Pass):
     """Run quantization aware training on PyTorch model."""
 
     _requires_user_script = True
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         import pytorch_lightning
         from packaging import version
 
         if version.parse(pytorch_lightning.__version__) >= version.parse("1.9.0"):
             from pytorch_lightning.loggers import Logger
         else:
             from pytorch_lightning.loggers import LightningLoggerBase as Logger
@@ -82,14 +83,15 @@
                 required=False,
                 default_value=False,
                 is_object=True,
                 description="Logger for training.",
             ),
             "gpus": PassConfigParam(type_=int, description="Number of GPUs to use."),
             "seed": PassConfigParam(type_=int, default_value=None, description="Random seed for training."),
+            "checkpoint_path": PassConfigParam(type_=str, default_value=None, description="Path to save checkpoints."),
         }
 
     def _run_for_config(self, model: PyTorchModel, config: Dict[str, Any], output_model_path: str) -> PyTorchModel:
         from olive.passes.pytorch.qat_utils import QatTrainer
 
         qat_trainer_config = self._config_class(**config)
         if Path(output_model_path).suffix != ".pt":
```

## olive/passes/snpe/conversion.py

```diff
@@ -3,14 +3,15 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from pathlib import Path
 from typing import Any, Callable, Dict, List, Union
 
 from pydantic import validator
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModel, SNPEModel, TensorFlowModel
 from olive.passes.olive_pass import Pass
 from olive.passes.pass_config import PassConfigParam
 from olive.snpe.constants import InputLayout, InputType
 from olive.snpe.tools.dev import get_dlc_io_config, to_dlc
 
 
@@ -40,15 +41,15 @@
 class SNPEConversion(Pass):
     """
     Convert ONNX or TensorFlow model to SNPE DLC.
     Uses snpe-tensorflow-to-dlc or snpe-onnx-to-dlc tools from the SNPE SDK.
     """
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "input_names": PassConfigParam(type_=List[str], required=True, description="List of input names."),
             "input_shapes": PassConfigParam(
                 type_=List[List[int]],
                 required=True,
                 description="List of input shapes. Must be the same length as input_names.",
             ),
```

## olive/passes/snpe/quantization.py

```diff
@@ -1,14 +1,15 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from pathlib import Path
 from typing import Any, Callable, Dict, List, Union
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import SNPEModel
 from olive.passes.olive_pass import Pass
 from olive.passes.pass_config import PassConfigParam
 from olive.snpe import SNPEDataLoader
 from olive.snpe.tools.dev import quantize_dlc
 from olive.strategy.search_parameter import Boolean
 
@@ -18,15 +19,15 @@
     Quantize SNPE model.
     Uses snpe-dlc-quantize tool from the SNPE SDK.
     """
 
     _requires_user_script = True
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "data_dir": PassConfigParam(
                 type_=str, required=True, is_path=True, description="Path to the data directory."
             ),
             "dataloader_func": PassConfigParam(
                 type_=Union[Callable[[str], SNPEDataLoader], str],
                 required=True,
```

## olive/passes/snpe/snpe_to_onnx.py

```diff
@@ -2,14 +2,15 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from typing import Any, Callable, Dict
 
 from pydantic import validator
 
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModel, SNPEModel
 from olive.passes.olive_pass import Pass
 from olive.passes.onnx.common import get_external_data_config, model_proto_to_olive_model
 from olive.passes.pass_config import PassConfigParam
 from olive.snpe import SNPEDevice
 from olive.snpe.tools.dev import dlc_to_onnx
 
@@ -24,15 +25,15 @@
 class SNPEtoONNXConversion(Pass):
     """
     Convert a SNPE DLC to ONNX to use with SNPE Execution Provider.
     Creates a ONNX graph with the SNPE DLC as a node.
     """
 
     @staticmethod
-    def _default_config() -> Dict[str, PassConfigParam]:
+    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "target_device": PassConfigParam(
                 type_=str,
                 default_value="cpu",
                 description="Target device for the ONNX model. Refer to olive.snpe.SNPEDevice for valid values.",
             ),
             "target_opset": PassConfigParam(type_=int, default_value=12, description="Target ONNX opset version."),
```

## olive/strategy/search_results.py

```diff
@@ -1,17 +1,18 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from copy import deepcopy
-from typing import Any, Dict, List, Tuple, Union
+from typing import Any, Dict, List, Tuple
 
 import numpy as np
 
 from olive.common.utils import hash_dict
+from olive.evaluator.metric import MetricResult
 
 
 class SearchResults:
     """
     This class stores the results of a search.
     """
 
@@ -38,35 +39,33 @@
         self.init_model_history = init_model_history
 
         # search results state
         self.search_point_hash_table = {}
         self.results = {}
         self.model_ids = {}
 
-    def record(
-        self, search_point: Dict[str, Dict[str, Any]], result: Dict[str, Union[float, int]], model_ids: List[str]
-    ):
+    def record(self, search_point: Dict[str, Dict[str, Any]], result: MetricResult, model_ids: List[str]):
         """
         Report the result of a configuration.
         """
         search_point_hash = hash_dict(search_point)
         self.search_point_hash_table[search_point_hash] = deepcopy(search_point)
         self.results[search_point_hash] = deepcopy(result)
         self.model_ids[search_point_hash] = model_ids
 
-    def check_goals(self, result: Dict[str, Union[float, int]]) -> bool:
+    def check_goals(self, result: MetricResult) -> bool:
         """
         Check if the result satisfies the constraints.
         """
         # if goals are not set, return True always
         if self.goals == {}:
             return True
 
         for obj, goal in self.goals.items():
-            if self.obj_mul[obj] * result[obj] < self.obj_mul[obj] * goal:
+            if self.obj_mul[obj] * result[obj].value < self.obj_mul[obj] * goal:
                 return False
         return True
 
     def sort_search_points(self, objectives: List[str] = None, apply_goals: bool = False) -> List[str]:
         """
         Return the search points sorted by the objectives.
         """
@@ -108,20 +107,20 @@
         else:
             assert set(objectives).issubset(self.objectives)
 
         search_point_hashes = []
         results = []
         for search_point_hash in self.results:
             result = self.results[search_point_hash]
-            if result == {}:
+            if not result:
                 continue
             if apply_goals and not self.check_goals(result):
                 continue
             search_point_hashes.append(search_point_hash)
-            results.append([self.obj_mul[obj] * result[obj] for obj in objectives])
+            results.append([self.obj_mul[obj] * result[obj].value for obj in objectives])
 
         return results, search_point_hashes
 
     def to_json(self):
         """
         Return a json representation of the search results.
         """
```

## olive/strategy/search_strategy.py

```diff
@@ -6,14 +6,15 @@
 from abc import ABC
 from copy import deepcopy
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 from pydantic import validator
 
 from olive.common.config_utils import ConfigBase, validate_config
+from olive.evaluator.metric import MetricResult
 from olive.strategy.search_algorithm import REGISTRY, SearchAlgorithm
 from olive.strategy.search_parameter import SearchParameter
 from olive.strategy.search_results import SearchResults
 
 logger = logging.getLogger(__name__)
 
 _VALID_EXECUTION_ORDERS = ["joint", "pass-by-pass"]
@@ -135,15 +136,19 @@
                 )
                 sorted_model_ids, sorted_search_points, sorted_results = self._search_results[
                     tuple(self._active_spaces_group)
                 ].sort_search_points(apply_goals=False)
             # TODO: this is a hack to get the best search point for the current search space group
             #      it totally work for joint execution order, but not for pass-by-pass
             if sorted_search_points and sorted_results:
-                best_search_point = (sorted_search_points[0], list(sorted_results[0].values()), sorted_model_ids[0])
+                best_search_point = (
+                    sorted_search_points[0],
+                    list(sorted_results[0].values()),
+                    sorted_model_ids[0],
+                )
                 self._best_search_points[tuple(self._active_spaces_group)] = best_search_point
                 init_model_id = best_search_point[2][-1]
 
         if len(self._spaces_groups) == 0:
             self._active_spaces_group = None
             return None
 
@@ -203,15 +208,15 @@
             "model_id": self._init_model_ids[tuple(self._active_spaces_group)],
             "passes": [(space_name, search_point[space_name]) for space_name in self._active_spaces_group],
         }
 
     def record_feedback_signal(
         self,
         search_point: Dict[str, Dict[str, Any]],
-        signal: Dict[str, float],
+        signal: MetricResult,
         model_ids: List[str],
         should_prune: bool = False,
     ):
         """
         Record the feedback signal for the given search point.
         """
         if not self._initialized:
```

## olive/strategy/search_algorithm/optuna_sampler.py

```diff
@@ -1,18 +1,19 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from abc import abstractmethod
-from typing import Any, Dict, Tuple, Union
+from typing import Any, Dict, Tuple
 
 import optuna
 
 from olive.common.config_utils import ConfigParam
 from olive.common.utils import hash_dict
+from olive.evaluator.metric import MetricResult
 from olive.strategy.search_algorithm.search_algorithm import SearchAlgorithm
 from olive.strategy.search_parameter import Categorical, Conditional, SpecialParamValue
 
 optuna.logging.set_verbosity(optuna.logging.WARNING)
 
 
 class OptunaSearchAlgorithm(SearchAlgorithm):
@@ -94,17 +95,15 @@
                 suggestion_name = f"{space_name}___{param_name}___{parent_vals_name}"
                 search_point[space_name][param_name] = trial.suggest_categorical(suggestion_name, options)
             else:
                 raise ValueError(f"Unsupported parameter type: {type(param)}")
             invalid_trial = invalid_trial or (search_point[space_name][param_name] == SpecialParamValue.INVALID)
         return trial, search_point, invalid_trial
 
-    def report(
-        self, search_point: Dict[str, Dict[str, Any]], result: Dict[str, Union[float, int]], should_prune: bool = False
-    ):
+    def report(self, search_point: Dict[str, Dict[str, Any]], result: MetricResult, should_prune: bool = False):
         search_point_hash = hash_dict(search_point)
         trial_id = self._trial_ids[search_point_hash]
         if should_prune:
             self._study.tell(trial_id, state=optuna.trial.TrialState.PRUNED)
         else:
-            objectives = [result[objective] for objective in self._objectives]
+            objectives = [result[objective].value for objective in self._objectives]
             self._study.tell(trial_id, objectives)
```

## olive/systems/common.py

```diff
@@ -14,21 +14,14 @@
 class SystemType(str, Enum):
     Docker = "Docker"
     Local = "LocalSystem"
     AzureML = "AzureML"
     PythonEnvironment = "PythonEnvironment"
 
 
-class Device(str, Enum):
-    CPU = "cpu"
-    GPU = "gpu"
-    NPU = "npu"
-    INTEL_MYRIAD = "intel_myriad"
-
-
 class AzureMLDockerConfig(ConfigBase):
     base_image: Optional[str] = None
     dockerfile: Optional[str] = None
     build_context_path: Optional[Union[Path, str]] = None
     conda_file_path: Optional[Union[Path, str]] = None
 
     @validator("dockerfile", "build_context_path", always=True)
```

## olive/systems/local.py

```diff
@@ -1,42 +1,51 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from typing import Any, Dict, List, Optional
 
-from olive.evaluator.metric import Metric
+from olive.evaluator.metric import Metric, MetricResult
 from olive.evaluator.olive_evaluator import OliveEvaluator, OliveEvaluatorFactory
+from olive.hardware.accelerator import AcceleratorSpec, Device
 from olive.model import CompositeOnnxModel, OliveModel
 from olive.passes.olive_pass import Pass
-from olive.systems.common import Device, SystemType
+from olive.systems.common import SystemType
 from olive.systems.olive_system import OliveSystem
 
 
 class LocalSystem(OliveSystem):
     system_type = SystemType.Local
 
-    def __init__(self, device: Device = Device.CPU):
-        self.device = device
-        super().__init__()
+    def __init__(self, accelerators: List[str] = None):
+        super().__init__(accelerators=accelerators)
 
     def run_pass(
         self,
         the_pass: Pass,
         model: OliveModel,
         output_model_path: str,
         point: Optional[Dict[str, Any]] = None,
     ) -> OliveModel:
         """
         Run the pass on the model at a specific point in the search space.
         """
         return the_pass.run(model, output_model_path, point)
 
-    def evaluate_model(self, model: OliveModel, metrics: List[Metric]) -> Dict[str, Any]:
+    def evaluate_model(self, model: OliveModel, metrics: List[Metric], accelerator: AcceleratorSpec) -> MetricResult:
         """
         Evaluate the model
         """
         if isinstance(model, CompositeOnnxModel):
             raise NotImplementedError()
 
+        device = accelerator.accelerator_type if accelerator else Device.CPU
+        execution_providers = accelerator.execution_provider if accelerator else None
+
         evaluator: OliveEvaluator = OliveEvaluatorFactory.create_evaluator_for_model(model)
-        return evaluator.evaluate(model, metrics, device=self.device)
+        return evaluator.evaluate(model, metrics, device=device, execution_providers=execution_providers)
+
+    @staticmethod
+    def get_supported_execution_providers():
+        import onnxruntime as ort
+
+        return ort.get_available_providers()
```

## olive/systems/olive_system.py

```diff
@@ -2,27 +2,28 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 from abc import ABC, abstractmethod
 from typing import Any, Dict, List, Optional
 
-from olive.evaluator.metric import Metric
+from olive.evaluator.metric import Metric, MetricResult
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import OliveModel
 from olive.passes.olive_pass import Pass
 from olive.systems.common import SystemType
 
 logger = logging.getLogger(__name__)
 
 
 class OliveSystem(ABC):
     system_type: SystemType
 
-    def __init__(self):
-        self.accelerators = []
+    def __init__(self, accelerators: List[str] = None):
+        self.accelerators = accelerators
 
     @abstractmethod
     def run_pass(
         self,
         the_pass: Pass,
         model: OliveModel,
         output_model_path: str,
@@ -30,12 +31,12 @@
     ) -> OliveModel:
         """
         Run the pass on the model at a specific point in the search space.
         """
         raise NotImplementedError()
 
     @abstractmethod
-    def evaluate_model(self, model: OliveModel, metrics: List[Metric]) -> Dict[str, Any]:
+    def evaluate_model(self, model: OliveModel, metrics: List[Metric], accelerator: AcceleratorSpec) -> MetricResult:
         """
         Evaluate the model
         """
         raise NotImplementedError()
```

## olive/systems/system_config.py

```diff
@@ -7,23 +7,23 @@
 from pathlib import Path
 from typing import Dict, List, Union
 
 from pydantic import validator
 
 from olive.azureml.azureml_client import AzureMLClientConfig
 from olive.common.config_utils import ConfigBase, validate_config
-from olive.systems.common import AzureMLDockerConfig, Device, LocalDockerConfig, SystemType
+from olive.systems.common import AzureMLDockerConfig, LocalDockerConfig, SystemType
 
 
 class TargetUserConfig(ConfigBase):
-    pass
+    accelerators: List[str] = None
 
 
 class LocalTargetUserConfig(TargetUserConfig):
-    device: Device = Device.CPU
+    pass
 
 
 class DockerTargetUserConfig(TargetUserConfig):
     local_docker_config: LocalDockerConfig
     is_dev: bool = False
 
 
@@ -32,15 +32,14 @@
     aml_compute: str
     aml_docker_config: AzureMLDockerConfig
     instance_count: int = 1
     is_dev: bool = False
 
 
 class PythonEnvironmentTargetUserConfig(TargetUserConfig):
-    device: Device = Device.CPU
     python_environment_path: Union[
         Path, str
     ]  # path to the python environment, e.g. /home/user/anaconda3/envs/myenv, /home/user/.virtualenvs/myenv
     environment_variables: Dict[str, str] = None  # os.environ will be updated with these variables
     prepend_to_path: List[str] = None  # paths to prepend to os.environ["PATH"]
 
     @validator("python_environment_path", "prepend_to_path", pre=True, each_item=True)
```

## olive/systems/azureml/aml_evaluation_runner.py

```diff
@@ -3,28 +3,36 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import argparse
 import json
 from pathlib import Path
 
 from olive.evaluator.metric import Metric
+from olive.hardware import AcceleratorSpec
 from olive.model import ModelConfig
 from olive.systems.local import LocalSystem
 from olive.systems.olive_system import OliveSystem
 from olive.systems.utils import get_model_config, parse_common_args
 
 
 def parse_metric_args(raw_args):
     parser = argparse.ArgumentParser("Metric config")
 
     parser.add_argument("--metric_config", type=str, help="pass config", required=True)
     parser.add_argument("--metric_user_script", type=str, help="metric user script")
     parser.add_argument("--metric_script_dir", type=str, help="metric script dir")
     parser.add_argument("--metric_data_dir", type=str, help="metric data dir")
 
+    return parser.parse_known_args(raw_args)
+
+
+def parse_accelerator_args(raw_args):
+    parser = argparse.ArgumentParser("Accelerator config")
+    parser.add_argument("--accelerator_config", type=str, help="accelerator config", required=True)
+
     return parser.parse_args(raw_args)
 
 
 def create_metric(metric_config, metric_args):
     for key, value in vars(metric_args).items():
         if key == "metric_config":
             continue
@@ -34,30 +42,35 @@
 
     p = Metric.from_json(metric_config)
     return p
 
 
 def main(raw_args=None):
     common_args, extra_args = parse_common_args(raw_args)
-    metric_args = parse_metric_args(extra_args)
+    metric_args, extra_args = parse_metric_args(extra_args)
+    accelerator_args = parse_accelerator_args(extra_args)
 
     # load metric
     with open(metric_args.metric_config) as f:
         metric_config = json.load(f)
     metric = create_metric(metric_config, metric_args)
 
     # load model
     model_config = get_model_config(common_args)
     model = ModelConfig.from_json(model_config).create_model()
 
+    with open(accelerator_args.accelerator_config) as f:
+        accelerator_config = json.load(f)
+    accelerator_spec = AcceleratorSpec(**accelerator_config)
+
     target: OliveSystem = LocalSystem()
 
     # metric result
-    metric_result = target.evaluate_model(model, [metric])
+    metric_result = target.evaluate_model(model, [metric], accelerator_spec)
 
     # save metric result json
     with open(Path(common_args.pipeline_output) / "metric_result.json", "w") as f:
-        json.dump(metric_result, f)
+        f.write(metric_result.json())
 
 
 if __name__ == "__main__":
     main()
```

## olive/systems/azureml/aml_pass_runner.py

```diff
@@ -4,37 +4,41 @@
 # --------------------------------------------------------------------------
 import argparse
 import json
 import shutil
 import tempfile
 from pathlib import Path
 
+from olive.hardware import AcceleratorSpec
 from olive.model import ModelConfig
 from olive.passes import REGISTRY as PASS_REGISTRY
 from olive.passes import FullPassConfig
 from olive.systems.utils import get_model_config, parse_common_args
 
 
 def parse_pass_config_arg(raw_args):
     parser = argparse.ArgumentParser("Pass config")
 
     # parse config arg
     parser.add_argument("--pass_config", type=str, help="pass config", required=True)
+    parser.add_argument("--pass_accelerator_type", type=str, help="pass accelerator type", default="cpu")
+    parser.add_argument(
+        "--pass_execution_provider", type=str, help="pass execution provider", default="CPUExecutionProvider"
+    )
 
     return parser.parse_known_args(raw_args)
 
 
-def parse_pass_args(pass_type, raw_args):
+def parse_pass_args(pass_type, accelerator_spec, raw_args):
     pass_class = PASS_REGISTRY[pass_type]
 
     parser = argparse.ArgumentParser(f"{pass_type} pass args")
 
-    # TODO: get accelerator specs from args when it is implemented
     # parse pass args
-    for param, param_config in pass_class.default_config().items():
+    for param, param_config in pass_class.default_config(accelerator_spec).items():
         if param_config.is_path:
             parser.add_argument(f"--pass_{param}", type=str, help=f"pass {param}", required=param_config.required)
 
     return parser.parse_args(raw_args)
 
 
 def create_pass(pass_config, pass_args):
@@ -68,15 +72,16 @@
             shutil.copy(old_path, new_path)
         else:
             new_path.mkdir(parents=True, exist_ok=True)
             shutil.copytree(old_path, new_path, dirs_exist_ok=True)
         common_args.model_path = str(new_path)
 
     # pass specific args
-    pass_args = parse_pass_args(pass_type, extra_args)
+    accelerator_spec = AcceleratorSpec(pass_config_arg.pass_accelerator_type, pass_config_arg.pass_execution_provider)
+    pass_args = parse_pass_args(pass_type, accelerator_spec, extra_args)
 
     # load input_model
     input_model_config = get_model_config(common_args)
     input_model = ModelConfig.from_json(input_model_config).create_model()
     input_model_path = str(Path(input_model.model_path).resolve()) if input_model.model_path is not None else None
 
     # load pass
```

## olive/systems/azureml/aml_system.py

```diff
@@ -15,15 +15,16 @@
 from azure.ai.ml.entities import BuildContext, Environment
 from azure.core.exceptions import HttpResponseError, ServiceResponseError
 
 from olive.azureml.azureml_client import AzureMLClientConfig
 from olive.common.config_utils import validate_config
 from olive.common.utils import retry_func
 from olive.constants import Framework
-from olive.evaluator.metric import Metric
+from olive.evaluator.metric import Metric, MetricResult
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ModelConfig, ModelStorageKind, OliveModel, ONNXModel
 from olive.passes.olive_pass import Pass
 from olive.systems.common import AzureMLDockerConfig, SystemType
 from olive.systems.olive_system import OliveSystem
 
 logger = logging.getLogger(__name__)
 
@@ -34,16 +35,17 @@
     def __init__(
         self,
         azureml_client_config: AzureMLClientConfig,
         aml_compute: str,
         aml_docker_config: Union[Dict[str, Any], AzureMLDockerConfig],
         instance_count: int = 1,
         is_dev: bool = False,
+        accelerators: List[str] = None,
     ):
-        super().__init__()
+        super().__init__(accelerators)
         self._assert_not_none(aml_docker_config)
         aml_docker_config = validate_config(aml_docker_config, AzureMLDockerConfig)
         azureml_client_config = validate_config(azureml_client_config, AzureMLClientConfig)
         self.azureml_client_config = azureml_client_config
         self.compute = aml_compute
         self.environment = self._create_environment(aml_docker_config)
         self.instance_count = instance_count
@@ -212,15 +214,15 @@
         return {"pass_config": Input(type=AssetTypes.URI_FILE, path=pass_config_path), **pass_args}
 
     def _create_step(
         self, name, display_name, description, aml_environment, code, compute, instance_count, inputs, script_name
     ):
         parameters = []
         for param, input in inputs.items():
-            if input.optional:
+            if isinstance(input, Input) and input.optional:
                 parameters.append(f"$[[--{param} ${{{{inputs.{param}}}}}]]")
             else:
                 parameters.append(f"--{param} ${{{{inputs.{param}}}}}")
         parameters.append("--pipeline_output ${{outputs.pipeline_output}}")
 
         cmd_line = f"python {script_name} {' '.join(parameters)}"
 
@@ -259,16 +261,24 @@
             logger.warning(
                 "This mode is only enabled for CI pipeline! "
                 + "It will overwrite the Olive package in AML computer with latest code."
             )
             project_folder = cur_dir.parent.parent
             shutil.copytree(project_folder, code_root / "olive", ignore=shutil.ignore_patterns("__pycache__"))
 
+        accelerator_info = {
+            "pass_accelerator_type": pass_config["accelerator"]["accelerator_type"],
+            "pass_execution_provider": pass_config["accelerator"]["execution_provider"],
+        }
         # prepare inputs
-        inputs = {**self._create_model_inputs(model.model_storage_kind), **self._create_pass_inputs(pass_path_params)}
+        inputs = {
+            **self._create_model_inputs(model.model_storage_kind),
+            **self._create_pass_inputs(pass_path_params),
+            **accelerator_info,
+        }
 
         # pass type
         pass_type = pass_config["type"]
 
         # aml command object
         cmd = self._create_step(
             name=pass_type,
@@ -285,14 +295,15 @@
         # model json
         model_json = model.to_json(check_object=True)
 
         # input argument values
         args = {
             **self._create_model_args(model_json, tmp_dir),
             **self._create_pass_args(pass_config, pass_path_params, tmp_dir),
+            **accelerator_info,
         }
 
         @pipeline()
         def pass_runner_pipeline():
             outputs = {}
             component = cmd(**args)
             outputs["pipeline_output"] = component.outputs.pipeline_output
@@ -354,16 +365,14 @@
             "metric_config": Input(type=AssetTypes.URI_FILE),
             "metric_user_script": Input(type=AssetTypes.URI_FILE, optional=True),
             "metric_script_dir": Input(type=AssetTypes.URI_FOLDER, optional=True),
             "metric_data_dir": Input(type=AssetTypes.URI_FOLDER, optional=True),
         }
 
     def _create_metric_args(self, metric_config: dict, tmp_dir: Path) -> Tuple[List[str], dict]:
-        metric_config["name"] = "result"
-        metric_config.pop("goal", None)
         metric_user_script = metric_config["user_config"]["user_script"]
         if metric_user_script:
             metric_user_script = Input(type=AssetTypes.URI_FILE, path=metric_user_script)
             metric_config["user_config"]["user_script"] = None
 
         metric_script_dir = metric_config["user_config"]["script_dir"]
         if metric_script_dir:
@@ -383,23 +392,23 @@
         return {
             "metric_config": metric_config,
             "metric_user_script": metric_user_script,
             "metric_script_dir": metric_script_dir,
             "metric_data_dir": metric_data_dir,
         }
 
-    def evaluate_model(self, model: OliveModel, metrics: List[Metric]) -> Dict[str, Any]:
+    def evaluate_model(self, model: OliveModel, metrics: List[Metric], accelerator: AcceleratorSpec) -> MetricResult:
         if model.framework == Framework.SNPE:
             raise NotImplementedError("SNPE model does not support azureml evaluation")
         if model.framework == Framework.OPENVINO:
             raise NotImplementedError("OpenVINO model does not support azureml evaluation")
 
         with tempfile.TemporaryDirectory() as tempdir:
             ml_client = self.azureml_client_config.create_client()
-            pipeline_job = self._create_pipeline_for_evaluation(tempdir, model, metrics)
+            pipeline_job = self._create_pipeline_for_evaluation(tempdir, model, metrics, accelerator)
 
             # submit job
             logger.debug("Submitting pipeline")
             job = retry_func(
                 ml_client.jobs.create_or_update,
                 [pipeline_job],
                 {"experiment_name": "olive-evaluation"},
@@ -423,45 +432,56 @@
             )
 
             metric_results = {}
             for metric in metrics:
                 metric_json = output_dir / "named-outputs" / metric.name / "metric_result.json"
                 if metric_json.is_file():
                     with metric_json.open() as f:
-                        metric_results[metric.name] = json.load(f)["result"]
+                        metric_results.update(json.load(f))
 
-            return metric_results
+            return MetricResult.parse_obj(metric_results)
 
-    def _create_pipeline_for_evaluation(self, tmp_dir: str, model: OliveModel, metrics: List[Metric]):
+    def _create_pipeline_for_evaluation(
+        self, tmp_dir: str, model: OliveModel, metrics: List[Metric], accelerator: AcceleratorSpec
+    ):
         tmp_dir = Path(tmp_dir)
 
         # model json
         model_json = model.to_json(check_object=True)
 
         # model args
         model_args = self._create_model_args(model_json, tmp_dir)
 
+        accelerator_config_path: Path = tmp_dir / "accelerator.json"
+        with accelerator_config_path.open("w") as f:
+            json.dump(accelerator.to_json(), f, sort_keys=True)
+
         @pipeline
         def evaluate_pipeline():
             outputs = {}
             for metric in metrics:
                 metric_tmp_dir = tmp_dir / metric.name
                 metric_component = self._create_metric_component(
-                    metric_tmp_dir, metric, model_args, model.model_storage_kind
+                    metric_tmp_dir, metric, model_args, model.model_storage_kind, accelerator_config_path
                 )
                 outputs[metric.name] = metric_component.outputs.pipeline_output
             return outputs
 
         pipeline_job = evaluate_pipeline()
         pipeline_job.settings.default_compute = self.compute
 
         return pipeline_job
 
     def _create_metric_component(
-        self, tmp_dir: Path, metric: Metric, model_args: Dict[str, Input], model_storage_kind: ModelStorageKind
+        self,
+        tmp_dir: Path,
+        metric: Metric,
+        model_args: Dict[str, Input],
+        model_storage_kind: ModelStorageKind,
+        accelerator_config_path: str,
     ):
         metric_json = metric.to_json(check_objects=True)
 
         # prepare code
         script_name = "aml_evaluation_runner.py"
         cur_dir = Path(__file__).resolve().parent
         code_file = cur_dir / script_name
@@ -473,20 +493,25 @@
                 "This mode is only enabled for CI pipeline! "
                 + "It will overwrite the Olive package in AML computer with latest code."
             )
             project_folder = cur_dir.parent.parent
             shutil.copytree(project_folder, code_root / "olive", ignore=shutil.ignore_patterns("__pycache__"))
 
         # prepare inputs
-        inputs = {**self._create_model_inputs(model_storage_kind), **self._create_metric_inputs()}
+        inputs = {
+            **self._create_model_inputs(model_storage_kind),
+            **self._create_metric_inputs(),
+            **{"accelerator_config": Input(type=AssetTypes.URI_FILE)},
+        }
 
         # metric type
         metric_type = metric_json["type"]
-        if metric_json["sub_type"] is not None:
-            metric_type = f"{metric_type}-{metric_json['sub_type']}"
+        if metric_json["sub_types"] is not None:
+            sub_type_name = ",".join([st["name"] for st in metric_json["sub_types"]])
+            metric_type = f"{metric_type}-{sub_type_name}"
 
         # aml command object
         cmd = self._create_step(
             name=metric_type,
             display_name=metric_type,
             description=f"Run olive {metric_type} evaluation",
             aml_environment=self.environment,
@@ -494,13 +519,17 @@
             compute=self.compute,
             instance_count=self.instance_count,
             inputs=inputs,
             script_name=script_name,
         )
 
         # input argument values
-        args = {**model_args, **self._create_metric_args(metric_json, tmp_dir)}
+        args = {
+            **model_args,
+            **self._create_metric_args(metric_json, tmp_dir),
+            **{"accelerator_config": Input(type=AssetTypes.URI_FILE, path=accelerator_config_path)},
+        }
 
         # metric component
         metric_component = cmd(**args)
 
         return metric_component
```

## olive/systems/docker/Dockerfile

```diff
@@ -9,13 +9,13 @@
             azureml-defaults \
             azureml-dataprep \
             onnxruntime \
             openvino \
             openvino-dev[tensorflow,onnx] \
             tensorflow \
             onnxconverter_common \
-            olive-ai==0.2.0
+            olive-ai==0.2.1
 
 ADD requirements.txt requirements.txt
 RUN pip install -r requirements.txt
 
 WORKDIR /olive
```

## olive/systems/docker/docker_system.py

```diff
@@ -10,29 +10,31 @@
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Union
 
 import docker
 
 import olive.systems.docker.utils as docker_utils
 from olive.common.config_utils import validate_config
-from olive.evaluator.metric import Metric
+from olive.evaluator.metric import Metric, MetricResult
+from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import OliveModel
 from olive.passes import Pass
 from olive.systems.common import LocalDockerConfig, SystemType
 from olive.systems.olive_system import OliveSystem
 
 logger = logging.getLogger(__name__)
 
 
 class DockerSystem(OliveSystem):
     system_type = SystemType.Docker
 
     BASE_DOCKERFILE = "Dockerfile"
 
     def __init__(self, local_docker_config: Union[Dict[str, Any], LocalDockerConfig], is_dev: bool = False):
+        super().__init__(accelerators=None)
         logger.info("Initializing Docker System...")
         local_docker_config = validate_config(local_docker_config, LocalDockerConfig)
         self.is_dev = is_dev
         self.docker_client = docker.from_env()
         self.run_params = local_docker_config.run_params
         try:
             self.image = self.docker_client.images.get(local_docker_config.image_name)
@@ -75,23 +77,23 @@
     ) -> OliveModel:
         """
         Run the pass on the model at a specific point in the search space.
         """
         logger.warning("DockerSystem.run_pass is not implemented yet.")
         raise NotImplementedError()
 
-    def evaluate_model(self, model: OliveModel, metrics: List[Metric]) -> Dict[str, Any]:
+    def evaluate_model(self, model: OliveModel, metrics: List[Metric], accelerator: AcceleratorSpec) -> Dict[str, Any]:
         container_root_path = Path("/olive-ws/")
         with tempfile.TemporaryDirectory() as tempdir:
-            metrics_res = {}
+            metrics_res = None
             metric_json = self._run_container(tempdir, model, metrics, container_root_path)
             if metric_json.is_file():
                 with metric_json.open() as f:
                     metrics_res = json.load(f)
-            return metrics_res
+            return MetricResult.parse_obj(metrics_res)
 
     def _run_container(self, tempdir, model: OliveModel, metrics: List[Metric], container_root_path: Path):
         eval_output_path = "eval_output"
         eval_output_name = "eval_res.json"
 
         volumes_list = []
         eval_file_mount_path, eval_file_mount_str = docker_utils.create_eval_script_mount(container_root_path)
```

## olive/systems/docker/eval.py

```diff
@@ -24,15 +24,15 @@
         model_json["config"]["model_path"] = model_path
     model = ModelConfig.from_json(model_json).create_model()
 
     evaluator: OliveEvaluator = OliveEvaluatorFactory.create_evaluator_for_model(model)
     metrics_res = evaluator.evaluate(model, evaluator_config.metrics)
 
     with open(os.path.join(output_path, f"{output_name}"), "w") as f:
-        json.dump(metrics_res, f)
+        f.write(metrics_res.json())
     logger.info(f"Metric result: {metrics_res}")
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
 
     parser.add_argument("--config", type=str, help="evaluation config")
```

## olive/systems/python_environment/python_environment_system.py

```diff
@@ -10,69 +10,77 @@
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Union
 
 import numpy as np
 import torch
 
 from olive.common.utils import run_subprocess
-from olive.evaluator.metric import Metric, MetricType
+from olive.evaluator.metric import (
+    Metric,
+    MetricResult,
+    MetricType,
+    flatten_metric_result,
+    get_latency_config_from_metric,
+)
 from olive.evaluator.olive_evaluator import OliveEvaluator, OnnxEvaluator
+from olive.hardware.accelerator import AcceleratorLookup, AcceleratorSpec, Device
 from olive.model import OliveModel, ONNXModel
 from olive.passes.olive_pass import Pass
-from olive.systems.common import Device, SystemType
+from olive.systems.common import SystemType
 from olive.systems.olive_system import OliveSystem
 from olive.systems.system_config import PythonEnvironmentTargetUserConfig
 
 logger = logging.getLogger(__name__)
 
 
 class PythonEnvironmentSystem(OliveSystem):
     system_type = SystemType.PythonEnvironment
 
     def __init__(
         self,
         python_environment_path: Union[Path, str],
-        device: Device = Device.CPU,
         environment_variables: Dict[str, str] = None,
         prepend_to_path: List[str] = None,
+        accelerators: List[str] = None,
     ):
-        super().__init__()
+        super().__init__(accelerators=accelerators)
         self.config = PythonEnvironmentTargetUserConfig(
             python_environment_path=python_environment_path,
-            device=device,
             environment_variables=environment_variables,
             prepend_to_path=prepend_to_path,
+            accelerators=accelerators,
         )
         self.environ = deepcopy(os.environ)
         if self.config.environment_variables:
             self.environ.update(self.config.environment_variables)
         if self.config.prepend_to_path:
             self.environ["PATH"] = os.pathsep.join(self.config.prepend_to_path) + os.pathsep + self.environ["PATH"]
         self.environ["PATH"] = str(self.config.python_environment_path) + os.pathsep + self.environ["PATH"]
 
-        # available eps. This will be populated the first time self.get_available_eps() is called.
+        # available eps. This will be populated the first time self.get_supported_execution_providers() is called.
         # used for caching the available eps
         self.available_eps = None
 
         # path to inference script
         self.inference_path = Path(__file__).parent.resolve() / "inference_runner.py"
+        self.device = self.accelerators[0] if self.accelerators else Device.CPU
 
     def run_pass(
         self,
         the_pass: Pass,
         model: OliveModel,
         output_model_path: str,
         point: Optional[Dict[str, Any]] = None,
     ) -> OliveModel:
         """
         Run the pass on the model at a specific point in the search space.
         """
         raise ValueError("PythonEnvironmentSystem does not support running passes.")
 
-    def evaluate_model(self, model: OliveModel, metrics: List[Metric]) -> Dict[str, Any]:
+    def evaluate_model(self, model: OliveModel, metrics: List[Metric], accelerator: AcceleratorSpec) -> MetricResult:
         """
         Evaluate the model
         """
         if not isinstance(model, ONNXModel):
             raise ValueError("PythonEnvironmentSystem can only evaluate ONNXModel.")
 
         # check if custom metric is present
@@ -81,15 +89,15 @@
 
         metrics_res = {}
         for metric in metrics:
             if metric.type == MetricType.ACCURACY:
                 metrics_res[metric.name] = self.evaluate_accuracy(model, metric)
             elif metric.type == MetricType.LATENCY:
                 metrics_res[metric.name] = self.evaluate_latency(model, metric)
-        return metrics_res
+        return flatten_metric_result(metrics_res)
 
     def evaluate_accuracy(self, model: ONNXModel, metric: Metric) -> float:
         """
         Evaluate the accuracy of the model.
         """
         dataloader, post_func, _ = OliveEvaluator.get_user_config(metric)
 
@@ -141,17 +149,15 @@
         return OliveEvaluator.compute_accuracy(metric, preds, targets)
 
     def evaluate_latency(self, model: ONNXModel, metric: Metric) -> float:
         """
         Evaluate the latency of the model.
         """
         dataloader, _, _ = OliveEvaluator.get_user_config(metric)
-        warmup_num = metric.metric_config.warmup_num
-        repeat_test_num = metric.metric_config.repeat_test_num
-        sleep_num = metric.metric_config.sleep_num
+        warmup_num, repeat_test_num, sleep_num = get_latency_config_from_metric(metric)
 
         latencies = []
         inference_settings = self.get_inference_settings(model, metric)
         io_config = model.get_io_config()
 
         with tempfile.TemporaryDirectory() as tmp_dir:
             tmp_dir_path = Path(tmp_dir)
@@ -173,15 +179,15 @@
 
             # run inference
             command = (
                 f"python {self.inference_path} --type {metric.type} --model_path"
                 f" {model.model_path} --inference_settings_path {inference_settings_path} --input_dir"
                 f" {input_dir} --output_dir  {output_dir} --warmup_num {warmup_num} --repeat_test_num"
                 f" {repeat_test_num} --sleep_num {sleep_num} --io_bind {metric.user_config.io_bind} --device"
-                f" {self.config.device}"
+                f" {self.device}"
             )
             run_subprocess(command, env=self.environ, check=True)
 
             # load output
             latencies = np.load(output_dir / "output.npy")
 
         return OliveEvaluator.compute_latency(metric, latencies)
@@ -199,46 +205,40 @@
 
         # if user doesn't not provide ep list, use default value([ep]). Otherwise, use the user's ep list
         if not inference_settings.get("execution_provider"):
             inference_settings["execution_provider"] = self.get_default_execution_provider(model)
 
         return inference_settings
 
-    def get_available_eps(self) -> List[str]:
+    def get_supported_execution_providers(self) -> List[str]:
         """
         Get the available execution providers.
         """
         if self.available_eps:
             return self.available_eps
 
         with tempfile.TemporaryDirectory() as temp_dir:
             available_eps_path = Path(__file__).parent.resolve() / "available_eps.py"
             output_path = Path(temp_dir).resolve() / "available_eps.pb"
             run_subprocess(
                 f"python {available_eps_path} --output_path {output_path}",
                 env=self.environ,
                 check=True,
             )
-            with open(output_path, "rb") as f:
+            with output_path.open("rb") as f:
                 available_eps = pickle.load(f)
             self.available_eps = available_eps
             return available_eps
 
     def get_execution_providers(self) -> List[str]:
         """
         Get the execution providers for the device.
         """
-        available_eps = self.get_available_eps()
-        eps_per_device = ONNXModel.EXECUTION_PROVIDERS.get(self.config.device)
-        eps = []
-        if eps_per_device:
-            for ep in available_eps:
-                if ep in eps_per_device:
-                    eps.append(ep)
-        return eps or available_eps
+        available_eps = self.get_supported_execution_providers()
+        return AcceleratorLookup.get_execution_providers_for_device_by_available_providers(self.device, available_eps)
 
     def get_default_execution_provider(self, model: ONNXModel) -> List[str]:
         """
         Get the default execution provider for the model.
         """
         # return first available ep as ort default ep
         available_providers = self.get_execution_providers()
@@ -266,15 +266,15 @@
                         "--output_path",
                         str(output_path),
                     ]
                 ),
                 env=self.environ,
                 check=True,
             )
-            with open(output_path, "rb") as f:
+            with output_path.open("rb") as f:
                 result = pickle.load(f)
             if result["valid"]:
                 return True
             else:
                 logger.warning(
                     f"Error: {result['error']} Olive will ignore this {ep}."
                     + f"Please make sure the environment with {ep} has the required dependencies."
```

## olive/workflows/run/config.py

```diff
@@ -46,15 +46,14 @@
         ]
         for key in to_del:
             del config[key]
         return Engine(config)
 
 
 class RunConfig(ConfigBase):
-    verbose: bool = False
     azureml_client: AzureMLClientConfig = None
     input_model: ModelConfig
     systems: Dict[str, SystemConfig] = None
     data_config: Dict[str, DataConfig] = {
         DefaultDataContainer.DATA_CONTAINER.value: DataConfig(),
         DEFAULT_HF_DATA_CONTAINER_NAME: DataConfig(
             name=DEFAULT_HF_DATA_CONTAINER_NAME,
```

## olive/workflows/run/run.py

```diff
@@ -7,17 +7,18 @@
 import os
 import subprocess
 from pathlib import Path
 from typing import Union
 
 import onnxruntime as ort
 
-from olive import set_default_logger_severity
+from olive.hardware import Device
+from olive.logging import set_default_logger_severity
 from olive.passes import Pass
-from olive.systems.common import Device, SystemType
+from olive.systems.common import SystemType
 from olive.workflows.run.config import RunConfig
 
 logger = logging.getLogger(__name__)
 
 
 def automatically_insert_passes(config):
     new_config_dict = json.loads(config.json())
@@ -65,14 +66,16 @@
             "OrtPerfTuning": ["psutil"],
             "QuantizationAwareTraining": ["pytorch-lightning"],
             "OpenVINOConversion": EXTRAS.get("openvino"),
             "OpenVINOQuantization": EXTRAS.get("openvino"),
             "IncQuantization": EXTRAS.get("inc"),
             "IncDynamicQuantization": EXTRAS.get("inc"),
             "IncStaticQuantization": EXTRAS.get("inc"),
+            "OptimumConversion": EXTRAS.get("optimum"),
+            "OptimumMerging": EXTRAS.get("optimum"),
         },
     }
 
     local_packages = []
     remote_packages = []
 
     # add dependencies for passes
@@ -152,13 +155,12 @@
                 clean_run_cache=pass_config.clean_run_cache,
             )
 
         # run
         best_execution = engine.run(
             input_model,
             config.engine.packaging_config,
-            config.verbose,
             config.engine.output_dir,
             config.engine.output_name,
             config.engine.evaluation_only,
         )
         return best_execution
```

## olive/workflows/snpe/evaluate/evaluate.py

```diff
@@ -5,18 +5,18 @@
 import json
 import logging
 from pathlib import Path
 from typing import Dict, Optional, Union
 
 import numpy as np
 
+from olive.hardware import Device
 from olive.model import SNPEModel
 from olive.snpe import SNPEProcessedDataLoader
 from olive.snpe.utils.local import get_snpe_target_arch
-from olive.systems.common import Device
 
 logger = logging.getLogger(__name__)
 
 
 def evaluate(model: str, config: Union[str, Dict], data: str, input_list_file: Optional[str] = "input_list.txt"):
     """Evaluate a model.
```

## test/integ_test/evaluator/azureml_eval/test_aml_evaluation.py

```diff
@@ -12,14 +12,16 @@
     get_latency_metric,
     get_onnx_model,
     get_pytorch_model,
 )
 
 import pytest
 
+from olive.evaluator.metric import joint_metric_key
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.model import ModelStorageKind, ONNXModel, PyTorchModel
 
 
 class TestAMLEvaluation:
     @pytest.fixture(scope="class", autouse=True)
     def setup(self):
         get_directories()
@@ -38,9 +40,11 @@
     @pytest.mark.parametrize(
         "model_cls,model_path,metric,expected_res",
         EVALUATION_TEST_CASE,
     )
     def test_evaluate_model(self, model_cls, model_path, metric, expected_res):
         aml_target = get_aml_target()
         olive_model = model_cls(model_path=model_path, model_storage_kind=ModelStorageKind.LocalFile)
-        actual_res = aml_target.evaluate_model(olive_model, [metric])[metric.name]
-        assert actual_res >= expected_res
+        actual_res = aml_target.evaluate_model(olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
+        for sub_type in metric.sub_types:
+            joint_key = joint_metric_key(metric.name, sub_type.name)
+            assert actual_res[joint_key].value >= expected_res
```

## test/integ_test/evaluator/azureml_eval/utils.py

```diff
@@ -38,30 +38,30 @@
         "post_processing_func": "post_process",
         "data_dir": data_dir,
         "dataloader_func": "create_dataloader",
     }
     accuracy_metric = Metric(
         name="accuracy",
         type=MetricType.ACCURACY,
-        sub_type=AccuracySubType.ACCURACY_SCORE,
+        sub_types=[{"name": AccuracySubType.ACCURACY_SCORE}],
         user_config=accuracy_metric_config,
     )
     return accuracy_metric
 
 
 def get_latency_metric():
     latency_metric_config = {
         "user_script": user_script,
         "data_dir": data_dir,
         "dataloader_func": "create_dataloader",
     }
     latency_metric = Metric(
         name="latency",
         type=MetricType.LATENCY,
-        sub_type=LatencySubType.AVG,
+        sub_types=[{"name": LatencySubType.AVG}],
         user_config=latency_metric_config,
     )
     return latency_metric
 
 
 def download_models():
     pytorch_model_config = {
```

## test/integ_test/evaluator/docker_eval/test_docker_evaluation.py

```diff
@@ -15,14 +15,16 @@
     get_onnx_model,
     get_openvino_model,
     get_pytorch_model,
 )
 
 import pytest
 
+from olive.evaluator.metric import joint_metric_key
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.model import ONNXModel, OpenVINOModel, PyTorchModel
 
 
 class TestDockerEvaluation:
     @pytest.fixture(scope="class", autouse=True)
     def setup(self):
         get_directories()
@@ -46,9 +48,11 @@
         "model_cls,model_config,metric,expected_res",
         EVALUATION_TEST_CASE,
     )
     @pytest.mark.skipif(platform.system() == "Windows", reason="Docker target does not support windows")
     def test_evaluate_model(self, model_cls, model_config, metric, expected_res):
         docker_target = get_docker_target()
         olive_model = model_cls(**model_config)
-        actual_res = docker_target.evaluate_model(olive_model, [metric])[metric.name]
-        assert actual_res >= expected_res
+        actual_res = docker_target.evaluate_model(olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
+        for sub_type in metric.sub_types:
+            joint_key = joint_metric_key(metric.name, sub_type.name)
+            assert actual_res[joint_key].value >= expected_res
```

## test/integ_test/evaluator/docker_eval/utils.py

```diff
@@ -37,30 +37,30 @@
         "post_processing_func": post_process,
         "data_dir": data_dir,
         "dataloader_func": dataloader_func,
     }
     accuracy_metric = Metric(
         name="accuracy",
         type=MetricType.ACCURACY,
-        sub_type=AccuracySubType.ACCURACY_SCORE,
+        sub_types=[{"name": AccuracySubType.ACCURACY_SCORE}],
         user_config=accuracy_metric_config,
     )
     return accuracy_metric
 
 
 def get_latency_metric(dataloader_func="create_dataloader"):
     latency_metric_config = {
         "user_script": user_script,
         "data_dir": data_dir,
         "dataloader_func": dataloader_func,
     }
     latency_metric = Metric(
         name="latency",
         type=MetricType.LATENCY,
-        sub_type=LatencySubType.AVG,
+        sub_types=[{"name": LatencySubType.AVG}],
         user_config=latency_metric_config,
     )
     return latency_metric
 
 
 def download_models():
     pytorch_model_config = {
```

## test/integ_test/evaluator/local_eval/test_local_evaluation.py

```diff
@@ -15,14 +15,16 @@
     get_pytorch_model,
     openvino_post_process,
     post_process,
 )
 
 import pytest
 
+from olive.evaluator.metric import joint_metric_key
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.model import ONNXModel, OpenVINOModel, PyTorchModel
 from olive.systems.local import LocalSystem
 
 
 class TestLocalEvaluation:
     @pytest.fixture(scope="class", autouse=True)
     def setup(self):
@@ -43,9 +45,11 @@
 
     @pytest.mark.parametrize(
         "model_cls,model_config,metric,expected_res",
         EVALUATION_TEST_CASE,
     )
     def test_evaluate_model(self, model_cls, model_config, metric, expected_res):
         olive_model = model_cls(**model_config)
-        actual_res = LocalSystem().evaluate_model(olive_model, [metric])[metric.name]
-        assert actual_res >= expected_res
+        actual_res = LocalSystem().evaluate_model(olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
+        for sub_type in metric.sub_types:
+            joint_key = joint_metric_key(metric.name, sub_type.name)
+            assert actual_res[joint_key].value >= expected_res
```

## test/integ_test/evaluator/local_eval/utils.py

```diff
@@ -84,32 +84,34 @@
 
 def get_accuracy_metric(post_process, dataloader=create_dataloader):
     accuracy_metric_config = {
         "post_processing_func": post_process,
         "data_dir": data_dir,
         "dataloader_func": dataloader,
     }
+    sub_types = [{"name": AccuracySubType.ACCURACY_SCORE}]
     accuracy_metric = Metric(
         name="accuracy",
         type=MetricType.ACCURACY,
-        sub_type=AccuracySubType.ACCURACY_SCORE,
+        sub_types=sub_types,
         user_config=accuracy_metric_config,
     )
     return accuracy_metric
 
 
 def get_latency_metric(dataloader=create_dataloader):
     latency_metric_config = {
         "data_dir": data_dir,
         "dataloader_func": dataloader,
     }
+    sub_types = [{"name": LatencySubType.AVG}]
     latency_metric = Metric(
         name="latency",
         type=MetricType.LATENCY,
-        sub_type=LatencySubType.AVG,
+        sub_types=sub_types,
         user_config=latency_metric_config,
     )
     return latency_metric
 
 
 def get_hf_accuracy_metric(post_process=hf_post_process, dataloader=create_hf_dataloader):
     return get_accuracy_metric(post_process, dataloader)
```

## test/unit_test/utils.py

```diff
@@ -94,42 +94,45 @@
 
 
 def create_fixed_dataloader(datadir, batchsize):
     dataloader = DataLoader(FixedDummyDataset(1))
     return dataloader
 
 
-def get_accuracy_metric(acc_subtype, random_dataloader=True):
+def get_accuracy_metric(*acc_subtype, random_dataloader=True, user_config=None):
     accuracy_metric_config = {"dataloader_func": create_dataloader if random_dataloader else create_fixed_dataloader}
+    sub_types = [{"name": sub, "goal": MetricGoal(type="threshold", value=0.99)} for sub in acc_subtype]
+    sub_types[0]["priority"] = 1
     accuracy_metric = Metric(
         name="accuracy",
         type=MetricType.ACCURACY,
-        sub_type=acc_subtype,
-        goal=MetricGoal(type="threshold", value=0.99),
-        user_config=accuracy_metric_config,
+        sub_types=sub_types,
+        user_config=user_config or accuracy_metric_config,
     )
     return accuracy_metric
 
 
 def get_custom_metric():
     custom_metric = Metric(
         name="custom",
         type=MetricType.CUSTOM,
+        sub_types=[{"name": "custom"}],
         user_config={"evaluate_func": "val", "user_script": "user_script"},
     )
     return custom_metric
 
 
-def get_latency_metric(lat_subtype):
+def get_latency_metric(*lat_subtype, user_config=None):
     latency_metric_config = {"dataloader_func": create_dataloader}
+    sub_types = [{"name": sub} for sub in lat_subtype]
     latency_metric = Metric(
         name="latency",
         type=MetricType.LATENCY,
-        sub_type=lat_subtype,
-        user_config=latency_metric_config,
+        sub_types=sub_types,
+        user_config=user_config or latency_metric_config,
     )
     return latency_metric
 
 
 def get_onnxconversion_pass(ignore_pass_config=True):
     onnx_conversion_config = {}
     p = create_pass_from_dict(OnnxConversion, onnx_conversion_config)
```

## test/unit_test/engine/test_engine.py

```diff
@@ -1,30 +1,31 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import json
 import logging
-import shutil
+import tempfile
 from pathlib import Path
 from test.unit_test.utils import (
     get_accuracy_metric,
     get_onnx_model,
     get_onnxconversion_pass,
     get_pytorch_model,
     pytorch_model_loader,
 )
 from unittest.mock import Mock, patch
 
 import pytest
 
 from olive.common.utils import hash_dict
 from olive.engine import Engine
-from olive.evaluator.metric import AccuracySubType
+from olive.evaluator.metric import AccuracySubType, MetricResult, joint_metric_key
 from olive.evaluator.olive_evaluator import OliveEvaluatorConfig
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.model import PyTorchModel
 from olive.passes.onnx import OnnxConversion, OnnxDynamicQuantization
 from olive.systems.local import LocalSystem
 
 
 # Please not your test case could still "pass" even if it throws exception to fail.
 # Please check log message to make sure your test case passes.
@@ -33,14 +34,16 @@
         # setup
         p = get_onnxconversion_pass()
         name = p.__class__.__name__
         system = LocalSystem()
         evaluator_config = OliveEvaluatorConfig(metrics=[get_accuracy_metric(AccuracySubType.ACCURACY_SCORE)])
 
         options = {
+            "output_dir": "./cache",
+            "output_name": "test",
             "cache_dir": "./cache",
             "clean_cache": True,
             "search_strategy": {
                 "execution_order": "joint",
                 "search_algorithm": "random",
             },
         }
@@ -91,43 +94,57 @@
         # setup
         pytorch_model = get_pytorch_model()
         input_model_id = hash_dict(pytorch_model.to_json())
         p, pass_config = get_onnxconversion_pass(ignore_pass_config=False)
         metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE)
         evaluator_config = OliveEvaluatorConfig(metrics=[metric])
         options = {
+            "output_dir": "./cache",
+            "output_name": "test",
             "cache_dir": "./cache",
             "clean_cache": True,
             "search_strategy": {
                 "execution_order": "joint",
                 "search_algorithm": "random",
             },
             "clean_evaluation_cache": True,
         }
-        engine = Engine(options, host=mock_local_system, target=mock_local_system, evaluator_config=evaluator_config)
-        engine.register(OnnxConversion, clean_run_cache=True)
         onnx_model = get_onnx_model()
+        metric_result_dict = {
+            joint_metric_key(metric.name, sub_metric.name): {
+                "value": 0.998,
+                "priority": sub_metric.priority,
+                "higher_is_better": sub_metric.higher_is_better,
+            }
+            for sub_metric in metric.sub_types
+        }
         mock_local_system.run_pass.return_value = onnx_model
-        mock_local_system.evaluate_model.return_value = {metric.name: 0.998}
-        model_id = f"0_{p.__class__.__name__}-{input_model_id}-{hash_dict(pass_config)}"
+        mock_local_system.evaluate_model.return_value = MetricResult.parse_obj(metric_result_dict)
+        mock_local_system.accelerators = ["CPU"]
+
+        engine = Engine(options, host=mock_local_system, target=mock_local_system, evaluator_config=evaluator_config)
+        engine.register(OnnxConversion, clean_run_cache=True)
+        model_id = f"0_{p.__class__.__name__}-{input_model_id}-{hash_dict(pass_config)}-{DEFAULT_CPU_ACCELERATOR}"
         expected_res = {
             model_id: {
                 "model_id": model_id,
                 "parent_model_id": input_model_id,
                 "metrics": {
-                    "value": {metric.name: 0.998},
-                    "cmp_direction": {metric.name: 1},
+                    "value": metric_result_dict,
+                    "cmp_direction": {},
                     "is_goals_met": True,
                 },
             }
         }
 
         # execute
-        actual_res = engine.run(pytorch_model)
-        accelerator_spec = 0
+        temp_dir = tempfile.TemporaryDirectory()
+        output_dir = Path(temp_dir.name)
+        actual_res = engine.run(pytorch_model, output_dir=output_dir)
+        accelerator_spec = DEFAULT_CPU_ACCELERATOR
         actual_res = actual_res[accelerator_spec]
 
         # make sure the input model always be in engine.footprints
         footprint = engine.footprints[accelerator_spec]
         assert input_model_id in footprint.nodes
         # make sure the input model always not in engine's pareto frontier
         assert input_model_id not in actual_res.nodes
@@ -135,63 +152,73 @@
         # assert
         assert len(actual_res.nodes) == 1
         assert model_id in actual_res.nodes
         assert actual_res.nodes[model_id].model_id == model_id
         for k, v in expected_res[model_id].items():
             if k == "metrics":
                 assert getattr(actual_res.nodes[model_id].metrics, "is_goals_met")
-            assert getattr(actual_res.nodes[model_id], k) == v
+            else:
+                assert getattr(actual_res.nodes[model_id], k) == v
         assert engine.get_model_json_path(actual_res.nodes[model_id].model_id).exists()
         mock_local_system.run_pass.assert_called_once()
-        mock_local_system.evaluate_model.assert_called_once_with(onnx_model, [metric])
+        mock_local_system.evaluate_model.assert_called_once_with(onnx_model, [metric], accelerator_spec)
 
     @patch("olive.engine.engine.LocalSystem")
     def test_run_no_search(self, mock_local_system):
         # setup
         pytorch_model = get_pytorch_model()
         metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE)
         evaluator_config = OliveEvaluatorConfig(metrics=[metric])
         options = {
+            "output_dir": "./cache",
+            "output_name": "test",
             "cache_dir": "./cache",
             "clean_cache": True,
             "search_strategy": None,
             "clean_evaluation_cache": True,
         }
-        engine = Engine(options, host=mock_local_system, target=mock_local_system, evaluator_config=evaluator_config)
-        engine.register(OnnxConversion, disable_search=True, clean_run_cache=True)
         onnx_model = get_onnx_model()
+        metric_result_dict = {
+            joint_metric_key(metric.name, sub_metric.name): {
+                "value": 0.998,
+                "priority": sub_metric.priority,
+                "higher_is_better": sub_metric.higher_is_better,
+            }
+            for sub_metric in metric.sub_types
+        }
         mock_local_system.run_pass.return_value = onnx_model
-        mock_local_system.evaluate_model.return_value = {metric.name: 0.998}
+        mock_local_system.evaluate_model.return_value = MetricResult.parse_obj(metric_result_dict)
+        mock_local_system.accelerators = ["CPU"]
 
+        engine = Engine(options, host=mock_local_system, target=mock_local_system, evaluator_config=evaluator_config)
+        engine.register(OnnxConversion, disable_search=True, clean_run_cache=True)
         # output model to output_dir
-        output_dir = Path("cache") / "output"
-        shutil.rmtree(output_dir, ignore_errors=True)
+        temp_dir = tempfile.TemporaryDirectory()
+        output_dir = Path(temp_dir.name)
 
-        # TODO: replace with the real accelerator spec
-        accelerator_spec = 0
-        expected_res = {"model": onnx_model.to_json(), "metrics": {metric.name: 0.998}}
+        accelerator_spec = DEFAULT_CPU_ACCELERATOR
+        expected_res = {"model": onnx_model.to_json(), "metrics": MetricResult.parse_obj(metric_result_dict)}
         expected_res["model"]["config"]["model_path"] = str(
             Path(output_dir / f"{accelerator_spec}_model.onnx").resolve()
         )
 
         # execute
         actual_res = engine.run(pytorch_model, output_dir=output_dir)
-        actual_res = list(actual_res.values())[0]
+        actual_res = actual_res[accelerator_spec]
 
         assert expected_res == actual_res
         assert Path(actual_res["model"]["config"]["model_path"]).is_file()
         model_json_path = Path(output_dir / f"{accelerator_spec}_model.json")
         assert model_json_path.is_file()
-        assert json.load(open(model_json_path, "r")) == actual_res["model"]
+        with open(model_json_path, "r") as f:
+            assert json.load(f) == actual_res["model"]
         result_json_path = Path(output_dir / f"{accelerator_spec}_metrics.json")
         assert result_json_path.is_file()
-        assert json.load(open(result_json_path, "r")) == actual_res["metrics"]
-
-        # clean up
-        shutil.rmtree(output_dir, ignore_errors=True)
+        with open(result_json_path, "r") as f:
+            assert json.load(f) == actual_res["metrics"].__root__
 
     def test_pass_exception(self, caplog):
         # Need explicitly set the propagate to allow the message to be logged into caplog
         # setup
         logger = logging.getLogger("olive")
         logger.propagate = True
 
@@ -208,56 +235,67 @@
                 },
             }
             engine = Engine(options, evaluator_config=evaluator_config, host=system, target=system)
             engine.register(OnnxConversion, clean_run_cache=True)
             model = PyTorchModel(model_loader=pytorch_model_loader, model_path=None)
 
             # execute
-            engine.run(model)
+            temp_dir = tempfile.TemporaryDirectory()
+            output_dir = Path(temp_dir.name)
+            engine.run(model, output_dir=output_dir)
 
             # assert
             assert "Exception: test" in caplog.text
 
+            # clean up: tempfile will be deleted automatically
+
     @patch("olive.engine.engine.LocalSystem")
     def test_run_evaluation_only(self, mock_local_system):
         # setup
         pytorch_model = get_pytorch_model()
         metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE)
         evaluator_config = OliveEvaluatorConfig(metrics=[metric])
         options = {
             "cache_dir": "./cache",
             "clean_cache": True,
             "search_strategy": None,
             "clean_evaluation_cache": True,
         }
-        engine = Engine(options, host=mock_local_system, target=mock_local_system, evaluator_config=evaluator_config)
-        engine.register(OnnxConversion, clean_run_cache=True)
         onnx_model = get_onnx_model()
+        metric_result_dict = {
+            joint_metric_key(metric.name, sub_metric.name): {
+                "value": 0.998,
+                "priority": sub_metric.priority,
+                "higher_is_better": sub_metric.higher_is_better,
+            }
+            for sub_metric in metric.sub_types
+        }
         mock_local_system.run_pass.return_value = onnx_model
-        mock_local_system.evaluate_model.return_value = {metric.name: 0.998}
+        mock_local_system.evaluate_model.return_value = MetricResult.parse_obj(metric_result_dict)
+        mock_local_system.accelerators = ["CPU"]
+
+        engine = Engine(options, host=mock_local_system, target=mock_local_system, evaluator_config=evaluator_config)
+        engine.register(OnnxConversion, clean_run_cache=True)
 
         # output model to output_dir
-        output_dir = Path("cache") / "output"
-        shutil.rmtree(output_dir, ignore_errors=True)
+        temp_dir = tempfile.TemporaryDirectory()
+        output_dir = Path(temp_dir.name)
 
-        expected_res = {metric.name: 0.998}
+        expected_res = MetricResult.parse_obj(metric_result_dict)
 
         # execute
         actual_res = engine.run(pytorch_model, output_dir=output_dir, evaluation_only=True)
-        actual_res = list(actual_res.values())[0]
+        accelerator_spec = DEFAULT_CPU_ACCELERATOR
+        actual_res = actual_res[accelerator_spec]
 
-        # TODO: replace with the real accelerator spec
-        accelerator_spec = 0
         assert expected_res == actual_res
         result_json_path = Path(output_dir / f"{accelerator_spec}_metrics.json")
         assert result_json_path.is_file()
-        assert json.load(open(result_json_path, "r")) == actual_res
-
-        # clean up
-        shutil.rmtree(output_dir, ignore_errors=True)
+        with open(result_json_path, "r") as f:
+            assert f.read() == actual_res.json()
 
     @patch.object(Path, "glob", return_value=[Path("cache") / "output" / "100_model.json"])
     @patch.object(Path, "unlink")
     def test_model_path_suffix(self, mock_glob, mock_unlink: Mock):
         # setup
         metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE)
         evaluator_config = OliveEvaluatorConfig(metrics=[metric])
@@ -271,18 +309,14 @@
         engine.register(OnnxConversion, clean_run_cache=True)
 
         engine.initialize()
 
         assert engine._new_model_number == 101
         assert mock_unlink.called
 
-        # output model to output_dir
-        output_dir = Path("cache") / "output"
-        shutil.rmtree(output_dir, ignore_errors=True)
-
     def test_model_path_suffix_with_exception(self):
         # setup
         metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE)
         evaluator_config = OliveEvaluatorConfig(metrics=[metric])
         options = {
             "cache_dir": "./cache",
             "clean_cache": True,
@@ -293,11 +327,7 @@
         engine.register(OnnxConversion, clean_run_cache=True)
         with patch.object(Path, "glob"):
             Path.glob.return_value = [Path("cache") / "output" / "435d_0.json"]
 
             with pytest.raises(ValueError) as exc_info:
                 engine.initialize()
                 assert str(exc_info.value) == "ValueError: invalid literal for int() with base 10: '435d'"
-
-        # output model to output_dir
-        output_dir = Path("cache") / "output"
-        shutil.rmtree(output_dir, ignore_errors=True)
```

## test/unit_test/engine/test_footprint.py

```diff
@@ -50,12 +50,16 @@
                 assert str(model_path).endswith(".pt")
             else:
                 assert inference_config is not None
                 assert str(model_path).endswith(".onnx")
 
     def test_plot_pareto_frontier(self):
         with tempfile.TemporaryDirectory() as tempdir:
+            self.fp.objective_dict = {
+                "accuracy-accuracy_score": {"higher_is_better": True, "priority": 1},
+                "latency-avg": {"higher_is_better": False, "priority": 2},
+            }
             self.fp.plot_pareto_frontier(
                 is_show=False,
                 save_path=Path(tempdir) / "pareto_frontier.html",
             )
             assert (Path(tempdir) / "pareto_frontier.html").exists()
```

## test/unit_test/engine/packaging/test_packaging_generator.py

```diff
@@ -1,22 +1,23 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-import shutil
+import tempfile
 import zipfile
 from pathlib import Path
 from test.unit_test.utils import get_accuracy_metric, get_pytorch_model
 
 from olive.engine import Engine
 from olive.engine.footprint import Footprint
 from olive.engine.packaging.packaging_config import PackagingConfig, PackagingType
 from olive.engine.packaging.packaging_generator import generate_output_artifacts
 from olive.evaluator.metric import AccuracySubType
 from olive.evaluator.olive_evaluator import OliveEvaluatorConfig
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.passes.onnx.conversion import OnnxConversion
 
 
 def test_generate_zipfile_artifacts():
     # setup
     metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE)
     evaluator_config = OliveEvaluatorConfig(metrics=[metric])
@@ -34,31 +35,29 @@
 
     input_model = get_pytorch_model()
 
     packaging_config = PackagingConfig()
     packaging_config.type = PackagingType.Zipfile
     packaging_config.name = "OutputModels"
 
-    output_dir = Path(__file__).parent / "outputs"
+    tempdir = tempfile.TemporaryDirectory()
+    output_dir = Path(tempdir.name) / "outputs"
 
     # execute
     engine.run(input_model=input_model, packaging_config=packaging_config, output_dir=output_dir)
 
     # assert
     artifacts_path = output_dir / "OutputModels.zip"
     assert artifacts_path.exists()
     with zipfile.ZipFile(artifacts_path, "r") as zip_ref:
         zip_ref.extractall(output_dir)
     assert (output_dir / "SampleCode").exists()
     assert (output_dir / "CandidateModels").exists()
     assert (output_dir / "ONNXRuntimePackages").exists()
 
-    # cleanup
-    shutil.rmtree(output_dir)
-
 
 def test_generate_zipfile_artifacts_no_search():
     # setup
     options = {
         "cache_dir": "./cache",
         "clean_cache": True,
         "clean_evaluation_cache": True,
@@ -68,45 +67,46 @@
 
     input_model = get_pytorch_model()
 
     packaging_config = PackagingConfig()
     packaging_config.type = PackagingType.Zipfile
     packaging_config.name = "OutputModels"
 
-    output_dir = Path(__file__).parent / "outputs"
+    tempdir = tempfile.TemporaryDirectory()
+    output_dir = Path(tempdir.name) / "outputs"
 
     # execute
     engine.run(input_model=input_model, packaging_config=packaging_config, output_dir=output_dir)
 
     # assert
     artifacts_path = output_dir / "OutputModels.zip"
     assert artifacts_path.exists()
     with zipfile.ZipFile(artifacts_path, "r") as zip_ref:
         zip_ref.extractall(output_dir)
     assert (output_dir / "SampleCode").exists()
     assert (output_dir / "CandidateModels").exists()
     assert (output_dir / "ONNXRuntimePackages").exists()
 
-    # cleanup
-    shutil.rmtree(output_dir)
-
 
 def test_generate_zipfile_artifacts_none_nodes():
     # setup
     packaging_config = PackagingConfig()
     packaging_config.type = PackagingType.Zipfile
     packaging_config.name = "OutputModels"
 
     foot_print = Footprint()
     pf_footprint = Footprint()
     pf_footprint.nodes = None
-    output_dir = Path(__file__).parent / "outputs"
+    tempdir = tempfile.TemporaryDirectory()
+    output_dir = Path(tempdir.name) / "outputs"
 
     # execute
-    generate_output_artifacts(packaging_config, foot_print, pf_footprint, output_dir)
+    generate_output_artifacts(
+        packaging_config, {DEFAULT_CPU_ACCELERATOR: foot_print}, {DEFAULT_CPU_ACCELERATOR: pf_footprint}, output_dir
+    )
 
     # assert
     artifacts_path = output_dir / "OutputModels.zip"
     assert not artifacts_path.exists()
 
 
 def test_generate_zipfile_artifacts_zero_len_nodes():
@@ -114,15 +114,18 @@
     packaging_config = PackagingConfig()
     packaging_config.type = PackagingType.Zipfile
     packaging_config.name = "OutputModels"
 
     foot_print = Footprint()
     pf_footprint = Footprint()
     pf_footprint.nodes = {}
-    output_dir = Path(__file__).parent / "outputs"
+    tempdir = tempfile.TemporaryDirectory()
+    output_dir = Path(tempdir.name) / "outputs"
 
     # execute
-    generate_output_artifacts(packaging_config, foot_print, pf_footprint, output_dir)
+    generate_output_artifacts(
+        packaging_config, {DEFAULT_CPU_ACCELERATOR: foot_print}, {DEFAULT_CPU_ACCELERATOR: pf_footprint}, output_dir
+    )
 
     # assert
     artifacts_path = output_dir / "OutputModels.zip"
     assert not artifacts_path.exists()
```

## test/unit_test/evaluator/test_olive_evaluator.py

```diff
@@ -4,14 +4,15 @@
 # --------------------------------------------------------------------------
 from test.unit_test.utils import get_accuracy_metric, get_latency_metric, get_onnx_model, get_pytorch_model
 from unittest.mock import patch
 
 import pytest
 
 from olive.evaluator.metric import AccuracySubType, LatencySubType
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.systems.local import LocalSystem
 
 
 class TestOliveEvaluator:
     @pytest.fixture(autouse=True)
     def setup(self):
         self.system = LocalSystem()
@@ -65,22 +66,23 @@
     )
     def test_evaluate_accuracy(self, olive_model, metric, acc_subtype, expected_res):
         # setup
         with patch(f"{acc_subtype}.measure") as mock_acc:
             mock_acc.return_value = expected_res
 
             # execute
-            actual_res = self.system.evaluate_model(olive_model, [metric])[metric.name]
+            actual_res = self.system.evaluate_model(olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
 
             # assert
             mock_acc.assert_called_once()
-            assert expected_res == actual_res
+            for sub_type in metric.sub_types:
+                assert expected_res == actual_res.get_value(metric.name, sub_type.name)
 
     LATENCY_TEST_CASE = [
-        (get_pytorch_model(), get_latency_metric(LatencySubType.AVG), 1),
+        (get_pytorch_model(), get_latency_metric(LatencySubType.AVG, LatencySubType.MAX), 1),
         (get_pytorch_model(), get_latency_metric(LatencySubType.MAX), 1),
         (get_pytorch_model(), get_latency_metric(LatencySubType.MIN), 1),
         (get_pytorch_model(), get_latency_metric(LatencySubType.P50), 1),
         (get_pytorch_model(), get_latency_metric(LatencySubType.P75), 1),
         (get_pytorch_model(), get_latency_metric(LatencySubType.P90), 1),
         (get_pytorch_model(), get_latency_metric(LatencySubType.P95), 1),
         (get_pytorch_model(), get_latency_metric(LatencySubType.P99), 1),
@@ -98,11 +100,40 @@
 
     @pytest.mark.parametrize(
         "olive_model,metric,expected_res",
         LATENCY_TEST_CASE,
     )
     def test_evaluate_latency(self, olive_model, metric, expected_res):
         # execute
-        actual_res = self.system.evaluate_model(olive_model, [metric])[metric.name]
+        actual_res = self.system.evaluate_model(olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
 
         # assert
-        assert expected_res > actual_res
+        for sub_type in metric.sub_types:
+            assert expected_res > actual_res.get_value(metric.name, sub_type.name)
+
+
+@pytest.mark.skip(reason="Requires custom onnxruntime build with mpi enabled")
+class TestDistributedOnnxEvaluator:
+    def test_evaluate(self):
+        from olive.model import DistributedOnnxModel
+
+        filepaths = ["examples/switch/model_4n_2l_8e_00.onnx", "examples/switch/model_4n_2l_8e_01.onnx"]
+        model = DistributedOnnxModel(filepaths, name="model_4n_2l_8e")
+
+        user_config = {
+            "user_script": "examples/switch/user_script.py",
+            "dataloader_func": "create_dataloader",
+            "batch_size": 1,
+        }
+        # accuracy_metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE, user_config=user_config)
+        latency_metric = get_latency_metric(LatencySubType.AVG, user_config=user_config)
+        # metrics = [accuracy_metric, latency_metric]
+        metrics = [latency_metric]
+
+        target = LocalSystem()
+
+        # execute
+        actual_res = target.evaluate_model(model, metrics)
+
+        # assert
+        for sub_type in latency_metric.sub_types:
+            assert actual_res.get_value(latency_metric.name, sub_type.name) > 1
```

## test/unit_test/model/test_pytorch_model.py

```diff
@@ -1,32 +1,40 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import shutil
+import tempfile
 import unittest
+from pathlib import Path
 
 import mlflow
 import pandas as pd
 import transformers
 from azureml.evaluate import mlflow as aml_mlflow
 
 from olive.model import PyTorchModel
 
 
 class TestPyTorchMLflowModel(unittest.TestCase):
     def setup(self):
-        self.model_path = "mlflow_test"
+        self.tempdir = tempfile.TemporaryDirectory()
+        self.root_dir = Path(self.tempdir.name)
+        self.model_path = str(self.root_dir.resolve() / "mlflow_test")
         self.task = "text-classification"
         self.architecture = "distilbert-base-uncased-finetuned-sst-2-english"
         self.original_model = transformers.AutoModelForSequenceClassification.from_pretrained(self.architecture)
         self.tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(self.architecture)
         self.input_text = ["Today was an amazing day!"]
         self.hf_conf = {
             "task_type": self.task,
         }
+
+        # cleanup the model path, otherwise, the test will fail after the firsr run.
+        shutil.rmtree(self.model_path, ignore_errors=True)
         aml_mlflow.hftransformers.save_model(
             self.original_model,
             self.model_path,
             tokenizer=self.tokenizer,
             config=self.original_model.config,
             hf_conf=self.hf_conf,
         )
@@ -51,14 +59,15 @@
             olive_model(input_ids=encoded_input["input_ids"], attention_mask=encoded_input["attention_mask"])
             .logits.argmax()
             .item()
         )
         olive_predict_result = [olive_model.config.id2label[olive_result]]
 
         assert mlflow_predict_result == olive_predict_result
+        self.tempdir.cleanup()
 
 
 class TestPyTorchHFModel(unittest.TestCase):
     def setup(self):
         # hf config values
         self.task = "text-classification"
         self.model_class = "DistilBertForSequenceClassification"
```

## test/unit_test/passes/common/test_user_script.py

```diff
@@ -3,34 +3,35 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import tempfile
 
 import pytest
 from pydantic import ValidationError
 
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.passes.onnx import OrtPerfTuning
 
 
 class TestUserScriptConfig:
     def test_no_config(self):
         config = {}
-        config_class, config = OrtPerfTuning.generate_search_space(config, True)
-        assert config_class and config
+        config = OrtPerfTuning.generate_search_space(DEFAULT_CPU_ACCELERATOR, config, True)
+        assert config
 
     def test_string_config(self):
         config = {"dataloader_func": "dataloader_func"}
         with pytest.raises(ValidationError):
-            OrtPerfTuning.generate_search_space(config, True)
+            OrtPerfTuning.generate_search_space(DEFAULT_CPU_ACCELERATOR, config, True)
 
     def test_object_config(self):
         def dataloader_func():
             return
 
         config = {"dataloader_func": dataloader_func}
-        config_class, config = OrtPerfTuning.generate_search_space(config, True)
-        assert config_class and config
+        config = OrtPerfTuning.generate_search_space(DEFAULT_CPU_ACCELERATOR, config, True)
+        assert config
 
     def test_with_user_script(self):
         user_script_file = tempfile.NamedTemporaryFile(delete=False, suffix=".py")
         config = {"dataloader_func": "dataloader_func", "user_script": user_script_file.name}
-        config_class, config = OrtPerfTuning.generate_search_space(config, True)
-        assert config_class and config
+        config = OrtPerfTuning.generate_search_space(DEFAULT_CPU_ACCELERATOR, config, True)
+        assert config
```

## test/unit_test/passes/onnx/test_transformer_optimization.py

```diff
@@ -5,31 +5,30 @@
 import tempfile
 from copy import deepcopy
 from pathlib import Path
 from test.unit_test.utils import get_onnx_model
 
 from onnxruntime.transformers.fusion_options import FusionOptions
 
-from olive.passes.olive_pass import create_pass_from_dict
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.passes.onnx import OrtTransformersOptimization
 from olive.passes.onnx.common import get_external_data_config
 from olive.systems.local import LocalSystem
 
 
 def test_fusion_options():
     config = {"model_type": "bart", "optimization_options": {"use_multi_head_attention": True}}
-    config_class, config = OrtTransformersOptimization.generate_search_space(config, True)
-    transformer_optimization = OrtTransformersOptimization(config_class, config)
+    config = OrtTransformersOptimization.generate_search_space(DEFAULT_CPU_ACCELERATOR, config, True)
+    transformer_optimization = OrtTransformersOptimization(DEFAULT_CPU_ACCELERATOR, config, True)
     run_config = deepcopy(config)
     del (
         run_config["float16"],
         run_config["input_int32"],
         run_config["keep_io_types"],
         run_config["force_fp32_ops"],
-        run_config["target_provider"],
     )
     for key in get_external_data_config():
         del run_config[key]
     transformer_optimization._set_fusion_options(run_config)
     olive_fusion_options = run_config["optimization_options"]
 
     ort_fusion_options = FusionOptions("bart")
@@ -40,13 +39,15 @@
 
 
 def test_ort_transformer_optimization_pass():
     # setup
     local_system = LocalSystem()
     input_model = get_onnx_model()
     config = {"model_type": "bert"}
-    p = create_pass_from_dict(OrtTransformersOptimization, config, disable_search=True)
+
+    config = OrtTransformersOptimization.generate_search_space(DEFAULT_CPU_ACCELERATOR, config, disable_search=True)
+    p = OrtTransformersOptimization(DEFAULT_CPU_ACCELERATOR, config, True)
     with tempfile.TemporaryDirectory() as tempdir:
         output_folder = str(Path(tempdir) / "onnx")
 
         # execute
         local_system.run_pass(p, input_model, output_folder)
```

## test/unit_test/passes/openvino/test_openvino_quantization.py

```diff
@@ -5,14 +5,15 @@
 import tempfile
 from pathlib import Path
 
 import torch
 from torchvision import transforms
 from torchvision.datasets import CIFAR10
 
+from olive.hardware import AcceleratorSpec
 from olive.model import PyTorchModel
 from olive.passes.olive_pass import create_pass_from_dict
 from olive.passes.openvino.conversion import OpenVINOConversion
 from olive.passes.openvino.quantization import OpenVINOQuantization
 from olive.systems.local import LocalSystem
 
 
@@ -29,15 +30,20 @@
             "algorithms": [
                 {
                     "name": "DefaultQuantization",
                     "params": {"target_device": "CPU", "preset": "performance", "stat_subset_size": 500},
                 }
             ],
         }
-        p = create_pass_from_dict(OpenVINOQuantization, config, disable_search=True)
+        p = create_pass_from_dict(
+            OpenVINOQuantization,
+            config,
+            disable_search=True,
+            accelerator_spec=AcceleratorSpec("cpu", "OpenVINOExecutionProvider"),
+        )
         output_folder = str(Path(tempdir) / "quantized")
 
         # execute
         quantized_model = local_system.run_pass(p, ov_model, output_folder)
 
         # assert
         assert Path(quantized_model.model_path).exists()
@@ -55,15 +61,20 @@
         model_loader=lambda torch_hub_model_path: torch.hub.load(torch_hub_model_path, pytorch_hub_model_name),
         model_path=torch_hub_model_path,
     )
     openvino_conversion_config = {
         "input_shape": [1, 3, 32, 32],
     }
 
-    p = create_pass_from_dict(OpenVINOConversion, openvino_conversion_config, disable_search=True)
+    p = create_pass_from_dict(
+        OpenVINOConversion,
+        openvino_conversion_config,
+        disable_search=True,
+        accelerator_spec=AcceleratorSpec("cpu", "OpenVINOExecutionProvider"),
+    )
     output_folder = str(Path(tempdir) / "openvino")
 
     # execute
     openvino_model = local_system.run_pass(p, pytorch_model, output_folder)
     return openvino_model
 
 
@@ -73,15 +84,15 @@
 
     class CifarDataLoader(DataLoader):
         def __init__(self, config, dataset):
             """
             Initialize config and dataset.
             :param config: created config with DATA_DIR path.
             """
-            if not isinstance(config, Dict):
+            if not isinstance(config, dict):
                 config = Dict(config)
             super().__init__(config)
             self.indexes, self.pictures, self.labels = self.load_data(dataset)
 
         def __len__(self):
             return len(self.labels)
```

## test/unit_test/passes/pytorch/test_quantization_aware_training.py

```diff
@@ -9,16 +9,21 @@
 from olive.passes.olive_pass import create_pass_from_dict
 from olive.passes.pytorch import QuantizationAwareTraining
 from olive.systems.local import LocalSystem
 
 
 def test_quantization_aware_training_pass_default():
     # setup
-    local_system = LocalSystem()
-    input_model = get_pytorch_model()
-    config = {"train_dataloader_func": create_dataloader}
-    p = create_pass_from_dict(QuantizationAwareTraining, config, disable_search=True)
+
     with tempfile.TemporaryDirectory() as tempdir:
+        local_system = LocalSystem()
+        input_model = get_pytorch_model()
+        config = {
+            "train_dataloader_func": create_dataloader,
+            "checkpoint_path": str(Path(tempdir) / "checkpoint"),
+        }
+
+        p = create_pass_from_dict(QuantizationAwareTraining, config, disable_search=True)
         output_folder = str(Path(tempdir) / "onnx")
 
         # execute
         local_system.run_pass(p, input_model, output_folder)
```

## test/unit_test/systems/test_local.py

```diff
@@ -4,15 +4,16 @@
 # --------------------------------------------------------------------------
 from test.unit_test.utils import get_accuracy_metric, get_custom_metric, get_latency_metric
 from unittest.mock import MagicMock, patch
 
 import pytest
 
 from olive.constants import Framework
-from olive.evaluator.metric import AccuracySubType, LatencySubType, MetricType
+from olive.evaluator.metric import AccuracySubType, LatencySubType, MetricResult, MetricType, joint_metric_key
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.systems.local import LocalSystem
 
 
 class TestLocalSystem:
     @pytest.fixture(autouse=True)
     def setup(self):
         self.system = LocalSystem()
@@ -57,25 +58,36 @@
     @patch("olive.evaluator.olive_evaluator.OnnxEvaluator._evaluate_custom")
     def test_evaluate_model(
         self, mock_evaluate_custom, mock_evaluate_latency, mock_evaluate_accuracy, mock_get_user_config, metric
     ):
         # setup
         olive_model = MagicMock()
         olive_model.framework = Framework.ONNX
-        expected_res = "0.382715310"
+        expected_res = MetricResult.parse_obj(
+            {
+                sub_metric.name: {
+                    "value": 0.382715310,
+                    "priority": sub_metric.priority,
+                    "higher_is_better": sub_metric.higher_is_better,
+                }
+                for sub_metric in metric.sub_types
+            }
+        )
         mock_evaluate_custom.return_value = expected_res
         mock_evaluate_latency.return_value = expected_res
         mock_evaluate_accuracy.return_value = expected_res
         mock_get_user_config.return_value = (None, None, None)
 
         # execute
-        actual_res = self.system.evaluate_model(olive_model, [metric])[metric.name]
+        actual_res = self.system.evaluate_model(olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
 
         # assert
         if metric.type == MetricType.ACCURACY:
-            mock_evaluate_accuracy.called_once_with(olive_model, metric, None, self.system.device, None)
-        elif metric.type == MetricType.LATENCY:
-            mock_evaluate_latency.called_once_with(olive_model, metric, None, self.system.device, None)
-        elif metric.type == MetricType.CUSTOM:
-            mock_evaluate_custom.called_once_with(olive_model, metric, None, None, self.system.device, None)
-
-        assert actual_res == expected_res
+            mock_evaluate_accuracy.called_once_with(olive_model, metric, None, "cpu", "CPUExecutionProvider")
+        if metric.type == MetricType.LATENCY:
+            mock_evaluate_latency.called_once_with(olive_model, metric, None, "cpu", "CPUExecutionProvider")
+        if metric.type == MetricType.CUSTOM:
+            mock_evaluate_custom.called_once_with(olive_model, metric, None, None, "cpu", "CPUExecutionProvider")
+
+        joint_keys = [joint_metric_key(metric.name, sub_metric.name) for sub_metric in metric.sub_types]
+        for joint_key in joint_keys:
+            assert actual_res[joint_key].value == 0.38271531
```

## test/unit_test/systems/azureml/test_aml_system.py

```diff
@@ -1,26 +1,31 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import json
 import os
+import shutil
 import tempfile
 from pathlib import Path
-from test.unit_test.utils import get_accuracy_metric, get_latency_metric, get_pytorch_model
+from test.unit_test.utils import ONNX_MODEL_PATH, get_accuracy_metric, get_latency_metric, get_pytorch_model
 from unittest.mock import MagicMock, Mock, patch
 
 import pytest
 from azure.ai.ml import Input, Output
 from azure.ai.ml.constants import AssetTypes
 
 from olive.azureml.azureml_client import AzureMLClientConfig
-from olive.evaluator.metric import AccuracySubType, LatencySubType
+from olive.evaluator.metric import AccuracySubType, LatencySubType, MetricResult
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.model import ModelStorageKind, ONNXModel
 from olive.passes.olive_pass import create_pass_from_dict
 from olive.passes.onnx.conversion import OnnxConversion
+from olive.systems.azureml.aml_evaluation_runner import main as aml_evaluation_runner_main
+from olive.systems.azureml.aml_pass_runner import main as aml_pass_runner_main
 from olive.systems.azureml.aml_system import AzureMLSystem
 from olive.systems.common import AzureMLDockerConfig
 
 
 class TestAzureMLSystem:
     @pytest.fixture(autouse=True)
     @patch("olive.systems.azureml.aml_system.Environment")
@@ -61,24 +66,26 @@
         olive_model = get_pytorch_model()
         output_folder = Path(__file__).absolute().parent / "output_metrics"
         mock_tempdir.return_value.__enter__.return_value = output_folder
         ml_client = MagicMock()
         self.system.azureml_client_config.create_client.return_value = ml_client
 
         # execute
-        res = self.system.evaluate_model(olive_model, [metric])[metric.name]
+        res = self.system.evaluate_model(olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
 
         # assert
-        mock_create_pipeline.assert_called_once_with(output_folder, olive_model, [metric])
+        mock_create_pipeline.assert_called_once_with(output_folder, olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
         ml_client.jobs.stream.assert_called_once()
         assert mock_retry_func.call_count == 2
         if metric.name == "accuracy":
-            assert res == 0.99618
+            for sub_type in metric.sub_types:
+                assert res.get_value(metric.name, sub_type.name) == 0.99618
         if metric.name == "latency":
-            assert res == 0.031415
+            for sub_type in metric.sub_types:
+                assert res.get_value(metric.name, sub_type.name) == 0.031415
 
     @patch("olive.systems.azureml.aml_system.shutil.copy")
     @patch("olive.systems.azureml.aml_system.retry_func")
     @patch("olive.systems.azureml.aml_system.AzureMLSystem._create_pipeline_for_pass")
     @patch("olive.systems.azureml.aml_system.tempfile.TemporaryDirectory")
     def test_run_pass(self, mock_tempdir, mock_create_pipeline, mock_retry_func, mock_copy):
         # setup
@@ -196,35 +203,43 @@
     def test__create_metric_component(self, mock_create_metric_args, mock_command, mock_copy, model_storage_kind):
         # setup
         tem_dir = Path(".")
         code_path = tem_dir / "code"
         metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE)
         metric.user_config = {}
         model_args = {"input": Input(type=AssetTypes.URI_FILE, path="path")}
-        metric_type = f"{metric.type}-{metric.sub_type}"
+        sub_type_name = ",".join([st.name for st in metric.sub_types])
+        metric_type = f"{metric.type}-{sub_type_name}"
         model_inputs = {
             "model_config": Input(type=AssetTypes.URI_FILE),
             "model_path": Input(type=AssetTypes.CUSTOM_MODEL)
             if model_storage_kind == ModelStorageKind.AzureMLModel
             else Input(type=AssetTypes.URI_FOLDER, optional=True),
             "model_script": Input(type=AssetTypes.URI_FILE, optional=True),
             "model_script_dir": Input(type=AssetTypes.URI_FOLDER, optional=True),
         }
         metric_inputs = {
             "metric_config": Input(type=AssetTypes.URI_FILE),
             "metric_user_script": Input(type=AssetTypes.URI_FILE, optional=True),
             "metric_script_dir": Input(type=AssetTypes.URI_FOLDER, optional=True),
             "metric_data_dir": Input(type=AssetTypes.URI_FOLDER, optional=True),
         }
-        inputs = {**model_inputs, **metric_inputs}
+        accelerator_config_path = tem_dir / "accelerator_config.json"
+        inputs = {
+            **model_inputs,
+            **metric_inputs,
+            "accelerator_config": Input(type=AssetTypes.URI_FILE),
+        }
         expected_res = MagicMock()
         mock_command.return_value.return_value = expected_res
 
         # execute
-        actual_res = self.system._create_metric_component(tem_dir, metric, model_args, model_storage_kind)
+        actual_res = self.system._create_metric_component(
+            tem_dir, metric, model_args, model_storage_kind, accelerator_config_path
+        )
 
         # assert
         assert actual_res == expected_res
         mock_command.assert_called_once_with(
             name=metric_type,
             display_name=metric_type,
             description=f"Run olive {metric_type} evaluation",
@@ -248,7 +263,253 @@
             if input.optional:
                 parameters.append(f"$[[--{param} ${{{{inputs.{param}}}}}]]")
             else:
                 parameters.append(f"--{param} ${{{{inputs.{param}}}}}")
         parameters.append("--pipeline_output ${{outputs.pipeline_output}}")
 
         return f"python {script_name} {' '.join(parameters)}"
+
+    @patch("olive.evaluator.olive_evaluator.OliveEvaluator.evaluate")
+    def test_aml_evaluation_runner(self, mock_evaluate, tmp_path):
+        mock_evaluate.return_value = MetricResult.parse_obj(
+            {"accuracy-accuracy_score": {"value": 0.5, "priority": 1, "higher_is_better": True}}
+        )
+
+        # create model_config.json
+        model_config = {
+            "config": {
+                "dummy_inputs_func": None,
+                "hf_config": {
+                    "components": None,
+                    "dataset": {
+                        "batch_size": 1,
+                        "data_name": "glue",
+                        "input_cols": ["sentence1", "sentence2"],
+                        "label_cols": ["label"],
+                        "split": "validation",
+                        "subset": "mrpc",
+                    },
+                    "model_class": None,
+                    "model_name": "Intel/bert-base-uncased-mrpc",
+                    "task": "text-classification",
+                    "use_ort_implementation": False,
+                },
+                "io_config": {
+                    "dynamic_axes": {
+                        "attention_mask": {"0": "batch_size", "1": "seq_length"},
+                        "input_ids": {"0": "batch_size", "1": "seq_length"},
+                        "token_type_ids": {"0": "batch_size", "1": "seq_length"},
+                    },
+                    "input_names": ["input_ids", "attention_mask", "token_type_ids"],
+                    "input_shapes": [[1, 128], [1, 128], [1, 128]],
+                    "input_types": ["int64", "int64", "int64"],
+                    "output_names": ["output"],
+                    "output_shapes": None,
+                    "output_types": None,
+                    "string_to_int_dim_params": None,
+                },
+                "model_loader": None,
+                "model_path": None,
+                "model_script": None,
+                "model_storage_kind": "folder",
+                "name": None,
+                "script_dir": None,
+                "version": None,
+            },
+            "type": "PyTorchModel",
+        }
+
+        with open(tmp_path / "model_config.json", "w") as f:
+            json.dump(model_config, f)
+
+        # create model.pt
+        # create metrics_config.json
+        metrics_config = {
+            "data_config": {
+                "components": {
+                    "dataloader": {
+                        "name": "default_dataloader",
+                        "params": {"batch_size": 1},
+                        "type": "default_dataloader",
+                    },
+                    "load_dataset": {
+                        "name": "huggingface_dataset",
+                        "params": {"data_name": "glue", "split": "validation", "subset": "mrpc"},
+                        "type": "huggingface_dataset",
+                    },
+                    "post_process_data": {
+                        "name": "text_classification_post_process",
+                        "params": {},
+                        "type": "text_classification_post_process",
+                    },
+                    "pre_process_data": {
+                        "name": "huggingface_pre_process",
+                        "params": {
+                            "input_cols": ["sentence1", "sentence2"],
+                            "label_cols": ["label"],
+                            "model_name": "Intel/bert-base-uncased-mrpc",
+                        },
+                        "type": "huggingface_pre_process",
+                    },
+                },
+                "default_components": {
+                    "dataloader": {"name": "default_dataloader", "params": {}, "type": "default_dataloader"},
+                    "load_dataset": {"name": "huggingface_dataset", "params": {}, "type": "huggingface_dataset"},
+                    "post_process_data": {
+                        "name": "text_classification_post_process",
+                        "params": {},
+                        "type": "text_classification_post_process",
+                    },
+                    "pre_process_data": {
+                        "name": "huggingface_pre_process",
+                        "params": {},
+                        "type": "huggingface_pre_process",
+                    },
+                },
+                "default_components_type": {
+                    "dataloader": "default_dataloader",
+                    "load_dataset": "huggingface_dataset",
+                    "post_process_data": "text_classification_post_process",
+                    "pre_process_data": "huggingface_pre_process",
+                },
+                "name": "_default_huggingface_dc",
+                "params_config": {
+                    "batch_size": 1,
+                    "data_name": "glue",
+                    "input_cols": ["sentence1", "sentence2"],
+                    "label_cols": ["label"],
+                    "model_name": "Intel/bert-base-uncased-mrpc",
+                    "split": "validation",
+                    "subset": "mrpc",
+                    "task_type": "text-classification",
+                },
+                "type": "HuggingfaceContainer",
+            },
+            "name": "result",
+            "sub_types": [
+                {
+                    "name": "accuracy_score",
+                }
+            ],
+            "type": "accuracy",
+            "user_config": {
+                "batch_size": 1,
+                "data_dir": None,
+                "dataloader_func": None,
+                "inference_settings": None,
+                "input_names": None,
+                "input_shapes": None,
+                "input_types": None,
+                "post_processing_func": None,
+                "script_dir": None,
+                "user_script": None,
+            },
+        }
+
+        with open(tmp_path / "metrics_config.json", "w") as f:
+            json.dump(metrics_config, f)
+
+        # create accelerator_config.json
+        accelerator_config = {"accelerator_type": "cpu", "execution_provider": "CPUExecutionProvider"}
+        with open(tmp_path / "accelerator_config.json", "w") as f:
+            json.dump(accelerator_config, f)
+
+        ouptut_dir = tmp_path / "pipeline_output"
+        ouptut_dir.mkdir()
+
+        args = [
+            "--model_config",
+            str(tmp_path / "model_config.json"),
+            "--metric_config",
+            str(tmp_path / "metrics_config.json"),
+            "--accelerator_config",
+            str(tmp_path / "accelerator_config.json"),
+            "--pipeline_output",
+            str(ouptut_dir),
+        ]
+        aml_evaluation_runner_main(args)
+        mock_evaluate.assert_called_once()
+
+    @patch("olive.passes.onnx.conversion.OnnxConversion.run")
+    def test_pass_runner(self, mock_conversion_run, tmp_path):
+        model_config = {
+            "config": {
+                "dummy_inputs_func": None,
+                "hf_config": {
+                    "components": None,
+                    "dataset": {
+                        "batch_size": 1,
+                        "data_name": "glue",
+                        "input_cols": ["sentence1", "sentence2"],
+                        "label_cols": ["label"],
+                        "split": "validation",
+                        "subset": "mrpc",
+                    },
+                    "model_class": None,
+                    "model_name": "Intel/bert-base-uncased-mrpc",
+                    "task": "text-classification",
+                    "use_ort_implementation": False,
+                },
+                "io_config": {
+                    "dynamic_axes": {
+                        "attention_mask": {"0": "batch_size", "1": "seq_length"},
+                        "input_ids": {"0": "batch_size", "1": "seq_length"},
+                        "token_type_ids": {"0": "batch_size", "1": "seq_length"},
+                    },
+                    "input_names": ["input_ids", "attention_mask", "token_type_ids"],
+                    "input_shapes": [[1, 128], [1, 128], [1, 128]],
+                    "input_types": ["int64", "int64", "int64"],
+                    "output_names": ["output"],
+                    "output_shapes": None,
+                    "output_types": None,
+                    "string_to_int_dim_params": None,
+                },
+                "model_loader": None,
+                "model_path": None,
+                "model_script": None,
+                "model_storage_kind": "folder",
+                "name": None,
+                "script_dir": None,
+                "version": None,
+            },
+            "type": "PyTorchModel",
+        }
+        pass_config = {
+            "accelerator": {"accelerator_type": "cpu", "execution_provider": "CPUExecutionProvider"},
+            "config": {
+                "all_tensors_to_one_file": True,
+                "external_data_name": None,
+                "save_as_external_data": False,
+                "script_dir": None,
+                "target_opset": 13,
+                "user_script": None,
+            },
+            "disable_search": True,
+            "type": "OnnxConversion",
+        }
+
+        with open(tmp_path / "model_config.json", "w") as f:
+            json.dump(model_config, f)
+
+        with open(tmp_path / "pass_config.json", "w") as f:
+            json.dump(pass_config, f)
+
+        ouptut_dir = tmp_path / "pipeline_output"
+        ouptut_dir.mkdir()
+        shutil.copy(ONNX_MODEL_PATH, ouptut_dir)
+        mock_conversion_run.return_value = ONNXModel(ouptut_dir / ONNX_MODEL_PATH.name)
+
+        args = [
+            "--model_config",
+            str(tmp_path / "model_config.json"),
+            "--pass_config",
+            str(tmp_path / "pass_config.json"),
+            "--pipeline_output",
+            str(ouptut_dir),
+            "--pass_accelerator_type",
+            "cpu",
+            "--pass_execution_provider",
+            "CPUExecutionProvider",
+        ]
+
+        aml_pass_runner_main(args)
+        mock_conversion_run.assert_called_once()
```

## test/unit_test/systems/docker/test_docker_system.py

```diff
@@ -5,15 +5,16 @@
 import tempfile
 from pathlib import Path
 from test.unit_test.utils import get_accuracy_metric, get_pytorch_model
 from unittest.mock import MagicMock, patch
 
 import pytest
 
-from olive.evaluator.metric import AccuracySubType
+from olive.evaluator.metric import AccuracySubType, joint_metric_key
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.systems.common import LocalDockerConfig
 from olive.systems.docker.docker_system import DockerSystem
 
 
 class TestDockerSystem:
     @pytest.fixture(autouse=True)
     def setup(self):
@@ -150,15 +151,15 @@
         eval_command = "eval_command"
         mock_create_evaluate_command.return_value = eval_command
 
         run_command = {"key": "val"}
         mock_create_run_command.return_value = run_command
 
         # execute
-        actual_res = docker_system.evaluate_model(olive_model, [metric])[metric.name]
+        actual_res = docker_system.evaluate_model(olive_model, [metric], DEFAULT_CPU_ACCELERATOR)
 
         # assert
         mock_create_eval_script_mount.called_once_with(container_root_path)
         mock_create_model_mount.called_once_with(mock_copy, container_root_path)
         vol_list = [eval_file_mount_str] + model_mount_str_list
         mock_create_metric_volumes_list.called_once_with(mock_copy, container_root_path, vol_list)
         mock_create_config_file.called_once_with(tempdir, mock_copy, mock_copy, container_root_path)
@@ -167,8 +168,10 @@
             eval_file_mount_path, model_mount_path, config_mount_path, output_mount_path, eval_output_name
         )
         mock_create_run_command.called_once_with(docker_system.run_params)
         volumes_list.append(config_file_mount_str)
         volumes_list.append(output_mount_str)
         mock_docker_client.containers.run.call_once_with(docker_system.image, eval_command, volumes_list, **run_command)
 
-        assert actual_res == 0.99618
+        for sub_type in metric.sub_types:
+            joint_key = joint_metric_key(metric.name, sub_type.name)
+            assert actual_res[joint_key].value == 0.99618
```

## test/unit_test/systems/python_environment/test_python_environment_system.py

```diff
@@ -5,80 +5,122 @@
 import sys
 from pathlib import Path
 from test.unit_test.utils import get_accuracy_metric, get_latency_metric, get_onnx_model
 from unittest.mock import patch
 
 import pytest
 
-from olive.evaluator.metric import AccuracySubType, LatencySubType
+from olive.evaluator.metric import AccuracySubType, LatencySubType, MetricResult, MetricType, joint_metric_key
 from olive.evaluator.olive_evaluator import OliveEvaluatorFactory
+from olive.hardware import DEFAULT_CPU_ACCELERATOR
 from olive.systems.python_environment import PythonEnvironmentSystem
 
 
 class TestPythonEnvironmentSystem:
     @pytest.fixture(autouse=True)
     def setup(self):
         # use the current python environment as the test environment
         executable_parent = Path(sys.executable).parent.resolve().as_posix()
         self.system = PythonEnvironmentSystem(executable_parent)
 
     def test_available_eps(self):
         import onnxruntime as ort
 
-        assert set(self.system.get_available_eps()) == set(ort.get_available_providers())
+        assert set(self.system.get_supported_execution_providers()) == set(ort.get_available_providers())
 
     @patch("olive.systems.python_environment.PythonEnvironmentSystem.evaluate_accuracy")
     @patch("olive.systems.python_environment.PythonEnvironmentSystem.evaluate_latency")
     def test_evaluate_model(self, mock_evaluate_latency, mock_evaluate_accuracy):
         # setup
         model = get_onnx_model()
         metrics = [get_accuracy_metric(AccuracySubType.ACCURACY_SCORE), get_latency_metric(LatencySubType.AVG)]
-        mock_evaluate_accuracy.return_value = 0.9
-        mock_evaluate_latency.return_value = 10
+
+        metrics_key = [
+            joint_metric_key(metric.name, sub_metric.name) for metric in metrics for sub_metric in metric.sub_types
+        ]
+
+        mock_return_value = {
+            sub_metric.name: {
+                "value": 0.9 if metric.type == MetricType.ACCURACY else 10,
+                "priority": sub_metric.priority,
+                "higher_is_better": sub_metric.higher_is_better,
+            }
+            for metric in metrics
+            for sub_metric in metric.sub_types
+        }
+
+        mock_evaluate_accuracy.return_value = MetricResult.parse_obj(
+            {AccuracySubType.ACCURACY_SCORE: mock_return_value[AccuracySubType.ACCURACY_SCORE]}
+        )
+        mock_evaluate_latency.return_value = MetricResult.parse_obj(
+            {LatencySubType.AVG: mock_return_value[LatencySubType.AVG]}
+        )
 
         # execute
-        res = self.system.evaluate_model(model, metrics)
+        res = self.system.evaluate_model(model, metrics, DEFAULT_CPU_ACCELERATOR)
 
         # assert
-        assert res == {"accuracy": 0.9, "latency": 10}
+        assert res[metrics_key[0]].value == 0.9
+        assert res[metrics_key[1]].value == 10
         assert mock_evaluate_accuracy.call_once_with(model, metrics[0])
         assert mock_evaluate_latency.call_once_with(model, metrics[1])
 
     @pytest.mark.skip(reason="Unable to patch static function calls from another function")
     @patch("olive.evaluator.olive_evaluator.OliveEvaluator.compute_accuracy")
     @patch("olive.systems.python_environment.python_environment_system.OliveEvaluator.compute_accuracy")
     def test_evaluate_accuracy(self, mock_compute_accuracy1, mock_compute_accuracy2):
         # setup
         model = get_onnx_model()
-        metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE, False)
-        mock_compute_accuracy1.return_value = 0.9
-        mock_compute_accuracy2.return_value = 0.9
+        metric = get_accuracy_metric(AccuracySubType.ACCURACY_SCORE, random_dataloader=False)
+        mock_value = MetricResult.parse_obj(
+            {
+                AccuracySubType.ACCURACY_SCORE: {
+                    "value": 0.9,
+                    "priority": 1,
+                    "higher_is_better": True,
+                }
+            }
+        )
+
+        mock_compute_accuracy1.return_value = mock_value
+        mock_compute_accuracy2.return_value = mock_value
 
         # expected result
         evaluator = OliveEvaluatorFactory.create_evaluator_for_model(model)
         expected_res = evaluator.evaluate(model, [metric])[metric.name]
 
         # execute
         actual_res = self.system.evaluate_accuracy(model, metric)
 
         # assert
-        assert actual_res == expected_res
+        assert actual_res[AccuracySubType.ACCURACY_SCORE].value == expected_res[AccuracySubType.ACCURACY_SCORE].value
         assert mock_compute_accuracy1.call_args.args[1] == mock_compute_accuracy2.call_args.args[1]
 
     @patch("olive.evaluator.olive_evaluator.OliveEvaluator.compute_latency")
     def test_evaluate_latency(self, mock_compute_latency):
         # setup
         model = get_onnx_model()
         metric = get_latency_metric(LatencySubType.AVG)
-        metric.metric_config.repeat_test_num = 5
-        mock_compute_latency.return_value = 10
+        metric_config = metric.sub_types[0].metric_config
+        metric_config.repeat_test_num = 5
+
+        mock_value = MetricResult.parse_obj(
+            {
+                LatencySubType.AVG: {
+                    "value": 10,
+                    "priority": 1,
+                    "higher_is_better": True,
+                }
+            }
+        )
 
+        mock_compute_latency.return_value = mock_value
         # expected result
         expected_res = 10
 
         # execute
         actual_res = self.system.evaluate_latency(model, metric)
 
         # assert
-        assert actual_res == expected_res
-        assert len(mock_compute_latency.call_args.args[1]) == metric.metric_config.repeat_test_num
+        assert actual_res[LatencySubType.AVG].value == expected_res
+        assert len(mock_compute_latency.call_args.args[1]) == metric_config.repeat_test_num
         assert all([latency > 0 for latency in mock_compute_latency.call_args.args[1]])
```

## Comparing `olive_ai-0.2.0.dist-info/LICENSE` & `olive_ai-0.2.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `olive_ai-0.2.0.dist-info/METADATA` & `olive_ai-0.2.1.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: olive-ai
-Version: 0.2.0
+Version: 0.2.1
 Summary: Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation.
 Home-page: https://microsoft.github.io/Olive/
 Download-URL: https://github.com/microsoft/Olive/tags
 Author: Microsoft Corporation
 Author-email: olivedevteam@microsoft.com
 License: MIT License
 Classifier: Development Status :: 3 - Alpha
@@ -47,11 +47,13 @@
 Provides-Extra: gpu
 Requires-Dist: onnxruntime-gpu ; extra == 'gpu'
 Provides-Extra: inc
 Requires-Dist: neural-compressor ; extra == 'inc'
 Provides-Extra: openvino
 Requires-Dist: openvino (==2022.3.0) ; extra == 'openvino'
 Requires-Dist: openvino-dev[onnx,tensorflow] (==2022.3.0) ; extra == 'openvino'
+Provides-Extra: optimum
+Requires-Dist: optimum ; extra == 'optimum'
 Provides-Extra: tf
 Requires-Dist: tensorflow (==1.15.0) ; extra == 'tf'
 
 Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation. Given a model and targeted hardware, Olive composes the best suitable optimization techniques to output the most efficient model(s) for inferencing on cloud or edge, while taking a set of constraints such as accuracy and latency into consideration.
```

## Comparing `olive_ai-0.2.0.dist-info/NOTICE.txt` & `olive_ai-0.2.1.dist-info/NOTICE.txt`

 * *Files identical despite different names*

## Comparing `olive_ai-0.2.0.dist-info/RECORD` & `olive_ai-0.2.1.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-olive/__init__.py,sha256=JeXDjq5jtS_5XZ6da73vAlgqliRBVo2b5_kmX06I2bs,1147
+olive/__init__.py,sha256=WvOuP_69GCWqsRqBGZZnntgpJPwfbMUWbOWtEnykVjo,608
 olive/cache.py,sha256=xWYybDcRial22-167M1_6fl8FdnSfD5q-5NgvQmM3P4,6064
-olive/constants.py,sha256=3_F-V7R6qQdEbbs0FHDdj-4UAWVZVmKlkm8gXrbRwt8,989
-olive/extra_dependencies.json,sha256=aPuZznz3G6xPf_W4MBlm4VbHeGbQWE3TdoIP0EBBqTs,469
+olive/constants.py,sha256=UFnLXf5aOzY4yZ_qA30oFC_yIgvklKjrhkBbnGgdGfA,1013
+olive/extra_dependencies.json,sha256=qUj4-RQ4tWEsWnSUu-C-hYVl6Ks0ytC9GNsLN7VWcwA,511
 olive/hf_utils.py,sha256=kIH9KBm7TQ2RidUFcrjrg3TAFkF1JAYMQIEUW-WKv0g,3758
-olive/logging.py,sha256=WpYNlKXOMc1wnI5JhYXHPWS7Z65Bv4-S5ILAFkzEweY,674
-olive/model.py,sha256=owY6XwSpXJIT3_LDHUAWWlvFVnWhKVytkkHncW1J4Rg,37681
+olive/logging.py,sha256=lIxta0wkP71QsfBDqP-zgjldVxiI9WDJsRXRar2HWyw,1210
+olive/model.py,sha256=txm-uqS62v-VIUjgz-INoyUXYIWFuqDM-a3YIEwsM-s,38321
 olive/azureml/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/azureml/azureml_client.py,sha256=-A5NiEjb0ncBx1gBONfCFXaqzR5olz8u2pWxb1TntuM,3383
 olive/common/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/common/auto_config.py,sha256=SRbhLSiCEQHfovyOPx3S23EPwen3_SEMD_3z9aUYk44,3453
-olive/common/config_utils.py,sha256=XPsJlXwsVYVnB8rPK1wvctZp6DlYfIIehT8EJcnTTtA,7158
+olive/common/config_utils.py,sha256=RBBTK-WmImxU3j7CPxzmM__7ZZbZkpJWmE32LQXDFKo,7878
 olive/common/import_lib.py,sha256=Pgr_l3HkdFoccz2R3hEhsis17zYYsKe51AjtJ8yevHY,1468
 olive/common/ort_inference.py,sha256=Y7SeN_vdHes-s4CHi2I4LWsjShfje-TTSFcmRjqpu88,2809
 olive/common/user_module_loader.py,sha256=Kvwl5WId0cqw12oSU4pZjKlfE8pJWJfMeP4oNVCdN-g,1783
 olive/common/utils.py,sha256=OzqQ0W-cx4o_hGwbsccSoAh4mPZuqaPKpes6yiSJrt8,5298
 olive/data/__init__.py,sha256=e6E8AVlCoJ-LNfbshkiyOVopkfgUmg6dRo9LH51Mafw,345
 olive/data/config.py,sha256=ncuSHAOT3nhdhGolNs2poNEimw9lgyVfWoQ7cpIhgcM,7641
 olive/data/constants.py,sha256=5TMcogTbQPhHrmSAILK0ZDr60M78g719F41jRO2Oink,1452
@@ -23,64 +23,69 @@
 olive/data/component/load_dataset.py,sha256=1cIULepF_5vXnaNAFky9eEqaYF7QHybfYskrEvX4AbM,2398
 olive/data/component/post_process_data.py,sha256=I9sNCfvxVxf_wgDRJy77s3PL3kYxiSDNqohYvD7LhPo,948
 olive/data/component/pre_process_data.py,sha256=8ZqMJGOs0aoDsXnIZnmN0igDXoXCa6y00EPR1mABQPQ,2103
 olive/data/container/__init__.py,sha256=poKzHFE5vcATW_ibY6-gr0EfbEuZt94Hyg16-5UK9pU,318
 olive/data/container/data_container.py,sha256=ZG3-VJ7L-WmqMEcdmlfv_LcoL8BG7cXCYvJ15-iiE-o,2734
 olive/data/container/huggingface_container.py,sha256=QBikrmivBNr-Ee9heCZmCZWqNzijPzceDqz5le9XnNQ,1044
 olive/engine/__init__.py,sha256=cxOEIC1JiSNFnBcueIYzKkCw3BKnv9LBFJzx7kPnDPw,345
-olive/engine/engine.py,sha256=zhcKPvFX4L7Tn4xobJvGcJf6Odz1dLEpDChnLzCt66s,35473
-olive/engine/footprint.py,sha256=WsAicQEweE7XDUNl2domJHEkeiWhUvGGJjRB1Gy9KzI,14347
+olive/engine/config.py,sha256=htCHEZ7YQlmzY64tbjBFmDeO9PaTYhQDOt9YL2adqog,1289
+olive/engine/engine.py,sha256=t2YmCYhWFXbQNe15TVbRYEzv69zhHY-lw6vBVbMdzJQ,38411
+olive/engine/footprint.py,sha256=hZsP77DvqXeDJu08uxYcC0aidRFFhBYv5TQvYSIQddQ,14617
 olive/engine/packaging/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/engine/packaging/packaging_config.py,sha256=c0MEtNZR4Qa2OjH4tWhvOpxiSRhl9ZCnTFD8F2gvLUE,598
-olive/engine/packaging/packaging_generator.py,sha256=DN78Guzvqxbc4FuA9aDN2hJaT3oMcPF3L89j6D9s4QM,11341
+olive/engine/packaging/packaging_generator.py,sha256=jTexb1NQ-Bbi6wfzX_Y7wxz49HFRFfiOs9GlZA58JHU,11782
 olive/engine/packaging/sample_code/ONNXModel/cpp/README.md,sha256=JnLAiNYgEZCvsP0rwG5rTOwn_px6XeLFjrchXUDWO4s,1588
 olive/engine/packaging/sample_code/ONNXModel/cpp/code_sample.cpp,sha256=4FejpIKRXN9-9LuWvkoFGMCgnuhRZxFA6m_9E6yrNQs,4990
 olive/engine/packaging/sample_code/ONNXModel/cs/README.md,sha256=V7tDnmXiisTAbiCec7Us-pUvI3XuJEc6EE1cO2WXt0A,1590
 olive/engine/packaging/sample_code/ONNXModel/cs/code_sample.cs,sha256=dSEn4oIUaeeDxN8gyL0hbVkR9RmCQ5nToSSchJyp4d8,4214
 olive/engine/packaging/sample_code/ONNXModel/python/README.md,sha256=oFBeSPhVUrL8oTUTb8QMxoaUgGT2PdShvCfxJG1lpnA,2304
 olive/engine/packaging/sample_code/ONNXModel/python/code_sample.py,sha256=CJRCH-9fuMbN9Q8UbO2oBZHqFYGTvLG54vHmT_tcTQY,2531
 olive/evaluator/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/evaluator/accuracy.py,sha256=_NEmQ3DYX_sTVUcFBo04Vj6VYMFd2NMwMXSZILYoHP0,3386
-olive/evaluator/metric.py,sha256=gXG1gMbh8xEUPwjMJbO-vpRVu-aoqf2Y5Ho83kSaBfg,4829
+olive/evaluator/metric.py,sha256=cmSwd-tBAhase1CS-wKZGt9IUMSZrb5i7uIWEu0QryE,6271
 olive/evaluator/metric_config.py,sha256=UortkXgRvqF0igFIFNf_hBh4Cp9ORSXQV4QWCTjiRpw,3255
-olive/evaluator/olive_evaluator.py,sha256=qranpBIg8kIUZta0KgtWWOootAGt2IVAwALhK4qjrl4,18538
+olive/evaluator/olive_evaluator.py,sha256=JqhuWHI1FF8ARHu57YnIyUSUGHxhHED4ffNbvGS1M8o,29490
+olive/hardware/__init__.py,sha256=FQyfsGe4DXxjGorkwSyr4yp2uOhi0W5XOhDIvQ8u6bQ,354
+olive/hardware/accelerator.py,sha256=LQpzgzt2-ejC1VM7yZuowQdrX9S5aqK5GX5YjC2rGUM,2975
 olive/passes/__init__.py,sha256=C5Jn-KpohWT72MiHkmMo_Ol818BFVyNqV8QhlUz7uHs,575
-olive/passes/olive_pass.py,sha256=ItkOHGkChVSi-TUPJdJbo-yvjpSX2xxOO22pKk5Wce4,16976
-olive/passes/pass_config.py,sha256=AqAu1mOF_cglibnL5IVEtR5Ws9phkokJepFTYVBo-VI,5315
-olive/passes/onnx/__init__.py,sha256=_NU0EKTJMryH_Bd-mvhnP6TBT6kf_1DKYZqw6TIpDqg,1506
-olive/passes/onnx/append_pre_post_processing_ops.py,sha256=q3MLF0uOwQbvEEzwTwBCS43O91RoyqpfpeiOz7R3xNk,4512
+olive/passes/olive_pass.py,sha256=4iVqgi7Q0i1wWzlh0Tf7Zcqgece6an-uk_6p4_huja8,18125
+olive/passes/pass_config.py,sha256=xd2ULXZWINgeBjyVDFNYC_l6ollliFq-pi9P300L-4Q,5138
+olive/passes/onnx/__init__.py,sha256=IMdXYpACazCYr1V6kT3UxztC1kj_CngP_EN-O7Wiv54,1681
+olive/passes/onnx/append_pre_post_processing_ops.py,sha256=YXES2-Pr3DtDFtcoP7swH9_TMiyaUJi5oonPl75-PpM,4600
 olive/passes/onnx/common.py,sha256=MGbkNA-60l92StUvFuOaH1ZDwiqO9jS2bplOwQf_ea4,6086
-olive/passes/onnx/conversion.py,sha256=vASzh16nn30rvlh2mK_UOw_DRSqDTiN2H9Dl86fwaXc,4818
-olive/passes/onnx/float16_conversion.py,sha256=pE4CEP8b96e1T-tBNNjYrEim4_S3eb5sgC18ywn4pxM,2861
-olive/passes/onnx/inc_quantization.py,sha256=X4WUZ2kcCGzaq-6q5T9YumKQioub9f3kksf08C8pIy0,11658
-olive/passes/onnx/insert_beam_search.py,sha256=wAgya5oPDZcjdI4M7SCpLKcqlvQCAaaon69-TXE2ySY,6355
-olive/passes/onnx/mixed_precision.py,sha256=Ed_0MHlKGnV0u_0_rs5khfC4jxQX3yDnZW5_h1awIiI,7114
-olive/passes/onnx/model_optimizer.py,sha256=J2TfBGzRV2V4L1pivhacA1oxPhM2axeyTOYui0u-OAI,5367
-olive/passes/onnx/perf_tuning.py,sha256=S3JYp06doXjVudU-sDIX91sybStsGWkFPd-POTp4LLY,12799
-olive/passes/onnx/quantization.py,sha256=lTogZ0NvvrVbfvaBQPnfHbJ2s3anSolsBIzOr9TwhjA,20463
-olive/passes/onnx/transformer_optimization.py,sha256=X_371mHXq2o7g1qbMPC9JsalAB4BFxVVMx2xDH4h7Zg,11428
-olive/passes/onnx/vitis_ai_quantization.py,sha256=EmLFOZQR-f7dM9qiot-fl7o2rftHOvjYMHtIWnQJWL0,11387
+olive/passes/onnx/conversion.py,sha256=gvQIkFIcRwhNBXgSVu5EhLAS4frYNj6q4kMTCKOIoSA,4906
+olive/passes/onnx/float16_conversion.py,sha256=iin1QfbQ8nwgLhegkPQQqrRRaCzFgpd9HlaIQB5GezA,2949
+olive/passes/onnx/inc_quantization.py,sha256=PfpScJmmFWf-oH_4QbyZ9NLjJF9NyEIKuIr3aiRwVI4,11812
+olive/passes/onnx/insert_beam_search.py,sha256=WAwO0zlZ_livhXrbn1BV4ZykQy0Jt_aJ3QY3ulUCeE8,6443
+olive/passes/onnx/mixed_precision.py,sha256=vzGXKBaOufFwxCvYv47f9JxnQQ9-uy7QsC0l3rQMCng,7202
+olive/passes/onnx/model_optimizer.py,sha256=K3qkQSR-k6YUH0vlCLLj5bcxXByUaZYQQ-KA5brz-Ko,5455
+olive/passes/onnx/optimum_conversion.py,sha256=ujdjijDZTI0tAqbRB8bi4VA1kFGcqBrYOWujKc5cr7w,1957
+olive/passes/onnx/optimum_merging.py,sha256=xobH4Jw4OVM1lnMpeqjkFZT7tFUqK7Wv9pPksQduH64,3372
+olive/passes/onnx/perf_tuning.py,sha256=JsWB2Uf23KCmkSk6QahENqMDyEdW7MXXJHZhPdtnzs8,13482
+olive/passes/onnx/quantization.py,sha256=351WxzAHlBsneyH3R0RpSaesLDLmYuNBEJCBWmFNGLg,20617
+olive/passes/onnx/transformer_optimization.py,sha256=IAFUMksGIOHXsgSAcni1CQfAzgLwtUolbhn8Go8WRss,5270
+olive/passes/onnx/vitis_ai_quantization.py,sha256=ChPte1B033J_34tbM7EtEcwgR5Dj7IT9H8A3MU5W8rE,11463
 olive/passes/onnx/vitis_ai/__init__.py,sha256=DcgGrXUD-CCGvc_b0QTUGCv0ds6zPyg_T_sUCKLnvUo,449
 olive/passes/onnx/vitis_ai/calibrate.py,sha256=xoqq0nnxn_S2OuFL4iCsqtn2A_zsqxj_gplv5ucCm00,7272
-olive/passes/onnx/vitis_ai/qdq_quantizer.py,sha256=VS0sV6pyww-AAUnru8b7Ijo16zfty9G9A_5s8JbaVfQ,7724
+olive/passes/onnx/vitis_ai/qdq_quantizer.py,sha256=HnHlGUa8_jX1biaDI9UZAgJr3u5qdxczJs-yS7q-3gs,7755
 olive/passes/onnx/vitis_ai/quant_utils.py,sha256=Fk8URhpxOiCAj_M4U8gul--zCYywbFDId1jPd_97CMk,4044
 olive/passes/onnx/vitis_ai/quantize.py,sha256=Lda9r994VVwBg1hhuWwQqQEEUE8tRUDMmEn9BSCj8LY,10774
-olive/passes/onnx/vitis_ai/refine.py,sha256=M0X_UHs-8tYtcaQ3Y_6lyzu82ODNom7ZI1ytB6SS5A0,18375
+olive/passes/onnx/vitis_ai/refine.py,sha256=YEKokJ2Y8UsGNEb15a4gNq1B1Ctflh19eXj-9cQxNcw,18348
 olive/passes/openvino/__init__.py,sha256=dgWxpxB8h0gZV6j_5qQCE-U0F9O3cVyqkmFZUKnUdYU,448
-olive/passes/openvino/conversion.py,sha256=UpTMFYFHT7NTao7aoFGzqBRqoqbdiPbVicCU5bUtYto,3901
-olive/passes/openvino/quantization.py,sha256=6Au5UXbNhCfLzhdfcfTEP2Z8dCV74vCFamSCggtlIug,4206
+olive/passes/openvino/conversion.py,sha256=DluP0TbwtLcIwcWdwtbvr34gQUqU4cphUioovvDZdPw,3989
+olive/passes/openvino/quantization.py,sha256=sE2m8g_pL5rQNBPr_DlleJCjoJKl-K7ndqRIP_ysc0E,4294
 olive/passes/pytorch/__init__.py,sha256=kBOv-wStAsDgrinWx0CRhKjoxUASAXw3jJXG-LUcrSs,375
 olive/passes/pytorch/cluster.py,sha256=vQVmFCO-Zpkvn3sdurh9vmP1ZRUn7tO7s2p0feYBnxg,7068
-olive/passes/pytorch/pytorch_lightning_utils.py,sha256=bdbZ3u1p28UC6WvUwV_MWniyNy7J4dRKkjDDZWkAr8U,974
-olive/passes/pytorch/qat_utils.py,sha256=ojNssmHQiHiR3KFjR8D0NC0BMrDIHfh_0F-E9GHnWWw,8272
-olive/passes/pytorch/quantization_aware_training.py,sha256=ZCsIYySsLtlh4xCUkMr8d46ox3wuGbq_gdBLL78RlxM,5827
+olive/passes/pytorch/pytorch_lightning_utils.py,sha256=K8GnHRzbYDoK7r8UVN9GD26ueJIBLqeHeBhvLbYoAxM,1044
+olive/passes/pytorch/qat_utils.py,sha256=LrOhJ3LSLCdw8BoXfPG0TXFe6c67P8mHAinXykNGeys,8382
+olive/passes/pytorch/quantization_aware_training.py,sha256=3CRCZFT5FEOlsVk1Dgw_plzyy4FFW1lfOjq9WSaCzCY,6035
 olive/passes/snpe/__init__.py,sha256=glmLIaMVKkjct_PqWk7x9Av-twMYU08ez9eJyNdUNgM,501
-olive/passes/snpe/conversion.py,sha256=x6SNAxuh3CL7mmdeK1t0zkGkyR6_nooSSJpIj0E61wA,4670
-olive/passes/snpe/quantization.py,sha256=PZA_di35ky36qT7B5KYsRZ8CWtPetPnUWRCiKYzLJkg,3521
-olive/passes/snpe/snpe_to_onnx.py,sha256=sv6Vj0VM-LDVZmLGiZCa_QeiNAUFFPsFT_KbK7yrM0k,2299
+olive/passes/snpe/conversion.py,sha256=DQLOkNvTVgfh87vhJHcrb7yUGUHG06Xr432TYwaFL7w,4758
+olive/passes/snpe/quantization.py,sha256=yNnRNWq6jp3sYIJVboPxFUMpmVJdyDqVeToR1AKmluc,3609
+olive/passes/snpe/snpe_to_onnx.py,sha256=aWfIvoHOUpCoCaYAfgxmk6sElTiv9pbikpTc9_VVhic,2387
 olive/passes/utils/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/passes/utils/whisper_prepost.py,sha256=L0ZvPgvMnJzFv6-GmuYM7344dmGO4YG42FmwtesNtYM,10242
 olive/snpe/__init__.py,sha256=rLvysVmSAT_xswOnM9JEp8hrwD4ZaW1niIE6OjZ8oBc,432
 olive/snpe/configure.py,sha256=WTzre-aTqUo1gmlUhfhBnUpo60SNPO4hUbWLFUWDBdo,2496
 olive/snpe/constants.py,sha256=goFGksLBaLXga7qTBaTfp2DROpL89dzyG6f9nJzc-Kg,1079
 olive/snpe/copy_libcdsprpc.ps1,sha256=uJfKdyPWGWZyVHy_pvjhGzYK1pKUocI5LTAo9_QkkX0,1046
 olive/snpe/create_python36_env.sh,sha256=ZjHN2qiQrPamGbpgr684zpazoape_uEDjF-TlgF7Qx4,1142
@@ -91,118 +96,120 @@
 olive/snpe/tools/inference.py,sha256=Qdr6JtBjkqw9QepAR6bsV3EWmqBdELIGS1iEoJSbFl4,17791
 olive/snpe/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 olive/snpe/utils/adb.py,sha256=4sBh533JJ7L-YRgcmGOHBbwkxhM7oSwfaXIVfVepd5Y,6759
 olive/snpe/utils/input_list.py,sha256=g0eE42qezkBAjPDcI4NqX_-OzYYyDSnD_PZt2d-LT48,8643
 olive/snpe/utils/local.py,sha256=ZFrHSkLfCNDu3KiQ0cqRjQNQz13u2BeqqYysz8pJR0k,4867
 olive/strategy/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/strategy/search_parameter.py,sha256=vCKWVQd1rv7w1dJNkRmTpyeiOg3nZy_G5ksLfewv9RU,11885
-olive/strategy/search_results.py,sha256=mYus_twnlbsncOAVLD3VWPO1XiBIBChEHEkxF0C5xao,5731
+olive/strategy/search_results.py,sha256=ZDWqdnKiNhjoz4h_4zz_CT4txT_TTQK6NH7f43FK4mI,5736
 olive/strategy/search_space.py,sha256=c7LZ1STudG8MtYemv271NFJsTlTYPGS8Qp4w3QmdCJw,4876
-olive/strategy/search_strategy.py,sha256=L4rMXCJT80MYk6TmBZWUJy6WtAlt7Ky_FW2KLSjf3Vg,10608
+olive/strategy/search_strategy.py,sha256=zUCWFbOSTZlXOw_-X9OAFIAS-sTXTnmsZbDcwoR6eNA,10731
 olive/strategy/utils.py,sha256=WSgSQYxDDypz7luegGXHnDellyqcN9pxXsc1oDUdzoU,2911
 olive/strategy/search_algorithm/__init__.py,sha256=v6oux-gPf1TAmiP9YZrNsqReLiVTpaxcaNP-JYmIrX4,598
 olive/strategy/search_algorithm/exhaustive.py,sha256=2lAgo65gZSIePXLAIKQJMa28SLVBcavu8DQ0Lfu6CpI,1153
-olive/strategy/search_algorithm/optuna_sampler.py,sha256=rYd5h5fqBn-D6XYHCmUQy5HM5C0m6JtI6IQYNSaOvEw,4403
+olive/strategy/search_algorithm/optuna_sampler.py,sha256=nbtj5PfGfRaLUl-LNF0KxI3Q2a5ee4PVLW_YV0PmJa0,4420
 olive/strategy/search_algorithm/random_sampler.py,sha256=Oe2uNYceFMOMieXk0c9Px0i4u9r5nl4NuSBzytm5OeE,2498
 olive/strategy/search_algorithm/search_algorithm.py,sha256=6aU-p0sfhgs0i60XRMSFkqU1WB74Xqu68i4dRzncyVE,2429
 olive/strategy/search_algorithm/tpe_sampler.py,sha256=TPDRq8cp34C5iiMcTtU3g_c7KDdsv1Cx_yrJQs1Fj1U,1876
 olive/systems/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-olive/systems/common.py,sha256=V6OkY30Ub4P8A2Mfghq_-FsSQUiffGPIUdtyVJb9lwc,2079
-olive/systems/local.py,sha256=Fex7UlRDNGF7I6njdgcVtC9k3jC7DctjUXriK2ayvCY,1535
-olive/systems/olive_system.py,sha256=XuZhy6z0UWZX48yuSG5pq2DoEvDXjLKgW1pmZ8UNlww,1192
-olive/systems/system_config.py,sha256=KbeklJlc2Lzzfx5yyVsN_ExcJdnMR5z7Il4KBY6vCsI,3618
+olive/systems/common.py,sha256=LLHhqJoM7Zes2zyL-lYkn2dqdwweqCOZMjDqibcUGtM,1970
+olive/systems/local.py,sha256=F-aQ5Y3eJgJlne8i9bt5du6XT1-VVm5lgbekEPmfhBw,1975
+olive/systems/olive_system.py,sha256=mHVwETXgmI5XeHQz_IfCfCkvhzjDcLyiPq3vEVNIBdE,1331
+olive/systems/system_config.py,sha256=TRnDpBHPUOSJ7aaDSU8wLSjezHFns74UK-P7GY7qhiU,3581
 olive/systems/utils.py,sha256=XM2pAKg8d9MeQQ9zdOESjcXjkiTyWkCx3wDHY-E7yi8,1507
 olive/systems/azureml/__init__.py,sha256=XMk00ngVPkLq8SAjhtDDXDUC6FDin5K_s_eu1FGi3qA,411
-olive/systems/azureml/aml_evaluation_runner.py,sha256=GzPa0Go0M5gid9vKxo05nFzywp71yMrb5Uw31dQA9V4,2076
-olive/systems/azureml/aml_pass_runner.py,sha256=C_wzCL78-218d5tvqFGsgmjpCpfzHPnFXz7qfl00xiE,4179
-olive/systems/azureml/aml_system.py,sha256=92jArMxfu-xTXtnaiLsoF24gcESP2iqO6W8kkHDbSbQ,21637
-olive/systems/docker/Dockerfile,sha256=AeCKubsZCoYb3vQSoxTMSYzVURr6mIb7SLHkInap7vI,697
+olive/systems/azureml/aml_evaluation_runner.py,sha256=Aerig9GvNdwIfsCpggFOqYVWALA7bZhMXhvSPi9IePA,2615
+olive/systems/azureml/aml_pass_runner.py,sha256=04rWoZKNzY3toD-jFlQ_m1k-jA5MUK_BJEJz6GdLBNo,4573
+olive/systems/azureml/aml_system.py,sha256=1_CoCnm8UAjsn0AKt2FlL-eiBQxN3xwyKMK1pMemRCo,22712
+olive/systems/docker/Dockerfile,sha256=TI9dJrPToNbV_SOHTyQsMmqVhmIC0669ajZL5J50fDM,697
 olive/systems/docker/__init__.py,sha256=nrCMDPi17qCk3iGuH-Hbpq9JIOYkR8nOdvWsfXy9uqM,407
 olive/systems/docker/dev_mount_cleanup.py,sha256=be00U9GViQ3vA7U1cosVxoJZvlGG8ATMPHhS651h-Sw,651
-olive/systems/docker/docker_system.py,sha256=z-holrqibpKQw7Raiw2oAo63rdxKePXap0464Svie7w,7426
-olive/systems/docker/eval.py,sha256=zvXXPT1-ieGtsrkDaepxeBVo7lR-7Ena9fSekkf4lrs,1776
+olive/systems/docker/docker_system.py,sha256=HGBwM0yE27srHwKvfEUzM5jOcb5ugdtyXfThB22F49k,7595
+olive/systems/docker/eval.py,sha256=lj9R3GX81NTts_b--i6oslgRD9al5JJ2gN_Qxk3atzw,1778
 olive/systems/docker/utils.py,sha256=DCASpfr-0AfAbTKWEa2AxctZqNoqBdyZ-ca4ozlvKfg,5352
 olive/systems/python_environment/__init__.py,sha256=cu6jFeXKLees7MkoNddzPVre_eE-FwXwA3pihv4yWi4,381
 olive/systems/python_environment/available_eps.py,sha256=27A0d6mAipcs4ixaxDd0IGR_fc4I2sXBnTnEkKnYon8,874
 olive/systems/python_environment/inference_runner.py,sha256=NF9X_piW8q-m-dBSTjZ_vVwsdLxj11l_LSobAnYVhuM,4050
 olive/systems/python_environment/is_valid_ep.py,sha256=CEZ4IbKZwv-uSGCHr_kkRAWBk-gLnOV0_EbmxSpEA68,1485
-olive/systems/python_environment/python_environment_system.py,sha256=lwlR2BMgOvfWLNw1f6bUgiM4eeMBUI9sf25gMjxUNpA,11513
+olive/systems/python_environment/python_environment_system.py,sha256=NB4E9Sm21O9H8w2AXmk80wgjHckv4I9FeOz0xybcyHQ,11659
 olive/workflows/__init__.py,sha256=VTbEMSuFgXo3_eTDSirDX1hrAGXD4Uk-RjHRVDRuM3U,287
 olive/workflows/run/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/workflows/run/__main__.py,sha256=_tSF5WY-Y5b_SqfJUkRhZxaLqmrXBZG2wNyzpKZBl8U,634
-olive/workflows/run/config.py,sha256=K8yn2m0e6u0k5sQx6gRs1q-O7nMbePdd9dogzOmYjl8,7204
-olive/workflows/run/run.py,sha256=pg42HOT0Fljd-oXewl8TYx9XgejgzG-5c-PFHuy3u0o,6319
+olive/workflows/run/config.py,sha256=7FZcLxG9d3-BiQWdU9bkanqEg_SCsB4vTkqzEjB2n1o,7178
+olive/workflows/run/run.py,sha256=s_rS9I0AyXhm31UB7tJDRmQC33fQMYUyLn9xnpBRQEw,6434
 olive/workflows/snpe/__init__.py,sha256=FMgPa6EGRkxZGo9okP6klGw5KFLmQtAwvwivYfBvU0g,388
 olive/workflows/snpe/convertquantize/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/workflows/snpe/convertquantize/__main__.py,sha256=re1UWNmJI_E9ubcrAEs4JeP_A4C3LBQMnVofw9TvI40,1339
 olive/workflows/snpe/convertquantize/convertquantize.py,sha256=NxkaswqxhO2ydbSFu_dgd0cYcgZRy1KORZvAvvPiiWg,4322
 olive/workflows/snpe/evaluate/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/workflows/snpe/evaluate/__main__.py,sha256=UAmzVqOnC90Ns3l8oBhHPheyEC2kncFgC-Gl7qXAoEQ,955
-olive/workflows/snpe/evaluate/evaluate.py,sha256=j6zyuUGbQQ_KEGv1Glz9IIYHWdhBqalxYkprUEk8IEU,2879
+olive/workflows/snpe/evaluate/evaluate.py,sha256=QqlMcpc8dttOuhRN-uxnezdkOfYFA2wIgjN2ETQVk8o,2873
 test/integ_test/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/integ_test/evaluator/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 test/integ_test/evaluator/azureml_eval/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/integ_test/evaluator/azureml_eval/test_aml_evaluation.py,sha256=qsJhbZgsqb6KP2BXEpUoT5wHpFVGL9-kVN0DweJI5GA,1587
+test/integ_test/evaluator/azureml_eval/test_aml_evaluation.py,sha256=Uxs3p1nOrvHgw9JZLnWbcp-3s9L6GQ4p4_U-ywgzrCM,1834
 test/integ_test/evaluator/azureml_eval/user_script.py,sha256=ZQUeQ2nwNGO7fu68kr1l8xVkOYaY6pQA2qsphhubtu4,555
-test/integ_test/evaluator/azureml_eval/utils.py,sha256=CuxHhpb2fZnynfnJ1FknoluU0N25_C5YEeVgue80r5s,4649
+test/integ_test/evaluator/azureml_eval/utils.py,sha256=HhkKLQQko6qnCsXE6FUyNauAHL0PEzUJ0OwbhZXiluc,4675
 test/integ_test/evaluator/docker_eval/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/integ_test/evaluator/docker_eval/test_docker_evaluation.py,sha256=Hzr6Qz6pb0rGcy94ldg_w-htTDGWJGvUOhIevKERmBI,2141
+test/integ_test/evaluator/docker_eval/test_docker_evaluation.py,sha256=97onlAbqDWn975uCZoej7ODZ9L6pC3vVqC6FMKWXQvk,2388
 test/integ_test/evaluator/docker_eval/user_script.py,sha256=TOJwatp9TqCOXZ4QkfZTmbr-yz8LZdibfwVTTIdjNPc,1787
-test/integ_test/evaluator/docker_eval/utils.py,sha256=RMD2AWcVlzvBBI_pxL6JoGpMICM053PLfcJ4AbiQo0o,4033
+test/integ_test/evaluator/docker_eval/utils.py,sha256=_d7oJvRj-F8aQOw6Nukim0-s0iFUHe7iBeay_odYeMI,4059
 test/integ_test/evaluator/local_eval/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/integ_test/evaluator/local_eval/test_local_evaluation.py,sha256=9X6d4oo2MCXTFLKwNZlvb0tMpd2c_f4ElkF0JRZSnfo,1942
-test/integ_test/evaluator/local_eval/utils.py,sha256=6CgK0YX6qIrA8urzAssGctKh0eY8yMeS2FT-ac-Q0zg,5153
+test/integ_test/evaluator/local_eval/test_local_evaluation.py,sha256=R2agrNiHP_a8l4XMm-olUQ6-j_LM8h6DYzPQHbuXICs,2189
+test/integ_test/evaluator/local_eval/utils.py,sha256=kP6FUIlJ7E3BynL5pBaFHBCyK4vLqH6RSFicosBEChA,5231
 test/unit_test/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/unit_test/conftest.py,sha256=QcrGihPk83MuprNN0jQ9LheTEJBzFlD7MPAS7JXoiS8,549
 test/unit_test/test_cache.py,sha256=t7QT38bemP19uOE6zmxZobxHWKjuvw26dZ7_O4tGkPs,4266
-test/unit_test/utils.py,sha256=qnTFeAWd1ybTkefrmTzrOcOLXe_w_Xoix9LGMGtasDE,5840
+test/unit_test/utils.py,sha256=M-7U_3-QDop5gToobP6vWTAokHPLmqITKEO-WPMJuUw,6084
 test/unit_test/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 test/unit_test/common/test_import_lib.py,sha256=zerqpZxMmBF_4SSpo4z0LlvWgWt1-MmSLPiOKW1bd9E,3560
 test/unit_test/common/test_retry.py,sha256=CQwYfLO9ALmGhajPN3eGqNjNdJ0lanLVQR6_5i39d_A,1247
 test/unit_test/data_container/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/unit_test/data_container/test_config.py,sha256=BplaVuj-06aEkIBMv4FIodRRnSN1fKyLbHkwI9YbzqM,1266
 test/unit_test/data_container/test_data_container.py,sha256=n8vGdkOp2OgJATNGyYuMI0WAOypv3UJhCHVA7-KiXAg,1899
 test/unit_test/engine/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-test/unit_test/engine/test_engine.py,sha256=BF46DZLti5aALrVdRvGRhz4U9IcFDf3t77LJeKzv0WE,12138
-test/unit_test/engine/test_footprint.py,sha256=WOYgWnPGI50sPq0CcNNc2G5qBTNtXQUua4fruVrnh8Y,2423
+test/unit_test/engine/test_engine.py,sha256=orZ1-zn5qTcs3FzFNWT0rE-KZ_StDh-qMPVur5HF8yY,13575
+test/unit_test/engine/test_footprint.py,sha256=BRcnE_fkYw9cArg3jZwlyNjulGGyEgfMoJQcu1iWsFU,2637
 test/unit_test/engine/packaging/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/unit_test/engine/packaging/test_packaging_generator.py,sha256=BzO5b8fhg8GUAlZdn7h8AIma4JT_x6NmkELjwygJ6MM,4141
+test/unit_test/engine/packaging/test_packaging_generator.py,sha256=r2m1EPiiefybsu-Ehu8rn7fUjD75CZeI2YiZrJdfDas,4404
 test/unit_test/evaluator/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/unit_test/evaluator/test_accuracy.py,sha256=08jwu_PfRN99SGhHQFQyN3g_hdCFfE0bP-kNJw8G7lE,3348
-test/unit_test/evaluator/test_olive_evaluator.py,sha256=tschThv-vlbRGdgqz6BdKYqGggSSqlCNfRz8Hotg6gU,4442
+test/unit_test/evaluator/test_metric.py,sha256=xU0soJb2Ty0Ylu_e2WpaUZqfH6qGYT2jo8IIC5Z8ctk,1908
+test/unit_test/evaluator/test_olive_evaluator.py,sha256=0qq0jrRmsX9kcXBzxXfIPTivKPcAZLYTNtBTAN28AS8,5803
 test/unit_test/hf_utils/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/unit_test/hf_utils/test_hf_utils.py,sha256=kXYwkT9iNRbFmceUkHXm4eYBkbIjZhIcvU9W9Ql9lfg,1021
 test/unit_test/model/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/unit_test/model/test_pytorch_model.py,sha256=qIOsIz2yO6mcbfNzlu2i7Hcf8HdN1aCpjFSWty1xreE,3040
+test/unit_test/model/test_pytorch_model.py,sha256=j0ZZP9vwRo-EH7jtnDPKqF82usKACCf_5ecvyZoKN1M,3403
 test/unit_test/passes/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
+test/unit_test/passes/test_pass_serialization.py,sha256=-pG3gBoyOCKkio1GxKmH3ks8hb-N5JHEbr_PRUBuSHU,878
 test/unit_test/passes/common/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/unit_test/passes/common/test_user_script.py,sha256=i5glLW9KtEo5Oqq-51Oj7v1dM4mAi_cwQVEnavNxzEw,1353
+test/unit_test/passes/common/test_user_script.py,sha256=cQC0w2gm5SZMQbYDs3_kVnYrppYp2LEbDyUHNGxTY7o,1411
 test/unit_test/passes/inc/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/unit_test/passes/inc/test_inc_quantization.py,sha256=CKJeCoPkmPxD06DuEMcu6C3Ty61l9RyHOrjeVJYE1T4,4669
 test/unit_test/passes/onnx/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/unit_test/passes/onnx/test_insert_beam_search.py,sha256=82RluiSA-d6-nt6EHb9OxbxRehZGHv8KipPMSG4dfiA,931
 test/unit_test/passes/onnx/test_mixed_precision.py,sha256=yl9GQvuwscUdHZbspHWYEacCcbcLTtVT4mlD9XJSRRw,632
 test/unit_test/passes/onnx/test_model_optimizer.py,sha256=vuyKR-aeY6kVIBRtnr_NdmFtIKC28vQEvOLtqH6lTdg,866
 test/unit_test/passes/onnx/test_perf_tuning.py,sha256=P4fZwJsdUh1k5u7ZHSNovEbXaO92mc1lE1iU4ZZwkgU,921
 test/unit_test/passes/onnx/test_pre_post_processing_op.py,sha256=nOlSoxU4eu3jspb2Ju69DhD-bNQO4iONPv9aBsxGgus,3327
-test/unit_test/passes/onnx/test_transformer_optimization.py,sha256=7yEsqLwno7fPTo2XPtFgnXaZBKCL8CCum2lUIQ6g2uU,2044
+test/unit_test/passes/onnx/test_transformer_optimization.py,sha256=Nfjg8qDYMd1k0m2SNpuSjuCqP4xwZfqZelg7GtDq7kQ,2131
 test/unit_test/passes/openvino/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/unit_test/passes/openvino/test_openvino_conversion.py,sha256=1xRxMxkKvDyx63Lu-IrWg-wNQeJuAW6NfIZaZvJjnLE,2062
-test/unit_test/passes/openvino/test_openvino_quantization.py,sha256=vFOldCpJmjzjY86q3ic5hoTF95gzXrWQDRM6HorV1ew,4362
+test/unit_test/passes/openvino/test_openvino_quantization.py,sha256=eoOKWzGRKPoCtG5gtdP42_bLnCh0lhfJZUxPI2quN8I,4643
 test/unit_test/passes/pytorch/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/unit_test/passes/pytorch/test_quantization_aware_training.py,sha256=BZfUnqzY33NQljEdoZM1Iq4LY_rt3ge3ysGxx5DtvQY,985
+test/unit_test/passes/pytorch/test_quantization_aware_training.py,sha256=QfGGZImzpE8pSILIsCzUedbQXRtu42ee1m0SwiVCHUg,1092
 test/unit_test/passes/vitis_ai/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 test/unit_test/passes/vitis_ai/test_vitis_ai_quantization.py,sha256=lCKjZkOQiez8fDbY41ia6jG1JVgZl3Io-5yIKES78A8,2245
 test/unit_test/systems/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/unit_test/systems/test_local.py,sha256=TsAJZKZNx6uPnl4P-7BM6M1nPuCj8ouFnYSJPsWxKBQ,3331
+test/unit_test/systems/test_local.py,sha256=uqJTCJLzTTxYEugxohN8XYnO9BP90YqErjiKwTJudIo,3914
 test/unit_test/systems/azureml/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/unit_test/systems/azureml/test_aml_system.py,sha256=63owo1jvRNzbnHFoPsOahmis1h_XQ93ASuJYh7X3mKg,11185
+test/unit_test/systems/azureml/test_aml_system.py,sha256=Yn1ukl6rOtAQXRluwfMCEfwjP_j2ErsZYwQuy8mWlL8,22000
 test/unit_test/systems/docker/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/unit_test/systems/docker/test_docker_system.py,sha256=kXzshj0wgPjHooz0eqm_DpxPIXsOinv9KnSQC-rrgbk,7650
+test/unit_test/systems/docker/test_docker_system.py,sha256=e6L38hZRAWFqxY58JlpSSNL-XctwzjhDh2W_kJETCwk,7863
 test/unit_test/systems/python_environment/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-test/unit_test/systems/python_environment/test_python_environment_system.py,sha256=kuO6TKWj55JyRQlquMWuqyrdkunq7Um1vCFrNcvXlxM,3625
-olive_ai-0.2.0.dist-info/LICENSE,sha256=ws_MuBL-SCEBqPBFl9_FqZkaaydIJmxHrJG2parhU4M,1141
-olive_ai-0.2.0.dist-info/METADATA,sha256=yGNgq4soKHMcdglsboQhOMdEq_Gn1svJ1t7Urxtm8qA,2700
-olive_ai-0.2.0.dist-info/NOTICE.txt,sha256=CWEXy-_vjZKDFwoYVgKszFpKKz9Hy6gEP8TYNxcuLi4,760804
-olive_ai-0.2.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-olive_ai-0.2.0.dist-info/top_level.txt,sha256=GNLByCTYfAa85hMwXMeBmCouNhYiH0opbXeGr4q8gXs,11
-olive_ai-0.2.0.dist-info/RECORD,,
+test/unit_test/systems/python_environment/test_python_environment_system.py,sha256=RWxjIgjnJ-XNEEHLGK9LgC6vMhHNZy-76Lr_Pr6VLtY,5246
+olive_ai-0.2.1.dist-info/LICENSE,sha256=ws_MuBL-SCEBqPBFl9_FqZkaaydIJmxHrJG2parhU4M,1141
+olive_ai-0.2.1.dist-info/METADATA,sha256=Ymh1ppWeUm2adxa5RvyGQzqkI3zK4G889HR9IrFrlbU,2768
+olive_ai-0.2.1.dist-info/NOTICE.txt,sha256=CWEXy-_vjZKDFwoYVgKszFpKKz9Hy6gEP8TYNxcuLi4,760804
+olive_ai-0.2.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+olive_ai-0.2.1.dist-info/top_level.txt,sha256=GNLByCTYfAa85hMwXMeBmCouNhYiH0opbXeGr4q8gXs,11
+olive_ai-0.2.1.dist-info/RECORD,,
```

