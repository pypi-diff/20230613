# Comparing `tmp/optool-0.5.1-py3-none-any.whl.zip` & `tmp/optool-0.6.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,39 +1,39 @@
-Zip file size: 52422 bytes, number of entries: 37
--rw-rw-rw-  2.0 unx        6 b- defN 23-Jun-05 20:26 optool/VERSION
--rw-r--r--  2.0 unx     6540 b- defN 23-Jun-05 20:26 optool/__init__.py
--rw-r--r--  2.0 unx      949 b- defN 23-Jun-05 20:26 optool/conversions.py
--rw-r--r--  2.0 unx     1094 b- defN 23-Jun-05 20:26 optool/languages.py
--rw-r--r--  2.0 unx     4095 b- defN 23-Jun-05 20:26 optool/logging.py
--rw-r--r--  2.0 unx     7753 b- defN 23-Jun-05 20:26 optool/math.py
--rw-r--r--  2.0 unx     1148 b- defN 23-Jun-05 20:26 optool/orthography.py
--rw-r--r--  2.0 unx     1814 b- defN 23-Jun-05 20:26 optool/parallel.py
--rw-rw-rw-  2.0 unx        0 b- defN 23-Jun-05 20:26 optool/py.typed
--rw-r--r--  2.0 unx    32913 b- defN 23-Jun-05 20:26 optool/uom.py
--rw-r--r--  2.0 unx      813 b- defN 23-Jun-05 20:26 optool/util.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-05 20:26 optool/fields/__init__.py
--rw-r--r--  2.0 unx     4553 b- defN 23-Jun-05 20:26 optool/fields/callables.py
--rw-r--r--  2.0 unx     4345 b- defN 23-Jun-05 20:26 optool/fields/containers.py
--rw-r--r--  2.0 unx     2312 b- defN 23-Jun-05 20:26 optool/fields/dataframe.py
--rw-r--r--  2.0 unx      811 b- defN 23-Jun-05 20:26 optool/fields/misc.py
--rw-r--r--  2.0 unx     7681 b- defN 23-Jun-05 20:26 optool/fields/numeric.py
--rw-r--r--  2.0 unx     5788 b- defN 23-Jun-05 20:26 optool/fields/quantities.py
--rw-r--r--  2.0 unx     6249 b- defN 23-Jun-05 20:26 optool/fields/series.py
--rw-r--r--  2.0 unx     2526 b- defN 23-Jun-05 20:26 optool/fields/symbolic.py
--rw-r--r--  2.0 unx     8588 b- defN 23-Jun-05 20:26 optool/fields/util.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-05 20:26 optool/optimization/__init__.py
--rw-r--r--  2.0 unx     4100 b- defN 23-Jun-05 20:26 optool/optimization/constraints.py
--rw-r--r--  2.0 unx     4505 b- defN 23-Jun-05 20:26 optool/optimization/helpers.py
--rw-r--r--  2.0 unx     8008 b- defN 23-Jun-05 20:26 optool/optimization/ode.py
--rw-r--r--  2.0 unx    21117 b- defN 23-Jun-05 20:26 optool/optimization/problem.py
--rw-r--r--  2.0 unx     7142 b- defN 23-Jun-05 20:26 optool/optimization/variables.py
--rw-r--r--  2.0 unx     3055 b- defN 23-Jun-05 20:26 optool/serialization/__init__.py
--rw-r--r--  2.0 unx     1317 b- defN 23-Jun-05 20:26 optool/serialization/datetime_objects.py
--rw-r--r--  2.0 unx      676 b- defN 23-Jun-05 20:26 optool/serialization/numpy_objects.py
--rw-r--r--  2.0 unx     2880 b- defN 23-Jun-05 20:26 optool/serialization/pandas_objects.py
--rw-r--r--  2.0 unx     1407 b- defN 23-Jun-05 20:26 optool/serialization/pint_objects.py
--rw-rw-rw-  2.0 unx     1081 b- defN 23-Jun-05 20:26 optool-0.5.1.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     7568 b- defN 23-Jun-05 20:26 optool-0.5.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-05 20:26 optool-0.5.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 23-Jun-05 20:26 optool-0.5.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2989 b- defN 23-Jun-05 20:26 optool-0.5.1.dist-info/RECORD
-37 files, 165922 bytes uncompressed, 47682 bytes compressed:  71.3%
+Zip file size: 65064 bytes, number of entries: 37
+-rw-rw-rw-  2.0 unx        6 b- defN 23-Jun-13 17:28 optool/VERSION
+-rw-r--r--  2.0 unx     1929 b- defN 23-Jun-13 17:28 optool/__init__.py
+-rw-r--r--  2.0 unx     2191 b- defN 23-Jun-13 17:28 optool/conversions.py
+-rw-r--r--  2.0 unx     6459 b- defN 23-Jun-13 17:28 optool/core.py
+-rw-r--r--  2.0 unx     2670 b- defN 23-Jun-13 17:28 optool/languages.py
+-rw-r--r--  2.0 unx     9554 b- defN 23-Jun-13 17:28 optool/logging.py
+-rw-r--r--  2.0 unx    10277 b- defN 23-Jun-13 17:28 optool/math.py
+-rw-r--r--  2.0 unx     4180 b- defN 23-Jun-13 17:28 optool/parallel.py
+-rw-rw-rw-  2.0 unx        0 b- defN 23-Jun-13 17:28 optool/py.typed
+-rw-r--r--  2.0 unx    32727 b- defN 23-Jun-13 17:28 optool/uom.py
+-rw-r--r--  2.0 unx     1618 b- defN 23-Jun-13 17:28 optool/util.py
+-rw-r--r--  2.0 unx      628 b- defN 23-Jun-13 17:28 optool/fields/__init__.py
+-rw-r--r--  2.0 unx     6206 b- defN 23-Jun-13 17:28 optool/fields/callables.py
+-rw-r--r--  2.0 unx     5786 b- defN 23-Jun-13 17:28 optool/fields/containers.py
+-rw-r--r--  2.0 unx     3937 b- defN 23-Jun-13 17:28 optool/fields/dataframe.py
+-rw-r--r--  2.0 unx     2456 b- defN 23-Jun-13 17:28 optool/fields/misc.py
+-rw-r--r--  2.0 unx    12294 b- defN 23-Jun-13 17:28 optool/fields/numeric.py
+-rw-r--r--  2.0 unx     9788 b- defN 23-Jun-13 17:28 optool/fields/quantities.py
+-rw-r--r--  2.0 unx     9218 b- defN 23-Jun-13 17:28 optool/fields/series.py
+-rw-r--r--  2.0 unx     3724 b- defN 23-Jun-13 17:28 optool/fields/symbolic.py
+-rw-r--r--  2.0 unx    11127 b- defN 23-Jun-13 17:28 optool/fields/util.py
+-rw-r--r--  2.0 unx      347 b- defN 23-Jun-13 17:28 optool/optimization/__init__.py
+-rw-r--r--  2.0 unx     5772 b- defN 23-Jun-13 17:28 optool/optimization/constraints.py
+-rw-r--r--  2.0 unx     5882 b- defN 23-Jun-13 17:28 optool/optimization/helpers.py
+-rw-r--r--  2.0 unx     7125 b- defN 23-Jun-13 17:28 optool/optimization/ode.py
+-rw-r--r--  2.0 unx    25679 b- defN 23-Jun-13 17:28 optool/optimization/problem.py
+-rw-r--r--  2.0 unx     8812 b- defN 23-Jun-13 17:28 optool/optimization/variables.py
+-rw-r--r--  2.0 unx     6382 b- defN 23-Jun-13 17:28 optool/serialization/__init__.py
+-rw-r--r--  2.0 unx     1610 b- defN 23-Jun-13 17:28 optool/serialization/datetime_objects.py
+-rw-r--r--  2.0 unx      832 b- defN 23-Jun-13 17:28 optool/serialization/numpy_objects.py
+-rw-r--r--  2.0 unx     3396 b- defN 23-Jun-13 17:28 optool/serialization/pandas_objects.py
+-rw-r--r--  2.0 unx     1643 b- defN 23-Jun-13 17:28 optool/serialization/pint_objects.py
+-rw-rw-rw-  2.0 unx     1081 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     4190 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2992 b- defN 23-Jun-13 17:28 optool-0.6.0.dist-info/RECORD
+37 files, 212617 bytes uncompressed, 60338 bytes compressed:  71.6%
```

## zipnote {}

```diff
@@ -3,26 +3,26 @@
 
 Filename: optool/__init__.py
 Comment: 
 
 Filename: optool/conversions.py
 Comment: 
 
+Filename: optool/core.py
+Comment: 
+
 Filename: optool/languages.py
 Comment: 
 
 Filename: optool/logging.py
 Comment: 
 
 Filename: optool/math.py
 Comment: 
 
-Filename: optool/orthography.py
-Comment: 
-
 Filename: optool/parallel.py
 Comment: 
 
 Filename: optool/py.typed
 Comment: 
 
 Filename: optool/uom.py
@@ -90,23 +90,23 @@
 
 Filename: optool/serialization/pandas_objects.py
 Comment: 
 
 Filename: optool/serialization/pint_objects.py
 Comment: 
 
-Filename: optool-0.5.1.dist-info/LICENSE.txt
+Filename: optool-0.6.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: optool-0.5.1.dist-info/METADATA
+Filename: optool-0.6.0.dist-info/METADATA
 Comment: 
 
-Filename: optool-0.5.1.dist-info/WHEEL
+Filename: optool-0.6.0.dist-info/WHEEL
 Comment: 
 
-Filename: optool-0.5.1.dist-info/top_level.txt
+Filename: optool-0.6.0.dist-info/top_level.txt
 Comment: 
 
-Filename: optool-0.5.1.dist-info/RECORD
+Filename: optool-0.6.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## optool/VERSION

```diff
@@ -1 +1 @@
-0.5.1
+0.6.0
```

## optool/__init__.py

```diff
@@ -1,159 +1,84 @@
-from __future__ import annotations
-
-from typing import Any, no_type_check
-
-import numpy as np
-import pydantic
-from pydantic import BaseConfig, Extra
-
-from optool.logging import LOGGER
-from optool.serialization import SerializationAssistant
-from optool.serialization.datetime_objects import DatetimeSerializer, ZoneInfoSerializer
-from optool.serialization.numpy_objects import NumpyNdArraySerializer
-from optool.serialization.pandas_objects import (PandasDataFrameSerializer, PandasDatetimeIndexSerializer,
-                                                 PandasRangeIndexSerializer, PandasSeriesSerializer)
-from optool.serialization.pint_objects import PintArraySerializer, PintQuantitySerializer, PintUnitSerializer
-
-
-def _recursive_dict_eq(dict1, dict2):
-    """Recursively compare two dictionaries, handling arrays."""
-    if set(dict1.keys()) != set(dict2.keys()):
-        return False
-
-    for key in dict1.keys():
-        val1, val2 = dict1[key], dict2[key]
-        if isinstance(val1, dict) and isinstance(val2, dict):
-            if not _recursive_dict_eq(val1, val2):
-                return False
-        elif not np.all(val1 == val2):
-            return False
-
-    return True
-
-
-class ImplementationError(Exception):
-    pass
-
-
-class BaseModel(pydantic.BaseModel):
-    """
-    Main base model used in this package.
-
-    Using this class as a superclass for all Pydantic models, their default behavior is adjusted in a very convenient
-    way. The default behavior is defined as follows:
-
-    - Model fields can be of arbitrary user types.
-    - The creation of new fields at runtime is forbidden.
-    - The validation on field assignment is enabled, not only on creation.
-    - All attributes starting with an underscore are private.
-    - Default values are also validated.
-    - Do not copy models on validation, simply keep them untouched.
-    - Allows to get the private field values as items of the `values` dictionary during validation.
-    - Customized JSON loader that can deserialize specified objects.
-    - Customized JSON encoder that can serialize specified objects.
-
-    Furthermore, comparing models for equality invokes a recursive comparison of the two dictionaries, which allows to
-    handle array-like elements.
-
-    See Also:
-        https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally.
-    """
-
-    def __init_subclass__(cls, **kwargs):
-        super().__init_subclass__(**kwargs)
-        cls.__doc__ = ''  # Null out the representation docstring of every subclass
-
-    # noinspection PyUnboundLocalVariable
-    @no_type_check
-    def __setattr__(self, name, value):
-        offer_private_attrs: bool = getattr(self.__config__, 'offer_private_attrs_during_validation', False)
-
-        if offer_private_attrs:
-            private_values = {name: getattr(self, name) for name in self.__private_attributes__ if hasattr(self, name)}
-            LOGGER.trace("Adding private attributes {} to the dictionary of {} with ID {} for validation.",
-                         list(private_values.keys()), self.__class__.__name__, id(self))
-            if any(key in self.__dict__ for key in private_values):
-                raise ImplementationError(f"Some of the private values {private_values.keys()} are already present as "
-                                          f"keys in the dictionary, see {self.__dict__.keys()}.")
-            keys_before = list(self.__dict__.keys()).copy()
-            self.__dict__.update(private_values)
-
-        try:
-            # Call overridden method
-            super().__setattr__(name, value)
-        finally:
-            # Always remove the private attributes
-            if offer_private_attrs:
-                LOGGER.trace("Removing private attributes {} from the dictionary of {} with ID {} again.",
-                             list(private_values.keys()), self.__class__.__name__, id(self))
-
-                for key in private_values:
-                    self.__dict__.pop(key)
-
-                keys_after = list(self.__dict__.keys()).copy()
-                if keys_before != keys_after:
-                    raise ImplementationError(f"The keys before are not equal to the ones after manipulation, "
-                                              f"i.e., {keys_before=} vs. {keys_after=}.")
-
-    def __eq__(self, other: Any) -> bool:
-        try:
-            return super().__eq__(other)
-        except Exception as e:
-            if str(e) != "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()":
-                raise e
-
-        if isinstance(other, BaseModel):
-            other = other.dict()
-        return _recursive_dict_eq(self.dict(), other)
-
-    class Config(BaseConfig):
-        arbitrary_types_allowed = True
-        """Allow arbitrary user types for fields."""
-
-        extra = Extra.forbid
-        """Forbids creation of new fields at runtime."""
-        # see https://pydantic-docs.helpmanual.io/usage/model_config/#options.
-
-        validate_assignment = True
-        """Enable validation on field assignment, not only on creation."""
-
-        underscore_attrs_are_private = True
-        """All attributes starting with an underscore are private."""
-
-        validate_all = True
-        """Validate also default values."""
-
-        copy_on_model_validation = 'none'
-        """Do not copy models on validation, simply keep them untouched."""
-
-        offer_private_attrs_during_validation = True
-        """Allows to get the private field values as items of the 'values' dictionary during validation."""
-
-        json_loads = SerializationAssistant.json_loader
-        """Customized JSON loader that can deserialize specified objects."""
-
-        json_encoders = SerializationAssistant.register(
-            NumpyNdArraySerializer(),
-            PintQuantitySerializer(),
-            PintUnitSerializer(),
-            PintArraySerializer(),
-            PandasDataFrameSerializer(),
-            PandasSeriesSerializer(),
-            PandasRangeIndexSerializer(),
-            PandasDatetimeIndexSerializer(),
-            ZoneInfoSerializer(),
-            DatetimeSerializer(),
-        )
-        """Customized JSON encoder that can serialize specified objects."""
-
-
-validate_arguments = pydantic.validate_arguments(config=dict(arbitrary_types_allowed=True))
 """
-Decorator to validate the arguments passed to a function.
+Generally usable utilities related to optimization problems.
 
-Returns:
-    :py:data:`~typing.Callable`:
+This package aims at enhancing the workflow of formulating numerical optimization problems.
+It allows to use units of measurements and execute optimizations in parallel.
+Furthermore, data validation and parsing is deeply integrated into the codebase, and serialization of various well-known
+data types is facilitated.
+
+## Content
+
+The source code is structured in a number of packages and modules, each of which serves a different purpose.
+
+### Subpackages
+
+````{list-table}
+:class: autosummary longtable
+:align: left
+
+* - {py:mod}`~optool.fields`
+  - ```{autodoc2-docstring} optool.fields
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.optimization`
+  - ```{autodoc2-docstring} optool.optimization
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.serialization`
+  - ```{autodoc2-docstring} optool.serialization
+    :parser: myst
+    :summary:
+    ```
+````
+
+### Submodules
+
+````{list-table}
+:class: autosummary longtable
+:align: left
+
+* - {py:mod}`~optool.conversions`
+  - ```{autodoc2-docstring} optool.conversions
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.core`
+  - ```{autodoc2-docstring} optool.core
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.languages`
+  - ```{autodoc2-docstring} optool.languages
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.logging`
+  - ```{autodoc2-docstring} optool.logging
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.math`
+  - ```{autodoc2-docstring} optool.math
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.parallel`
+  - ```{autodoc2-docstring} optool.parallel
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.uom`
+  - ```{autodoc2-docstring} optool.uom
+    :parser: myst
+    :summary:
+    ```
+* - {py:mod}`~optool.util`
+  - ```{autodoc2-docstring} optool.util
+    :parser: myst
+    :summary:
+    ```
 
-Note:
-    This is equal to :py:func:`pydantic.validate_arguments` but with ``arbitrary_types_allowed`` set to :py:data:`True`.
+````
 """
```

## optool/conversions.py

```diff
@@ -1,25 +1,55 @@
+"""
+Date and time conversions for time-series data.
+
+This module is dedicated to providing reliable conversions between common time-series data representations.
+It offers functions that transform {py:class}`pandas.DatetimeIndex` objects into sample times or intervals expressed in
+seconds and vice versa.
+"""
+
 from datetime import datetime, timedelta
 
 import numpy as np
 import pandas as pd
 from pandas import DatetimeIndex
 
 from optool.uom import UNITS, Quantity
 
 
 def datetime_index_to_samples(timestamps: DatetimeIndex) -> Quantity:
+    """
+    Converts a {py:class}`~pandas.DatetimeIndex` to a {py:data}`~optool.uom.Quantity` object representing the sample
+    times in seconds.
+
+    :param timestamps: The absolute timestamps.
+    :return: A quantity object with the sample times in seconds since the first timestamp.
+    """
     duration = (timestamps - timestamps[0]).to_pytimedelta()
     sample_times_seconds = np.array([val.total_seconds() for val in duration])
     return Quantity(sample_times_seconds, UNITS.second)
 
 
 def datetime_index_to_intervals(timestamps: DatetimeIndex) -> Quantity:
+    """
+    Converts a {py:class}`~pandas.DatetimeIndex` to a {py:data}`~optool.uom.Quantity` object representing the intervals
+    between timestamps in seconds.
+
+    :param timestamps: The absolute timestamps.
+    :return: A Quantity object with the intervals between the timestamps in seconds.
+    """
     intervals_seconds = np.diff(timestamps.astype(np.int64)) / 10**9
     return Quantity(intervals_seconds, UNITS.second)
 
 
 def samples_to_datetime_index(start: datetime, sample_times: Quantity) -> DatetimeIndex:
+    """
+    Converts a {py:data}`~optool.uom.Quantity` object representing sample times into a {py:class}`~pandas.DatetimeIndex`
+    object.
+
+    :param start: The starting date and time.
+    :param sample_times: A quantity object with the sample times in seconds since the start time.
+    :return: The absolute timestamps.
+    """
     sample_times_seconds = sample_times.m_as(UNITS.second)
     duration = [timedelta(0, float(second)) for second in sample_times_seconds]
     datetime_values = [start + val for val in duration]
     return pd.DatetimeIndex(datetime_values)
```

## optool/languages.py

```diff
@@ -1,17 +1,48 @@
+"""
+Facilitating Internationalization and Localization.
+
+This module assists internationalization (i18n) and localization (L10n) processes.
+It helps to adapt software to various languages, regional nuances, and technical requirements of a target locale.
+This adaptation is achieved by externalizing all strings and marking them for translation, usually by wrapping them in a
+function, named simply as an underscore (`_`). The core function in this module is gettext, which returns the hard-coded
+translations for these strings.
+The function {py:func}`set_language` further enables the selection of the desired language for the application, thus
+ensuring smooth multilingual operation.
+
+::::{admonition} Example
+:class: example dropdown
+
+Declare a string that is to be translated as follows:
+```python
+from optool.languages import gettext as _
+
+my_translated_str = _("A sentence to translate")
+```
+"""
+
 import gettext as _gettext
 import locale
 from pathlib import Path
 
 from optool.logging import LOGGER
 
 _TRANSLATOR = None
 
 
 def gettext(message):
+    """
+    Returns the translation of a given message.
+
+    If no translation is found for the current language, a warning is logged and the original message is returned.
+
+    :param message: The string to be translated.
+    :return: The translated string if found in the language catalog, else the original string.
+    """
+
     if _TRANSLATOR is None:
         return _gettext.gettext(message)
 
     # noinspection PyUnresolvedReferences
     info = _TRANSLATOR.info()
     # noinspection PyProtectedMember,PyUnresolvedReferences
     catalog = _TRANSLATOR._catalog
@@ -23,11 +54,19 @@
                        message, info["language"])
 
     # noinspection PyUnresolvedReferences
     return _TRANSLATOR.gettext(message)
 
 
 def set_language(language: str, locale_directory: Path):
+    """
+    Sets the language for the application and loads the corresponding translation catalog.
+
+    :param language: The language to be set. It should correspond to one of the locales available in the locale
+        directory.
+    :param locale_directory: The path to the directory containing the locale files for different languages.
+    """
+
     locale.setlocale(locale.LC_ALL, language)
 
     global _TRANSLATOR
     _TRANSLATOR = _gettext.translation('messages', localedir=str(locale_directory), languages=[language])
```

## optool/logging.py

```diff
@@ -1,112 +1,265 @@
+"""
+Utilities for streamlining logging for Python applications.
+
+This module provides comprehensive logging capabilities, contributing to effective debugging and information tracking.
+It furnishes classes that filter log messages based on different criteria, ensuring only relevant information is logged.
+The module includes functions to set up the logger, measure and log the execution time of functions, further enhanced by
+a decorator for function execution timing.
+It also incorporates data elements defining available logging levels, message formats, logger settings, and an alias to
+a logger instance. Overall, this module enables developers to implement robust logging mechanisms in their Python
+applications.
+
+::::{admonition} Example
+:class: example dropdown
+
+Setting up a logger works as for example in the following.
+```python
+import sys
+from optool.logging import LogFilter, MessageFilter, setup_logger
+
+log_filter = LogFilter(process="MainProcess")
+log_filter.add(
+    MessageFilter(module="optool.math", level="INFO"),
+    MessageFilter(module="optool.uom", level="INFO"),
+)
+setup_logger(sink=sys.stdout, filter=log_filter, level='DEBUG')
+```
+
+It can then be used everywhere in the code as follows:
+```python
+from optool.logging import LOGGER
+
+LOGGER.info("The value {!r} is logged on level info.", 13.0)
+```
+::::
+
+"""
+
 import time
 from fnmatch import fnmatch
 from functools import wraps
-from io import TextIOWrapper
-from types import FunctionType, MethodType
-from typing import Any, Callable, Dict, List, Literal, Optional, Union
+from typing import Any, Callable, Dict, List, Literal, Optional, Protocol, TextIO, Union
 
 from loguru import logger
 from pydantic import StrictStr, validate_arguments
 
-from optool.fields.misc import NonEmptyStr
-
 LogLevels = Literal['TRACE', 'DEBUG', 'INFO', 'SUCCESS', 'WARNING', 'ERROR', 'CRITICAL']
+"""
+A Literal type representing available levels of logging.
+
+The levels are, in increasing order of severity: `TRACE`, `DEBUG`, `INFO`, `SUCCESS`, `WARNING`, `ERROR`, `CRITICAL`.
+"""
 
 
 class MessageFilter:
+    """
+    A class that filters log messages based on specific criteria.
+
+    The criteria available include the module name, function name, process name, and log level.
+
+    :param module: Name of the module to filter on.
+    :param function: Name of the function to filter on.
+    :param process: Name of the process to filter on.
+    :param level: The minimum log level of messages to accept.
+    """
+
     __slots__ = 'module', 'function', 'process', 'level'
 
     @validate_arguments
     def __init__(self,
-                 module: Optional[NonEmptyStr] = None,
-                 function: Optional[NonEmptyStr] = None,
-                 process: Optional[NonEmptyStr] = None,
-                 level: LogLevels = 'CRITICAL'):
-
-        self.module = module
-        self.function = function
-        self.process = process
+                 module: Optional[str] = None,
+                 function: Optional[str] = None,
+                 process: Optional[str] = None,
+                 level: LogLevels = 'CRITICAL') -> None:
+
+        self.module = None if module == "" else module
+        self.function = None if function == "" else function
+        self.process = None if process == "" else process
         self.level = logger.level(level)
 
     def is_accepted(self, record: Dict[str, Any]) -> bool:
+        """
+        Determines whether a given log record is accepted by this filter.
+
+        :param record: The log record to evaluate.
+        :return: {py:data}`True` if the record is accepted, {py:data}`False` otherwise.
+        """
+
         if self.module is not None and self.module != record["name"]:
             return True
         if self.function is not None and self.function != record["function"]:
             return True
         if self.process is not None and self.process != record["process"].process__name:
             return True
 
         return record["level"].no >= self.level.no
 
 
 class LogFilter:
+    """
+    A class that filters log messages based on a global minimum level, the process name, and a list of message filters.
+
+    :param minimum_level: The minimum log level of messages to accept.
+    :param process: The name of the process to filter on.
+    """
+
     __slots__ = 'level', 'process', '_filters'
 
     @validate_arguments
-    def __init__(self, *, minimum_level: LogLevels = 'TRACE', process: StrictStr = "*"):
+    def __init__(self, *, minimum_level: LogLevels = 'TRACE', process: StrictStr = "*") -> None:
         self.level = logger.level(minimum_level)
         self.process = process
         self._filters: List[MessageFilter] = []
 
-    def __call__(self, record):
+    def __call__(self, record) -> bool:
+        """
+        Determines whether a given log record is accepted by this filter.
+
+        :param record: The log record to evaluate.
+        :return: {py:data}`True` if the record is accepted, {py:data}`False` otherwise.
+        """
         if (record["level"].no < self.level.no) or not fnmatch(record["process"].name, self.process):
             return False
 
         # noinspection PyShadowingBuiltins
         return all(filter.is_accepted(record) for filter in self._filters)
 
     # noinspection PyShadowingBuiltins
-    def add(self, filter: Union[MessageFilter, List[MessageFilter]]):
-        if isinstance(filter, MessageFilter):
-            self._filters.append(filter)
-        else:
-            self._filters.extend(filter)
-
-
-FORMAT = "{time:YYYY-MM-DD HH:mm:ss.SSS} | {process.name} | <level>{level: <8}</level> | " \
-         "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>{exception}"
+    def add(self, *filters: MessageFilter) -> None:
+        """
+        Adds one or more message filters to this log filter.
+
+        :param filters: One or more {py:class}`MessageFilter` objects to add.
+        """
+        for filter in filters:
+            if isinstance(filter, MessageFilter):
+                self._filters.append(filter)
+
+
+FORMAT = r"{time:YYYY-MM-DD HH:mm:ss.SSS} | {process.name} | <level>{level: <8}</level> | " \
+         r"<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>{exception}"
+"""
+A string defining the formatting for log messages.
+
+The log message format includes:
+- Time of the log message.
+- Name of the process that produced the log message.
+- Level of the log message.
+- Name of the module, function and line number where the log message originated.
+- The actual log message.
+- Any exception information (if applicable).
+"""
 
 logger_settings = dict(
     # rotation="10 MB",
     serialize=False,
     format=FORMAT,
-    # format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}\n{exception}",
     diagnose=True,
     backtrace=True,
     enqueue=True,
 )
+"""
+A dictionary of settings for the logger.
+
+These settings include:
+- `serialize`: Specify if log messages are serialized to a structured format. (Default: {py:data}`False`)
+- `format`: The format string to use for log messages. (Default: {py:data}`FORMAT`)
+- `diagnose`: Specify if diagnostics information is added to the log messages. (Default: {py:data}`True`)
+- `backtrace`: Specify if a stack trace is added to the log messages when an exception occurs. (Default:
+  {py:data}`True`)
+- `enqueue`: Specify if log messages are added to a queue to be processed asynchronously. (Default: {py:data}`True`)
+"""
+
+
+class Writable(Protocol):
+
+    def write(self, message) -> None:
+        pass
 
 
 # noinspection PyShadowingBuiltins
-def setup_logger(sink, filter, level):
+def setup_logger(sink: Union[TextIO, Writable, str], filter: Optional[Callable[[Any], bool]], level: LogLevels) -> None:
+    """
+    Sets up the logger with a given sink, filter, and level.
+
+    :param sink: Where the log messages should be sent. This can be either a {py:class}`~io.TextIOWrapper` object (e.g.
+        {py:data}`sys.stdout`) or a string specifying a filename.
+    :param filter: The filter to use for the log messages.
+    :param level: The minimum log level to record.
+    :raises ValueError: If sink is not a {py:class}`~io.TextIOWrapper` object or a string.
+    """
+
     logger.remove()
-    if isinstance(sink, TextIOWrapper):
-        logger.add(sink, filter=filter, level=level, **logger_settings)
-    elif isinstance(sink, str):
-        logger.add(sink, mode="w", filter=filter, level=level, **logger_settings)
+    if isinstance(sink, str):
+        logger.add(sink, mode="w", filter=filter, level=level, **logger_settings)  # type: ignore
     else:
-        raise ValueError(f"The sink must either be a TextIOWrapper or a string, "
-                         f"but is {sink.__class__.__name__!r}.")
+        logger.add(sink, filter=filter, level=level, **logger_settings)  # type: ignore
 
 
 LOGGER = logger
-
-
-def time_function(func, log_level, *args, **kwargs):
-    # See https://loguru.readthedocs.io/en/stable/resources/recipes.html#
-    # logging-entry-and-exit-of-functions-with-a-decorator
+"""
+Alias to an instance of the
+[Loguru logger](https://loguru.readthedocs.io/en/stable/api/logger.html#module-loguru._logger).
+"""
+
+
+def time_function(func: Callable, log_level: LogLevels, *args, **kwargs):
+    """
+    Times the execution of a function and logs the result.
+
+    :param func: The function to time.
+    :param log_level: The level at which to log the result.
+    :param args: Positional arguments to pass to the function.
+    :param kwargs: Keyword arguments to pass to the function.
+    :return: The result of the function.
+
+    :::{seealso}
+    https://loguru.readthedocs.io/en/stable/resources/recipes.html#logging-entry-and-exit-of-functions-with-a-decorator
+    :::
+    """
     start = time.time()
     result = func(*args, **kwargs)
     end = time.time()
     LOGGER.log(log_level, "Function {!r} executed in {:f} s.", func.__name__, end - start)
     return result
 
 
-def timeit(func: Optional[Union[Callable, FunctionType, MethodType]] = None, /, *, log_level: str = "DEBUG"):
+def timeit(func: Optional[Callable] = None, /, *, log_level: LogLevels = "DEBUG"):
+    """
+    A decorator to time the execution of a function and log the result.
+
+    :param func: The function to be timed. If {py:data}`None`, returns a decorator to be used.
+    :param log_level: The level at which to log the result.
+    :return: If `func` is not {py:data}`None`, returns the result of the function. Otherwise, returns a decorator.
+
+    ::::{admonition} Examples
+    :class: example dropdown
+
+    Annotate any function to log the time it takes to execute it as follows:
+
+    ```python
+    from optool.logging import timeit
+
+    @timeit
+    def my_function_to_time(*args):
+        pass
+    ```
+
+    To specify a log level other than `DEBUG`, specify the desired level (here `INFO`) as an argument:
+
+    ```python
+    from optool.logging import timeit
+
+    @timeit(log_level='INFO')
+    def my_function_to_time(*args):
+        pass
+    ```
+    ::::
+    """
 
     def func_wrapper(*args, **kwargs):
         return time_function(func, log_level, *args, **kwargs)
 
     if func is not None:
         return func_wrapper
```

## optool/math.py

```diff
@@ -1,34 +1,50 @@
-import re
-from enum import Enum, auto
+"""
+Collection of classes and functions geared towards facilitating computations involving both numeric and symbolic values.
+
+This module offers various functions for performing a series of checks and operations on numerical and unit-based data.
+For instance, the module contains functions to verify if a value or unit is dimensionless, to determine if a numeric
+value has an offset, or to check if two values or units are compatible.
+A set of utility functions provides detailed inspection capabilities on numerical data, allowing to ascertain whether a
+value is zero, non-zero, or not a number (NaN), among others.
+Moreover, the module includes functions to confirm the data structure type of the provided data, identifying whether
+it's a scalar, an array, or a vector.
+"""
+
+from enum import Enum
 from numbers import Number
-from typing import Dict, List, Optional, Union, get_args
+from typing import Literal, Optional, Union, get_args
 
 import casadi
 import numpy as np
 
-from optool.logging import LOGGER
 from optool.uom import UNITS, Quantity, Unit
-from optool.util import StrEnum
 
-NUMERIC_TYPES = Union[Number, np.ndarray, Quantity]  # type:ignore #: Numbers with or without units of measurement
-SYMBOLIC_TYPES = Union[casadi.SX]  #: Symbols used by the supported modeling languages
+NUMERIC_TYPES = Union[Number, np.ndarray, Quantity]  # type:ignore
+"""Numbers with or without units of measurement"""
 
-
-class VectorRepresentation(Enum):
-    COLUMN = 0  #: Convenient shortcut to describe column vectors, where the entries are stacked vertically
-    ROW = 1  #: Convenient shortcut to describe row vectors, where the entries are stacked horizontally
+SYMBOLIC_TYPES = Union[casadi.SX]
+"""Symbols used by the supported modeling languages"""
 
 
-class Direction(StrEnum):
-    ASCENDING = auto()
-    DESCENDING = auto()
+class VectorRepresentation(Enum):
+    """Description of vector layouts."""
+    COLUMN = 0
+    """Convenient shortcut to describe column vectors, where the entries are stacked vertically"""
+    ROW = 1
+    """Convenient shortcut to describe row vectors, where the entries are stacked horizontally"""
 
 
 def has_offset(value_or_unit: Union[NUMERIC_TYPES, Unit]) -> bool:
+    """
+    Checks whether a given numeric value or unit has an offset in its measurement, such as Celsius.
+
+    :param value_or_unit: Numeric value or unit to check.
+    :return: {py:data}`True` if the given value or unit has an offset in its measurement, {py:data}`False` otherwise.
+    """
     if isinstance(value_or_unit, Unit):
         return has_offset(Quantity(1.0, value_or_unit))
     # noinspection PyProtectedMember
     return isinstance(value_or_unit, Quantity) and not value_or_unit._is_multiplicative
 
 
 def _get_unit(quantity_or_unit: Union[Quantity, Unit, str]) -> Unit:
@@ -39,172 +55,212 @@
     if isinstance(quantity_or_unit, Quantity):
         return quantity_or_unit.units
     raise ValueError(
         f"The input argument must either be a quantity or a unit, but is {quantity_or_unit.__class__.__name__}")
 
 
 def is_dimensionless(value_or_unit: Union[NUMERIC_TYPES, Unit, str]) -> bool:
+    """
+    Checks if a given value or unit is dimensionless.
+
+    :param value_or_unit: The value or unit to check.
+    :return: {py:data}`True` if the given value or unit is dimensionless, {py:data}`False` otherwise.
+    """
     if isinstance(value_or_unit, (Quantity, Unit, str)):
         return _get_unit(value_or_unit).dimensionless
     return True
 
 
 def is_compatible(value_or_unit: Union[NUMERIC_TYPES, Unit, str], unit: Optional[Union[Unit, str]]) -> bool:
+    """
+    Checks if a given value or unit is compatible with a specific unit.
+
+    :param value_or_unit: The value or unit to check.
+    :param unit: The specific unit to check compatibility against.
+    :return: {py:data}`True` if the given value or unit is compatible with the specific unit, {py:data}`False`
+        otherwise.
+    """
     if unit is None:
         return is_dimensionless(value_or_unit)
     if isinstance(unit, str):
         unit = UNITS.parse_units(unit)
     both_are_dimensionless = is_dimensionless(value_or_unit) and is_dimensionless(unit)
     units_are_compatible = isinstance(value_or_unit,
                                       (Quantity, Unit, str)) and _get_unit(value_or_unit).is_compatible_with(unit)
     return both_are_dimensionless or units_are_compatible
 
 
-def isnonnan(value: NUMERIC_TYPES):
+def is_non_nan(value: NUMERIC_TYPES):
+    """
+    Checks if a given numeric value is not NaN.
+
+    :param value: The numeric value to check.
+    :return: {py:data}`True` if the given numeric value is not NaN, {py:data}`False` otherwise.
+    """
     return ~np.isnan(value)  # type: ignore
 
 
-def iszero(value: NUMERIC_TYPES):
+def is_zero(value: NUMERIC_TYPES):
+    """
+    Checks if a given numeric value is zero.
+
+    :param value: The numeric value to check.
+    :return: {py:data}`True` if the given numeric value is zero, {py:data}`False` otherwise.
+    """
     return value == 0
 
 
-def isnonzero(value: NUMERIC_TYPES):
+def is_nonzero(value: NUMERIC_TYPES):
+    """
+    Checks if a given numeric value is not zero.
+
+    :param value: The numeric value to check.
+    :return: {py:data}`True` if the given numeric value is not zero, {py:data}`False` otherwise.
+    """
     return value != 0
 
 
-def isnumeric(value) -> bool:
+def is_numeric(value) -> bool:
+    """
+    Checks if a given value is numeric.
+
+    :param value: The value to check.
+    :return: {py:data}`True` if the given value is numeric, {py:data}`False` otherwise.
+    """
     return isinstance(value, get_args(NUMERIC_TYPES))
 
 
 def is_symbolic(value) -> bool:
+    """
+    Checks if a given value is symbolic.
+
+    :param value: The value to check.
+    :return: {py:data}`True` if the given value is symbolic, {py:data}`False` otherwise.
+    """
     return isinstance(value, SYMBOLIC_TYPES) or (  # type: ignore
         isinstance(value, Quantity) and is_symbolic(value.magnitude))  # type: ignore
 
 
-def isscalar(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES]) -> bool:
+def is_scalar(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES]) -> bool:
+    """
+    Checks if a given value is scalar.
+
+    :param value: The value to check.
+    :return: {py:data}`True` if the given value is scalar, {py:data}`False` otherwise.
+    """
     return num_elements(value) == 1
 
 
-def isarray(value: NUMERIC_TYPES) -> bool:
+def is_array(value: NUMERIC_TYPES) -> bool:
+    """
+    Checks if a given numeric value is an array.
+
+    :param value: The numeric value to check.
+    :return: {py:data}`True` if the given numeric value is an array, {py:data}`False` otherwise.
+    """
     if isinstance(value, Number):
         return True
     if isinstance(value, np.ndarray):
         return np.ndim(value) == 1
     if isinstance(value, Quantity):
-        return isarray(value.magnitude)
+        return is_array(value.magnitude)
     raise TypeError(f"Unsupported type {type(value)}")
 
 
-def isvector(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES],
-             representation: Optional[VectorRepresentation] = None) -> bool:
+def is_vector(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES],
+              representation: Optional[VectorRepresentation] = None) -> bool:
+    """Checks if a given value is a (column or row) vector.
+
+    :param value: The value to check
+    :param representation: The representation of the vector
+    :returns: {py:data}`True` if the value has two dimensions and is either a row or a column vector, depending on the
+        requested axis, or {py:data}`False` otherwise.
+
+    :::{seealso}
+    {py:func}`is_column`, {py:func}`is_row`, {py:class}`VectorRepresentation`
+    :::
     """
-    Return :py:data:`True` if value is a (column or row) vector, and :py:data:`False` otherwise.
-
-    Args:
-        value: The value to check
-        representation: The representation of the vector
+    if not representation:
+        return is_column(value) or is_row(value)
+    return is_column(value) if representation is VectorRepresentation.COLUMN else is_row(value)
 
-    Returns:
-        :py:data:`True` if the value has two dimensions and is either a row or a column vector, depending on the
-        requested axis, or :py:data:`False` otherwise.
 
-    See Also:
-        :py:func:`iscolumn`, :py:func:`isrow`, :py:class:`VectorRepresentation`
+def is_column(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES]) -> bool:
     """
-    if not representation:
-        return iscolumn(value) or isrow(value)
-    return iscolumn(value) if representation is VectorRepresentation.COLUMN else isrow(value)
-
+    Checks if a given numeric or symbolic value is a column (i.e., has a vertical vector representation).
 
-def iscolumn(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES]) -> bool:
+    :param value: The numeric or symbolic value to check.
+    :return: {py:data}`True` if the given value is a column, {py:data}`False` otherwise.
+    """
     if isinstance(value, Number):
         return True
     if isinstance(value, np.ndarray):
-        return isscalar(value) or (value.ndim > 1 and value.shape[1] == 1)
+        return is_scalar(value) or (value.ndim > 1 and value.shape[1] == 1)
     if isinstance(value, casadi.SX):
         return value.is_column()
     if isinstance(value, Quantity):
-        return iscolumn(value.magnitude)
+        return is_column(value.magnitude)
     raise TypeError(f"Unsupported type {type(value)}")
 
 
-def isrow(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES]) -> bool:
+def is_row(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES]) -> bool:
+    """
+    Checks if a given numeric or symbolic value is a row (i.e., has a horizontal vector representation).
+
+    :param value: The numeric or symbolic value to check.
+    :return: {py:data}`True` if the given value is a row, {py:data}`False` otherwise.
+    """
     if isinstance(value, Number):
         return True
     if isinstance(value, np.ndarray):
-        return isscalar(value) or (value.ndim > 1 and value.shape[0] == 1)
+        return is_scalar(value) or (value.ndim > 1 and value.shape[0] == 1)
     if isinstance(value, casadi.SX):
         return value.is_row()
     if isinstance(value, Quantity):
-        return isrow(value.magnitude)
+        return is_row(value.magnitude)
     raise TypeError(f"Unsupported type {type(value)}")
 
 
 def num_elements(value: Union[NUMERIC_TYPES, SYMBOLIC_TYPES]) -> int:
+    """
+    Determines the number of elements in a given numeric or symbolic value.
+
+    :param value: The numeric or symbolic value to count elements in.
+    :return: The number of elements in the given value.
+    """
     if isinstance(value, Quantity):
         return num_elements(value.magnitude)
-    if isnumeric(value):
+    if is_numeric(value):
         return np.size(value)  # type: ignore
     if isinstance(value, casadi.SX):
         return value.numel()
     raise TypeError(f"Unsupported type {type(value)}")
 
 
-def is_monotonic(value: NUMERIC_TYPES, direction: Direction = Direction.ASCENDING, strict: bool = False) -> bool:
+def is_monotonic(value: NUMERIC_TYPES,
+                 direction: Literal['ascending', 'descending'] = 'ascending',
+                 strict: bool = False) -> bool:
+    """
+    Checks if a given numeric value is monotonic in a specific direction.
+
+    :param value: The numeric value to check.
+    :param direction: The direction in which to check monotonicity.
+    :param strict: A boolean indicating whether to enforce strict monotonicity.
+                   If {py:data}`True`, the function checks for strictly increasing or decreasing values.
+                   Otherwise, the function allows for equal successive values.
+    :return: {py:data}`True` if the given value is monotonic in the specified direction, {py:data}`False` otherwise.
+    """
     if num_elements(value) < 2:
         raise ValueError(f"Requires at least 2 elements, but got only {num_elements(value)}.")
-    if not all(isnonnan(value)):
-        raise ValueError(f"The array contains {np.sum(~isnonnan(value))} NaN value(s).")
+    if not all(is_non_nan(value)):
+        raise ValueError(f"The array contains {np.sum(~is_non_nan(value))} NaN value(s).")
 
     difference = np.diff(value)  # type: ignore
-    if direction is Direction.ASCENDING:
+    if direction == 'ascending':
         if strict:
             return bool(np.all(difference > 0))
         return bool(np.all(difference >= 0))
-    if strict:
-        return bool(np.all(difference < 0))
-    return bool(np.all(difference <= 0))
-
-
-_REFERENCES: Dict[str, Quantity] = {}
-
-
-def is_value_satisfying(quantity: Quantity, criterion: str) -> bool:
-    stripped = criterion.replace(" ", "")
-    try:
-        relation = re.match(r"^(<=|==|!=|>=|<|≤|=|≠|≥|>)", stripped).group()  # type: ignore
-        reference_string = stripped.lstrip(relation)
-    except AttributeError as e:
-        raise ValueError(f"The given criterion {criterion!r} does not start with any of the following "
-                         f"relational symbols: <|≤|<=|=|==|!=|≠|≥|>=|>") from e
-
-    if reference_string in _REFERENCES:
-        reference = _REFERENCES[reference_string]
-    else:
-        try:
-            reference = Quantity(reference_string)
-            _REFERENCES[reference_string] = reference
-        except Exception as e:
-            raise ValueError(f"Cannot create a quantity from {reference_string!r}.") from e
-
-    if not is_compatible(quantity, reference.units):
-        raise ValueError(f"Cannot compare quantity with units {quantity.units!r} to {reference.units!r}.")
-
-    LOGGER.debug("From {}, we interpret the following: Is {} {} {} true?", criterion, quantity, relation, reference)
-    if relation == "<":
-        return bool(np.all(quantity < reference))
-    elif relation in ["≤", "<="]:
-        return bool(np.all(quantity <= reference))
-    elif relation in ["=", "=="]:
-        return bool(np.all(quantity == reference))
-    elif relation in ["!=", "≠"]:
-        return bool(np.all(quantity != reference))
-    elif relation in ["≥", ">="]:
-        return bool(np.all(quantity >= reference))
-    elif relation == ">":
-        return bool(np.all(quantity > reference))
-    else:
-        raise ValueError(f"Unsupported relational symbol {relation!r}.")
-
-
-def consecutive(data: np.ndarray, step_size: int = 1) -> List[np.ndarray]:
-    return np.split(data, np.where(np.diff(data) != step_size)[0] + 1)
+    if direction == 'descending':
+        if strict:
+            return bool(np.all(difference < 0))
+        return bool(np.all(difference <= 0))
+    raise ValueError(f"Direction must be ascending or descending, but is {direction!r}.")
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## optool/parallel.py

```diff
@@ -1,48 +1,122 @@
+"""
+Utilities to execute code in parallel across multiple processes.
+"""
+
 import os
 from multiprocessing import Pool, current_process
-from typing import Callable, Iterable
+from typing import Any, Callable, Iterable
 
 from pydantic import DirectoryPath, StrictInt
 
-from optool import BaseModel
+from optool.core import BaseModel
 from optool.logging import LOGGER, LogFilter, LogLevels, setup_logger, timeit
 
 
 def _get_system_cpu_counts() -> int:
     if count := os.cpu_count():
         return count
     raise ValueError("Cannot determine number of CPUs of this system.")
 
 
 class ParallelExecutor(BaseModel):
+    """
+    Utility class to execute a function in parallel across multiple processes.
+
+    ::::{admonition} Example
+    :class: example dropdown
+
+    ```python
+    import os
+    import tempfile
+    from pathlib import Path
+
+    from optool import LOGGER
+    from optool.parallel import ParallelExecutor
+
+    # Define a simple function that we want to execute in parallel
+    def calculate(n):
+        LOGGER.debug("Performing calculation with {}.", n)
+        return n * n
+
+
+    if __name__ == '__main__':
+        temporary_directory = tempfile.TemporaryDirectory(suffix=None, prefix="test_parallel_")
+        log_path = Path(temporary_directory.name)
+
+        # Define the inputs for the function
+        inputs = list(range(10))
+
+        # Create an instance of ParallelExecutor
+        LOGGER.info("Executing function {!r} with arguments {} in parallel.", calculate.__name__, inputs)
+        executor = ParallelExecutor(function=calculate,
+                                    log_sink=log_path,
+                                    log_level='INFO',
+                                    processes=4)
+
+        # Execute the function in parallel
+        results = executor.run(inputs)
+        LOGGER.info("Done. Solver output is written to {}. Results are {}.", log_path, results)
+
+        log_files = os.listdir(log_path)
+        LOGGER.info("Content of output directory is: {}", log_files)
+    ```
+    """
+
     function: Callable
+    """The function to execute."""
+
     log_sink: DirectoryPath
+    """The directory path where to store the logs."""
+
     log_level: LogLevels = 'TRACE'
+    """The level of logging, default to 'TRACE'."""
+
     processes: StrictInt = _get_system_cpu_counts()
+    """The number of processes to spawn, defaults to the system CPU count."""
 
     @timeit(log_level='INFO')
-    def run(self, *args):
+    def run(self, *args: Any) -> Any:
+        """
+        Executes the function across multiple processes.
+
+        :param args: Arguments to be passed to the function.
+        :return: A list of results from each process.
+        """
         if len(args) == 1 and isinstance(args, Iterable):
             args = args[0]
         processes = min([self.processes, len(args)])
         LOGGER.info("Executing function {} on {} processes in parallel.", self.function.__name__, processes)
         # TODO: Figure out if this is actually necessary, and if so, how to extract the logger and add it again
         # LOGGER.remove()  # Default "sys.stderr" sink cannot be pickled
         with Pool(processes=processes) as pool:
             out = pool.map(self.run_subprocess, args)
         return out
 
-    def run_subprocess(self, arg):
+    def run_subprocess(self, arg: Any) -> Any:
+        """
+        Sets up the logger for a subprocess, runs the function on a single argument, and then tears down the logger.
+
+        :param arg: The argument to be passed to the function.
+        :return: The result of the function execution.
+        """
         self.setup_subprocess_logger()
         out = timeit(self.function, log_level='INFO')(arg)
         self.tear_down_subprocess_logger()
         return out
 
-    def setup_subprocess_logger(self):
+    def setup_subprocess_logger(self) -> None:
+        """
+        Sets up the logger for a subprocess.
+        """
         process = current_process()
         log_file_name = str(self.log_sink.absolute() / f"log_{process.name}.log")
         setup_logger(sink=log_file_name, filter=LogFilter(), level=self.log_level)
 
     @staticmethod
-    def tear_down_subprocess_logger():
-        LOGGER.complete()  # make sure the queue (consumed by a thread started internally) is left in a stable state
+    def tear_down_subprocess_logger() -> None:
+        """
+        Tears down the logger for a subprocess.
+
+        This makes sure the queue (consumed by a thread started internally) is left in a stable state.
+        """
+        LOGGER.complete()
```

## optool/uom.py

```diff
@@ -1,988 +1,1100 @@
+"""
+Utilities for managing units of measurement and physical quantities.
+
+This module is a comprehensive suite for managing units of measurement and representing physical quantities in a
+precise, unambiguous way.
+It sets up a unit registry and provides a matching definition of a physical quantity with an associated quantity and
+unit.
+The module further offers a great variety of classes that specify physical dimensions such as length, mass, area, speed,
+density, electrical charge, and more. These classes can be used as generic type annotations for the Pydantic-compatible
+fields {py:class}`~optool.fields.quantities.UnitLike`, {py:class}`~optool.fields.quantities.QuantityLike`,
+{py:class}`optool.fields.series.SeriesLike`, etc.
+"""
+
 from typing import Optional
 
 import pint
 import pint_pandas
 
 UNITS = pint.get_application_registry()
 """
-The application's :py:class:`pint.UnitRegistry`, retrieved via :py:func:`pint.get_application_registry`.
+The application's {py:class}`pint.UnitRegistry`, retrieved via {py:func}`pint.get_application_registry`.
 
-See Also:
-    See https://stackoverflow.com/a/68089489 for why this is a good idea.
+:::{seealso}
+See [the answer on stackoverflow](https://stackoverflow.com/a/68089489) for why this is a good idea.
+:::
 """
 
 UNITS.define('square_meter = meter**2 = m² = m2')
 UNITS.define('cubic_meter = meter**3 = m3')
 
 # Exchange rates as of May 10, 2022, 3:48 p.m., taken from https://finance.yahoo.com/currency-converter/
 UNITS.define('USD = [currency] = $ = usd')
 UNITS.define('CHF = USD / 0.9918 = _ = chf')
 UNITS.define('EUR = USD / 0.9484 = € = eur')
 
 Quantity = UNITS.Quantity  # Should use this registry, see https://github.com/hgrecco/pint/issues/1480
-"""Representation of a physical quantity with a value and associated unit."""
-Quantity.__doc__ = \
-    """
-    Representation of a physical quantity with a value and associated unit.
-
-    This class represents a physical quantity, consisting of a numerical value and an associated unit. It provides
-    functionality for performing mathematical operations and conversions between different units.
-
-    Note:
-        This class is a reference to :py:class:`pint.Quantity` that has :py:data:`.UNITS` as its
-        :py:class:`pint.UnitRegistry`.
-
-    Attributes:
-        magnitude: The numerical value of the quantity.
-        units: The unit associated with the quantity.
-
-    Examples::
-
-        # Creating a Quantity object:
+"""
+Representation of a physical quantity with a value and associated unit.
 
-        >>> from optool.uom import Quantity, UNITS
-        >>> length = Quantity(5, UNITS.meter)
-        >>> print(length)
-        5 meter
+This class represents a physical quantity, consisting of a numerical value and an associated unit. It provides
+functionality for performing mathematical operations and conversions between different units.
 
-        # Performing arithmetic operations:
 
-        >>> width = Quantity(3, UNITS.meter)
-        >>> area = length * width
-        >>> print(area)
-        15 meter ** 2
+:param magnitude: The numerical value of the quantity.
+:param units: The unit associated with the quantity.
 
-        # Converting between units:
+:::{note}
+This class is a reference to {py:class}`pint.Quantity` that has {py:data}`UNITS` as its {py:class}`pint.UnitRegistry`.
+:::
+
+::::{admonition} Examples
+:class: example, dropdown
+
+- Creating a Quantity object:
+  ```python
+  from optool.uom import Quantity, UNITS
+  length = Quantity(5, UNITS.meter)
+  print(length)
+  ```
+  yields `5 meter`.
+
+- Performing arithmetic operations:
+  ```python
+  width = Quantity(3, UNITS.meter)
+  area = length * width
+  print(area)
+  ```
+  yields `15 meter ** 2`.
+
+- Converting between units:
+  ```python
+  length = Quantity(1000, UNITS.millimeter)
+  print(length.to(UNITS.meter))
+  ```
+  yields `1 meter`.
 
-        >>> length = Quantity(1000, UNITS.millimeter)
-        >>> print(length.to(UNITS.meter))
-        1 meter
-    """
+::::
+"""
 
 Unit = UNITS.Unit
-"""Representation of a unit of measurement."""
-Unit.__doc__ = \
-    """
-    Representation of a unit of measurement.
-
-    This class represents a unit of measurement, which provides a standard and consistent way to quantify and compare
-    quantities in their respective domains.
-
-    Note:
-        This class is a reference to :py:class:`pint.Unit`.
-
-    Examples::
-
-        # Creating a Unit object:
+"""
+Representation of a unit of measurement.
 
-        >>> from optool.uom import UNITS
-        >>> meter = UNITS.meter
-        >>> print(meter)
-        meter
+This class represents a unit of measurement, which provides a standard and consistent way to quantify and compare
+quantities in their respective domains.
 
-        # Performing arithmetic operations:
+:::{note}
+This class is a reference to {py:class}`pint.Unit`.
+:::
+
+::::{admonition} Examples
+:class: example, dropdown
+
+- Creating a Unit object:
+  ```python
+  from optool.uom import UNITS
+  meter = UNITS.meter
+  print(meter)
+  ```
+  yields `meter`.
+
+- Performing arithmetic operations:
+  ```python
+  from optool.uom import UNITS
+  area = UNITS.meter**2
+  print(area)
+  ```
+  yields `meter ** 2`.
+
+- Parsing strings:
+  ```python
+  from optool.uom import UNITS
+  kilometer = UNITS.parse_units("km")
+  print(kilometer)
+  ```
+  yields `kilometer`.
+"""
 
-        >>> area = UNITS.meter**2
-        >>> print(area)
-        meter ** 2
+pint_pandas.PintType.ureg = UNITS
 
-        # Parsing strings:
 
-        >>> kilometer = UNITS.parse_units("km")
-        >>> print(kilometer)
-        kilometer
+class PhysicalDimension:
     """
+    Base class representing a physical dimension.
 
-pint_pandas.PintType.ureg = UNITS
-
+    The dimensionality attribute is a string representing the SI unit of the specific physical dimension. Subclasses of
+    `PhysicalDimension` represent specific physical dimensions and should provide an appropriate dimensionality.
 
-class PhysicalDimension:
+    :::{note}
+    This class does not implement specific behavior and should not be instantiated directly. Instead, use or create a
+    subclass that represents a specific physical dimension.
+    :::
+
+    :::{seealso}
+    [Wikipedia: Dimensional analysis](wiki:Dimensional_analysis)
+    :::
+    """
     dimensionality: Optional[str] = None
+    """String representing the SI unit of the specific physical dimension."""
 
 
 class Absement(PhysicalDimension):
     """
     A measure of sustained displacement of an object from its initial position, i.e. a measure of how far away and for
     how long. In SI units, it is usually measured in meter-seconds (m·s).
 
-    See Also:
-        `Wikipedia: Absement <https://en.wikipedia.org/wiki/Absement>`_
+    :::{seealso}
+    [Wikipedia: Absement](wiki:Absement)
+    :::
     """
     strict = False
     dimensionality = '[length] * [time]'
 
 
 class Acceleration(PhysicalDimension):
     """
     Rate of change of the velocity of an object with respect to time. In SI units, it is usually measured in meters per
     square seconds (m/s²).
 
-    See Also:
-        `Wikipedia: Acceleration <https://en.wikipedia.org/wiki/Acceleration>`_
+    :::{seealso}
+    [Wikipedia: Acceleration](wiki:Acceleration)
+    :::
     """
     strict = False
     dimensionality = '[acceleration]'
 
 
 class Action(PhysicalDimension):
     """
     Energy multiplied by a duration, used to describe how a physical system has changed over time.
 
-    See Also:
-        `Wikipedia: Action (physics) <https://en.wikipedia.org/wiki/Action_(physics)>`_
+    :::{seealso}
+    [Wikipedia: Action (physics)](wiki:Action_(physics))
+    :::
     """
     strict = False
     dimensionality = '[energy] * [time]'
 
 
 class AmountOfSubstance(PhysicalDimension):
     """
     Number of elementary entities of a substance. It is one of the seven fundamental physical quantities in both the
     International System of Units (SI) and the International System of Quantities (ISQ). The SI base unit of time is the
     mole (mol).
 
     One mole contains exactly 6.022 140 76 × 10²³ elementary entities. This number is the fixed numerical value of
     the Avogadro constant, NA, when expressed in the unit 1/mol and is called the Avogadro number.
 
-    See Also:
-        `Wikipedia: Amount of substance <https://en.wikipedia.org/wiki/Amount_of_substance>`_
+    :::{seealso}
+    [Wikipedia: Amount of substance](wiki:Amount_of_substance)
+    :::
     """
     strict = False
     dimensionality = '[substance]'
 
 
 class Angle(PhysicalDimension):
     """
     A dimensionless measure describing the relative position of two beams to each other.
 
-    See Also:
-        `Wikipedia: Angle <https://en.wikipedia.org/wiki/Angle>`_
+    :::{seealso}
+    [Wikipedia: Angle](wiki:Angle)
+    :::
     """
     strict = False
     dimensionality = '[]'
 
 
 class AngularAcceleration(PhysicalDimension):
     """
     The time rate of change of angular velocity. In SI units, it is usually measured in radians per square second
     (rad/s²).
 
-    See Also:
-        `Wikipedia: Angular acceleration <https://en.wikipedia.org/wiki/Angular_acceleration>`_
+    :::{seealso}
+    [Wikipedia: Angular acceleration](wiki:Angular_acceleration)
+    :::
     """
     strict = False
     dimensionality = '[] / [time] ** 2'
 
 
 class AngularVelocity(PhysicalDimension):
     """
     A measure of how fast the angular position or orientation of an object changes with time. In SI units, it is usually
     measured in radians per second (rad/s).
 
-    See Also:
-        `Wikipedia: Angular velocity <https://en.wikipedia.org/wiki/Angular_velocity>`_
+    :::{seealso}
+    [Wikipedia: Angular velocity](wiki:Angular_velocity)
+    :::
     """
     strict = False
     dimensionality = '[] / [time]'
 
 
 class Area(PhysicalDimension):
     """
     Measure of a region's size on a surface. In SI units, it is usually measured in square meters (m²).
 
-    See Also:
-        `Wikipedia: Area <https://en.wikipedia.org/wiki/Area>`_
+    :::{seealso}
+    [Wikipedia: Area](wiki:Area)
+    :::
     """
     strict = False
     dimensionality = '[area]'
 
 
 class AreaDensity(PhysicalDimension):
     """
     Measure of the mass per unit area of a two-dimensional object. In SI units, it is usually measured in kilograms per
     square meters (kg/m²).
 
-    See Also:
-        `Wikipedia: Area density <https://en.wikipedia.org/wiki/Area_density>`_
+    :::{seealso}
+    [Wikipedia: Area density](wiki:Area_density)
+    :::
     """
     strict = False
     dimensionality = '[mass] / [area]'
 
 
 class CatalyticActivity(PhysicalDimension):
     """
     Measure for quantifying the catalytic activity of enzymes and other catalysts. The SI derived unit for measuring the
     catalytic activity of a catalyst is the katal, which is quantified in moles per second (mol/s).
 
-    See Also:
-        `Wikipedia: Catalysis <https://en.wikipedia.org/wiki/Catalysis>`_
+    :::{seealso}
+    [Wikipedia: Catalysis](wiki:Catalysis)
+    :::
     """
     strict = False
     dimensionality = '[activity]'
 
 
 class Concentration(PhysicalDimension):
     """
     Measure of the concentration of a chemical species in terms of amount of substance per unit volume of solution. In
     SI units, it is usually measured in moles per liter (mol/l).
 
-    See Also:
-        `Wikipedia: Molar concentration <https://en.wikipedia.org/wiki/Molar_concentration>`_
+    :::{seealso}
+    [Wikipedia: Molar concentration](wiki:Molar_concentration)
+    :::
     """
     strict = False
     dimensionality = '[concentration]'
 
 
 class Density(PhysicalDimension):
     """
     Measure of a substance's mass per unit of volume. In SI units, it is usually measured in kilograms per cubic meters
     (kg/m3).
 
-    See Also:
-        `Wikipedia: Density <https://en.wikipedia.org/wiki/Density>`_
+    :::{seealso}
+    [Wikipedia: Density](wiki:Density)
+    :::
     """
     strict = False
     dimensionality = '[density]'
 
 
 class Dimensionless(PhysicalDimension):
     """
     A quantity to which no physical dimension is assigned, with a corresponding SI unit of measurement of one.
 
-    See Also:
-        `Wikipedia: Dimensionless quantity <https://en.wikipedia.org/wiki/Dimensionless_quantity>`_
+    :::{seealso}
+    [Wikipedia: Dimensionless quantity](wiki:Dimensionless_quantity)
+    :::
     """
     strict = False
     dimensionality = '[]'
 
 
 class ElectricalConductivity(PhysicalDimension):
     """
     A measure of a material's ability to conduct electric current. Electrical conductivity is also called specific
-    conductance and is the reciprocal of :py:class:`ElectricalResistivity`. In SI units, it is usually measured in
+    conductance and is the reciprocal of {py:class}`ElectricalResistivity`. In SI units, it is usually measured in
     siemens per metre (S/m).
 
-    See Also:
-        `Wikipedia: Electrical resistivity and conductivity
-        <https://en.wikipedia.org/wiki/Electrical_resistivity_and_conductivity>`_
+    :::{seealso}
+    [Wikipedia: Electrical resistivity and conductivity
+       ](wiki:Electrical_resistivity_and_conductivity)
+    :::
     """
     strict = False
     dimensionality = '[conductance] / [length]'
 
 
 class ElectricalResistivity(PhysicalDimension):
     """
     A measure of how strongly a material resists electric current. Electrical resistivity is also called specific
     electrical resistance or volume resistivity. In SI units, it is usually measured in ohm-meters (Ω⋅m).
 
-    See Also:
-        `Wikipedia: Electrical resistivity and conductivity
-        <https://en.wikipedia.org/wiki/Electrical_resistivity_and_conductivity>`_
+    :::{seealso}
+    [Wikipedia: Electrical resistivity and conductivity
+       ](wiki:Electrical_resistivity_and_conductivity)
+    :::
     """
     strict = False
     dimensionality = '[resistivity]'
 
 
 class ElectricCapacitance(PhysicalDimension):
     """
     The capability of a material object or device to store electric charge, measured by the change in charge in response
     to a difference in electric potential. The unit commonly used in the SI unit system is the farad (F).
 
-    See Also:
-        `Wikipedia: Capacitance <https://en.wikipedia.org/wiki/Capacitance>`_
+    :::{seealso}
+    [Wikipedia: Capacitance](wiki:Capacitance)
+    :::
     """
     strict = False
     dimensionality = '[capacitance]'
 
 
 class ElectricCharge(PhysicalDimension):
     """
     The physical property of matter that causes charged matter to experience a force when placed in an electromagnetic
     field. The units commonly used in the SI unit system are the coulomb (C) and the ampere-hours (A·h).
 
-    See Also:
-        `Wikipedia: Electric charge <https://en.wikipedia.org/wiki/Electric_charge>`_
+    :::{seealso}
+    [Wikipedia: Electric charge](wiki:Electric_charge)
+    :::
     """
     strict = False
     dimensionality = '[charge]'
 
 
 class ElectricConductance(PhysicalDimension):
     """
     A measure for the ease with which an electric current passes. Its reciprocal quantity is
-    :py:class:`ElectricalResistance`. The unit commonly used in the SI unit system is the siemens (S).
+    {py:class}`ElectricResistance`. The unit commonly used in the SI unit system is the siemens (S).
 
-    See Also:
-        `Wikipedia: Electrical resistance and conductance
-        <https://en.wikipedia.org/wiki/Electrical_resistance_and_conductance>`_
+    :::{seealso}
+    [Wikipedia: Electrical resistance and conductance
+       ](wiki:Electrical_resistance_and_conductance)
+    :::
     """
     strict = False
     dimensionality = '[conductance]'
 
 
 class ElectricCurrent(PhysicalDimension):
     """
     A measure for the net rate of flow of electric charge through a surface or into a control volume. It is one of the
     seven fundamental physical quantities in both the International System of Units (SI) and the International System of
     Quantities (ISQ). The SI base unit of time is the ampere (A).
 
-    See Also:
-        `Wikipedia: Electric current <https://en.wikipedia.org/wiki/Electric_current>`_
+    :::{seealso}
+    [Wikipedia: Electric current](wiki:Electric_current)
+    :::
     """
     strict = False
     dimensionality = '[current]'
 
 
 class ElectricInductance(PhysicalDimension):
     """
     The ratio of the induced voltage to the rate of change of current causing it. The unit commonly used in the SI unit
     system is the henry (H).
 
-    See Also:
-        `Wikipedia: Inductance <https://en.wikipedia.org/wiki/Inductance>`_
+    :::{seealso}
+    [Wikipedia: Inductance](wiki:Inductance)
+    :::
     """
     strict = False
     dimensionality = '[inductance]'
 
 
 class ElectricPermittivity(PhysicalDimension):
     """
     A measure of the electric polarizability of a dielectric. TIn SI units, it is usually measured in farads per meter
     (F/m).
 
-    See Also:
-        `Wikipedia: Permittivity <https://en.wikipedia.org/wiki/Permittivity>`_
+    :::{seealso}
+    [Wikipedia: Permittivity](wiki:Permittivity)
+    :::
     """
     strict = False
     dimensionality = '[capacitance] / [length]'
 
 
 class ElectricPotential(PhysicalDimension):
     """
     A measure for the amount of work energy needed to move a unit of electric charge from a reference point to the
     specific point in an electric field. The unit commonly used in the SI unit system is the volt (V).
 
-    See Also:
-        `Wikipedia: Electric potential <https://en.wikipedia.org/wiki/Electric_potential>`_
+    :::{seealso}
+    [Wikipedia: Electric potential](wiki:Electric_potential)
+    :::
     """
     strict = False
     dimensionality = '[electric_potential]'
 
 
 class ElectricResistance(PhysicalDimension):
     """
     A measure for the ease with which an electric current passes. Its reciprocal quantity is
-    :py:class:`ElectricConductance`. The unit commonly used in the SI unit system is the ohm (Ω).
+    {py:class}`ElectricConductance`. The unit commonly used in the SI unit system is the ohm (Ω).
 
-    See Also:
-        `Wikipedia: Electrical resistance and conductance
-        <https://en.wikipedia.org/wiki/Electrical_resistance_and_conductance>`_
+    :::{seealso}
+    [Wikipedia: Electrical resistance and conductance
+       ](wiki:Electrical_resistance_and_conductance)
+    :::
     """
     strict = False
     dimensionality = '[resistance]'
 
 
 class Energy(PhysicalDimension):
     """
     The quantitative property that is transferred to a body or to a physical system, recognizable in the performance of
     work and in the form of heat and light. The unit commonly used in the SI unit system is the joule (J).
 
-    See Also:
-        `Wikipedia: Energy <https://en.wikipedia.org/wiki/Energy>`_
+    :::{seealso}
+    [Wikipedia: Energy](wiki:Energy)
+    :::
     """
     strict = False
     dimensionality = '[energy]'
 
 
 class Entropy(PhysicalDimension):
     """
     A scientific concept, as well as a measurable physical property, that is most commonly associated with a state of
     disorder, randomness, or uncertainty. It has dimensions of energy divided by temperature. In SI units, it is usually
     measured in joules per kelvin (J/K).
 
-    See Also:
-        `Wikipedia: Entropy <https://en.wikipedia.org/wiki/Entropy>`_
+    :::{seealso}
+    [Wikipedia: Entropy](wiki:Entropy)
+    :::
     """
     strict = False
     dimensionality = '[entropy]'
 
 
 class Fluidity(PhysicalDimension):
     """
-    The reciprocal of :py:class:`Viscosity`. In SI units, it is usually reciprocal poise (1/P), sometimes called the
+    The reciprocal of {py:class}`Viscosity`. In SI units, it is usually reciprocal poise (1/P), sometimes called the
     `rhe`.
 
-    See Also:
-        `Wikipedia: Fluidity <https://en.wikipedia.org/wiki/Fluidity>`_
+    :::{seealso}
+    [Wikipedia: Fluidity](wiki:Fluidity)
+    :::
     """
     strict = False
     dimensionality = '[fluidity]'
 
 
 class Force(PhysicalDimension):
     """
     An influence that can change the motion of an object. The unit commonly used in the SI unit system is the newton
     (N).
 
-    See Also:
-        `Wikipedia: Force <https://en.wikipedia.org/wiki/Force>`_
+    :::{seealso}
+    [Wikipedia: Force](wiki:Force)
+    :::
     """
     strict = False
     dimensionality = '[force]'
 
 
 class Frequency(PhysicalDimension):
     """
     Number of occurrences of a repeating event per unit of time. The unit commonly used in the SI unit system is the
     hertz (Hz).
 
-    See Also:
-        `Wikipedia: Frequency <https://en.wikipedia.org/wiki/Frequency>`_
+    :::{seealso}
+    [Wikipedia: Frequency](wiki:Frequency)
+    :::
     """
     strict = False
     dimensionality = '[frequency]'
 
 
 class Illuminance(PhysicalDimension):
     """
     A measure of how much the incident light illuminates the surface, wavelength-weighted by the luminosity function to
     correlate with human brightness perception. The unit commonly used in the SI unit system is the lux (lx).
 
-    See Also:
-        `Wikipedia: Illuminance <https://en.wikipedia.org/wiki/Illuminance>`_
+    :::{seealso}
+    [Wikipedia: Illuminance](wiki:Illuminance)
+    :::
     """
     strict = False
     dimensionality = '[illuminance]'
 
 
 class Impulse(PhysicalDimension):
     """
     The integral of a force over a time interval for which it acts. In SI units, it is usually measured in newton-
     seconds (N⋅s).
 
-    See Also:
-        `Wikipedia: Impulse (physics) <https://en.wikipedia.org/wiki/Impulse_(physics)>`_
+    :::{seealso}
+    [Wikipedia: Impulse (physics)](wiki:Impulse_(physics))
+    :::
     """
     strict = False
     dimensionality = '[length] * [mass] / [time]'
 
 
 class Information(PhysicalDimension):
     """
     A measure for the capacity of some standard data storage system or communication channel. The unit commonly used in
     the SI unit system is the bit.
 
-    See Also:
-        `Wikipedia: Units of information <https://en.wikipedia.org/wiki/Units_of_information>`_
+    :::{seealso}
+    [Wikipedia: Units of information](wiki:Units_of_information)
+    :::
     """
     strict = False
     dimensionality = '[]'
 
 
 class InformationRate(PhysicalDimension):
     """
     A measure for the speed of data transmission. In SI units, it is usually measured in bits per second (bit/s).
 
-    See Also:
-        `Wikipedia: Bit rate <https://en.wikipedia.org/wiki/Bit_rate>`_
+    :::{seealso}
+    [Wikipedia: Bit rate](wiki:Bit_rate)
+    :::
     """
     strict = False
     dimensionality = '[frequency]'
 
 
 class Intensity(PhysicalDimension):
     """
     A measure for the power transferred per unit area, where the area is measured on the plane perpendicular to the
     direction of propagation of the energy. In SI units, it is usually measured in watts per square meter (W/m²).
 
-    See Also:
-        `Wikipedia: Intensity (physics) <https://en.wikipedia.org/wiki/Intensity_(physics)>`_
+    :::{seealso}
+    [Wikipedia: Intensity (physics)](wiki:Intensity_(physics))
+    :::
     """
     strict = False
     dimensionality = '[intensity]'
 
 
 class IonizingRadiation(PhysicalDimension):
     """
     A measure of the energy that is emitted by certain types of atomic nuclei or subatomic particles, such as alpha
     particles, beta particles, and gamma rays. The units commonly used in the SI unit system are the gray (Gy) and the
     sievert (Sv).
 
-    See Also:
-        `Wikipedia: Ionizing radiation <https://en.wikipedia.org/wiki/Ionizing_radiation>`_
+    :::{seealso}
+    [Wikipedia: Ionizing radiation](wiki:Ionizing_radiation)
+    :::
     """
     strict = False
     dimensionality = '[energy] / [mass]'
 
 
 class KinematicViscosity(PhysicalDimension):
     """
     A measure of a fluid's resistance to flow. It is defined as the ratio of the dynamic viscosity of a fluid to its
     density. In SI units, it is usually measured in square meters per second (m^2/s).
 
-    See Also:
-        `Wikipedia: Kinematic Viscosity <https://en.wikipedia.org/wiki/Viscosity#Kinematic_viscosity>`_
+    :::{seealso}
+    [Wikipedia: Kinematic Viscosity](wiki:Viscosity#Kinematic_viscosity)
+    :::
     """
     strict = False
     dimensionality = '[kinematic_viscosity]'
 
 
 class Length(PhysicalDimension):
     """
     A measure of distance. It is one of the seven fundamental physical quantities in both the International System of
     Units (SI) and the International System of Quantities (ISQ). The SI base unit of time is the meter (m).
 
-    See Also:
-        `Wikipedia: Length <https://en.wikipedia.org/wiki/Length>`_
+    :::{seealso}
+    [Wikipedia: Length](wiki:Length)
+    :::
     """
     strict = False
     dimensionality = '[length]'
 
 
 class Luminance(PhysicalDimension):
     """
     A photometric measure of the luminous intensity per unit area of light travelling in a given direction. In SI units,
     it is usually measured in candela per square metre (cd/m²).
 
-    See Also:
-        `Wikipedia: Luminance <https://en.wikipedia.org/wiki/Luminance>`_
+    :::{seealso}
+    [Wikipedia: Luminance](wiki:Luminance)
+    :::
     """
     strict = False
     dimensionality = '[luminance]'
 
 
 class LuminousFlux(PhysicalDimension):
     """
     A measure of the perceived power of light. The unit commonly used in the SI unit system is the lumen (lm).
 
-    See Also:
-        `Wikipedia: Luminous flux <https://en.wikipedia.org/wiki/Luminous_flux>`_
+    :::{seealso}
+    [Wikipedia: Luminous flux](wiki:Luminous_flux)
+    :::
     """
     strict = False
     dimensionality = '[luminous_flux]'
 
 
 class LuminousIntensity(PhysicalDimension):
     """
     A measure of the wavelength-weighted power emitted by a light source in a particular direction per unit solid angle,
     based on the luminosity function, a standardized model of the sensitivity of the human eye. It is one of the seven
     fundamental physical quantities in both the International System of Units (SI) and the International System of
     Quantities (ISQ). The SI base unit of time is the candela (cd).
 
-    See Also:
-        `Wikipedia: Luminous intensity <https://en.wikipedia.org/wiki/Luminous_intensity>`_
+    :::{seealso}
+    [Wikipedia: Luminous intensity](wiki:Luminous_intensity)
+    :::
     """
     strict = False
     dimensionality = '[luminosity]'
 
 
 class MagneticFieldStrength(PhysicalDimension):
     """
     A measure of the intensity of a magnetic field. In SI units, it is usually measured in amperes per meter (A/m).
 
-    See Also:
-        `Wikipedia: Magnetic field - The H-field <https://en.wikipedia.org/wiki/Magnetic_field#The_H-field>`_
+    :::{seealso}
+    [Wikipedia: Magnetic field - The H-field](wiki:Magnetic_field#The_H-field)
+    :::
     """
     strict = False
     dimensionality = '[magnetic_field_strength]'
 
 
 class MagneticFlux(PhysicalDimension):
     """
     A measure of how many magnetic field lines are passing through a given surface area, that is, a measure of the
     strength of the magnetic field. The unit commonly used in the SI unit system is the weber (Wb).
 
-    See Also:
-        `Wikipedia: Magnetic flux <https://en.wikipedia.org/wiki/Magnetic_flux>`_
+    :::{seealso}
+    [Wikipedia: Magnetic flux](wiki:Magnetic_flux)
+    :::
     """
     strict = False
     dimensionality = '[magnetic_flux]'
 
 
 class MagneticFluxDensity(PhysicalDimension):
     """
     A measure of the actual magnetic field within a material considered as a concentration of magnetic field lines, or
     flux, per unit cross-sectional area. It is also called the magnitude of the magnetic field. The unit commonly used
     in the SI unit system is the tesla (T).
 
-    See Also:
-        `Wikipedia: Magnetic field - The B-field <https://en.wikipedia.org/wiki/Magnetic_field#The_B-field>`_
+    :::{seealso}
+    [Wikipedia: Magnetic field - The B-field](wiki:Magnetic_field#The_B-field)
+    :::
     """
     strict = False
     dimensionality = '[magnetic_field]'
 
 
 class MagneticPermeability(PhysicalDimension):
     """
     A measure of magnetization that a material obtains in response to an applied magnetic field. In SI units, it is
     usually measured in henries per meter (H/m) or equivalently in newtons per ampere squared (N/A²).
 
-    See Also:
-        `Wikipedia: Permeability (electromagnetism)
-        <https://en.wikipedia.org/wiki/Permeability_(electromagnetism)>`_
+    :::{seealso}
+    [Wikipedia: Permeability (electromagnetism)
+       ](wiki:Permeability_(electromagnetism))
+    :::
     """
     strict = False
     dimensionality = '[inductance] / [length]'
 
 
 class MagnetomotiveForce(PhysicalDimension):
     """
     The property of certain substances or phenomena that give rise to magnetic fields. The unit commonly used in the SI
     unit system is the ampere (A).
 
-    See Also:
-        `Wikipedia: Magnetomotive force <https://en.wikipedia.org/wiki/Magnetomotive_force>`_
+    :::{seealso}
+    [Wikipedia: Magnetomotive force](wiki:Magnetomotive_force)
+    :::
     """
     strict = False
     dimensionality = '[magnetomotive_force]'
 
 
 class Mass(PhysicalDimension):
     """
     One of the seven fundamental physical quantities in both the International System of Units (SI) and the
     International System of Quantities (ISQ). The SI base unit of time is the kilogram (kg).
 
-    See Also:
-        `Wikipedia: Mass <https://en.wikipedia.org/wiki/Mass>`_
+    :::{seealso}
+    [Wikipedia: Mass](wiki:Mass)
+    :::
     """
     strict = False
     dimensionality = '[mass]'
 
 
 class MassFlowRate(PhysicalDimension):
     """
     A measure of how much mass of a substance passes per unit of time. In SI units, it is usually measured in kilograms
     per second (kg/s).
 
-    See Also:
-        `Wikipedia: Mass flow rate <https://en.wikipedia.org/wiki/Mass_flow_rate>`_
+    :::{seealso}
+    [Wikipedia: Mass flow rate](wiki:Mass_flow_rate)
+    :::
     """
     strict = False
     dimensionality = '[mass] / [time]'
 
 
 class MolarEntropy(PhysicalDimension):
     """
     A measure of entropy per mole. In SI units, it is usually measured in joules per mole per kelvin (J/(mol⋅K)).
 
-    See Also:
-        `Wikipedia: Entropy <https://en.wikipedia.org/wiki/Entropy>`_
+    :::{seealso}
+    [Wikipedia: Entropy](wiki:Entropy)
+    :::
     """
     strict = False
     dimensionality = '[molar_entropy]'
 
 
 class MomentOfInertia(PhysicalDimension):
     """
     A measure of how much torque is needed for a desired angular acceleration about a rotational axis, similar to how
     mass determines the force needed for a desired acceleration. It is also known as the rotational inertia. In SI
     units, it is usually measured in kilogram-square meters (kg⋅m²).
 
-    See Also:
-        `Wikipedia: Moment of inertia <https://en.wikipedia.org/wiki/Moment_of_inertia>`_
+    :::{seealso}
+    [Wikipedia: Moment of inertia](wiki:Moment_of_inertia)
+    :::
     """
     strict = False
     dimensionality = '[mass] * [length] ** 2'
 
 
 class Momentum(PhysicalDimension):
     """
     The product of the mass and velocity of an object. In SI units, it is usually measured in kilogram metre per second
     (kg⋅m/s), which is equivalent to the newton-second (N⋅s).
 
-    See Also:
-        `Wikipedia: Momentum <https://en.wikipedia.org/wiki/Momentum>`_
+    :::{seealso}
+    [Wikipedia: Momentum](wiki:Momentum)
+    :::
     """
     strict = False
     dimensionality = '[momentum]'
 
 
 class Power(PhysicalDimension):
     """
     The amount of energy transferred or converted per unit time. The unit commonly used in the SI unit system is the
     watt (W).
 
-    See Also:
-        `Wikipedia: Power (physics) <https://en.wikipedia.org/wiki/Power_(physics)>`_
+    :::{seealso}
+    [Wikipedia: Power (physics)](wiki:Power_(physics))
+    :::
     """
     strict = False
     dimensionality = '[power]'
 
 
 class Pressure(PhysicalDimension):
     """
     The force applied perpendicular to the surface of an object per unit area over which that force is distributed. The
     unit commonly used in the SI unit system is the pascal (Pa).
 
-    See Also:
-        `Wikipedia: Pressure <https://en.wikipedia.org/wiki/Pressure>`_
+    :::{seealso}
+    [Wikipedia: Pressure](wiki:Pressure)
+    :::
     """
     strict = False
     dimensionality = '[pressure]'
 
 
 class Radiance(PhysicalDimension):
     """
     A measure of the radiant flux emitted, reflected, transmitted or received by a given surface, per unit solid angle
     per unit projected area. In SI units, it is usually measured in watts per steradian per square metre (W/(sr·m²)).
 
-    See Also:
-        `Wikipedia: Radiance <https://en.wikipedia.org/wiki/Radiance>`_
+    :::{seealso}
+    [Wikipedia: Radiance](wiki:Radiance)
+    :::
     """
     strict = False
     dimensionality = '[power] / [length] ** 2'
 
 
 class RadiantIntensity(PhysicalDimension):
     """
     A measure of the radiant flux emitted, reflected, transmitted or received, per unit solid angle. In SI units, it is
     usually measured in watts per steradian (W/sr).
 
-    See Also:
-        `Wikipedia: Radiant intensity <https://en.wikipedia.org/wiki/Radiant_intensity>`_
+    :::{seealso}
+    [Wikipedia: Radiant intensity](wiki:Radiant_intensity)
+    :::
     """
     strict = False
     dimensionality = '[power]'
 
 
 class RadiationDoseAbsorbed(PhysicalDimension):
     """
     A measure for the amount of energy deposited per unit of mass. The unit commonly used in the SI unit system is the
     gray (Gy).
 
-    See Also:
-        `Wikipedia: Absorbed dose <https://en.wikipedia.org/wiki/Absorbed_dose>`_
+    :::{seealso}
+    [Wikipedia: Absorbed dose](wiki:Absorbed_dose)
+    :::
     """
     strict = False
     dimensionality = '[energy] / [mass]'
 
 
 class RadiationDoseEffective(PhysicalDimension):
     """
     A measure for the effective dose of radiation received by a human or some other living organism. The unit commonly
     used in the SI unit system is the sievert (Sv).
 
-    See Also:
-        `Wikipedia: Effective dose (radiation) <https://en.wikipedia.org/wiki/Effective_dose_(radiation)>`_
+    :::{seealso}
+    [Wikipedia: Effective dose (radiation)](wiki:Effective_dose_(radiation))
+    :::
     """
     strict = False
     dimensionality = '[energy] / [mass]'
 
 
 class Radioactivity(PhysicalDimension):
     """
     A measure for the activity of a radioactive material in which one nucleus decays per unit of time. The unit commonly
     used in the SI unit system is the becquerel (Bq).
 
-    See Also:
-        `Wikipedia: Radioactive decay <https://en.wikipedia.org/wiki/Radioactive_decay>`_
+    :::{seealso}
+    [Wikipedia: Radioactive decay](wiki:Radioactive_decay)
+    :::
     """
     strict = False
     dimensionality = '[] / [time]'
 
 
 class SolidAngle(PhysicalDimension):
     """
     A measure of the amount of the field of view from some particular point that a given object covers. The unit
     commonly used in the SI unit system is the steradian (sr).
 
-    See Also:
-        `Wikipedia: Radioactive decay <https://en.wikipedia.org/wiki/Radioactive_decay>`_
+    :::{seealso}
+    [Wikipedia: Radioactive decay](wiki:Radioactive_decay)
+    :::
     """
     strict = False
     dimensionality = '[]'
 
 
 class SpecificHeatCapacity(PhysicalDimension):
     """
     A measure for the amount of heat energy required to raise the temperature of a substance per unit of mass. In SI
     units, it is usually measured in joules per kelvin per kilogram (J/(kg·K)).
 
-    See Also:
-        `Wikipedia: Specific heat capacity <https://en.wikipedia.org/wiki/Specific_heat_capacity>`_
+    :::{seealso}
+    [Wikipedia: Specific heat capacity](wiki:Specific_heat_capacity)
+    :::
     """
     strict = False
     dimensionality = '[energy] / [mass] / [temperature]'
 
 
 class Speed(PhysicalDimension):
     """
     The magnitude of the change of an object's position over time or the magnitude of the change of the object's
-    position per unit of time. Speed is not the same as :py:class:`Velocity`, which is a vector quantity that has both
+    position per unit of time. Speed is not the same as {py:class}`Velocity`, which is a vector quantity that has both
     magnitude and direction. In SI units, it is usually measured in meters per second (m/s).
 
-    See Also:
-        `Wikipedia: Speed <https://en.wikipedia.org/wiki/Speed>`_
+    :::{seealso}
+    [Wikipedia: Speed](wiki:Speed)
+    :::
     """
     strict = False
     dimensionality = '[speed]'
 
 
 class Temperature(PhysicalDimension):
     """
     A physical quantity that expresses quantitatively the perceptions of hotness and coldness. It is one of the seven
     fundamental physical quantities in both the International System of Units (SI) and the International System of
     Quantities. The SI base unit of time is the kelvin (K).
 
-    See Also:
-        `Wikipedia: Temperature <https://en.wikipedia.org/wiki/Temperature>`_
+    :::{seealso}
+    [Wikipedia: Temperature](wiki:Temperature)
+    :::
     """
     strict = False
     dimensionality = '[temperature]'
 
 
 class ThermalConductance(PhysicalDimension):
     """
     A measure of how much heat passes in unit time through a plate of particular area and thickness when its opposite
     faces differ in temperature by one kelvin. In SI units, it is usually measured in watts per kelvin (W/K).
 
-    See Also:
-        `Wikipedia: Thermal conductivity <https://en.wikipedia.org/wiki/Thermal_conductivity>`_
+    :::{seealso}
+    [Wikipedia: Thermal conductivity](wiki:Thermal_conductivity)
+    :::
     """
     strict = False
     dimensionality = '[power] / [temperature]'
 
 
 class ThermalConductivity(PhysicalDimension):
     """
     A measure of a material's ability to conduct heat. In SI units, it is usually measured in watts per meter per kelvin
     (W/(m·K)).
 
-    See Also:
-        `Wikipedia: Thermal conductivity <https://en.wikipedia.org/wiki/Thermal_conductivity>`_
+    :::{seealso}
+    [Wikipedia: Thermal conductivity](wiki:Thermal_conductivity)
+    :::
     """
     strict = False
     dimensionality = '[power] / ([length] * [temperature])'
 
 
 class ThermalInsulance(PhysicalDimension):
     """
     A measure of how well a two-dimensional barrier, such as a layer of insulation, a window or a complete wall or
     ceiling, resists the conductive flow of heat. In SI units, it is usually measured in square meter-kelvins per watt
     (K·m²/W).
 
-    See Also:
-        `Wikipedia: R-value (insulation) <https://en.wikipedia.org/wiki/R-value_%28insulation%29>`_
+    :::{seealso}
+    [Wikipedia: R-value (insulation)](wiki:R-value_%28insulation%29)
+    :::
     """
     strict = False
     dimensionality = '[temperature] * [length] ** 2 / [power]'
 
 
 class ThermalResistance(PhysicalDimension):
     """
     The inverse of thermal conductance. It is a convenient measure to use in multi-component design since thermal
     resistances are additive when occurring in series. In SI units, it is usually measured in kelvins per watt (K/W).
 
-    See Also:
-        `Wikipedia: Thermal conductivity <https://en.wikipedia.org/wiki/Thermal_conductivity>`_
+    :::{seealso}
+    [Wikipedia: Thermal conductivity](wiki:Thermal_conductivity)
+    :::
     """
     strict = False
     dimensionality = '[temperature] / [power]'
 
 
 class ThermalResistivity(PhysicalDimension):
     """
-    The reciprocal of :py:class:`ThermalConductivity`. It is a measure of a material's ability to resists the conductive
+    The reciprocal of {py:class}`ThermalConductivity`. It is a measure of a material's ability to resists the conductive
     flow of heat. TIn SI units, it is usually measured in kelvin-meters per watt (m·K)/W).
 
-    See Also:
-        `Wikipedia: Thermal conductivity <https://en.wikipedia.org/wiki/Thermal_conductivity>`_
+    :::{seealso}
+    [Wikipedia: Thermal conductivity](wiki:Thermal_conductivity)
+    :::
     """
     strict = False
     dimensionality = '[length] * [temperature] / [power]'
 
 
 class ThermalTransmittance(PhysicalDimension):
     """
     A measure for the rate of transfer of heat through matter, typically expressed as a U-value. In SI units, it is
     usually measured in watts per square metre per kelvin (W/(m²·K)).
 
-    See Also:
-        `Wikipedia: Thermal transmittance <https://en.wikipedia.org/wiki/Thermal_transmittance>`_
+    :::{seealso}
+    [Wikipedia: Thermal transmittance](wiki:Thermal_transmittance)
+    :::
     """
     strict = False
     dimensionality = '[power] / ([length] ** 2 * [temperature])'
 
 
 class Time(PhysicalDimension):
     """
     One of the seven fundamental physical quantities in both the International System of Units (SI) and the
     International System of Quantities (ISQ). The SI base unit of time is the second (s).
 
-    See Also:
-        `Wikipedia: Time <https://en.wikipedia.org/wiki/Time>`_
+    :::{seealso}
+    [Wikipedia: Time](wiki:Time)
+    :::
     """
     strict = False
     dimensionality = '[time]'
 
 
 class Torque(PhysicalDimension):
     """
-    The rotational equivalent of linear :py:class:`Force`. In SI units, it is usually measured in newton-meters (N·m).
+    The rotational equivalent of linear {py:class}`Force`. In SI units, it is usually measured in newton-meters (N·m).
 
-    See Also:
-        `Wikipedia: Torque <https://en.wikipedia.org/wiki/Torque>`_
+    :::{seealso}
+    [Wikipedia: Torque](wiki:Torque)
+    :::
     """
     strict = False
     dimensionality = '[torque]'
 
 
 class Velocity(PhysicalDimension):
     """
     The directional speed of an object in motion as an indication of its rate of change in position as observed from a
     particular frame of reference and as measured by a particular standard of time. It is a physical vector quantity.
     Hence, both magnitude and direction are needed to define it. The scalar absolute value (magnitude) of velocity is
-    :py:class:`Speed`. In SI units, it is usually measured in meters per second (m/s).
+    {py:class}`Speed`. In SI units, it is usually measured in meters per second (m/s).
 
-    See Also:
-        `Wikipedia: Velocity <https://en.wikipedia.org/wiki/Velocity>`_
+    :::{seealso}
+    [Wikipedia: Velocity](wiki:Velocity)
+    :::
     """
     strict = False
     dimensionality = '[velocity]'
 
 
 class Viscosity(PhysicalDimension):
     """
     A measure of a fluid's resistance to deformation at a given rate. The unit commonly used in the SI unit system is
     the poise (P).
 
-    See Also:
-        `Wikipedia: Viscosity <https://en.wikipedia.org/wiki/Viscosity>`_
+    :::{seealso}
+    [Wikipedia: Viscosity](wiki:Viscosity)
+    :::
     """
     strict = False
     dimensionality = '[viscosity]'
 
 
 class Volume(PhysicalDimension):
     """
     A measure of three-dimensional space. In SI units, it is usually measured in cubic meters (m3).
 
-    See Also:
-        `Wikipedia: Volume <https://en.wikipedia.org/wiki/Volume>`_
+    :::{seealso}
+    [Wikipedia: Volume](wiki:Volume)
+    :::
     """
     strict = False
     dimensionality = '[volume]'
 
 
 class VolumetricFlowRate(PhysicalDimension):
     """
     The volume of fluid that passes per unit time. It is also known as volume flow rate, or volume velocity. In SI
     units, it is usually measured in cubic metres per second (m3/s).
 
-    See Also:
-        `Wikipedia: Volumetric flow rate <https://en.wikipedia.org/wiki/Volumetric_flow_rate>`_
+    :::{seealso}
+    [Wikipedia: Volumetric flow rate](wiki:Volumetric_flow_rate)
+    :::
     """
     strict = False
     dimensionality = '[volumetric_flow_rate]'
 
 
 class WaveNumber(PhysicalDimension):
     """
     The spatial frequency of a wave, measured in cycles per unit distance. SI units, it is usually measured in
     reciprocal of meters (1/m).
 
-    See Also:
-        `Wikipedia: Wavenumber <https://en.wikipedia.org/wiki/Wavenumber>`_
+    :::{seealso}
+    [Wikipedia: Wavenumber](wiki:Wavenumber)
+    :::
     """
     strict = False
     dimensionality = '[wavenumber]'
```

## optool/util.py

```diff
@@ -1,36 +1,55 @@
+"""
+General utility classes used within this package.
+
+This module contains utility classes designed to augment and simplify coding tasks.
+"""
+
 from __future__ import annotations
 
 from enum import Enum
 
 import numpy as np
 
-from optool import BaseModel
+from optool.core import BaseModel
 from optool.fields.misc import NonEmptyStr
 
 
 class StrEnum(str, Enum):
+    """
+    Python {py:class}`~enum.Enum` that inherits from {py:class}`str` to complement {py:class}`~enum.IntEnum` in the
+    standard library.
+
+    It can be used to create string enums, where the enum members are instances of {py:class}`str`.
+
+    The implementation ensures that a lower case string of the {py:class}`enum.Enum` member is produced as its value.
+    """
 
     def _generate_next_value_(self, *_):
         return self.lower()
 
 
 class ValueRange(BaseModel, frozen=True):
     """Ranges of the normed values of the optimization variables."""
 
     name: NonEmptyStr
     """The name of the decision variable."""
 
     min: float
+    """The smallest value."""
     avg: float
+    """The average (arithmetic mean) value."""
     max: float
+    """The greatest value."""
     max_abs: float
+    """The greatest absolute value."""
 
     @classmethod
-    def of(cls, name: str, val: np.ndarray):
+    def of(cls, name: str, val: np.ndarray) -> ValueRange:
         """
+        Creates a value range by analyzing the numeric array specified.
 
-        Args:
-            name: The name of the decision variable.
-            val: The normed values as array.
+        :param name: The name of the decision variable.
+        :param val: The normed values as array.
+        :returns: The value range of the array specified.
         """
         return cls(name=name, min=np.min(val), avg=float(np.mean(val)), max=np.max(val), max_abs=np.max(np.abs(val)))
```

## optool/fields/__init__.py

```diff
@@ -0,0 +1,40 @@
+00000000: 2222 220a 4375 7374 6f6d 2050 7964 616e  """.Custom Pydan
+00000010: 7469 632d 636f 6d70 6174 6962 6c65 2066  tic-compatible f
+00000020: 6965 6c64 7320 666f 7220 6461 7461 2076  ields for data v
+00000030: 616c 6964 6174 696f 6e20 616e 6420 7061  alidation and pa
+00000040: 7273 696e 672e 0a0a 5468 6973 2070 6163  rsing...This pac
+00000050: 6b61 6765 2069 7320 6120 636f 6d70 7265  kage is a compre
+00000060: 6865 6e73 6976 6520 746f 6f6c 6b69 7420  hensive toolkit 
+00000070: 666f 7220 6164 7661 6e63 6564 2064 6174  for advanced dat
+00000080: 6120 7661 6c69 6461 7469 6f6e 2062 6173  a validation bas
+00000090: 6564 206f 6e20 7468 6520 5079 7468 6f6e  ed on the Python
+000000a0: 206c 6962 7261 7279 0a5b 5079 6461 6e74   library.[Pydant
+000000b0: 6963 5d28 6874 7470 733a 2f2f 646f 6373  ic](https://docs
+000000c0: 2e70 7964 616e 7469 632e 6465 762f 6c61  .pydantic.dev/la
+000000d0: 7465 7374 2f29 2e0a 4974 2070 726f 7669  test/)..It provi
+000000e0: 6465 7320 6120 7375 6974 6520 6f66 2063  des a suite of c
+000000f0: 7573 746f 6d20 5079 6461 6e74 6963 2d63  ustom Pydantic-c
+00000100: 6f6d 7061 7469 626c 6520 6669 656c 6473  ompatible fields
+00000110: 2064 6573 6967 6e65 6420 746f 2065 6e66   designed to enf
+00000120: 6f72 6365 2073 7065 6369 6669 6320 636f  orce specific co
+00000130: 6e73 7472 6169 6e74 7320 6f6e 2076 6172  nstraints on var
+00000140: 696f 7573 2064 6174 6120 7479 7065 732c  ious data types,
+00000150: 0a69 6e63 6c75 6469 6e67 206e 756d 6265  .including numbe
+00000160: 7273 2c20 4e75 6d70 7920 6172 7261 7973  rs, Numpy arrays
+00000170: 2c20 7175 616e 7469 7469 6573 2c20 7379  , quantities, sy
+00000180: 6d62 6f6c 6963 2065 7870 7265 7373 696f  mbolic expressio
+00000190: 6e73 2c20 636f 6e74 6169 6e65 7273 2c20  ns, containers, 
+000001a0: 6361 6c6c 6162 6c65 732c 2061 6e64 2050  callables, and P
+000001b0: 616e 6461 7320 5365 7269 6573 2061 6e64  andas Series and
+000001c0: 0a44 6174 6146 7261 6d65 732e 0a54 6865  .DataFrames..The
+000001d0: 7365 2063 7573 746f 6d20 6669 656c 6473  se custom fields
+000001e0: 2065 6e73 7572 6520 7468 6174 2064 6174   ensure that dat
+000001f0: 6120 6164 6865 7265 7320 746f 2074 6865  a adheres to the
+00000200: 2073 7065 6369 6669 6320 7265 7175 6972   specific requir
+00000210: 656d 656e 7473 206f 6620 6f70 7469 6d69  ements of optimi
+00000220: 7a61 7469 6f6e 206d 6f64 656c 732c 2073  zation models, s
+00000230: 7570 706f 7274 696e 6720 726f 6275 7374  upporting robust
+00000240: 2061 6e64 0a72 656c 6961 626c 6520 6d6f   and.reliable mo
+00000250: 6465 6c20 636f 6e73 7472 7563 7469 6f6e  del construction
+00000260: 2061 6e64 2065 7865 6375 7469 6f6e 2e0a   and execution..
+00000270: 2222 220a                                """.
```

## optool/fields/callables.py

```diff
@@ -1,37 +1,33 @@
+"""
+Pydantic-compatible field types for objects that are callable.
+
+This module provides utilities to validate and enforce constraints on Python callable objects.
+A callable in Python is any object that can be called like a function.
+The validation includes checks on the number of parameters and the types of both these parameters and the return value.
+"""
+
 from __future__ import annotations
 
 import inspect
 from typing import Any, Callable, Optional, Type, Union, get_type_hints
 
 from pydantic.fields import ModelField
 
 from optool.fields.util import check_only_one_specified, get_type_validator, update_object_schema
 
 
-class CallableParameterError(ValueError):
-
-    def __init__(self, *, spec: str, expected: Any, value: inspect.FullArgSpec) -> None:
-        super().__init__(f"expected the {spec} {expected}, but got a value with argument specification {value}")
-
-
-class UnverifiableCallableParameterError(ValueError):
-
-    def __init__(self, *, spec: str, expected: Any, value: inspect.FullArgSpec) -> None:
-        super().__init__(f"cannot verify the callable with argument specification {value} "
-                         f"for expected {spec} {expected}")
-
-
 class ConstrainedCallable:
     """
-    Pydantic-compatible field type for :py:class:`typing.Callable` objects.
+    Pydantic-compatible field type for {py:class}`typing.Callable` objects.
 
-    See Also:
-        `Pydantic documentation: Custom Data Types <https://docs.pydantic.dev/usage/types/#custom-data-types>`_ and
-        class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`
+    :::{seealso}
+    [Pydantic documentation: Custom Data Types](https://docs.pydantic.dev/usage/types/#custom-data-types) and
+    {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`
+    :::
     """
 
     num_params: Optional[int] = None
     param_types: Optional[tuple[type, ...]]
     return_type: Optional[type] = None
 
     @classmethod
@@ -93,12 +89,55 @@
         return arg_spec
 
 
 def concallable(*,
                 num_params: Optional[int] = None,
                 param_types: Union[type, tuple[type, ...], None] = None,
                 return_type: Optional[type] = None) -> Type[Callable]:
+    """
+    Function to create a constrained callable value.
+
+    The constraints could be the number of parameters, types of parameters, and the return type.
+
+    :param num_params: The number of parameters for the callable. Default is {py:data}`None`.
+    :param param_types: The type or types of parameters for the callable. If a single type is specified, it is converted
+        into a tuple. Default is {py:data}`None`.
+    :param return_type: The return type of the callable. Default is {py:data}`None`.
+    :return: A new type that is a subclass of {py:class}`ConstrainedCallable`.
+    :raises ValueError: If both `num_params` and `param_types` are specified.
+
+    :::{seealso}
+    {py:func}`pydantic.conint`, {py:func}`pydantic.confloat`, etc.
+    :::
+    """
     check_only_one_specified(num_params, param_types, "Cannot specify both number and types of parameters.")
     if isinstance(param_types, type):
         param_types = (param_types,)
     namespace = dict(num_params=num_params, param_types=param_types, return_type=return_type)
     return type('ConstrainedCallableValue', (ConstrainedCallable,), namespace)
+
+
+class CallableParameterError(ValueError):
+    """
+    Raised when the callable parameter does not meet the expectations.
+
+    :param spec: The specification for the callable parameter.
+    :param expected: The expected callable parameter.
+    :param value: The actual value of the callable parameter.
+    """
+
+    def __init__(self, *, spec: str, expected: Any, value: inspect.FullArgSpec) -> None:
+        super().__init__(f"expected the {spec} {expected}, but got a value with argument specification {value}")
+
+
+class UnverifiableCallableParameterError(ValueError):
+    """
+    Raised when it's not possible to verify the callable parameter.
+
+    :param spec: The specification for the callable parameter.
+    :param expected: The expected callable parameter.
+    :param value: The actual value of the callable parameter.
+    """
+
+    def __init__(self, *, spec: str, expected: Any, value: inspect.FullArgSpec) -> None:
+        super().__init__(f"cannot verify the callable with argument specification {value} "
+                         f"for expected {spec} {expected}")
```

## optool/fields/containers.py

```diff
@@ -1,19 +1,28 @@
+"""
+Pydantic-compatible field types for data containers such as lists.
+
+This module offers functionalities to create and manage typed and constrained containers that allow type checking and
+validation.
+This ensures that data integrity is maintained as elements are added or manipulated within the container.
+"""
+
 import types
 import typing
 from typing import TYPE_CHECKING, Callable, Iterable, List, Optional, Type, TypeVar, Union, get_args
 
 import pydantic
 from pydantic import ValidationError, validate_model
 from pydantic.error_wrappers import ErrorWrapper
 
-from optool import BaseModel
+from optool.core import BaseModel
 from optool.fields.util import ValidationFunc
 
 T = TypeVar("T")
+"""Type variable without an upper bound, specifying the type of the elements of a constrained list."""
 
 # Define _CustomList as a workaround for: https://github.com/python/mypy/issues/11427
 #
 # According to this issue, the typeshed contains a "white lie" (it adds MutableSequence to the ancestry of list), which
 # completely messes with the type inference.
 
 if TYPE_CHECKING:
@@ -38,16 +47,20 @@
     return _ListModel
 
 
 class ConstrainedMutatingList(_CustomList[T]):
     """
     Typed container that runs validation when an element is added to the list.
 
-    See Also:
-        https://github.com/pydantic/pydantic/issues/496#issuecomment-904308727
+    This ability provides runtime assurance that all elements adhere to specified conditions, which may include checks
+    such as type compliance, value range, or any custom-defined criteria.
+
+    :::{seealso}
+    https://github.com/pydantic/pydantic/issues/496#issuecomment-904308727
+    :::
     """
     min_items: Optional[int] = None
     max_items: Optional[int] = None
     custom_validators: Union[ValidationFunc, Iterable[ValidationFunc], None] = None
 
     _ListModel = None
 
@@ -112,10 +125,23 @@
 
 @typing.no_type_check
 def conlist(item_type: Type[T],
             *,
             min_items: Optional[int] = None,
             max_items: Optional[int] = None,
             custom: Union[ValidationFunc, Iterable[ValidationFunc], None] = None) -> Type[List[T]]:
+    """
+    Generates a list type with constraints on the length of the list and on the type of items it is allowed to contain.
+
+    This function creates a list type with optional minimum and maximum length constraints. Additionally, it can enforce
+    a list to have a specific type of items and also applies custom validation functions.
+
+    :param item_type: The type of items the list should contain.
+    :param min_items: The minimum number of items the list should contain. (Default: {py:data}`None`)
+    :param max_items: The maximum number of items the list should contain. (Default: {py:data}`None`)
+    :param custom: Custom validation functions. This could be a single function or an iterable of functions. (Default:
+        {py:data}`None`)
+    :return: A list type with the specified constraints.
+    """
     namespace = dict(min_items=min_items, max_items=max_items, custom_validators=custom)
     return types.new_class(  # type:ignore
         'ConstrainedMutatingListValue', (ConstrainedMutatingList[item_type],), {}, lambda ns: ns.update(namespace))
```

## optool/fields/dataframe.py

```diff
@@ -1,29 +1,32 @@
+"""
+Pydantic-compatible field types for [Pandas](https://pypi.org/project/pandas/) DataFrame objects.
+
+This module contains classes that provide Pydantic-compatible field types specifically tailored for Pandas DataFrame
+objects.
+These custom fields allow developers to enforce specific index types.
+"""
+
 from typing import TYPE_CHECKING, Any, Optional, Type
 
 import pandas as pd
 from pandas import DatetimeIndex, Index, TimedeltaIndex
 from pydantic.fields import ModelField
 
 from optool.fields.util import get_type_validator, update_object_schema
 
 
-class IndexTypeError(ValueError):
-
-    def __init__(self, *, expected: Type[Index], value: pd.DataFrame) -> None:
-        super().__init__(f"expected index type {expected}, but got a DataFrame with index type {type(value.index)}")
-
-
 class ConstrainedDataFrame:
     """
-    Pydantic-compatible field type for :py:class:`pandas.DataFrame` objects, which allows to specify the index type.
+    Pydantic-compatible field type for {py:class}`pandas.DataFrame` objects, which allows to specify the index type.
 
-    See Also:
-        `Pydantic documentation: Custom Data Types <https://docs.pydantic.dev/usage/types/#custom-data-types>`_ and
-        class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`
+    :::{seealso}
+    [Pydantic documentation: Custom Data Types](https://docs.pydantic.dev/usage/types/#custom-data-types) and
+    {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`
+    :::
     """
 
     strict: bool = True
     index_type: Type[Index] = pd.RangeIndex
 
     @classmethod
     def __get_validators__(cls):
@@ -46,24 +49,59 @@
     @classmethod
     def validate_index_type(cls, val: pd.DataFrame, field: ModelField) -> pd.DataFrame:
         if cls.index_type is None or isinstance(val.index, cls.index_type):
             return val
         raise IndexTypeError(expected=cls.index_type, value=val)
 
 
+class DataFrameLike(ConstrainedDataFrame):
+    """
+    Pydantic-compatible field type for {py:class}`~pandas.DataFrame` objects.
+
+    Assigned values not already of type {py:class}`~pandas.DataFrame` are parsed using the regular constructor
+    {py:class}`DataFrame(val) <pandas.DataFrame>`.
+    """
+    strict = False
+
+
+class DatetimeDataFrame(ConstrainedDataFrame):
+    """
+    Pydantic-compatible field type for {py:class}`~pandas.DataFrame` objects, the index of which must be of type
+    {py:class}`~pandas.DatetimeIndex`.
+
+    Assigned values not already of type {py:class}`~pandas.DataFrame` are parsed using the regular constructor
+    {py:class}`DataFrame(val) <pandas.DataFrame>`.
+    """
+    strict = False
+    index_type = DatetimeIndex
+
+
+class TimedeltaDataFrame(ConstrainedDataFrame):
+    """
+    Pydantic-compatible field type for {py:class}`~pandas.DataFrame` objects, the index of which must be of type
+    {py:class}`~pandas.TimedeltaIndex`.
+
+    Assigned values not already of type {py:class}`~pandas.DataFrame` are parsed using the regular constructor
+    {py:class}`DataFrame(val) <pandas.DataFrame>`.
+    """
+    strict = False
+    index_type = TimedeltaIndex
+
+
 if TYPE_CHECKING:
-    DataFrameLike = pd.DataFrame
-    DatetimeDataFrame = pd.DataFrame
-    TimedeltaDataFrame = pd.DataFrame
-
-else:
-
-    class DataFrameLike(ConstrainedDataFrame):
-        strict = False
-
-    class DatetimeDataFrame(ConstrainedDataFrame):
-        strict = False
-        index_type = DatetimeIndex
-
-    class TimedeltaDataFrame(ConstrainedDataFrame):
-        strict = False
-        index_type = TimedeltaIndex
+    from typing_extensions import TypeAlias
+
+    DataFrameLike: TypeAlias = pd.DataFrame  # type: ignore[no-redef] # noqa: F811
+    DatetimeDataFrame: TypeAlias = pd.DataFrame  # type: ignore[no-redef] # noqa: F811
+    TimedeltaDataFrame: TypeAlias = pd.DataFrame  # type: ignore[no-redef] # noqa: F811
+
+
+class IndexTypeError(ValueError):
+    """
+    Raised when the type of index of a {py:class}`pandas.DataFrame` does not meet the expectations.
+
+    :param expected: The expected type of the index.
+    :param value: The DataFrame that causes the error due to its index type.
+    """
+
+    def __init__(self, *, expected: Type[Index], value: pd.DataFrame) -> None:
+        super().__init__(f"expected index type {expected}, but got a DataFrame with index type {type(value.index)}")
```

## optool/fields/misc.py

```diff
@@ -1,34 +1,79 @@
+"""
+Miscellaneous Pydantic-compatible field types for custom validations.
+
+This module contains a collection of Pydantic-compatible field types, designed for enforcing specific validation rules
+on strings and numbers, including non-empty strings, number bounds within the [0, 1] interval, finiteness checks, etc.
+"""
+
 from typing import TYPE_CHECKING
 
 from pydantic import ConstrainedFloat, ConstrainedStr
 
+
+class NonEmptyStr(ConstrainedStr):
+    """Pydantic-compatible field type for non-empty strings, with leading and trailing spaces removed."""
+    strict = True
+    strip_whitespace = True
+    min_length = 1
+
+
+class FractionFloat(ConstrainedFloat):
+    """
+    Pydantic-compatible field type for numbers, enforcing them to be greater than or equal to zero and smaller than
+    or equal to one.
+    """
+    strict = False
+    ge = 0.0
+    le = 1.0
+    allow_inf_nan = False
+
+
+class PositiveFiniteFloat(ConstrainedFloat):
+    """
+    Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and greater
+    than zero.
+    """
+    strict = False
+    gt = 0
+    allow_inf_nan = False
+
+
+class NonNegativeFiniteFloat(ConstrainedFloat):
+    """
+    Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and greater
+    than or equal to zero.
+    """
+    strict = False
+    ge = 0
+    allow_inf_nan = False
+
+
+class NegativeFiniteFloat(ConstrainedFloat):
+    """
+    Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and smaller
+    than zero.
+    """
+    strict = False
+    lt = 0
+    allow_inf_nan = False
+
+
+class NonPositiveFiniteFloat(ConstrainedFloat):
+    """
+    Pydantic-compatible field type for numbers, enforcing them to be finite (i.e., not NaN or infinite) and smaller
+    than or equal to zero.
+    """
+    strict = False
+    le = 0
+    allow_inf_nan = False
+
+
 if TYPE_CHECKING:
-    NonEmptyStr = str
+    from typing_extensions import TypeAlias
 
-    FractionFloat = float
-    PositiveFiniteFloat = float
-    NonNegativeFiniteFloat = float
-
-else:
-
-    class NonEmptyStr(ConstrainedStr):
-        strict = True
-        strip_whitespace = True
-        min_length = 1
-
-    class FractionFloat(ConstrainedFloat):
-        """A number that needs to be greater or equal to zero and smaller or equal to one."""
-        strict = False
-        ge = 0.0
-        le = 1.0
-        allow_inf_nan = False
-
-    class PositiveFiniteFloat(ConstrainedFloat):
-        strict = False
-        gt = 0
-        allow_inf_nan = False
-
-    class NonNegativeFiniteFloat(ConstrainedFloat):
-        strict = False
-        ge = 0
-        allow_inf_nan = False
+    NonEmptyStr: TypeAlias = str  # type: ignore[no-redef] # noqa: F811
+    FractionFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
+    PositiveFiniteFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
+    NonNegativeFiniteFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
+    NegativeFiniteFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
+    NonPositiveFiniteFloat: TypeAlias = float  # type: ignore[no-redef] # noqa: F811
```

## optool/fields/numeric.py

```diff
@@ -1,79 +1,82 @@
+"""
+Pydantic-compatible field types for [Numpy](https://pypi.org/project/numpy/) arrays.
+
+This module provides a suite of Pydantic-compatible field types tailored for validation and control of Numpy array
+objects.
+Utilizing these custom fields in Pydantic models enables stricter control over Numpy data structure, aiding in enforcing
+desired data shapes, dimensions, and immutability constraints.
+"""
+
 from __future__ import annotations
 
 import itertools
 import numbers
-from typing import TYPE_CHECKING, Any, ClassVar, Generic, Iterable, Optional, Type, TypeVar, Union
+from typing import TYPE_CHECKING, Any, ClassVar, Generic, Iterable, Literal, Optional, Tuple, Type, TypeVar, Union
 
 import numpy as np
 import pydantic
 from pydantic.fields import ModelField
 
 from optool.fields.util import (WrongTypeError, check_only_one_specified, check_sub_fields_level, get_subtype_validator,
                                 get_type_validator, update_object_schema)
 
+T = TypeVar("T", bound=np.generic)
+"""Type variable with an upper bound of {py:class}`numpy.generic`."""
 
-class ShapeError(ValueError):
+TypesParseableToNdArrays = Union[np.ndarray[Any, np.dtype[T]], numbers.Number, Iterable]
+"""
+Type alias for a union of types that can be parsed into objects of type {py:class}`numpy.ndarray`.
 
-    def __init__(self, *, expected: tuple[int, ...], value: np.ndarray) -> None:
-        super().__init__(f"expected the shape {expected}, but got a value with shape {value.shape}")
+:type: typing.Union[numpy.ndarray[typing.Any, numpy.dtype[numpy.generic]], numbers.Number, typing.Iterable]
 
+The union includes the following types:
 
-class DimensionsError(ValueError):
-
-    def __init__(self, *, expected: int, value: np.ndarray) -> None:
-        super().__init__(f"expected {expected} dimension(s), but got a value with {np.ndim(value)} dimension(s)")
-
-
-class ArrayWriteableError(ValueError):
-
-    def __init__(self, *, expected: bool, value: np.ndarray) -> None:
-        super().__init__(f"expected writeable is {expected}, "
-                         f"but got a value with writeable flag set to {value.flags.writeable}")
-
-
-T = TypeVar("T", bound=np.generic)  # Allow storing everything in ndarray
+- N-dimensional arrays.
+- Any type of number, including integers, floating points, complex numbers, etc.
+- Any object capable of returning its members one at a time, including lists, tuples, strings, dictionaries, sets, etc.
+"""
 
 
 # Due to the generic class, Pydantic has to be tricked out such that the automatic creation of schemas is working.
 class ConstrainedNdArray(pydantic.BaseModel, Generic[T]):
-    """
-    Pydantic-compatible field type for :py:class:`numpy.ndarray` objects, which allows to specify the data-type.
+    """Pydantic-compatible field type for {py:class}`numpy.ndarray` objects, which allows to specify the data-type.
 
     The approach is inspired by https://github.com/cheind/pydantic-numpy.
 
-    See Also:
-        `Pydantic documentation: Custom Data Types <https://docs.pydantic.dev/usage/types/#custom-data-types>`_ and
-        class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`
+    :::{seealso}
+    [Pydantic documentation: Custom Data Types](https://docs.pydantic.dev/usage/types/#custom-data-types) and
+    {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`.
+    :::
     """
 
     strict: ClassVar[bool] = True
-    strict_subtypes: ClassVar[bool] = True
+    strict_datatype: ClassVar[bool] = True
     dimensions: ClassVar[Optional[int]] = None
-    shape_spec: ClassVar[Optional[tuple[int, ...]]] = None
-    writeable: ClassVar[bool] = True
+    shape: ClassVar[Optional[Tuple[Optional[int], ...]]] = None
+    writeable: ClassVar[Literal['keep', 'make_true', 'make_false', 'check_true', 'check_false']] = 'keep'
 
     @classmethod
     def __get_validators__(cls):
         if cls.strict:
             yield get_type_validator(np.ndarray)
-        if cls.strict_subtypes:
+        if cls.strict_datatype:
             yield get_subtype_validator(np.ndarray, lambda x: x.dtype)
 
         if not cls.strict:
             yield cls.validate_ndarray
         yield cls.validate_dimensions
         yield cls.validate_shape
         yield cls.validate_writeable
 
     @classmethod
     def __modify_schema__(cls, field_schema: dict[str, Any], field: Optional[ModelField]):
         update_object_schema(field_schema,
                              dimensions=cls.dimensions,
-                             shape_spec=cls.shape_spec,
+                             shape=cls.shape,
                              writeable=cls.writeable,
                              datatype=field.sub_fields[0].type_.__name__ if field and field.sub_fields else None)
 
     @classmethod
     def validate_ndarray(cls, val: Any, field: ModelField) -> np.ndarray:
         if not isinstance(val, Iterable):
             val = [val]  # otherwise, np.asarray returns something weird
@@ -84,118 +87,203 @@
             array = np.asarray(val, dtype=expected_subtype)
         else:
             try:
                 array = np.asarray(val)
             except Exception as e:
                 raise WrongTypeError(expected=(np.ndarray, numbers.Number, Iterable), value=val) from e
 
-        array.setflags(write=cls.writeable)
+        if cls.writeable == 'make_true':
+            array.setflags(write=True)
+        elif cls.writeable == 'make_false':
+            array.setflags(write=False)
         return array
 
     @classmethod
     def validate_dimensions(cls, val: np.ndarray) -> np.ndarray:
         if cls.dimensions is None or cls.dimensions == val.ndim:
             return val
         raise DimensionsError(expected=cls.dimensions, value=val)
 
     @classmethod
     def validate_shape(cls, val: np.ndarray) -> np.ndarray:
-        if cls.shape_spec is None:
+        if cls.shape is None:
             return val
-        if all(cls._compare_dim(*dims) for dims in itertools.zip_longest(cls.shape_spec, val.shape)):
+        if all(cls._compare_dim(*dims) for dims in itertools.zip_longest(cls.shape, val.shape)):
             return val
-        raise ShapeError(expected=cls.shape_spec, value=val)
+        raise ShapeError(expected=cls.shape, value=val)
 
     @classmethod
     def validate_writeable(cls, val: np.ndarray) -> np.ndarray:
-        if val.flags.writeable == cls.writeable:
-            return val
-        raise ArrayWriteableError(expected=cls.writeable, value=val)
+        if cls.writeable == 'check_true' and val.flags.writeable is False:
+            raise ArrayWriteableError(expected=True, value=val)
+        if cls.writeable == 'check_false' and val.flags.writeable is True:
+            raise ArrayWriteableError(expected=False, value=val)
+        return val
 
     @classmethod
     def _compare_dim(cls, expected: Optional[int], actual: Optional[int]) -> bool:
-        return actual == expected or expected == -1
-
-
-def conndarray(*,
-               strict: bool = False,
-               dimensions: Optional[int] = None,
-               shape: Optional[tuple[int, ...]] = None,
-               writeable: bool = True) -> Type[np.ndarray]:
-    """
-    Creates a Pydantic-compatible field type for :py:class:`numpy.ndarray` objects, which allows specifying constraints
-    on the accepted values.
-
-    Args:
-        strict: If :py:data:`True` only values of type :py:class:`numpy.ndarray` are accepted. (Default:
-            :py:data:`False`)
-        dimensions: The expected dimensions as in :py:func:`numpy.ndim`.
-        shape: The shape expected. One shape dimension can be ``-1`` indicating that this dimension is arbitrary.
-        writeable: Boolean flag indicating whether the :py:class:`numpy.ndarray` object is mutable or not.
+        return actual == expected or expected is None
 
-    Returns:
-        A new Pydantic-compatible field type.
 
-    See Also:
-        Method :py:func:`pydantic.conint` or similar of :py:mod:`pydantic`.
+def conndarray(
+        *,
+        strict: bool = False,
+        dimensions: Optional[int] = None,
+        shape: Optional[Tuple[int, ...]] = None,
+        writeable: Literal['keep', 'make_true', 'make_false', 'check_true',
+                           'check_false'] = 'keep') -> Type[np.ndarray]:
+    """Creates a Pydantic-compatible field type for {py:class}`numpy.ndarray` objects, which allows specifying
+    constraints on the accepted values.
+
+    :param strict: If {py:data}`True` only values of type {py:class}`numpy.ndarray` are accepted. (Default:
+            {py:data}`False`)
+    :param dimensions: The expected dimensions as in {py:attr}`~numpy.ndarray.ndim`.
+    :param shape: The shape expected. One shape dimension can be {py:data}`None` indicating that this dimension is
+        arbitrary.
+    :param writeable: Specification on how to deal with the `writeable` flag of the {py:class}`numpy.ndarray` object.
+    :returns: A new Pydantic-compatible field type.
+
+    :::{seealso}
+    {py:func}`pydantic.conint` or similar of {py:mod}`pydantic`.
+    :::
     """
     check_only_one_specified(dimensions, shape, "Cannot specify both dimensions and shape.")
-    namespace = dict(strict=strict, dimensions=dimensions, shape_spec=shape, writeable=writeable)
+    namespace = dict(strict=strict, dimensions=dimensions, shape=shape, writeable=writeable)
     return type('ConstrainedNdArrayValue', (ConstrainedNdArray,), namespace)  # type: ignore
 
 
+class NdArrayLike(ConstrainedNdArray[T]):
+    """
+    Pydantic-compatible field type for {py:class}`numpy.ndarray` objects.
+
+    Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
+    subtype specified using type hinting (e.g., ``NdArrayLike[np.int_]``) is also not enforced, but rather used to set
+    the data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
+    """
+    strict = False
+    strict_datatype = False
+
+
+class Array(ConstrainedNdArray[T]):
+    """
+    Pydantic-compatible field type for one-dimensional {py:class}`numpy.ndarray` objects.
+
+    Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
+    subtype specified using type hinting (e.g., ``Array[np.int_]``) is also not enforced, but rather used to set the
+    data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
+    """
+    strict = False
+    strict_datatype = False
+    dimensions = 1
+
+
+class ImmutableArray(ConstrainedNdArray[T]):
+    """
+    Pydantic-compatible field type for one-dimensional immutable {py:class}`numpy.ndarray` objects.
+
+    Assigned values not already of type {py:class}`~numpy.ndarray` are parsed using {py:func}`numpy.asarray`. The
+    subtype specified using type hinting (e.g., ``ImmutableArray[np.int_]``) is also not enforced, but rather used to
+    set the data-type of the numeric values. Accepted types to be parsed are {py:data}`TypesParseableToNdArrays`.
+
+    Immutability is established by setting the flag ``writeable`` to {py:data}`False`.
+    """
+    strict = False
+    strict_datatype = False
+    dimensions = 1
+    writeable = 'make_false'
+
+
+class StrictNdArray(ConstrainedNdArray[T]):
+    """
+    Pydantic-compatible field type for {py:class}`numpy.ndarray` objects.
+
+    Assigned values must be of type {py:class}`~numpy.ndarray`. Furthermore, the subtype specified using type hinting
+    (e.g., ``StrictNdArray[np.int_]``) must also match the data-type of the numeric values.
+    """
+    strict = True
+    strict_datatype = True
+
+
+class Row(ConstrainedNdArray[T]):
+    """
+    Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing row vectors.
+
+    Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting (e.g.,
+    ``Row[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
+    """
+    strict = True
+    strict_datatype = False
+    shape = (1, None)
+
+
+class Column(ConstrainedNdArray[T]):
+    """
+    Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing column vectors.
+
+    Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting (e.g.,
+    ``Column[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
+    """
+    strict = True
+    strict_datatype = False
+    shape = (None, 1)
+
+
+class Matrix(ConstrainedNdArray[T]):
+    """
+    Pydantic-compatible field type for two-dimensional {py:class}`numpy.ndarray` objects representing matrices.
+
+    Assigned values must be of type {py:class}`~numpy.ndarray`. However, the subtype specified using type hinting (e.g.,
+    ``Matrix[np.int_]``) is also not enforced, but rather used to set the data-type of the numeric values.
+    """
+    strict = True
+    strict_datatype = False
+    dimensions = 2
+
+
 if TYPE_CHECKING:
+    from typing_extensions import TypeAlias
+
+    NdArrayLike: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
+    Array: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
+    ImmutableArray: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
+    StrictNdArray: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
+    Row: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
+    Column: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
+    Matrix: TypeAlias = np.ndarray[Any, np.dtype[T]]  # type: ignore[no-redef] # noqa: F811
+
+
+class ShapeError(ValueError):
+    """
+    Raised when the shape of a numpy array does not meet the expectations.
+
+    :param expected: The expected shape of the array, {py:data}`None` indicating arbitrary length of the corresponding
+        dimension.
+    :param value: The array that causes the error due to its shape.
+    """
+
+    def __init__(self, *, expected: Tuple[Optional[int], ...], value: np.ndarray) -> None:
+        super().__init__(f"expected the shape {expected}, but got a value with shape {value.shape}")
 
-    NdArrayLike = Union[np.ndarray[Any, np.dtype[T]], numbers.Number, Iterable]
-    Array = Union[np.ndarray[Any, np.dtype[T]], numbers.Number, Iterable]
-    ImmutableArray = Union[np.ndarray[Any, np.dtype[T]], numbers.Number, Iterable]
-
-    StrictNdArray = np.ndarray[Any, np.dtype[T]]
-    Row = np.ndarray[Any, np.dtype[T]]
-    Column = np.ndarray[Any, np.dtype[T]]
-    Matrix = np.ndarray[Any, np.dtype[T]]
-
-else:
-
-    class NdArrayLike(ConstrainedNdArray[T]):
-        strict = False
-        strict_subtypes = False
-
-    class Array(ConstrainedNdArray[T]):
-        """Pydantic-compatible field type for one-dimensional :py:class:`numpy.ndarray` objects."""
-        strict = False
-        strict_subtypes = False
-        dimensions = 1
-
-    class ImmutableArray(Array[T]):
-        """
-        Pydantic-compatible field type for one-dimensional :py:class:`numpy.ndarray` objects, where the flag
-        ``writeable`` is set to :py:data:`False`.
-        """
-        strict = False
-        strict_subtypes = False
-        writeable = False
-
-    class StrictNdArray(ConstrainedNdArray[T]):
-        strict = True
-        strict_subtypes = True
-
-    class Row(ConstrainedNdArray[T]):
-        """Pydantic-compatible field type for two-dimensional :py:class:`numpy.ndarray` objects representing row
-        vectors."""
-        strict = True
-        strict_subtypes = False
-        shape_spec = (1, -1)
-
-    class Column(ConstrainedNdArray[T]):
-        """Pydantic-compatible field type for two-dimensional :py:class:`numpy.ndarray` objects representing column
-        vectors."""
-        strict = True
-        strict_subtypes = False
-        shape_spec = (-1, 1)
-
-    class Matrix(ConstrainedNdArray[T]):
-        """Pydantic-compatible field type for two-dimensional :py:class:`numpy.ndarray` objects representing
-        matrices."""
-        strict = True
-        strict_subtypes = False
-        dimensions = 2
+
+class DimensionsError(ValueError):
+    """
+    Raised when the number of dimensions of a {py:class}`numpy.ndarray` does not meet the expectations.
+
+    :param expected: The expected number of dimensions.
+    :param value: The array that causes the error due to its number of dimensions.
+    """
+
+    def __init__(self, *, expected: int, value: np.ndarray) -> None:
+        super().__init__(f"expected {expected} dimension(s), but got a value with {np.ndim(value)} dimension(s)")
+
+
+class ArrayWriteableError(ValueError):
+    """
+    Raised when the writeable flag of a {py:class}`numpy.ndarray` does not meet the expectations.
+
+    :param expected: The expected state of the writeable flag.
+    :param value: The array that causes the error due to its writeable flag state.
+    """
+
+    def __init__(self, *, expected: bool, value: np.ndarray) -> None:
+        super().__init__(f"expected writeable is {expected}, "
+                         f"but got a value with writeable flag set to {value.flags.writeable}")
```

## optool/fields/quantities.py

```diff
@@ -1,52 +1,43 @@
+"""
+Pydantic-compatible field types for unit and quantity objects of [Pint](https://pypi.org/project/Pint/).
+
+This module introduces Pydantic-compatible field types designed to handle and validate objects related to units of
+measurements.
+Through the use of these custom fields, it is possible to impose desired dimensionality requirements on units and
+quantities within Pydantic models, which strengthens the data integrity throughout the codebase.
+"""
+
 from __future__ import annotations
 
 from numbers import Number
 from typing import TYPE_CHECKING, Any, ClassVar, Generic, Optional, TypeVar
 
 import pydantic
 from pydantic import ValidationError
 from pydantic.fields import ModelField
 
 from optool.fields.util import (WrongTypeError, check_validation_is_passed_on_to_sub_types, get_dimension,
                                 get_subfield_schema, get_subtype_validator, get_type_validator, update_object_schema)
 from optool.uom import UNITS, PhysicalDimension, Quantity, Unit
 
-
-class DimensionalityError(ValueError):
-
-    def __init__(self, *, expected: Optional[str], value: Quantity) -> None:
-        super().__init__(f"expected the dimensionality {expected}, "
-                         f"but got a value with dimensionality {value.dimensionality}")
-
-
-class UnsupportedMagnitudeConversion(ValueError):
-
-    def __init__(self, *, value: Any) -> None:
-        super().__init__(f"the value of {type(value)} cannot be converted automatically")
-
-
-class UnitParseError(ValueError):
-
-    def __init__(self, *, unit: str) -> None:
-        super().__init__(f"cannot parse the unit {unit}")
-
-
 D = TypeVar("D", bound=PhysicalDimension)
+"""Type variable with an upper bound of {py:class}`~optool.uom.PhysicalDimension`."""
 
 
 # Due to the generic class, Pydantic has to be tricked out such that the automatic creation of schemas is working.
 class ConstrainedUnit(pydantic.BaseModel, Generic[D]):
     """
-    Pydantic-compatible field type for :py:class:`pint.Unit` objects, which allows to specify the desired
+    Pydantic-compatible field type for {py:class}`~optool.uom.Unit` objects, which allows to specify the desired
     dimensionality.
 
-    See Also:
-        `Pydantic documentation: Custom Data Types <https://docs.pydantic.dev/usage/types/#custom-data-types>`_ and
-        class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`
+    :::{seealso}
+    [Pydantic documentation: Custom Data Types](https://docs.pydantic.dev/usage/types/#custom-data-types) and
+    {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`
+    :::
     """
     strict: ClassVar[bool] = True
 
     @classmethod
     def __get_validators__(cls):
         yield get_type_validator(Unit) if cls.strict else cls.validate_unit
         yield cls.validate_dimensionality
@@ -74,34 +65,38 @@
         dimension = get_dimension(field, 0)
         if dimension is None or val.dimensionality == UNITS.get_dimensionality(dimension.dimensionality):
             return val
         raise DimensionalityError(expected=dimension.dimensionality, value=val)
 
 
 T = TypeVar("T")  # Allow storing everything as magnitude in Quantity
+"""
+Type variable to specify the type of the magnitude of a {py:attr}`~optool.uom.Quantity`.
+"""
 
 
 # Due to the generic class, Pydantic has to be tricked out such that the automatic creation of schemas is working.
 class ConstrainedQuantity(pydantic.BaseModel, Generic[D, T]):
     """
-    Pydantic-compatible field type for :py:class:`optool.uom.Quantity` objects, which allows to specify the desired
+    Pydantic-compatible field type for {py:class}`~optool.uom.Quantity` objects, which allows to specify the desired
     dimensionality.
 
-    See Also:
-        Class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`.
+    :::{seealso}
+    Class {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`.
+    :::
     """
 
     strict: ClassVar[bool] = True
-    strict_subtypes: ClassVar[bool] = True
+    strict_magnitude: ClassVar[bool] = True
 
     @classmethod
     def __get_validators__(cls):
         if cls.strict:
             yield get_type_validator(Quantity)
-        if cls.strict_subtypes:
+        if cls.strict_magnitude:
             yield get_subtype_validator(Quantity, lambda x: type(x.m))
 
         if not cls.strict:
             yield cls.validate_quantity
         yield cls.validate_dimensionality
         yield cls.validate_magnitude
 
@@ -136,29 +131,130 @@
         valid_value, error = magnitude_field.validate(val.m, {}, loc='magnitude')
         if error:
             raise ValidationError([error], cls)
 
         return Quantity(valid_value, val.u)
 
 
+class UnitLike(ConstrainedUnit[D], Unit):
+    """
+    Pydantic-compatible field type for {py:class}`~optool.uom.Unit` objects.
+
+    Assigned values not already of type {py:class}`~optool.uom.Unit` are parsed using `UNITS.parse_units(...)`. The
+    subtype specified using type hinting (e.g., `UnitLike[Length]`) is used to check if the unit has the correct
+    dimensionality.
+
+    ::::{admonition} Example
+    :class: example dropdown
+
+    ```python
+    from optool import BaseModel
+    from optool.fields.quantities import UnitLike
+    from optool.uom import Mass
+
+    class ExampleModel(BaseModel):
+        mass_unit: UnitLike[Mass]
+
+    mdl = BaseModel(mass_unit="kg")
+    ```
+    """
+    strict = False
+
+
+class StrictUnit(ConstrainedUnit[D], Unit):
+    """
+    Pydantic-compatible field type for {py:class}`~optool.uom.Unit` objects.
+
+    Assigned values must be of type {py:class}`~optool.uom.Unit`. The subtype specified using type hinting (e.g.,
+    `UnitLike[Length]`) is used to check if the unit has the correct dimensionality.
+    """
+    strict = True
+
+
+class QuantityLike(ConstrainedQuantity[D, T], Quantity):
+    """
+    Pydantic-compatible field type for {py:class}`~optool.uom.Quantity` objects.
+
+    Assigned values not already of type {py:class}`~optool.uom.Quantity` are parsed using the regular constructor
+    {py:class}`Quantity(val) <optool.uom.Quantity>`.
+
+    The two subtypes specified using type hinting (e.g., `QuantityLike[Length, PositiveInt]`) are used to check if the
+    unit has the expected dimensionality and the magnitude matches the expected specification. For the latter, the
+    validation and parsing is forwarded to the specific type, e.g., in the example above, the value is validated using
+    the implementation provided by {py:class}`pydantic.PositiveInt`.
+
+    ::::{admonition} Example
+    :class: example dropdown
+
+    ```python
+    from optool import BaseModel
+    from optool.fields.quantities import QuantityLike
+    from optool.fields.misc import PositiveFiniteFloat
+    from optool.uom import Mass
+
+    class ExampleModel(BaseModel):
+        mass: QuantityLike[Mass, PositiveFiniteFloat]
+
+    mdl = BaseModel(mass="5 kg")
+    ```
+    """
+    strict = False
+    strict_magnitude = False
+
+
+class StrictQuantity(ConstrainedQuantity[D, T], Quantity):
+    """
+    Pydantic-compatible field type for {py:class}`~optool.uom.Quantity` objects.
+
+    Assigned values must be of type {py:class}`~optool.uom.Quantity`.
+
+    The two subtypes specified using type hinting (e.g., `QuantityLike[Length, PositiveInt]`) are used to check if the
+    unit has the expected dimensionality and the magnitude matches the expected specification. For the latter, the
+    validation and parsing is forwarded to the specific type, e.g., in the example above, the value is validated using
+    the implementation provided by {py:class}`pydantic.PositiveInt`.
+    """
+    strict = True
+    strict_magnitude = False
+
+
 if TYPE_CHECKING:
-    UnitLike = Unit
-    StrictUnit = Unit
+    from typing_extensions import TypeAlias
 
-    QuantityLike = Quantity
-    StrictQuantity = Quantity
+    UnitLike: TypeAlias = Unit  # type: ignore[no-redef] # noqa: F811
+    StrictUnit: TypeAlias = Unit  # type: ignore[no-redef] # noqa: F811
+    QuantityLike: TypeAlias = Quantity  # type: ignore[no-redef] # noqa: F811
+    StrictQuantity: TypeAlias = Quantity  # type: ignore[no-redef] # noqa: F811
 
-else:
 
-    class UnitLike(ConstrainedUnit[D], Unit):
-        strict = False
+class DimensionalityError(ValueError):
+    """
+    Raised when an incorrect dimensionality is encountered.
 
-    class StrictUnit(ConstrainedUnit[D], Unit):
-        strict = True
+    :param expected: The expected dimensionality
+    :param value: The value that causes the error due to its incorrect dimensionality.
+    """
 
-    class QuantityLike(ConstrainedQuantity[D, T], Quantity):
-        strict = False
-        strict_subtypes = False
+    def __init__(self, *, expected: Optional[str], value: Quantity) -> None:
+        super().__init__(f"expected the dimensionality {expected}, "
+                         f"but got a value with dimensionality {value.dimensionality}")
+
+
+class UnsupportedMagnitudeConversion(ValueError):
+    """
+    Raised when a value cannot be automatically converted to the expected magnitude.
+
+    :param value: The value that causes the error due to unsupported automatic conversion.
+    """
+
+    def __init__(self, *, value: Any) -> None:
+        super().__init__(f"the value of {type(value)} cannot be converted automatically")
+
+
+class UnitParseError(ValueError):
+    """
+    Raised when a unit string cannot be parsed.
 
-    class StrictQuantity(ConstrainedQuantity[D, T], Quantity):
-        strict = True
-        strict_subtypes = False
+    :param unit: The unit string that causes the error due to parsing issues.
+    """
+
+    def __init__(self, *, unit: str) -> None:
+        super().__init__(f"cannot parse the unit {unit}")
```

## optool/fields/series.py

```diff
@@ -1,7 +1,16 @@
+"""
+Pydantic-compatible field types for [Pandas](https://pypi.org/project/pandas/) series objects.
+
+This module contains classes that provide Pydantic-compatible field types specifically tailored for Pandas series
+objects.
+These custom fields allow developers to enforce specific data types including their physical dimensionality and the
+index type.
+"""
+
 from __future__ import annotations
 
 from numbers import Number
 from typing import TYPE_CHECKING, Any, ClassVar, Generic, Iterable, Optional, Sequence, Type, TypeVar, cast
 
 import numpy as np
 import pandas as pd
@@ -11,44 +20,26 @@
 from pydantic import ValidationError
 from pydantic.fields import ModelField
 
 from optool.fields.util import (WrongTypeError, check_sub_fields_level, get_subfield_schema, get_type_validator,
                                 update_object_schema)
 from optool.uom import PhysicalDimension, Quantity
 
-
-class IndexTypeError(ValueError):
-
-    def __init__(self, *, expected: Type[Index], value: pd.Series) -> None:
-        super().__init__(f"expected index type {expected}, but got a series with index type {type(value.index)}")
-
-
-class DimensionalityError(ValueError):
-
-    def __init__(self, *, expected: str, value: pd.Series) -> None:
-        super().__init__(f"expected the dimensionality {expected}, but got a series with data-type {value.dtype}")
-
-
-class ArrayWriteableError(ValueError):
-
-    def __init__(self, *, expected: bool, value: np.ndarray) -> None:
-        super().__init__(f"expected writeable is {expected}, "
-                         f"but got a value with writeable flag set to {value.flags.writeable}")
-
-
-T = TypeVar("T")  # Allow storing everything as data-type in Series
+T = TypeVar("T")
+"""Type variable without an upper bound, specifying the type of the data of {py:class}`pandas.Series`."""
 
 
 class ConstrainedSeries(pydantic.BaseModel, Generic[T]):
     """
-    Pydantic-compatible field type for :py:class:`pandas.Series` objects, which allows to specify the data-type.
+    Pydantic-compatible field type for {py:class}`pandas.Series` objects, which allows to specify the data-type.
 
-    See Also:
-        `Pydantic documentation: Custom Data Types <https://docs.pydantic.dev/usage/types/#custom-data-types>`_ and
-        class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`
+    :::{seealso}
+    [Pydantic documentation: Custom Data Types](https://docs.pydantic.dev/usage/types/#custom-data-types) and
+    {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`
+    :::
     """
 
     strict: ClassVar[bool] = True
     index_type: ClassVar[Type[Index]] = pd.RangeIndex
 
     @classmethod
     def __get_validators__(cls):
@@ -145,24 +136,83 @@
         return val
 
     @classmethod
     def _is_physical_dimension(cls, field: ModelField) -> bool:
         return field.type_ == Any or issubclass(field.type_, PhysicalDimension)
 
 
+class SeriesLike(ConstrainedSeries[T]):
+    """
+    Pydantic-compatible field type for {py:class}`pandas.Series` objects.
+
+    Assigned values not already of type {py:class}`~pandas.Series` are parsed using
+    {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic type
+    annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is verified. If the
+    generic type annotation is different, the validation and parsing of each data element is forwarded to the specific
+    type specified, e.g., in the example above, the value is validated using the implementation provided by
+    {py:class}`pydantic.PositiveInt`.
+    """
+    strict = False
+
+
+class DatetimeSeries(ConstrainedSeries[T]):
+    """
+    Pydantic-compatible field type for {py:class}`pandas.Series` objects, the index of which must be of type
+    {py:class}`~pandas.DatetimeIndex`.
+
+    Assigned values not already of type {py:class}`~pandas.Series` are parsed using
+    {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic type
+    annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is verified. If the
+    generic type annotation is different, the validation and parsing of each data element is forwarded to the specific
+    type specified, e.g., in the example above, the value is validated using the implementation provided by
+    {py:class}`pydantic.PositiveInt`.
+    """
+    strict = False
+    index_type = DatetimeIndex
+
+
+class TimedeltaSeries(ConstrainedSeries[T]):
+    """
+    Pydantic-compatible field type for {py:class}`pandas.Series` objects, the index of which must be of type
+    {py:class}`~pandas.TimedeltaIndex`.
+
+    Assigned values not already of type {py:class}`~pandas.Series` are parsed using
+    {py:class}`Series(val) <pandas.Series>`. If a {py:class}`~optool.uom.PhysicalDimension` is specified as generic type
+    annotation, a {py:class}`pint_pandas.PintArray` is created and the corresponding dimensionality is verified. If the
+    generic type annotation is different, the validation and parsing of each data element is forwarded to the specific
+    type specified, e.g., in the example above, the value is validated using the implementation provided by
+    {py:class}`pydantic.PositiveInt`.
+    """
+    strict = False
+    index_type = TimedeltaIndex
+
+
 if TYPE_CHECKING:
-    SeriesLike = pd.Series
-    DatetimeSeries = pd.Series
-    TimedeltaSeries = pd.Series
-
-else:
-
-    class SeriesLike(ConstrainedSeries[T]):
-        strict = False
-
-    class DatetimeSeries(ConstrainedSeries[T]):
-        strict = False
-        index_type = DatetimeIndex
-
-    class TimedeltaSeries(ConstrainedSeries[T]):
-        strict = False
-        index_type = TimedeltaIndex
+    from typing_extensions import TypeAlias
+
+    SeriesLike: TypeAlias = pd.Series  # type: ignore[no-redef] # noqa: F811
+    DatetimeSeries: TypeAlias = pd.Series  # type: ignore[no-redef] # noqa: F811
+    TimedeltaSeries: TypeAlias = pd.Series  # type: ignore[no-redef] # noqa: F811
+
+
+class IndexTypeError(ValueError):
+    """
+    Raised when the type of index of a {py:class}`pandas.Series` does not meet the expectations.
+
+    :param expected: The expected type of the index.
+    :param value: The series that causes the error due to its index type.
+    """
+
+    def __init__(self, *, expected: Type[Index], value: pd.Series) -> None:
+        super().__init__(f"expected index type {expected}, but got a series with index type {type(value.index)}")
+
+
+class DimensionalityError(ValueError):
+    """
+    Raised when the dimensionality of a {py:class}`pandas.Series` does not meet the expectations.
+
+    :param expected: The expected dimensionality.
+    :param value: The series that causes the error due to its dimensionality.
+    """
+
+    def __init__(self, *, expected: str, value: pd.Series) -> None:
+        super().__init__(f"expected the dimensionality {expected}, but got a series with data-type {value.dtype}")
```

## optool/fields/symbolic.py

```diff
@@ -1,35 +1,41 @@
+"""
+Pydantic-compatible field types for objects that are used to create symbolic expressions.
+
+This module focuses on providing data validation for symbolic expressions implemented via the
+[CasADi](https://web.casadi.org) library.
+CasADi is a symbolic framework for automatic differentiation and optimal control, which is used extensively for
+nonlinear optimization and algorithmic differentiation.
+With the Pydantic-compatible fields of this module, it can be ensured that any CasADi symbolic expressions used in a
+Pydantic model adheres to specific dimensional requirements, thereby reducing the risk of shape-related errors in the
+symbolic computations.
+"""
+
 from __future__ import annotations
 
 import itertools
-from typing import TYPE_CHECKING, Optional
+from typing import TYPE_CHECKING, Optional, Tuple
 
 import casadi
 from pydantic.fields import ModelField
 
 from optool.fields.util import get_type_validator, update_object_schema
 
 
-class ShapeError(ValueError):
-
-    def __init__(self, *, expected: tuple[int, ...], value: casadi.SX) -> None:
-        super().__init__(f"expected the shape {expected}, "
-                         f"but got a value with shape ('called size' in CasADi) {value.size()}")
-
-
 class ConstrainedCasadiSymbol:
     """
-    Pydantic-compatible field type for :py:class:`casadi.SX` objects.
+    Pydantic-compatible field type for {py:class}`casadi.SX` objects.
 
-    See Also:
-        `Pydantic documentation: Custom Data Types <https://docs.pydantic.dev/usage/types/#custom-data-types>`_ and
-        class :py:class:`pydantic.types.ConstrainedInt` or similar of :py:mod:`pydantic`
+    :::{seealso}
+    [Pydantic documentation: Custom Data Types](https://docs.pydantic.dev/usage/types/#custom-data-types) and
+    {py:class}`pydantic.types.ConstrainedInt` or similar of {py:mod}`pydantic`.
+    :::
     """
 
-    shape: Optional[tuple[int, ...]] = None
+    shape: Optional[Tuple[Optional[int], ...]] = None
 
     @classmethod
     def __get_validators__(cls):
         yield get_type_validator(casadi.SX)
         yield cls.validate_shape
 
     @classmethod
@@ -40,35 +46,51 @@
     def validate_shape(cls, val: casadi.SX, field: ModelField) -> casadi.SX:
         if cls.shape is None or all(cls._compare_dim(*dims) for dims in itertools.zip_longest(cls.shape, val.size())):
             return val
         raise ShapeError(expected=cls.shape, value=val)
 
     @classmethod
     def _compare_dim(cls, expected: Optional[int], actual: Optional[int]) -> bool:
-        return actual == expected or expected == -1
+        return actual == expected or expected is None
+
+
+class CasadiScalar(ConstrainedCasadiSymbol):
+    """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing scalars."""
+    shape = (1, 1)
+
+
+class CasadiRow(ConstrainedCasadiSymbol):
+    """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing row vectors."""
+    shape = (1, None)
+
+
+class CasadiColumn(ConstrainedCasadiSymbol):
+    """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing column vectors."""
+    shape = (None, 1)
+
+
+class CasadiMatrix(ConstrainedCasadiSymbol):
+    """Pydantic-compatible field type for two-dimensional {py:class}`casadi.SX` objects representing matrices."""
+    shape = (None, None)
 
 
 if TYPE_CHECKING:
+    from typing_extensions import TypeAlias
+
+    CasadiScalar: TypeAlias = casadi.SX  # type: ignore[no-redef] # noqa: F811
+    CasadiRow: TypeAlias = casadi.SX  # type: ignore[no-redef] # noqa: F811
+    CasadiColumn: TypeAlias = casadi.SX  # type: ignore[no-redef] # noqa: F811
+    CasadiMatrix: TypeAlias = casadi.SX  # type: ignore[no-redef] # noqa: F811
+
+
+class ShapeError(ValueError):
+    """
+    Raised when the shape of a CasADi SX variable does not meet the expectations.
+
+    :param expected: The expected shape of the array, {py:data}`None` indicating arbitrary length of the corresponding
+        dimension.
+    :param value: The CasADi SX variable that causes the error due to its shape.
+    """
 
-    CasadiScalar = casadi.SX
-    CasadiRow = casadi.SX
-    CasadiColumn = casadi.SX
-    CasadiMatrix = casadi.SX
-
-else:
-
-    class CasadiScalar(ConstrainedCasadiSymbol):
-        """Pydantic-compatible field type for two-dimensional :py:class:`casadi.SX` objects representing scalars."""
-        shape = (1, 1)
-
-    class CasadiRow(ConstrainedCasadiSymbol):
-        """Pydantic-compatible field type for two-dimensional :py:class:`casadi.SX` objects representing row vectors."""
-        shape = (1, -1)
-
-    class CasadiColumn(ConstrainedCasadiSymbol):
-        """Pydantic-compatible field type for two-dimensional :py:class:`casadi.SX` objects representing column
-        vectors."""
-        shape = (-1, 1)
-
-    class CasadiMatrix(ConstrainedCasadiSymbol):
-        """Pydantic-compatible field type for two-dimensional :py:class:`casadi.SX` objects representing matrices."""
-        shape = (-1, -1)
+    def __init__(self, *, expected: Tuple[Optional[int], ...], value: casadi.SX) -> None:
+        super().__init__(f"expected the shape {expected}, "
+                         f"but got a value with shape ('called size' in CasADi) {value.size()}")
```

## optool/fields/util.py

```diff
@@ -1,101 +1,142 @@
+"""
+Utility functions facilitating the implementation of the Pydantic-compatible fields in the other modules of this
+package.
+
+This module contains various utility functions designed to augment and simplify coding tasks related to the
+Pydantic-compatible fields implemented in the other modules of this package.
+"""
+
 from __future__ import annotations
 
 import re
-from typing import Any, Callable, Dict, Iterable, Optional, Type, TypeVar, Union
+from typing import Any, Callable, Dict, Iterable, Optional, Tuple, Type, TypeVar, Union
 
 import numpy as np
 import pydantic
 from pydantic.fields import ModelField
 from pydantic.validators import find_validators
 
-from optool import BaseModel
+from optool.core import BaseModel
 from optool.uom import PhysicalDimension
 
-TypeDefinition = Union[type, tuple[type, ...]]
+TypeDefinition = Union[Type, Tuple[Type, ...]]
+"""Type alias used to annotate function parameters and return types."""
+
 ValidationFunc = Callable[[Any], Any]
+"""Type alias used to annotate callable function accepting a single parameter and returning a value of any type."""
 
 T = TypeVar("T")
+"""Type variable that can represent any type, used for creating generic function parameters and return types."""
 
 
 class WrongTypeError(ValueError):
+    """
+    Raised when the type of the value specified does not meet the expectations.
+
+    :param expected: The expected type.
+    :param value: The value that causes the error due to its type
+    """
 
     def __init__(self, *, expected: TypeDefinition, value: Any) -> None:
         super().__init__(f"expected {expected}, but got {value}")
 
 
 class WrongSubTypeError(ValueError):
+    """
+    Raised when the subtype of a value does not meet the expectations.
+
+    :param expected_type: The expected main type.
+    :param expected_subtype: The expected subtype.
+    :param actual_subtype: The actual subtype of the value.
+    :param value: The value that causes the error due to its subtype.
+    """
 
     def __init__(self, *, expected_type: TypeDefinition, expected_subtype: TypeDefinition,
                  actual_subtype: TypeDefinition, value: Any) -> None:
-        super().__init__(f"expected sub-type {expected_subtype} of {expected_type}, "
-                         f"but got sub-type {actual_subtype} of {type(value)}")
+        super().__init__(f"expected subtype {expected_subtype} of {expected_type}, "
+                         f"but got subtype {actual_subtype} of {type(value)}")
 
 
 class ArbitrarySubTypeError(ValueError):
+    """
+    Raised when the subtype of a {py:class}`ModelField` is not handled by any specific validators.
+
+    :param name: The name of the field.
+    :param field: The model field with the problematic subtype.
+    """
 
     def __init__(self, *, name: str, field: ModelField) -> None:
         sub_type = None if field.sub_fields is None else field.sub_fields[0].type_
-        super().__init__(f"the sub-field of {name!r} has the type {field.type_} (with sub-type {sub_type}), "
+        super().__init__(f"the sub-field of {name!r} has the type {field.type_} (with subtype {sub_type}), "
                          f"but {field.type_} does not offer any specific validators that would be able to handle "
-                         f"sub-types")
+                         f"subtypes")
 
 
 class _ConfigWithArbitraryTypesNotAllowed(BaseModel.Config):
     arbitrary_types_allowed = False
 
 
 def has_specific_type_validator(type_: Type[Any]) -> bool:
     """
-    Determines if the type specified has one or more validators that are more specific than just the
-    `arbitrary_type_validator` that is used when `arbitrary_types_allowed` of Config is set to :py:data:`True`.
+    Determines if the type specified has one or more validators specific validators.
 
-    Args:
-        type_: The type to analyze.
+    A specific validator is a validator that is not just the `arbitrary_type_validator` used when
+    {py:attr}`pydantic.Config.arbitrary_types_allowed` is set to {py:data}`True`.
 
-    Returns:
-        :py:data:`True` if the type specified has a validator that is different from the `arbitrary_type_validator`,
-        :py:data:`False` otherwise.
+    :param type_: The type to analyze.
+    :returns: {py:data}`True` if the type specified has a validator that is different from the
+        `arbitrary_type_validator`, {py:data}`False` otherwise.
     """
 
     try:
         next(find_validators(type_, _ConfigWithArbitraryTypesNotAllowed))
         return True
     except Exception as e:
         if re.match("no validator found for <.*?>, see `arbitrary_types_allowed` in Config", str(e)):
             return False
         raise e
 
 
 def check_validation_is_passed_on_to_sub_types(name: str, field: ModelField) -> None:
+    """
+    Checks if the validation is passed on to subtypes of the provided field.
+
+    :param name: The name of the field.
+    :param field: The model field to check.
+    :raises optool.fields.util.ArbitrarySubTypeError: If validation is not passed on to subtypes
+    """
     if field.sub_fields is None:
         return
     if not has_specific_type_validator(field.type_):
         raise ArbitrarySubTypeError(name=name, field=field)
     for sub_field in field.sub_fields:
         check_validation_is_passed_on_to_sub_types(field.name, sub_field)
 
 
 def check_sub_fields_level(field: ModelField) -> None:
+    """
+    Checks if a {py:class}`pydantic.ModelField` has generic types more than one level deep.
+
+    :param field: The model field to check.
+    :raises ValueError: If generic types more than one level deep are found.
+    """
     if field.sub_fields is None:
         return
     if field.sub_fields[0].sub_fields:
         raise ValueError(f"Generic types more than one level deep are currently not supported. "
                          f"Got {field.sub_fields[0].type_} and {field.sub_fields[0].sub_fields[0].type_}.")
 
 
 def get_type_validator(expected_type: Type[T]) -> Callable[[Any], T]:
     """
     Creates a validation function that checks if the input argument is of the expected type.
 
-    Args:
-        expected_type: The type the resulting validator will enforce.
-
-    Returns:
-        A new function that can be used to validate if an input value is an instance of the type specified.
+    :param expected_type: The type the resulting validator will enforce.
+    :returns: A new function that can be used to validate if an input value is an instance of the type specified.
     """
 
     def validate_type(value: Any) -> T:
         if isinstance(value, expected_type):
             return value
         raise WrongTypeError(expected=expected_type, value=value)
 
@@ -103,20 +144,17 @@
 
 
 def get_subtype_validator(object_type: Type[T], subtype_provider: Callable[[T],
                                                                            Type]) -> Callable[[Any, ModelField], T]:
     """
     Creates a validation function that checks if the subtype of the input argument is of the expected type.
 
-    Args:
-        object_type: The type the resulting validator will enforce.
-        subtype_provider: Callable to get the subtype of the provided value.
-
-    Returns:
-        A new function that can be used to validate if an input value is an instance of the type specified.
+    :param object_type: The type the resulting validator will enforce.
+    :param subtype_provider: Callable to get the subtype of the provided value.
+    :returns: A new function that can be used to validate if an input value is an instance of the type specified.
     """
 
     def validate_subtype(value: Any, field: ModelField) -> T:
         if field.sub_fields:
             expected_subtype = field.sub_fields[0].type_
             actual_subtype = subtype_provider(value)
             if expected_subtype != actual_subtype:
@@ -130,87 +168,85 @@
 
         return value
 
     return validate_subtype
 
 
 def check_only_one_specified(first: Any, second: Any, message: str) -> None:
+    """
+    Checks if either the first or the second argument is {py:data}`None`.
+
+    :param first: The first object to check.
+    :param second: The second object to check.
+    :param message: The error message to raise if both are specified.
+    :raises ValueError: If both 'first' and 'second' are specified, i.e. not {py:data}`None`.
+    """
     first_present = first if isinstance(first, bool) else first is not None
     second_present = second if isinstance(second, bool) else second is not None
     if first_present and second_present:
         raise ValueError(message)
 
 
 def get_subfield_schema(field: Optional[ModelField], subfield_index: int) -> Optional[Dict[str, Any]]:
     """
     Creates a schema of the sub-field of the model field specified.
 
-    Args:
-        field: The model field.
-        subfield_index: The index of the sub-field of interest.
-
-    Returns:
-        The schema representing the sub-field of the model field if it is present, :py:data:`None` otherwise.
+    :param field: The model field.
+    :param subfield_index: The index of the sub-field of interest.
+    :returns: The schema representing the sub-field of the model field if it is present, {py:data}`None` otherwise.
     """
     if field is None or field.sub_fields is None:
         return None
     subfield_schema = pydantic.schema_of(field.sub_fields[subfield_index].type_)
     subfield_schema.pop('title', None)
     return subfield_schema
 
 
 def get_dimension(field: Optional[ModelField], subfield_index: int) -> Optional[PhysicalDimension]:
     """
     Gets the physical dimension associated to the model field specified.
 
-    Args:
-        field: The model field.
-        subfield_index: The index of the sub-field of interest.
-
-    Returns:
-        The physical dimension associated to the model field if it is present, :py:data:`None` otherwise.
+    :param field: The model field.
+    :param subfield_index: The index of the sub-field of interest.
+    :returns: The physical dimension associated to the model field if it is present, {py:data}`None` otherwise.
     """
     if field is None or field.sub_fields is None:
         return None
     dimension = field.sub_fields[subfield_index].type_
     if dimension == Any:
         return None
     if issubclass(dimension, PhysicalDimension):
         return dimension
 
     raise TypeError(f"Unsupported {dimension}, should be a {PhysicalDimension.__name__!r} or 'typing.Any'.")
 
 
 def update_object_schema(field_schema: Dict[str, Any], **properties) -> None:
-    """
-    Updates the field schema with object properties, ignoring :py:data:`None` values.
+    """Updates the field schema with object properties, ignoring {py:data}`None` values.
 
-    Updates the dictionary with a key ``type`` set to ``object`` and a key ``property``, the value of which is a
-    dictionary containing all properties specified that are not :py:data:`None`.
+    Updates the dictionary with a key `type` set to `object` and a key `property`, the value of which is a dictionary
+    containing all properties specified that are not {py:data}`None`.
 
-    Args:
-        field_schema: The field schema to update.
-        **properties: The properties
+    :param field_schema: The field schema to update.
+    :param properties: The properties
     """
     field_schema |= {"type": "object", "properties": {k: v for (k, v) in properties.items() if v is not None}}
 
 
 def validate(value: T,
              validators: Union[bool, ValidationFunc, Iterable[ValidationFunc]],
              msg_template: Optional[str] = None) -> T:
     """
     Validates a given value based on the validator function(s) specified.
 
-    Args:
-        value: The value to validate.
-        validators: The validator function(s).
-        msg_template: The message to show in case the validation fails, may contain ``{value}`` to refer to the value.
-
-    Returns:
-        The given value in case the validation is successful.
+    :param value: The value to validate.
+    :param validators: The validator function(s).
+    :param msg_template: The message to show in case the validation fails, may contain ``{value}`` to refer to the
+        value.
+    :returns: The given value in case the validation is successful.
     """
     msg_template = msg_template or "Validation failed for {value}"
     error = ValueError(msg_template.format(value=value))
     if isinstance(validators, bool):
         if validators:
             return value
         raise error
@@ -226,9 +262,16 @@
 
     return value
 
 
 def validate_each(value: Iterable,
                   validators: Union[bool, ValidationFunc, Iterable[ValidationFunc]],
                   msg_template: Optional[str] = None) -> None:
+    """
+    Validates each element in an iterable according to the provided validators.
+
+    :param value: The iterable whose elements need to be validated.
+    :param validators: A single or a collection of validation functions to apply on each element.
+    :param msg_template: An optional error message template used if validation fails.
+    """
     for (i, element) in enumerate(value):
         validate(element, validators, f'While validating element {i}: {msg_template}')
```

## optool/optimization/__init__.py

```diff
@@ -0,0 +1,22 @@
+00000000: 2222 220a 4661 6369 6c69 7461 7469 6e67  """.Facilitating
+00000010: 2074 6865 2066 6f72 6d75 6c61 7469 6f6e   the formulation
+00000020: 2061 6e64 2073 6f6c 7574 696f 6e20 6f66   and solution of
+00000030: 206e 6f6e 6c69 6e65 6172 2070 726f 6772   nonlinear progr
+00000040: 616d 6d69 6e67 2028 4e4c 5029 2070 726f  amming (NLP) pro
+00000050: 626c 656d 732e 0a0a 4974 2063 6f6d 6573  blems...It comes
+00000060: 2077 6974 6820 6120 636f 6c6c 6563 7469   with a collecti
+00000070: 6f6e 206f 6620 6d6f 6475 6c65 7320 7468  on of modules th
+00000080: 6174 2066 6163 696c 6974 6174 6520 7468  at facilitate th
+00000090: 6520 7265 7072 6573 656e 7461 7469 6f6e  e representation
+000000a0: 206f 6620 6465 6369 7369 6f6e 2076 6172   of decision var
+000000b0: 6961 626c 6573 2c20 636f 6e73 7472 6169  iables, constrai
+000000c0: 6e74 732c 206f 7264 696e 6172 790a 6469  nts, ordinary.di
+000000d0: 6666 6572 656e 7469 616c 2065 7175 6174  fferential equat
+000000e0: 696f 6e73 2028 4f44 4573 292c 2061 6e64  ions (ODEs), and
+000000f0: 206f 7665 7261 6c6c 2070 726f 626c 656d   overall problem
+00000100: 2066 6f72 6d75 6c61 7469 6f6e 2e0a 4974   formulation..It
+00000110: 2061 6c73 6f20 7072 6f76 6964 6573 2075   also provides u
+00000120: 7469 6c69 7469 6573 2066 6f72 2064 6562  tilities for deb
+00000130: 7567 6769 6e67 2061 6e64 2061 6e61 6c79  ugging and analy
+00000140: 7a69 6e67 2073 6f6c 7665 7220 7265 7370  zing solver resp
+00000150: 6f6e 7365 732e 0a22 2222 0a              onses..""".
```

## optool/optimization/constraints.py

```diff
@@ -1,26 +1,33 @@
+"""
+Representation of constraints of a numerical optimization problem.
+
+This module defines constraints for optimization problems.
+"""
+
 import math
 from abc import ABC
 from typing import Any, Dict, Optional, Union, final
 
 import numpy as np
+from casadi import casadi
 from pydantic import Field, StrictStr, root_validator, validator
 
-from optool import BaseModel
-from optool.fields.misc import PositiveFiniteFloat
+from optool.core import BaseModel
+from optool.fields.misc import NonEmptyStr, PositiveFiniteFloat
 from optool.fields.numeric import ImmutableArray
 from optool.fields.quantities import QuantityLike
 from optool.fields.symbolic import CasadiColumn
 from optool.math import num_elements
 from optool.uom import Quantity, Unit
 
 
 class OptimizationConstraint(BaseModel, ABC):
     """
-    Constraint for an NLP formulation.
+    Constraint for a nonlinear program (NLP) formulation.
 
     This handle class allows to define a set of constraints which is then used for an NLP formulation.
 
     The set of constraints may be scalar or a vector.
     """
 
     name: StrictStr = ""
@@ -37,18 +44,18 @@
     """Directly creates a constraint object from a symbolic expression."""
 
     expression: QuantityLike[Any, Union[ImmutableArray, CasadiColumn]] = Field(allow_mutation=False)
     """The expression of the constraint."""
 
 
 class BoxConstraint(OptimizationConstraint):
-    """Creates a box constraint."""
+    """Box constraint that constrains an expression to be within (or equal to) the bounds specified."""
 
     expression: QuantityLike[Any, CasadiColumn] = Field(allow_mutation=False,
-                                                        exclude=True)  # Cannot handle casadi elements
+                                                        exclude=True)  # Cannot serialize casadi elements
     """The expression of the constraint."""
 
     lower_bound: Optional[QuantityLike[Any, ImmutableArray]] = Field(allow_mutation=False)
     """The smallest values the expression should be able to take."""
 
     upper_bound: Optional[QuantityLike[Any, ImmutableArray]] = Field(allow_mutation=False)
     """The greatest values the expression should be able to take."""
@@ -81,25 +88,62 @@
         reference_unit = other_units.popitem()[1]  # removes one item
 
         if any(not reference_unit.is_compatible_with(other) for other in list(other_units.values())):
             raise ValueError(f"Not all numerical values have compatible units, see {array_units}")
 
         return values
 
-    def get_symbols(self) -> CasadiColumn:
+    def get_symbols(self) -> casadi.SX:
+        """Column-oriented vector of [CasADi](https://web.casadi.org) symbols."""
         return self.expression.magnitude
 
     def get_dimensionless_lower_bound(self) -> np.ndarray:
+        """
+        Gets the dimensionless lower bound of the constraint.
+
+        If no lower bound is specified, negative infinity is returned.
+
+        :return: A {py:class}`numpy.ndarray` representing the dimensionless lower bound of the constraint.
+        """
         if self.lower_bound is None:
             return np.full((self.length(),), -math.inf)
         return self.lower_bound.m_as(self.get_unit())
 
     def get_dimensionless_upper_bound(self) -> np.ndarray:
+        """
+        Gets the dimensionless upper bound of the constraint.
+
+        If no upper bound is specified, positive infinity is returned.
+
+        :return: A {py:class}`numpy.ndarray` representing the dimensionless upper bound of the constraint.
+        """
         if self.upper_bound is None:
             return np.full((self.length(),), math.inf)
         return self.upper_bound.m_as(self.get_unit())
 
     def get_unit(self) -> Unit:
+        """
+        Gets the unit of the constraint's expression.
+
+        :return: A {py:data}`optool.uom.Unit` object representing the unit of the expression.
+        """
         return self.expression.units
 
     def length(self) -> int:
+        """
+        The number of elements (i.e., the vector length) of this constraint's expression.
+
+        :return: An integer representing the number of elements in the constraint's expression.
+        """
         return self.get_symbols().numel()
+
+
+class Equation(BaseModel, frozen=True):
+    """Representation of a formula that describes the equality of two expressions, by connecting them with the equals
+    sign."""
+
+    name: NonEmptyStr
+    """The name of the equation."""
+    lhs: QuantityLike[Any, CasadiColumn]
+    """The left-hand side of the equation."""
+    rhs: QuantityLike[Any, CasadiColumn]
+    """The right-hand side of the equation."""
```

## optool/optimization/helpers.py

```diff
@@ -1,14 +1,22 @@
+"""
+Utilities that assist the formulation and analysis of optimization problems.
+
+This module provides ancillary features for the optimization process.
+It houses tools for organizing debug information, handling solver responses, and managing Ipopt options.
+"""
+
 from datetime import timedelta
 from typing import TYPE_CHECKING, Any, Dict
 
+import humanize
 import pandas as pd
 from pydantic import Field, StrictBool, StrictFloat, StrictStr
 
-from optool import BaseModel, orthography
+from optool.core import BaseModel
 from optool.fields.containers import ConstrainedMutatingList
 from optool.fields.misc import NonEmptyStr
 from optool.fields.quantities import QuantityLike
 from optool.util import ValueRange
 
 if TYPE_CHECKING:
     ValueRangeList = list
@@ -36,17 +44,18 @@
         separator = f"{prefix}{'-' * 60}"
         details = [
             f"Debug information for the optimization problem entitled '{self.problem_name}'",
             f"{prefix}Normed values of the decision variables (as seen by the solver):", separator
         ]
 
         self._append_normed_values(details, prefix, self.normed_variable_values)
-        details.extend((separator, f"{prefix}Normed values of the lagrange multipliers of the constraints "
-                        f"(as seen by the solver):"))
-
+        details.extend((
+            separator,
+            f"{prefix}Normed values of the lagrange multipliers of the constraints (as seen by the solver):",
+        ))
         self._append_normed_values(details, prefix, self.normed_constraints_lagrange_multipliers)
         details.append(separator)
         return details
 
     @staticmethod
     def _append_normed_values(details: list[str], prefix: str, normed_values: list[ValueRange]) -> None:
         attributes_to_show = ["min", "avg", "max", "max_abs"]
@@ -68,57 +77,88 @@
 class SolverResponse(BaseModel, frozen=True):
     """The response returned by the solver."""
 
     problem_name: StrictStr
     """The name of the optimization problem."""
 
     function_value: QuantityLike[Any, StrictFloat]
+    """The final value of the objective."""
+
     success: StrictBool
+    """Indication if the optimization problem was solved successfully."""
+
     solver_status: Dict[str, Any]
+    """Details on the status of the numerical solver."""
+
     debug_info: DebugInfo
+    """Details of the solution found that can be used to fix potential problems or analyze the performance."""
 
     def get_return_status(self) -> str:
+        """Gets the return status of the solver."""
         return self.solver_status["return_status"]
 
     def get_number_of_iterations(self) -> int:
+        """Gets the number of iterations used by the solver."""
         return int(self.solver_status["iter_count"])
 
     def get_solver_time(self) -> timedelta:
+        """
+        Gets the time it took the solver to find the solution.
+
+        :return: The "wall time", i.e. actual physical time it took to solve the problem.
+
+        :::{seealso}
+        [Wikipedia: Elapsed real time](wiki:Elapsed_real_time)
+        :::
+        """
+
         # See https://groups.google.com/g/casadi-users/c/dMSGV8KII30?pli=1 for an explanation of both
         # 't_wall_total' and 't_proc_total' and the difference between them.
         return timedelta(seconds=self.solver_status["t_wall_total"])
 
-    def guarantee_success(self):
+    def guarantee_success(self) -> None:
+        """
+        Checks if the optimization was successful.
+
+        :raises UnsuccessfulOptimization: If the optimization was not successful
+        """
         if not self.success:
             raise UnsuccessfulOptimization(f"The problem entitled '{self.problem_name}' was not solved successfully, "
                                            f"but returned with '{self.get_return_status()}'.")
 
-    def get_message(self):
+    def get_message(self) -> str:
+        """
+        Returns a message summarizing the results of the optimization.
+
+        :return: A string message summarizing the results of the optimization
+        """
         success_msg = "" if self.success else "NOT "
-        duration_str = orthography.naturaldelta(self.get_solver_time(), minimum_unit="microseconds")
+        duration_str = humanize.naturaldelta(self.get_solver_time(), minimum_unit="microseconds")
         return f"The optimization problem {self.problem_name!r} was {success_msg}solved successfully " \
                f"after {self.get_number_of_iterations()} iterations and {duration_str} " \
                f"with return status {self.get_return_status()!r}."
 
 
 class IpoptOption(BaseModel, frozen=True):
     """
     The options available in Ipopt.
 
-    See Also:
-        `Ipopt documentation <https://coin-or.github.io/Ipopt/OPTIONS.html#OPT_print_options_documentation>`_
+    :::{seealso}
+    [Ipopt documentation](https://coin-or.github.io/Ipopt/OPTIONS.html#OPT_print_options_documentation)
+    :::
     """
 
     category: NonEmptyStr
     """The category of the option."""
 
     name: NonEmptyStr
     """The name of the option."""
 
     values: NonEmptyStr
     """The possible values to set."""
 
     description: NonEmptyStr
     """The description of the option."""
 
-    def pretty_print(self):
+    def pretty_print(self) -> None:
+        """Prints a formatted message describing the option."""
         print(f"{self.name}: ({self.category})\t{self.values}:\n{self.description}")
```

## optool/optimization/ode.py

```diff
@@ -1,44 +1,48 @@
-from abc import ABC, abstractmethod
-from enum import auto
-from typing import Any, Dict, final
+"""
+Representation of ordinary differential equations (ODE)s.
+
+This module contains numerical integration methods that simplify the implementation of ODEs with respect to their use in
+an optimization context.
+"""
+
+from typing import Any, Dict, Protocol, final
 
 from casadi import casadi
 from pandas import DatetimeIndex
 from pydantic import root_validator
 
-from optool import BaseModel, validate_arguments
 from optool.conversions import datetime_index_to_intervals
+from optool.core import BaseModel
 from optool.fields.callables import concallable
 from optool.fields.misc import NonEmptyStr
 from optool.fields.quantities import QuantityLike
 from optool.fields.symbolic import CasadiColumn
 from optool.fields.util import validate
 from optool.logging import LOGGER
 from optool.math import num_elements
+from optool.optimization.constraints import Equation
 from optool.uom import UNITS, Quantity
-from optool.util import StrEnum
 
 
 class OrdinaryDifferentialEquation(BaseModel, frozen=True):
     r"""
     Representation of an ordinary differential equation (ODE).
 
     An ODE is described by the mathematical formula of the following form,
 
-    .. math::
-        \dot{x} = f(x, u),
+    $$ \dot{x} = f(x, u), $$
 
-    where :math:`x` represents the state variable and :math:`u` the input variable that includes both controlled signals
-    and disturbances.
+    where $x$ represents the state variable and $u$ the input variable that includes both controlled signals and
+    disturbances.
 
     The corresponding implementation requires the definition of the state variable, the input variable, and the function
-    :math:`f`. The transcription then follows a multiple shooting approach, where the continuity is ensured via
-    so-called gap closing constraints. In order to use the associated vector-based formulation, both :math:`x` and
-    :math:`u` should be specified as column vectors, where :math:`u` has one element less than :math:`x`.
+    $f$. The transcription then follows a multiple shooting approach, where the continuity is ensured via so-called gap
+    closing constraints. In order to use the associated vector-based formulation, both $x$ and $u$ should be specified
+    as column vectors, where $u$ has one element less than $x$.
     """
 
     name: NonEmptyStr
     """The name of the ordinary differential equation."""
     state_variable: QuantityLike[Any, CasadiColumn]
     """The array of state variables of the ordinary differential equation."""
     input_variable: QuantityLike[Any, CasadiColumn]
@@ -79,106 +83,83 @@
                 f"The vector of time-derivatives for the ODE function {name} must have the same number of "
                 f"elements than the vector of input variables, but have {time_derivative.numel()} and "
                 f"{num_input_variables}.")
 
         return values
 
 
-class Equation(BaseModel, frozen=True):
-    """Representation of a formula that expresses the equality of two expressions, by connecting them with the equals
-    sign."""
-
-    lhs: QuantityLike[Any, CasadiColumn]
-    """The left-hand side of the equation."""
-    rhs: QuantityLike[Any, CasadiColumn]
-    """The right-hand side of the equation."""
-
+class IntegrationMethod(Protocol):
 
-class BaseIntegrationMethod(ABC):
-
-    @classmethod
-    @abstractmethod
-    def integrate(cls, ode: OrdinaryDifferentialEquation, timestamps: DatetimeIndex) -> Equation:
-        raise NotImplementedError()
+    def integrate(self, ode: OrdinaryDifferentialEquation, timestamps: DatetimeIndex) -> Equation:
+        pass
 
 
-class ForwardEuler(BaseIntegrationMethod):
+class ForwardEuler:
     r"""
     The Euler method for numerical integration.
 
     The forward Euler method, also simply referred to as the Euler method, is the most basic explicit method for
     numerical integration of ODEs and is the simplest Runge–Kutta method.
 
-    One step of the Euler method from :math:`i` to :math:`i+1` is given by
+    One step of the Euler method from $i$ to $i+1$ is given by
 
-    .. math::
-        x[i+1] = x[i] + T_s[i] \cdot f \left( x[i], u[i] \right),
+    $$ x[i+1] = x[i] + T_s[i] \cdot f \left( x[i], u[i] \right), $$
 
-    where :math:`T_s[i]` is the time between index :math:`i` and :math:`i+1`.
+    where $T_s[i]$ is the time between index $i$ and $i+1$.
 
-    See Also:
-        `Wikipedia: Euler method <https://en.wikipedia.org/wiki/Euler_method>`_
+    :::{seealso}
+    [Wikipedia: Euler method](wiki:Euler_method)
+    :::
     """
 
     @classmethod
     def integrate(cls, ode: OrdinaryDifferentialEquation, timestamps: DatetimeIndex) -> Equation:
         LOGGER.debug("Integrating {} with {}.", ode.name, cls.__name__)
 
         time_intervals = datetime_index_to_intervals(timestamps)
         time_derivative = ode.function(ode.state_variable[1:], ode.input_variable)
-        return Equation(lhs=ode.state_variable[1:], rhs=ode.state_variable[:-1] + time_intervals * time_derivative)
+        next_state = ode.state_variable[:-1] + time_intervals * time_derivative
+        return Equation(name=ode.name, lhs=ode.state_variable[1:], rhs=next_state)
 
 
-class RungeKutta4(BaseIntegrationMethod):
+class RungeKutta4:
     r"""
     Fourth-order method of the Runge–Kutta for numerical integration.
 
     Runge-Kutta 4 is the fourth-order method of the Runge–Kutta family, which is the most widely used Runge-Kutta method
     and thus also referred to as the classic Runge–Kutta method or simply the Runge–Kutta method.
 
-    One step of the Runge-Kutta method from :math:`i` to :math:`i + 1` is given by
+    One step of the Runge-Kutta method from $i$ to $i + 1$ is given by
 
-    .. math::
-        x[i+1] = x[i] + \frac{1}{6}\, T_s[i] \big( k_1 + 2k_2 + 2k_3 + k_4 \big),
 
-    where the parameters :math:`k_1, \ldots, k_4` are recursively defined as follows:
+    $$ x[i+1] = x[i] + \frac{1}{6}\, T_s[i] \big( k_1 + 2k_2 + 2k_3 + k_4 \big), $$
 
-    .. math::
+    where the parameters $k_1, \ldots, k_4$ are recursively defined as follows:
+
+    $$
         k_1 &= f \big( x[i], u[i] \big), \\
         k_2 &= f \big( x[i] + k_1 \tfrac{T_s[i]}{2}, u[i] \big), \\
         k_3 &= f \big( x[i] + k_2 \tfrac{T_s[i]}{2}, u[i] \big), \\
         k_4 &= f \big( x[i] + k_3 T_s[i], u[i] \big).
+    $$
 
-    Note that, if the time-derivative of :math:`x` is not dependent on :math:`x` itself, i.e., :math:`f(x,u) = f(u)`,
-    all parameters above are equal, i.e., :math:`k_1 = k_2 = k_3 = k_4`. Hence, the Runge-Kutta method is equal to the
+    Note that, if the time-derivative of $x$ is not dependent on $x$ itself, i.e., $f(x,u) = f(u)$,
+    all parameters above are equal, i.e., $k_1 = k_2 = k_3 = k_4$. Hence, the Runge-Kutta method is equal to the
     Euler method.
 
-    See Also:
-        `Wikipedia: Runge–Kutta methods <https://en.wikipedia.org/wiki/Runge–Kutta_methods>`_
+    :::{seealso}
+    [Wikipedia: Runge–Kutta methods](wiki:Runge–Kutta_methods)
+    :::
     """
 
     @classmethod
     def integrate(cls, ode: OrdinaryDifferentialEquation, timestamps: DatetimeIndex) -> Equation:
         LOGGER.debug("Integrating {} with {}.", ode.name, cls.__name__)
 
         time_intervals = datetime_index_to_intervals(timestamps)
         k1 = ode.function(ode.state_variable[1:], ode.input_variable)
         k2 = ode.function(ode.state_variable[1:] + time_intervals / 2 * k1, ode.input_variable)
         k3 = ode.function(ode.state_variable[1:] + time_intervals / 2 * k2, ode.input_variable)
         k4 = ode.function(ode.state_variable[1:] + time_intervals * k3, ode.input_variable)
         next_state = ode.state_variable[1:] + time_intervals / 6 * (k1 + 2 * k2 + 2 * k3 + k4)
 
-        return Equation(lhs=ode.state_variable[1:], rhs=next_state)
-
-
-class IntegrationMethod(StrEnum):
-    FORWARD_EULER = auto()
-    RUNGE_KUTTA_4 = auto()
-
-    @classmethod
-    @validate_arguments
-    def integrate(cls, ode: OrdinaryDifferentialEquation, timestamps: DatetimeIndex) -> Equation:
-        if cls.FORWARD_EULER:
-            return ForwardEuler.integrate(ode, timestamps)
-        if cls.RUNGE_KUTTA_4:
-            return RungeKutta4.integrate(ode, timestamps)
-        raise NotImplementedError(f"Missing case for {cls.value}.")
+        return Equation(name=ode.name, lhs=ode.state_variable[1:], rhs=next_state)
```

## optool/optimization/problem.py

```diff
@@ -1,47 +1,48 @@
+"""
+Representation of optimization problem that can be defined and solved without the need for cumbersome notation.
+
+This module is a provides the core implementation for formulating optimization problems and managing their components.
+It provides a consistent framework to encapsulate problem elements, promoting efficient organization and manipulation of
+complex problem structures, thereby simplifying the optimization process.
+"""
+
 import io
 import re
 from abc import ABC, abstractmethod
 from contextlib import redirect_stdout
 from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union, final
 
 import casadi
 import numpy as np
-from pandas import DatetimeIndex
-from pydantic import StrictFloat, StrictInt, StrictStr, validator
+from pydantic import StrictFloat, StrictInt, StrictStr, validate_arguments, validator
 
-from optool import BaseModel, validate_arguments
+from optool.core import BaseModel
 from optool.fields.containers import conlist
 from optool.fields.numeric import ImmutableArray
 from optool.fields.quantities import QuantityLike, UnitLike
 from optool.fields.symbolic import CasadiColumn, CasadiScalar
 from optool.fields.util import validate, validate_each
 from optool.logging import LOGGER
 from optool.math import is_symbolic, num_elements
-from optool.optimization.constraints import BoxConstraint, ExpressionConstraint, OptimizationConstraint
+from optool.optimization.constraints import BoxConstraint, Equation, ExpressionConstraint, OptimizationConstraint
 from optool.optimization.helpers import DebugInfo, IpoptOption, SolverResponse
-from optool.optimization.ode import IntegrationMethod, OrdinaryDifferentialEquation
 from optool.optimization.variables import CasadiVariable, OptimizationVariable
 from optool.uom import UNITS, Quantity, Unit
 from optool.util import ValueRange
 
-T = TypeVar("T", bound=OptimizationConstraint)
-
 
 @validate_arguments
 def _find_element(name: str, elements: list[Any]) -> Optional[int]:
     """
     Finds the index at which the element with the specified name is located in the specified list of elements.
 
-    Args:
-        name: The name of the variable or constraint.
-        elements: The list of elements to search.
-
-    Returns:
-        The index at which the element is located, or :py:data:`None` if the element is not present in the list.
+    :param name: The name of the variable or constraint.
+    :param elements: The list of elements to search.
+    :returns: The index at which the element is located, or {py:data}`None` if the element is not present in the list.
     """
 
     validate_each(elements, lambda x: hasattr(x, "name"), "Element is missing attribute 'name', see {value}.")
     index = [i for (i, element) in enumerate(elements) if name == element.name]
     if not index:
         return None
     if len(index) == 1:
@@ -62,138 +63,253 @@
     elif formulation == "QP":
         return casadi.qpsol(*inputs)
     else:
         raise ValueError(f"Unknown type '{formulation}'. Use either 'NLP' or 'QP'.")
 
 
 class ProblemElements(BaseModel):
+    """
+    Container for storing and managing the variables and constraints of an optimization problem.
+
+    :param variables: A list of variables of the optimization problem.
+    :param constraints: A list of constraints of the optimization problem.
+    """
 
     @staticmethod
     def _is_distinct(values):
+        """
+        Validates if all elements in the list have distinct names.
+
+        :param values: A list of elements.
+        :return: The input list if validation is successful.
+        """
         names = [element.name for element in values]
         validate(names, len(names) == len(set(names)), 'All names must be distinct, but have {value}.')
         return values
 
     variables: conlist(OptimizationVariable, custom=_is_distinct) = []  # type:ignore[valid-type]
     """A list of variables of the optimization problem."""
     constraints: conlist(OptimizationConstraint, custom=_is_distinct) = []  # type:ignore[valid-type]
     """A list of constraints of the optimization problem."""
 
     def has_variable(self, name: str) -> bool:
+        """
+        Checks if a variable with the specified name exists in the problem.
+
+        :param name: Name of the variable.
+        :return: {py:data}`True` if variable exists, {py:data}`False` otherwise.
+        """
         return _find_element(name, self.variables) is not None
 
     def get_variable(self, name: str) -> OptimizationVariable:
+        """
+        Retrieves a variable by its name.
+
+        :param name: Name of the variable.
+        :return: The variable with the specified name.
+        :raises ValueError: If no variable with the specified name is found.
+        """
         index = _find_element(name, self.variables)
         if index is None:
             raise ValueError(f"The variable {name!r} is not present. Here is a list of all available variables:"
                              f"\n{[var.name for var in self.variables]}")
         return self.variables[index]
 
     def has_constraint(self, name: str) -> bool:
+        """
+        Checks if a constraint with the specified name exists in the problem.
+
+        :param name: Name of the constraint.
+        :return: {py:data}`True` if constraint exists, {py:data}`False` otherwise
+        """
+
         return _find_element(name, self.constraints) is not None
 
     def get_constraint(self, name: str) -> OptimizationConstraint:
+        """
+        Retrieves a constraint by its name.
+
+        :param name: Name of the constraint.
+        :return: The constraint with the specified name.
+        :raises ValueError: If no constraint with the specified name is found.
+        """
         index = _find_element(name, self.constraints)
         if index is None:
             raise ValueError(f"The constraint {name!r} is not present. Here is a list of all available constraints:"
                              f"\n{[var.name for var in self.constraints]}")
         return self.constraints[index]
 
 
 class OptimizationProblem(ProblemElements, ABC):  # Abstract base class
     """
-    Define and solve an optimization program.
+    Definition of an optimization program.
 
     Convenient interface to define and solve an optimization problem without the need for cumbersome notation.
     """
 
     name: StrictStr = ""
     """The name of the optimization problem."""
 
     @abstractmethod
     def parse(self, solver: str, options: Dict[str, Any]) -> None:
         """
         Parses the problem as specified such that it can be solved afterward.
 
-        Args:
-            solver: A solver specified as string, e.g., `ipopt`.
-            options: A Dictionary of options.
+        :param solver: A solver specified as string, e.g., `ipopt`.
+        :param options: A Dictionary of options.
 
-        Example::
+        :::{admonition} Example
+        :class: example
 
-        >>> obj.parse('ipopt')
+        ```python
+        obj.parse('ipopt')
+        ```
         """
         raise NotImplementedError()
 
     @abstractmethod
     def solve(self) -> SolverResponse:
         """
         Solves the formulated optimization problem.
 
         Solves the optimization problem and writes the results into the solution fields of the optimization variables.
 
-        Example::
+        :::{admonition} Example
+        :class: example
 
-        >>> status = obj.solve()
+        ```python
+        status = obj.solve()
+        ```
         """
         raise NotImplementedError()
 
     @staticmethod
-    @abstractmethod
-    def get_parser_options(type: str = "NLP", solver: str = "ipopt") -> Dict[str, Tuple[str, str]]:
-        raise NotImplementedError()
-
-    @staticmethod
-    @abstractmethod
-    def get_solver_options(formulation: str = "NLP", solver: str = "ipopt") -> frozenset[IpoptOption]:
-        raise NotImplementedError()
+    def casadi(name=""):
+        return CasadiProblem(name=name)
 
     @abstractmethod
     def new_variable(self, name: str, n: int, unit: Optional[Unit] = None) -> OptimizationVariable:
+        """
+        Creates and adds a new optimization variable to the optimization problem.
+
+        :param name: Name of the new variable.
+        :param n: Number of elements of the new variable.
+        :param unit: Physical unit of the new variable.
+        :return: The variable created.
+        """
         raise NotImplementedError()
 
     @abstractmethod
     def add_equality_constraint(self, lhs, rhs, nominal_value=None) -> OptimizationConstraint:
+        """
+        Adds an equality constraint forcing the left-hand side (lhs) to be equal to the right-hand side (rhs).
+
+        :param lhs: Left-hand side expression.
+        :param rhs: Right-hand side expression.
+        :param nominal_value: The nominal value to be used for the equality constraint.
+        :return: The constraint added.
+        """
         raise NotImplementedError()
 
     @abstractmethod
     def add_box_constraint(self, lower_bound, expression, upper_bound, nominal_value=None) -> OptimizationConstraint:
+        """
+        Adds a box constraint that forces the expression to be within (or equal to) the bounds specified.
+
+        :param lower_bound: The lower bound for the box constraint.
+        :param expression: The expression to be constrained.
+        :param upper_bound: The upper bound for the box constraint.
+        :param nominal_value: The nominal value to be used for the box constraint.
+        :return: The constraint added.
+        """
         raise NotImplementedError()
 
     @abstractmethod
     def add_greater_than_constraint(self, lower_bound, expression, nominal_value=None) -> OptimizationConstraint:
+        """
+        Adds a constraint that forces the expression to be greater than or equal to a lower bound.
+
+        :param lower_bound: The lower bound for the constraint.
+        :param expression: The expression to be constrained.
+        :param nominal_value: The nominal value to be used for the constraint.
+        :return: The constraint added.
+        """
         raise NotImplementedError()
 
     @abstractmethod
     def add_less_than_constraint(self, expression, upper_bound, nominal_value=None) -> OptimizationConstraint:
+        """
+        Adds a constraint that forces the expression to be smaller than or equal to a lower bound.
+
+        :param expression: The expression to be constrained.
+        :param upper_bound: The upper bound for the constraint.
+        :param nominal_value: The nominal value to be used for the constraint.
+        :return: The added constraint.
+        """
         raise NotImplementedError()
 
     @abstractmethod
-    def add_continuity_constraint(self, integration_method: IntegrationMethod, ode: OrdinaryDifferentialEquation,
-                                  timestamps: DatetimeIndex) -> BoxConstraint:
+    def add_equation(self, equation: Equation) -> BoxConstraint:
+        """
+        Adds an equation that describes the equality of two expressions.
+
+        :param equation: The equation to add.
+        :return: The added constraint.
+        """
         raise NotImplementedError()
 
     @staticmethod
-    def casadi(name=""):
-        return CasadiProblem(name=name)
+    @abstractmethod
+    def get_parser_options(formulation: str = "NLP", solver: str = "ipopt") -> Dict[str, Tuple[str, str]]:
+        """
+        Retrieves the solver options for a given solver and formulation.
+
+        :param formulation: The formulation for which to retrieve the options.
+        :param solver: The solver for which to retrieve the options.
+        :return: A dictionary describing the available parser options.
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    @abstractmethod
+    def get_solver_options(formulation: str = "NLP", solver: str = "ipopt") -> frozenset[IpoptOption]:
+        """
+        Retrieves the parser options for a given solver and formulation.
+
+        :param formulation: The formulation for which to retrieve the options.
+        :param solver: The solver for which to retrieve the options.
+        :return: A set of the available solver options.
+        """
+        raise NotImplementedError()
 
 
 # noinspection PyArgumentList
 def robust_divide_units(numerator: Optional[Unit], denominator: Optional[Unit]) -> Optional[Unit]:
+    """
+    Performs division between two units in a way that tolerates {py:data}`None` values.
+
+    :param numerator: The numerator unit.
+    :param denominator: The denominator unit.
+    :return: The result of division, if applicable. If both numerator and denominator are {py:data}`None`, returns
+    {py:data}`None`.
+    """
     if numerator is None and denominator is None:
         return None
     elif numerator is None:
         return 1 / denominator  # type: ignore
     elif denominator is None:
         return numerator
     return numerator / denominator
 
 
+_T = TypeVar("_T", bound=OptimizationConstraint)
+
+
 class CasadiProblem(OptimizationProblem):
-    """Representation of an optimization problem using the modeling language CasADi."""
+    """Representation of an optimization problem using the modeling language [CasADi](https://web.casadi.org)."""
 
     _solver: casadi.Function = None
 
     objective: Optional[QuantityLike[Any, CasadiScalar]] = None
     """The objective to minimize."""
 
     @final
@@ -305,15 +421,15 @@
 
         return SolverResponse(problem_name=self.name,
                               function_value=function_value,
                               success=status["success"],
                               solver_status=status,
                               debug_info=debug_info)
 
-    def _get_constraints(self, constraint_type: Type[T]) -> list[T]:
+    def _get_constraints(self, constraint_type: Type[_T]) -> List[_T]:
         return list(filter(lambda x: isinstance(x, constraint_type), self.constraints))
 
     def _get_normed_replicated_variable_values(self, field_name: str) -> np.ndarray:
         normed_values = [getattr(val, field_name) / val.nominal_values for val in self.variables]
         dimensionless_values = [val.m_as(UNITS.dimensionless) for val in normed_values]
         return np.concatenate(dimensionless_values)  # type: ignore
 
@@ -400,20 +516,17 @@
                                    expression=expression / nominal_value,
                                    upper_bound=upper_bound / nominal_value,
                                    nominal_value=nominal_value)
         self.constraints.append(constraint)
         return constraint
 
     @validate_arguments
-    def add_continuity_constraint(self, integration_method: IntegrationMethod, ode: OrdinaryDifferentialEquation,
-                                  timestamps: DatetimeIndex) -> BoxConstraint:
-
-        eq = integration_method.integrate(ode, timestamps)
-        constraint = self.add_equality_constraint(eq.lhs, eq.rhs)
-        constraint.name = ode.name
+    def add_equation(self, equation: Equation) -> BoxConstraint:
+        constraint = self.add_equality_constraint(equation.lhs, equation.rhs)
+        constraint.name = equation.name
         return constraint
 
     @staticmethod
     def get_parser_options(formulation: str = "NLP", solver: str = "ipopt") -> Dict[str, Tuple[str, str]]:
         inputs = ("solver", solver, {'x': casadi.SX.sym("", 1, 1)})
         dummy_solver = _get_dummy_solver(formulation, inputs)
```

## optool/optimization/variables.py

```diff
@@ -1,40 +1,39 @@
+"""
+Representation of decision variables for Nonlinear Programs (NLP)s.
+
+This module offers utilities for defining and manipulating decision variables in an NLP formulation.
+With functions to handle the initialization, updates, and querying of variable sets, this module streamlines the
+creation and management of decision variables, providing a flexible and intuitive interface for designing numerical
+optimization problems.
+"""
+
 import math
 from abc import ABC, abstractmethod
 from typing import Any, Dict, Optional, Union, final
 
 import casadi
 import numpy as np
 from pydantic import StrictBool, root_validator, validator
 
-from optool import BaseModel
+from optool.core import BaseModel
 from optool.fields.misc import NonEmptyStr
 from optool.fields.numeric import ImmutableArray
 from optool.fields.quantities import QuantityLike, UnitLike
 from optool.fields.util import validate
 from optool.logging import LOGGER
 from optool.math import SYMBOLIC_TYPES, has_offset, num_elements
 from optool.uom import UNITS, Quantity, Unit
 
 
 class OptimizationVariable(BaseModel, ABC):
-    """
-    Set of decision variables for an NLP formulation.
-
-    This handle class allows to define a set of decision variables which is then used for an NLP formulation.
-
-    The set of decision variables may be scalar or a vector.
-    """
+    """Abstract representation of a decision variable for the formulation of a nonlinear program (NLP)."""
 
     _frozen_nominal_values: StrictBool = False
-    """
-    Indicator flag to ensure that nominal values cannot be changed anymore.
-
-    Shouldn't be set manually.
-    """
+    """Indicator flag to ensure that nominal values cannot be changed anymore. Shouldn't be set manually."""
 
     name: NonEmptyStr
     """The name of the decision variable."""
 
     unit: Optional[UnitLike] = None
     """The physical unit of the decision variable."""
 
@@ -68,14 +67,15 @@
             array_lengths, numerical_values, unique_lengths = cls._get_array_lengths(values)
 
             if np.size(unique_lengths) == 1 and unique_lengths > 1:
                 return Quantity(np.full(unique_lengths, value.magnitude), value.units)
 
         return value
 
+    @final
     @validator('initial_guess', 'nominal_values', allow_reuse=True)
     def _is_finite(cls, value):
         return validate(value, lambda x: np.all(np.isfinite(x)), 'All array elements must be finite, but have {value}.')
 
     @final
     @validator('lower_bounds', 'upper_bounds', allow_reuse=True)
     def _is_not_nan(cls, value):
@@ -112,41 +112,67 @@
     @staticmethod
     def _get_array_lengths(values: Dict):
         numerical_values = [key for (key, value) in values.items() if isinstance(value, Quantity)]
         array_lengths: Dict[str, int] = {key: num_elements(values[key].magnitude) for key in numerical_values}
         unique_lengths = np.unique(list(array_lengths.values()))
         return array_lengths, numerical_values, unique_lengths
 
-    @abstractmethod
-    def length(self) -> int:
-        """Gets the number of symbols."""
-        raise NotImplementedError()
-
     @property
     @abstractmethod
     def normed(self) -> SYMBOLIC_TYPES:
+        r"""
+        The normed symbols, i.e., the actual decision variables not multiplied by the nominal value.
+
+        The normed symbols are the actual variable seen by the optimizer. These values should ideally be in the range of
+        $\pm 1$ to enhance numerical stability. However, formulating an optimization problem can become inconvenient if
+        the user always has to keep track of the normalization value. Hence, {py:attr}`OptimizationVariable.regular`
+        offers the _actual_ variable value that includes both the nominal value and the physical unit specified.
+        """
         raise NotImplementedError()
 
     @property
     @abstractmethod
     def regular(self) -> Quantity:
+        """
+        The regular symbols, i.e., the decision variables multiplied by the nominal value.
+
+        The regular symbols typically are used to formulate any constraints of the optimization problem. They include
+        both the nominal value and the physical unit specified.
+        """
+        raise NotImplementedError()
+
+    @abstractmethod
+    def length(self) -> int:
+        """
+        The number of elements (i.e., the vector length) of this optimization variable.
+
+        :return: An integer representing the number of elements of this optimization variable.
+        """
         raise NotImplementedError()
 
     def _freeze_nominal_values(self):
         if not self._frozen_nominal_values:
             LOGGER.debug("Freezing nominal values of variable named {}.", self.name)
             self._frozen_nominal_values = True
 
     @staticmethod
     def casadi(name: str, n: int, unit: Optional[UnitLike] = None):
+        """
+        Creates a [CasADi](https://web.casadi.org) variable.
+
+        :param name: Name of the new variable.
+        :param n: Number of elements of the new variable.
+        :param unit: Physical unit of the new variable.
+        :return: The variable created.
+        """
         return CasadiVariable(name, n, unit)
 
 
 class CasadiVariable(OptimizationVariable):
-    """Representation of a decision variable using the modeling language CasADi."""
+    """Representation of a decision variable using the modeling language [CasADi](https://web.casadi.org)."""
 
     _symbols: casadi.SX
 
     def __init__(self, name: str, n: int, unit: Optional[UnitLike] = None):
         super().__init__(name=name,
                          unit=unit,
                          initial_guess=Quantity(np.zeros((n,)), unit),
```

## optool/serialization/__init__.py

```diff
@@ -1,60 +1,129 @@
+"""
+Serialization of a variety of well-known Python data types.
+
+The package features a collection of modules, each dedicated to the data types of a specific third-party package.
+Accordingly, each module implements dedicated serializer classes that handle the respective serialization and
+deserialization processes.
+"""
+
 from __future__ import annotations
 
 import json
 from abc import ABC, abstractmethod
 from collections import OrderedDict
 from typing import Any, Callable, Dict, ForwardRef, Generic, Type, TypeVar, Union, get_args
 
 from optool.logging import LOGGER
 
 T = TypeVar("T")
+"""Generic type variable used for defining the type of the objects a particular {py:class}`Serializer` can handle."""
+
 AllowedSerializedDictKeys = Union[str, int, float, bool, None]
+"""Type alias describing all types that are allowed as as keys of a dictionary that represents a serialized object."""
 
 
 class Serializer(ABC, Generic[T]):
-    _type_T: type
+    """
+    An abstract base class that defines a serializer for objects of a specific type.
+
+    The serializer provides methods to convert objects of the specified type to a dictionary of primitive types and
+    vice versa, thereby enabling the serialization and deserialization of objects for storage or transmission.
+
+    The actual serialization and deserialization logic is to be implemented in subclasses. The type of the objects the
+    corresponding serializers can handle are specified via generic type annotations.
+
+    ::::{admonition} Example
+    :class: example
+    To serialize objects of type {py:class}`zoneinfo.ZoneInfo`, the following implementation could be used.
+
+    ```python
+    from zoneinfo import ZoneInfo
+
+    class ZoneInfoSerializer(Serializer[ZoneInfo]):
+
+        def serialize(self, obj: ZoneInfo) -> Dict[AllowedSerializedDictKeys, Any]:
+           return {'key': obj.key}
+
+        def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> ZoneInfo:
+            return ZoneInfo(raw['key'])
+    ```
+
+    To enable serialization of all {py:class}`zoneinfo.ZoneInfo` within fields or sub-fields of any
+    {py:class}`pydantic.BaseModel`, the class is added to the {py:class}`SerializationAssistant` via
+    ```python
+    SerializationAssistant.register(ZoneInfoSerializer())
+    ```
+    the output of which is set {py:attr}`pydantic.BaseConfig.json_encoders`.
+    ::::
+
+    :::{seealso}
+    {py:class}`SerializationAssistant` and implementation of {py:attr}`optool.BaseModel.Config.json_encoders`.
+    :::
+    """
+
+    _type_T: Type
 
     def __init_subclass__(cls) -> None:
         # Get the generic type. Approach taken from https://stackoverflow.com/a/71720366
         # noinspection PyUnresolvedReferences
         cls._type_T = get_args(cls.__orig_bases__[0])[0]  # type: ignore
 
     @classmethod
-    def get_type(cls) -> type:
+    def get_type(cls) -> Type:
+        """
+        Gets the type of the objects this serializer is supposed to handle.
+
+        :return: The type of the objects to serialize and deserialize.
+        """
         return cls._type_T
 
     @classmethod
     def get_type_name(cls) -> str:
+        """
+        Gets the name of the type of the objects this serializer is supposed to handle.
+
+        :return: {py:meth}`get_type` converted to a string.
+        """
         return str(cls.get_type())
 
     @abstractmethod
     def serialize(self, obj: T) -> Dict[AllowedSerializedDictKeys, Any]:
-        """Serializes an object to a dictionary of primitive types."""
+        """
+        Serializes an object to a dictionary of primitive types.
+
+        :param obj: The object to serialize.
+        :return: Dictionary describing the serialized object, potentially containing values the need further
+            serialization.
+        """
         raise NotImplementedError()
 
     @abstractmethod
     def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> T:
-        """Deserializes a dictionary of primitive types to an object."""
+        """
+        Deserializes a dictionary of primitive types to an object.
+
+        :param raw: The raw descriptive content of the object from which the latter is to be created.
+        :return: The deserialized object.
+        """
         raise NotImplementedError()
 
 
 class SerializationAssistant:
+    """Utility class holding all serializers registered."""
+
     _serializers: Dict[str, Serializer] = {}
 
     @classmethod
     def register(cls, *serializers: Serializer) -> Dict[Union[Type[Any], str, ForwardRef], Callable]:
         """
         Registers the serializers specified.
 
-        Args:
-            *serializers: The serializers to register
-
-        Returns:
-            Dictionary mapping types to the corresponding JSON encoders.
+        :param serializers: The serializers to register
+        :returns: Dictionary mapping types to the corresponding JSON encoders.
         """
         for serializer in serializers:
             obj_type = serializer.get_type()
             obj_type_name = serializer.get_type_name()
             LOGGER.debug("Registering serializer for {}.", obj_type)
             if obj_type_name in cls._serializers:
                 raise ValueError(f"There is already an entry in the registry for {obj_type_name}.")
@@ -68,14 +137,23 @@
         def _encode_obj(obj: T) -> Dict[AllowedSerializedDictKeys, Any]:
             return OrderedDict({'obj_type': serializer.get_type_name()}, **serializer.serialize(obj))
 
         return _encode_obj
 
     @classmethod
     def json_loader(cls, raw: Union[str, bytes]) -> Any:
+        """
+        Loads a JSON object from a string or bytes and parses it into a Python object.
+
+        This method uses the custom object pair hook function defined in this class to parse the
+        JSON object, providing flexibility in how the JSON object is converted to a Python object.
+
+        :param raw: The JSON object to be loaded, represented as a string or bytes.
+        :return: The Python object resulting from parsing the JSON object.
+        """
         return json.loads(raw, object_pairs_hook=cls._parse_raw)
 
     @classmethod
     def _parse_raw(cls, tuples: list[tuple[Any, Any]]) -> Any:
         dct = OrderedDict(tuples)
 
         if 'obj_type' not in dct:
```

## optool/serialization/datetime_objects.py

```diff
@@ -1,26 +1,35 @@
+"""
+Serialization of date-time objects.
+
+This module offers custom serialization of {py:class}`~zoneinfo.ZoneInfo` and {py:class}`~datetime.datetime` objects.
+"""
+
 from __future__ import annotations
 
 from datetime import datetime
 from typing import Any, Dict
 from zoneinfo import ZoneInfo
 
 from optool.serialization import AllowedSerializedDictKeys, Serializer
 
 
 class ZoneInfoSerializer(Serializer[ZoneInfo]):
+    """Serializer for {py:class}`zoneinfo.ZoneInfo` objects."""
 
     def serialize(self, obj: ZoneInfo) -> Dict[AllowedSerializedDictKeys, Any]:
         return {'key': obj.key}
 
     def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> ZoneInfo:
         return ZoneInfo(raw['key'])
 
 
 class DatetimeSerializer(Serializer[datetime]):
+    """Serializer for {py:class}`datetime.datetime` objects."""
+
     _DICT_KEY_VALUE = 'value'
     _DICT_KEY_TIMEZONE = 'timezone'
     _DICT_KEY_FORMAT = 'format'
     _DATE_FMT = "%Y-%m-%d %H:%M:%S.%f"
     _DATE_FMT_TZ = _DATE_FMT + " %z"
 
     def serialize(self, obj: datetime) -> Dict[AllowedSerializedDictKeys, Any]:
```

## optool/serialization/numpy_objects.py

```diff
@@ -1,18 +1,24 @@
+"""
+Serialization of [Numpy](https://pypi.org/project/numpy/) objects.
+
+This module offers custom serialization of {py:class}`~numpy.ndarray` objects.
+"""
+
 from __future__ import annotations
 
 from typing import Any, Dict
 
 import numpy as np
 
 from optool.serialization import AllowedSerializedDictKeys, Serializer
 
 
 class NumpyNdArraySerializer(Serializer[np.ndarray]):
-    """Serializer for :py:class:`numpy.ndarray` objects."""
+    """Serializer for {py:class}`numpy.ndarray` objects."""
 
     def serialize(self, obj: np.ndarray) -> Dict[AllowedSerializedDictKeys, Any]:
         return {"datatype": obj.dtype.name, "writeable": obj.flags.writeable, "values": obj.tolist()}
 
     def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> np.ndarray:
         value = np.asarray(raw["values"], dtype=raw["datatype"])
         value.setflags(write=raw['writeable'])
```

## optool/serialization/pandas_objects.py

```diff
@@ -1,28 +1,38 @@
+"""
+Serialization of [Pandas](https://pypi.org/project/pandas/) objects.
+
+This module offers custom serialization of {py:class}`~pandas.RangeIndex`, {py:class}`~pandas.DatetimeIndex`,
+{py:class}`~pandas.Series`, and {py:class}`~pandas.DataFrame` objects.
+"""
+
 from __future__ import annotations
 
 from datetime import datetime
 from typing import Any, Dict
 
 import pandas as pd
 from pint_pandas import PintArray
 
 from optool.serialization import AllowedSerializedDictKeys, Serializer
 
 
 class PandasRangeIndexSerializer(Serializer[pd.RangeIndex]):
+    """Serializer for {py:class}`pandas.RangeIndex` objects."""
 
     def serialize(self, obj: pd.RangeIndex) -> Dict[AllowedSerializedDictKeys, Any]:
         return dict(start=obj.start, stop=obj.stop, step=obj.step, name=obj.name)
 
     def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> pd.RangeIndex:
         return pd.RangeIndex(**raw)
 
 
 class PandasDatetimeIndexSerializer(Serializer[pd.DatetimeIndex]):
+    """Serializer for {py:class}`pandas.DatetimeIndex` objects."""
+
     _DICT_KEY_VALUES = 'values'
     _DICT_KEY_FORMAT = 'format'
     _DICT_KEY_FREQUENCY = 'freq'
     _DICT_KEY_TIMEZONE = 'timezone'
     _DATE_FMT = "%Y-%m-%d %H:%M:%S.%f"
     _DATE_FMT_TZ = _DATE_FMT + " %z"
 
@@ -47,25 +57,28 @@
         if freq := raw[self._DICT_KEY_FREQUENCY]:
             obj.freq = freq
 
         return obj
 
 
 class PandasSeriesSerializer(Serializer[pd.Series]):
+    """Serializer for {py:class}`pandas.Series` objects."""
 
     def serialize(self, obj: pd.Series) -> Dict[AllowedSerializedDictKeys, Any]:
         if obj.ndim != 1:
             raise ValueError(f"The number of dimensions of a pandas series must be 1, but is {obj.ndim}.")
         return dict(name=obj.name, index=obj.index, data=obj.values)
 
     def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> pd.Series:
         return pd.Series(**raw)
 
 
 class PandasDataFrameSerializer(Serializer[pd.DataFrame]):
+    """Serializer for {py:class}`pandas.DataFrame` objects."""
+
     _DICT_KEY_INDEX = '__index__'
 
     def serialize(self, obj: pd.DataFrame) -> Dict[AllowedSerializedDictKeys, Any]:
         columns_dict = obj.to_dict(orient='list')
         for key in columns_dict:
             if isinstance(obj[key].values, PintArray):
                 columns_dict[key] = obj[key].values
```

## optool/serialization/pint_objects.py

```diff
@@ -1,39 +1,46 @@
+"""
+Serialization of [Pint](https://pypi.org/project/Pint/) objects.
+
+This module offers custom serialization of {py:class}`~optool.uom.Quantity`, {py:class}`~optool.uom.Unit`, and
+{py:class}`~pint_pandas.PintArray` objects.
+"""
+
 from __future__ import annotations
 
 from typing import Any, Dict
 
 from pint.util import to_units_container
 from pint_pandas import PintArray
 
 from optool.serialization import AllowedSerializedDictKeys, Serializer
 from optool.uom import UNITS, Quantity, Unit
 
 
 class PintQuantitySerializer(Serializer[Quantity]):
-    """Serializer for :py:class:`optool.uom.Quantity` objects."""
+    """Serializer for {py:class}`optool.uom.Quantity` objects."""
 
     def serialize(self, obj: Quantity) -> Dict[AllowedSerializedDictKeys, Any]:
         return {'mag': obj.m, 'unit': obj.u}
 
     def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> Quantity:
         return Quantity(raw['mag'], raw['unit'])
 
 
 class PintUnitSerializer(Serializer[Unit]):
-    """Serializer for :py:class:`pint.Unit` objects."""
+    """Serializer for {py:class}`optool.uom.Unit` objects."""
 
     def serialize(self, obj: Unit) -> Dict[AllowedSerializedDictKeys, Any]:
         return dict(to_units_container(obj))  # type: ignore
 
     def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> Unit:
         return UNITS.Unit(UNITS.UnitsContainer(raw))
 
 
 class PintArraySerializer(Serializer[PintArray]):
-    """Serializer for :py:class:`pint_pandas.PintArray` objects."""
+    """Serializer for {py:class}`pint_pandas.PintArray` objects."""
 
     def serialize(self, obj: PintArray) -> Dict[AllowedSerializedDictKeys, Any]:
         return {'mag': obj.quantity.m, 'unit': obj.quantity.u}
 
     def deserialize(self, raw: Dict[AllowedSerializedDictKeys, Any]) -> PintArray:
         return PintArray(raw['mag'], raw['unit'])
```

## Comparing `optool-0.5.1.dist-info/LICENSE.txt` & `optool-0.6.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `optool-0.5.1.dist-info/RECORD` & `optool-0.6.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-optool/VERSION,sha256=q6lRYmyGkM5JPLyPAYIFu0aAA_YfvwD9PTxxzrq8AXc,6
-optool/__init__.py,sha256=ZjTQIUA8BfuiU67VD_ASAQF2MQWZovtwMuc-TraaNX4,6540
-optool/conversions.py,sha256=eUbpDZ4Ep1MB-o-QdMYHUiiz0QlzCaRp21wzVWfUvFU,949
-optool/languages.py,sha256=TLG-Ez5dqiyIy4JlHxy39WV9FgXPDcjFrghufIlZEsY,1094
-optool/logging.py,sha256=GidpPg6pZwcjsUOKMrC1YT-K6YZmZLY3RAB4wSgF4s0,4095
-optool/math.py,sha256=GEBNu0CJkb157VsOrrjEZizp25aDiWAH0AF536rYKC0,7753
-optool/orthography.py,sha256=KjLnK8KWMW6_d29Nuu8aAmK9NdgH-vscP5UlFMyWyB0,1148
-optool/parallel.py,sha256=W1fXrDSSTL9-SJXO9yN1zYltyahsyObHBV4cvHRNUqo,1814
+optool/VERSION,sha256=l6XW5UCmEg0Jw53bZn4Ojiusf8wv_vgTuC4I_WA2W84,6
+optool/__init__.py,sha256=su-p6dH_StDid9r8jiyHVOupEzhXhO0pD3VayuKZte8,1929
+optool/conversions.py,sha256=kryQ3PIEgIBqCBl3GgPrrcWMgVBR_3o-JOZYXlMsncQ,2191
+optool/core.py,sha256=OkLG3hOhEUBJZQ5bYX5sh0j3WOQ_8g3OyD8-5M1YXkA,6459
+optool/languages.py,sha256=h0IhjTnZ7L7bPFFbhVmaQN960phrXbOt5eon0IB10oY,2670
+optool/logging.py,sha256=FD6yqPAT2XdAK-ETl7w5aQweoVHBhaHrN77HcwPKcWY,9554
+optool/math.py,sha256=8hHg6BXNp6bbFeE5gIRb-__ZVTRmWQXUdhK2maZU-58,10277
+optool/parallel.py,sha256=S8PsUvSDPvUsrh8D8OI52HaRM00xDusB0B2cdBbfgWU,4180
 optool/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-optool/uom.py,sha256=nhaPBFGGj3UOzW6wkWdeP3xCNlTJ5FJ-WOB8f60IbIE,32913
-optool/util.py,sha256=uOS7r895g9Lvj9iFMztTvIDh9SX9SO8c7b8g66FZ-E8,813
-optool/fields/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-optool/fields/callables.py,sha256=JSVq95pzcbG6-1_Zdbr2IJetnyhjwfanCcnk58_KYp4,4553
-optool/fields/containers.py,sha256=9pvJ517cUlwFOMXwV4V1KE2sZFuX5xH2Mn6xDTr6Wbo,4345
-optool/fields/dataframe.py,sha256=nGv281iFWtkLW2O1hQvcDd-4-nKxS8BsW2lDXfg7jKo,2312
-optool/fields/misc.py,sha256=bR-CNJwiZiYVPQgsj1boGB06lwNvhBdZpRWZViDa8aY,811
-optool/fields/numeric.py,sha256=0jEzqb-zG8FLyW7QAZZcJX-TBlsNXKANVTaraVlA3D4,7681
-optool/fields/quantities.py,sha256=2Hc8FUOZmYA8sRFHHahXM5s-b-NOukikY8kySps0Ums,5788
-optool/fields/series.py,sha256=q8FM2TEdW2dgJxld2JZ26sYwPlh45340sHrwFovzOGM,6249
-optool/fields/symbolic.py,sha256=6hMx-mtxZQhvthSfuxzAuvc-oSVyleMcluOlaefUCQQ,2526
-optool/fields/util.py,sha256=00S-JYRQ7U34TmWpltowcCzrUPDKwKUwMV40bPLU0xY,8588
-optool/optimization/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-optool/optimization/constraints.py,sha256=aM7HGOvf_rL-iiLbThpXGozlGJa4A_iujCK-7PF8NlE,4100
-optool/optimization/helpers.py,sha256=-tS_EF6p-mYFFulj7O-S0ZUCk01Vg0v9GRZRMUQAsZ4,4505
-optool/optimization/ode.py,sha256=Le4fCE6FvEP0AmcMrvkRJ9f6gGuH1HWootowdSkVnck,8008
-optool/optimization/problem.py,sha256=LziHxq7TyNqfY124dKM29Lj5FIRmBcAy4PDzh8Xqb8A,21117
-optool/optimization/variables.py,sha256=d4bIraZARShhe0sL3Q_uRnsQ0WtjNeZO47mbXzY15Gg,7142
-optool/serialization/__init__.py,sha256=GX4MUMb1QKRC_Zo5_JsfQDWfLiBL0HM3Af6OmyRuFo8,3055
-optool/serialization/datetime_objects.py,sha256=YaCHFJLYhfbQJeJnchpo2eQPActARuMuYOGlNLW4tfs,1317
-optool/serialization/numpy_objects.py,sha256=NKmdoMjnGsxZZb3-_xZ8vxllbDdg7eI-mWIipW0Hwp4,676
-optool/serialization/pandas_objects.py,sha256=layoYfhGtMLJeHZL4nwaeR2kuBqFK-OP1JBFlDPe4_Y,2880
-optool/serialization/pint_objects.py,sha256=ox5JZPKIMftUdTBAUQg3cJcFD0j-PMzxo89J5FYxTH4,1407
-optool-0.5.1.dist-info/LICENSE.txt,sha256=hrghB2ojre3BR1nxeXTWOsbhc7CzFIRaC2r1Ez1gV10,1081
-optool-0.5.1.dist-info/METADATA,sha256=pfLJMOVXYkvIqMOkb9vuuelWk83a4OGP-Zde2oNCQUU,7568
-optool-0.5.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-optool-0.5.1.dist-info/top_level.txt,sha256=rBfjZiBTokmEUSgWAvNw23rTh-MvuILbz6o2LeWprko,7
-optool-0.5.1.dist-info/RECORD,,
+optool/uom.py,sha256=CcqNS3Yq61SwYtGMKxwmeYP_cJv1ZLcLIRQR03tjLxc,32727
+optool/util.py,sha256=bgNco_mgCjfnC3ppoa1A61_YUC5yH6lvcXRC6XBLdkU,1618
+optool/fields/__init__.py,sha256=qBtAfZKOr4VE14R-ZMhHcek4uuEH7gWQ1Obtoa3gLxo,628
+optool/fields/callables.py,sha256=oaHE36UMU3dKz-1gudA76PKkweXBzMpODFxg3A-X9oQ,6206
+optool/fields/containers.py,sha256=c4JPgWnDy1Vgpxv4dCh8-tw-PbPgVKbcrDjmHNSttUA,5786
+optool/fields/dataframe.py,sha256=4VR89l-TT2HAs6n5ZOZeqFHFipPth9ptMF1Z9X3m8Zw,3937
+optool/fields/misc.py,sha256=-cowM9ZF6YE-O0TP4TVZwmujuF2FoMMAj4IhvGMIlnY,2456
+optool/fields/numeric.py,sha256=d8IUg3gsJ3q1hbu1JMsjD2UrfCorc7G0WqFJkvgbohY,12294
+optool/fields/quantities.py,sha256=z3NTFvTJovJkYCTmuk_LSsCf3QD1DqquzWoMxzazAxw,9788
+optool/fields/series.py,sha256=yiELcUahjiPbtGJp0Ls4pAv61K5G6JotAxNbo8iXxKY,9218
+optool/fields/symbolic.py,sha256=1f5faPx29FVQrN9ooM078d9d9Igm8h-zVzjLqqt6zQk,3724
+optool/fields/util.py,sha256=ORjvwyUmarfqoCac6rFpNlL_61AMbtZFJsotU6BjXCs,11127
+optool/optimization/__init__.py,sha256=EtHlu2Q0SOjHT_kjgZ9n70FvfOwR6f63-pIbqzpR8Jg,347
+optool/optimization/constraints.py,sha256=tJeiSBQWJEwcD5Xcmc_XVcNqDD6Yk9SGCStt6BETmCk,5772
+optool/optimization/helpers.py,sha256=KTWWUSjA2r8iVDc_7b67TKDVMgwUCDbjtBd7DXhIA8Q,5882
+optool/optimization/ode.py,sha256=OA2EK8RXmQ-69mOQy6Itg0nbAvI3mi05YDKlN5aDfKQ,7125
+optool/optimization/problem.py,sha256=K8pJi5wjdmR6UEzSe8ATstTKb0ct_SJw4lCPSZfsZ7c,25679
+optool/optimization/variables.py,sha256=6THq0l9nEbvMJXfqFZ30KWoZtLgQapaeIFZdzC7lHJk,8812
+optool/serialization/__init__.py,sha256=yUvmOZGj-_pTqyBzoQhIRp1PeN29hee-wYePW1r04x0,6382
+optool/serialization/datetime_objects.py,sha256=60bxDqmJUoGFROPMirQbmFetqaL06nuAwGNsdvsW4_Q,1610
+optool/serialization/numpy_objects.py,sha256=e9P9Dg5Nkv_UDLwjG1KKjxNtPVE9ruzcQ6XZXozigw4,832
+optool/serialization/pandas_objects.py,sha256=5k3LGti3qtf3LdkZoMyRgFhARRqTJeWEz5mq7XUQ6AU,3396
+optool/serialization/pint_objects.py,sha256=gSKF3GwnUqSUVt8JK4VtxKAbH7DLepei_jCNXrpirek,1643
+optool-0.6.0.dist-info/LICENSE.txt,sha256=hrghB2ojre3BR1nxeXTWOsbhc7CzFIRaC2r1Ez1gV10,1081
+optool-0.6.0.dist-info/METADATA,sha256=_yRax25b6vNl9sWZ48V3eJAUBCpaE6QWEmfHPD_o0VU,4190
+optool-0.6.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+optool-0.6.0.dist-info/top_level.txt,sha256=rBfjZiBTokmEUSgWAvNw23rTh-MvuILbz6o2LeWprko,7
+optool-0.6.0.dist-info/RECORD,,
```

