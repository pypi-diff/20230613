# Comparing `tmp/sparclclient-1.2.0b4.dev2-py3-none-any.whl.zip` & `tmp/sparclclient-1.2.1.dev0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,20 @@
-Zip file size: 31814 bytes, number of entries: 20
--rw-rw-r--  2.0 unx     8039 b- defN 23-Apr-30 18:22 sparcl/Results.py
--rw-rw-r--  2.0 unx     1035 b- defN 23-May-19 17:32 sparcl/__init__.py
--rw-rw-r--  2.0 unx      798 b- defN 23-Mar-15 14:12 sparcl/big_retrieve.py
--rw-rw-r--  2.0 unx    31709 b- defN 23-May-19 14:22 sparcl/client.py
--rw-rw-r--  2.0 unx      953 b- defN 23-Feb-08 18:38 sparcl/conf.py
--rw-rw-r--  2.0 unx      887 b- defN 23-Mar-15 15:44 sparcl/dls_376.py
--rw-rw-r--  2.0 unx     3813 b- defN 23-Apr-20 21:46 sparcl/exceptions.py
--rw-rw-r--  2.0 unx     5002 b- defN 23-Mar-26 21:24 sparcl/fields.py
--rw-rw-r--  2.0 unx     9254 b- defN 23-May-12 19:28 sparcl/gather_2d.py
--rw-rw-r--  2.0 unx     1329 b- defN 23-Apr-30 15:12 sparcl/resample_spectra.py
--rw-rw-r--  2.0 unx    13112 b- defN 23-Feb-08 18:38 sparcl/type_conversion.py
--rw-rw-r--  2.0 unx     1867 b- defN 23-Feb-28 20:09 sparcl/unsupported.py
--rw-rw-r--  2.0 unx     4682 b- defN 23-Mar-26 21:03 sparcl/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Feb-08 18:38 sparcl/benchmarks/__init__.py
--rw-rw-r--  2.0 unx     9667 b- defN 23-Feb-08 18:38 sparcl/benchmarks/benchmarks.py
--rw-rw-r--  2.0 unx     1576 b- defN 23-May-19 17:33 sparclclient-1.2.0b4.dev2.dist-info/LICENSE
--rw-rw-r--  2.0 unx      872 b- defN 23-May-19 17:33 sparclclient-1.2.0b4.dev2.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-19 17:33 sparclclient-1.2.0b4.dev2.dist-info/WHEEL
--rw-rw-r--  2.0 unx        7 b- defN 23-May-19 17:33 sparclclient-1.2.0b4.dev2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1601 b- defN 23-May-19 17:33 sparclclient-1.2.0b4.dev2.dist-info/RECORD
-20 files, 96295 bytes uncompressed, 29222 bytes compressed:  69.7%
+Zip file size: 30756 bytes, number of entries: 18
+-rw-rw-r--  2.0 unx     8107 b- defN 23-Jun-12 19:59 sparcl/Results.py
+-rw-rw-r--  2.0 unx     1108 b- defN 23-Jun-12 22:56 sparcl/__init__.py
+-rw-rw-r--  2.0 unx    31551 b- defN 23-Jun-12 20:16 sparcl/client.py
+-rw-rw-r--  2.0 unx      928 b- defN 23-Jun-12 19:59 sparcl/conf.py
+-rw-rw-r--  2.0 unx     3801 b- defN 23-Jun-12 19:59 sparcl/exceptions.py
+-rw-rw-r--  2.0 unx     5126 b- defN 23-Jun-12 19:59 sparcl/fields.py
+-rw-rw-r--  2.0 unx     8685 b- defN 23-Jun-12 20:17 sparcl/gather_2d.py
+-rw-rw-r--  2.0 unx     1282 b- defN 23-Jun-12 19:59 sparcl/resample_spectra.py
+-rw-rw-r--  2.0 unx    12965 b- defN 23-Jun-12 19:59 sparcl/type_conversion.py
+-rw-rw-r--  2.0 unx     1848 b- defN 23-Jun-12 19:59 sparcl/unsupported.py
+-rw-rw-r--  2.0 unx     4685 b- defN 23-Jun-12 19:59 sparcl/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 22-Aug-23 21:07 sparcl/benchmarks/__init__.py
+-rw-rw-r--  2.0 unx     9424 b- defN 23-Jun-12 19:59 sparcl/benchmarks/benchmarks.py
+-rw-r--r--  2.0 unx     1576 b- defN 23-Jun-12 22:57 sparclclient-1.2.1.dev0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx      870 b- defN 23-Jun-12 22:57 sparclclient-1.2.1.dev0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-12 22:57 sparclclient-1.2.1.dev0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        7 b- defN 23-Jun-12 22:57 sparclclient-1.2.1.dev0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1440 b- defN 23-Jun-12 22:57 sparclclient-1.2.1.dev0.dist-info/RECORD
+18 files, 93495 bytes uncompressed, 28414 bytes compressed:  69.6%
```

## zipnote {}

```diff
@@ -1,25 +1,19 @@
 Filename: sparcl/Results.py
 Comment: 
 
 Filename: sparcl/__init__.py
 Comment: 
 
-Filename: sparcl/big_retrieve.py
-Comment: 
-
 Filename: sparcl/client.py
 Comment: 
 
 Filename: sparcl/conf.py
 Comment: 
 
-Filename: sparcl/dls_376.py
-Comment: 
-
 Filename: sparcl/exceptions.py
 Comment: 
 
 Filename: sparcl/fields.py
 Comment: 
 
 Filename: sparcl/gather_2d.py
@@ -39,23 +33,23 @@
 
 Filename: sparcl/benchmarks/__init__.py
 Comment: 
 
 Filename: sparcl/benchmarks/benchmarks.py
 Comment: 
 
-Filename: sparclclient-1.2.0b4.dev2.dist-info/LICENSE
+Filename: sparclclient-1.2.1.dev0.dist-info/LICENSE
 Comment: 
 
-Filename: sparclclient-1.2.0b4.dev2.dist-info/METADATA
+Filename: sparclclient-1.2.1.dev0.dist-info/METADATA
 Comment: 
 
-Filename: sparclclient-1.2.0b4.dev2.dist-info/WHEEL
+Filename: sparclclient-1.2.1.dev0.dist-info/WHEEL
 Comment: 
 
-Filename: sparclclient-1.2.0b4.dev2.dist-info/top_level.txt
+Filename: sparclclient-1.2.1.dev0.dist-info/top_level.txt
 Comment: 
 
-Filename: sparclclient-1.2.0b4.dev2.dist-info/RECORD
+Filename: sparclclient-1.2.1.dev0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sparcl/Results.py

```diff
@@ -1,21 +1,22 @@
 """Containers for results from SPARCL Server.
 These include results of client.retrieve() client.find().
 """
 
 from collections import UserList
+
 #!import copy
 from sparcl.utils import _AttrDict
-#from sparcl.gather_2d import bin_spectra_records
+
+# from sparcl.gather_2d import bin_spectra_records
 import sparcl.exceptions as ex
 from warnings import warn
 
 
 class Results(UserList):
-
     def __init__(self, dict_list, client=None):
         super().__init__(dict_list)
         self.hdr = dict_list[0]
         self.recs = dict_list[1:]
         self.client = client
         self.fields = client.fields
         self.to_science_fields()
@@ -48,50 +49,50 @@
 
     # Convert Internal field names to Science field names.
     # SIDE-EFFECT: modifies self.recs
     def to_science_fields(self):  # from_orig
         newrecs = list()
         for rec in self.recs:
             newrec = dict()
-            dr = rec['_dr']
+            dr = rec["_dr"]
             keep = True
             for orig in rec.keys():
-                if orig == '_dr':
+                if orig == "_dr":
                     # keep DR around unchanged. We need it to rename back
                     # to Internal Field Names later.
                     newrec[orig] = rec[orig]
                 else:
                     new = self.fields._science_name(orig, dr)
                     if new is None:
-                        keep = False # We don't have name mapping, toss rec
+                        keep = False  # We don't have name mapping, toss rec
                     newrec[new] = rec[orig]
             if keep:
                 newrecs.append(_AttrDict(newrec))
         self.recs = newrecs
 
     # Convert Science field names to Internal field names.
     def to_internal_fields(self):
         for rec in self.recs:
-            dr = rec.get('_dr')
+            dr = rec.get("_dr")
             for new in rec.keys():
-                if new == '_dr':
+                if new == "_dr":
                     # keep DR around unchanged. We need it to rename back
                     # to Internal Field Names later.
                     continue
                 new = self.fields._internal_name(new, dr)
                 rec[new] = rec.pop(new)
 
     def science_to_internal_fields(self):
         newrecs = list()
         for rec in self.recs:
             newrec = dict()
-            dr = rec['_dr']
+            dr = rec["_dr"]
             keep = True
             for sci_name in rec.keys():
-                if sci_name == '_dr':
+                if sci_name == "_dr":
                     # keep DR around unchanged. We need it to rename back
                     # to Internal Field Names later.
                     newrec[sci_name] = rec[sci_name]
                 else:
                     new = self.fields._internal_name(sci_name, dr)
                     if new is None:
                         keep = False
@@ -112,71 +113,79 @@
         Returns:
             reordered (:class:`~sparcl.Results.Retrieved`): Contains header and
                                                             reordered records.
             # none_idx (:obj:`list`): List of indices where record is None.
 
         """
         if len(ids_og) <= 0:
-            msg = (f'The list of IDs passed to the reorder method '
-                   f'does not contain any sparcl_ids or specIDs.')
+            msg = (
+                f"The list of IDs passed to the reorder method "
+                f"does not contain any sparcl_ids or specIDs."
+            )
             raise ex.NoIDs(msg)
         elif len(self.recs) <= 0:
-            msg = (f'The retrieved or found results did not '
-                   f'contain any records.')
+            msg = (
+                "The retrieved or found results did not "
+                "contain any records."
+            )
             raise ex.NoRecords(msg)
         else:
             # Transform science fields to internal fields
             new_recs = self.science_to_internal_fields()
             # Get the ids or specids from retrieved records
             if type(ids_og[0]) == str:
-                ids_re = [f['id'] for f in new_recs]
+                ids_re = [f["id"] for f in new_recs]
             elif type(ids_og[0]) == int:
-                ids_re = [f['specid'] for f in new_recs]
+                ids_re = [f["specid"] for f in new_recs]
             # Enumerate the original ids
             dict_og = {x: i for i, x in enumerate(ids_og)}
             # Enumerate the retrieved ids
             dict_re = {x: i for i, x in enumerate(ids_re)}
             # Get the indices of the original ids. Set to None if not found
             idx = [dict_re.get(key, None) for key in dict_og.keys()]
             # Get the indices of None values
             none_idx = [i for i, v in enumerate(idx) if v is None]
             # Reorder the retrieved records
             reordered = [self.recs[i] for i in idx if i is not None]
             # Insert dummy record(s) if applicable
             dummy_record = "{'id': None, 'specid': None, '_dr': 'SDSS-DR16'}"
             for i in none_idx:
-                reordered.insert(i, {'id': None, 'specid': None,
-                                     '_dr': 'SDSS-DR16'})
+                reordered.insert(
+                    i, {"id": None, "specid": None, "_dr": "SDSS-DR16"}
+                )
             reordered.insert(0, self.hdr)
             meta = reordered[0]
             if len(none_idx) > 0:
-                msg = (f'{len(none_idx)} sparcl_ids or specIDs were '
-                       f'not found in '
-                       f'the database. Use "client.missing()" '
-                       f'to get a list of the unavailable IDs. '
-                       f'To maintain correct reordering, a dummy '
-                       f'record has been placed at the indices '
-                       f'where no record was found. Those '
-                       f'indices are: {none_idx}. The dummy '
-                       f'record will appear as follows: '
-                       f'{dummy_record}. ')
-                meta['status'].update({'warnings': [msg]})
+                msg = (
+                    f"{len(none_idx)} sparcl_ids or specIDs were "
+                    f"not found in "
+                    f'the database. Use "client.missing()" '
+                    f"to get a list of the unavailable IDs. "
+                    f"To maintain correct reordering, a dummy "
+                    f"record has been placed at the indices "
+                    f"where no record was found. Those "
+                    f"indices are: {none_idx}. The dummy "
+                    f"record will appear as follows: "
+                    f"{dummy_record}. "
+                )
+                meta["status"].update({"warnings": [msg]})
                 warn(msg, stacklevel=2)
         return Results(reordered, client=self.client)
 
 
 # For results of retrieve()
 class Retrieved(Results):
     """Holds spectra records (and header)."""
 
     def __init__(self, dict_list, client=None):
         super().__init__(dict_list, client=client)
 
     def __repr__(self):
-        return f'Retrieved Results: {len(self.recs)} records'
+        return f"Retrieved Results: {len(self.recs)} records"
+
 
 #!    def bin_spectra(self):
 #!        """Align flux from all records by common wavelength bin.
 #!
 #!        A value of nan is used where a record does not contain a flux
 #!        value for a specific bin.
 #!
@@ -202,16 +211,16 @@
 class Found(Results):
     """Holds metadata records (and header)."""
 
     def __init__(self, dict_list, client=None):
         super().__init__(dict_list, client=client)
 
     def __repr__(self):
-        return f'Find Results: {len(self.recs)} records'
+        return f"Find Results: {len(self.recs)} records"
 
     @property
     def ids(self):
         """List of unique identifiers of matched records."""
         dr = list(self.fields.all_drs)[0]
-        idfld = self.fields._science_name('id', dr)
+        idfld = self.fields._science_name("id", dr)
 
         return [d.get(idfld) for d in self.recs]
```

## sparcl/__init__.py

```diff
@@ -10,25 +10,27 @@
 #
 # '0.3.0-alpha3.23' is an invalid value for Version.
 #  Error: Start and end with a letter or numeral containing only ASCII
 #  numeric and '.', '_' and '-'.
 #
 # https://semver.org/ yields possible versions that violate PEP-0440
 
-#__version__ = '0.3.21'
-#__version__ = '0.1a3.dev22'
-#__version__ = '0.3.0-alpha3.23'
-#__version__ = '0.3.22'
+# __version__ = '0.3.21'
+# __version__ = '0.1a3.dev22'
+# __version__ = '0.3.0-alpha3.23'
+# __version__ = '0.3.22'
 
 # must mach: [N!]N(.N)*[{a|b|rc}N][.postN][.devN]
 # Example of a correct version string: '0.4.0a3.dev35'
-#__version__ = '0.4.0b1.dev8'
-#__version__ = '0.4.0b1.dev10'
-#__version__ = '1.0.0'
-#__version__ = '1.0.0b1.dev7'
-#__version__ = '1.0.0b1.dev8'
-#__version__ = '1.0.0b1.dev9'
-#__version__ = '1.0.1b2.dev1'
-#__version__ = '1.1rc1'
-#__version__ = '1.1rc2'
-#__version__ = '1.1'
-__version__ = '1.2.0b4.dev2'
+# __version__ = '0.4.0b1.dev8'
+# __version__ = '0.4.0b1.dev10'
+# __version__ = '1.0.0'
+# __version__ = '1.0.0b1.dev7'
+# __version__ = '1.0.0b1.dev8'
+# __version__ = '1.0.0b1.dev9'
+# __version__ = '1.0.1b2.dev1'
+# __version__ = '1.1rc1'
+# __version__ = '1.1rc2'
+# __version__ = '1.1'
+# __version__ = '1.2.0b4'
+# __version__ = '1.2.0'  # Release
+__version__ = '1.2.1.dev0'
```

## sparcl/client.py

```diff
@@ -10,38 +10,44 @@
 #   ## Returns NOTHING if everything works, else lists errors.
 
 ############################################
 # Python Standard Library
 from urllib.parse import urlencode, urlparse
 from warnings import warn
 import pickle
+
 #!from pathlib import Path
 import tempfile
+
 ############################################
 # External Packages
 import requests
+
 ############################################
 # Local Packages
 from sparcl.fields import Fields
 import sparcl.utils as ut
 import sparcl.exceptions as ex
+
 #!import sparcl.type_conversion as tc
 from sparcl import __version__
 from sparcl.Results import Found, Retrieved
 
 
-MAX_CONNECT_TIMEOUT = 3.1    # seconds
-MAX_READ_TIMEOUT = 150 * 60   # seconds
+MAX_CONNECT_TIMEOUT = 3.1  # seconds
+MAX_READ_TIMEOUT = 150 * 60  # seconds
 MAX_NUM_RECORDS_RETRIEVED = int(24e3)  # Minimum Hard Limit = 25,000
 #!MAX_NUM_RECORDS_RETRIEVED = int(5e4) #@@@ Reduce !!!
 
 
-_pat_hosts = ['sparc1.datalab.noirlab.edu',
-              'sparc2.datalab.noirlab.edu',
-              'astrosparcl.datalab.noirlab.edu']
+_pat_hosts = [
+    "sparc1.datalab.noirlab.edu",
+    "sparc2.datalab.noirlab.edu",
+    "astrosparcl.datalab.noirlab.edu",
+]
 
 # Upload to PyPi:
 #   python3 -m build --wheel
 #   twine upload dist/*
 
 # Use Google Style Python Docstrings so autogen of Sphinx doc works:
 #  https://www.sphinx-doc.org/en/master/usage/extensions/example_google.html
@@ -67,25 +73,25 @@
 #
 # data0 = client.retrieve(specids,columns='flux')
 # f'{len(str(data0)):,}'   # -> '3,435,687'
 #
 # dataall = client.retrieve(specids,columns=allc)
 # f'{len(str(dataall)):,}' # -> '27,470,052'
 
-_PROD  = 'https://astrosparcl.datalab.noirlab.edu'  # noqa: E221
-_STAGE = 'https://sparclstage.datalab.noirlab.edu'  # noqa: E221
-_PAT   = 'https://sparc1.datalab.noirlab.edu'       # noqa: E221
-_DEV   = 'http://localhost:8050'                    # noqa: E221
+_PROD = "https://astrosparcl.datalab.noirlab.edu"  # noqa: E221
+_STAGE = "https://sparclstage.datalab.noirlab.edu"  # noqa: E221
+_PAT = "https://sparc1.datalab.noirlab.edu"  # noqa: E221
+_DEV = "http://localhost:8050"  # noqa: E221
 
 
-#client_version = pkg_resources.require("sparclclient")[0].version
+# client_version = pkg_resources.require("sparclclient")[0].version
 client_version = __version__
 
-DEFAULT = 'DEFAULT'
-ALL = 'ALL'
+DEFAULT = "DEFAULT"
+ALL = "ALL"
 RESERVED = set([DEFAULT, ALL])
 
 
 ###########################
 # ## Convenience Functions
 
 # Following can be done with:
@@ -95,15 +101,16 @@
 #!    """Return intersection of all LISTS."""
 #!    return set(lists[0]).intersection(*lists[1:])
 
 
 ###########################
 # ## The Client class
 
-class SparclClient():  # was SparclApi()
+
+class SparclClient:  # was SparclApi()
     """Provides interface to SPARCL Server.
     When using this to report a bug, set verbose to True. Also print
     your instance of this.  The results will include important info
     about the Client and Server that is usefule to Developers.
 
     Args:
         url (:obj:`str`, optional): Base URL of SPARC Server. Defaults
@@ -128,79 +135,85 @@
             Server against the one expected by the Client. Throws an
             error if the Client is a major version or more behind.
 
     """
 
     KNOWN_GOOD_API_VERSION = 9.0  # @@@ Change this on Server version increment
 
-    def __init__(self, *,
-                 url=_PROD,
-                 verbose=False,
-                 connect_timeout=1.1,    # seconds
-                 read_timeout=90 * 60):  # seconds
-        """Create client instance.
-        """
+    def __init__(
+        self,
+        *,
+        url=_PROD,
+        verbose=False,
+        connect_timeout=1.1,  # seconds
+        read_timeout=90 * 60,
+    ):  # seconds
+        """Create client instance."""
         self.rooturl = url.rstrip("/")
-        self.apiurl = f'{self.rooturl}/sparc'
+        self.apiurl = f"{self.rooturl}/sparc"
         self.apiversion = None
         self.verbose = verbose
         #!self.internal_names = internal_names
-        self.c_timeout = min(MAX_CONNECT_TIMEOUT,
-                             float(connect_timeout))  # seconds
-        self.r_timeout = min(MAX_READ_TIMEOUT,  # seconds
-                             float(read_timeout))
+        self.c_timeout = min(
+            MAX_CONNECT_TIMEOUT, float(connect_timeout)
+        )  # seconds
+        self.r_timeout = min(MAX_READ_TIMEOUT, float(read_timeout))  # seconds
 
         # require response within this num seconds
         # https://2.python-requests.org/en/master/user/advanced/#timeouts
         # (connect timeout, read timeout) in seconds
         self.timeout = (self.c_timeout, self.r_timeout)
-        #@@@ read timeout should be a function of the POST payload size
+        # @@@ read timeout should be a function of the POST payload size
 
         if verbose:
-            print(f'apiurl={self.apiurl}')
+            print(f"apiurl={self.apiurl}")
 
         # Get API Version
         try:
-            endpoint = f'{self.apiurl}/version/'
+            endpoint = f"{self.apiurl}/version/"
             verstr = requests.get(endpoint, timeout=self.timeout).content
         except requests.ConnectionError as err:
-            msg = f'Could not connect to {endpoint}. {str(err)}'
+            msg = f"Could not connect to {endpoint}. {str(err)}"
             if urlparse(url).hostname in _pat_hosts:
-                msg += 'Did you enable VPN?'
+                msg += "Did you enable VPN?"
             raise ex.ServerConnectionError(msg) from None  # disable chaining
 
         self.apiversion = float(verstr)
 
         expected_api = SparclClient.KNOWN_GOOD_API_VERSION
         if (int(self.apiversion) - int(expected_api)) >= 1:
-            msg = (f'The SPARCL Client you are running expects an older '
-                   f'version of the API services. '
-                   f'Please upgrade to the latest "sparclclient".  '
-                   f'The Client you are using expected version '
-                   f'{SparclClient.KNOWN_GOOD_API_VERSION} but got '
-                   f'{self.apiversion} from the SPARCL Server '
-                   f'at {self.apiurl}.')
+            msg = (
+                f"The SPARCL Client you are running expects an older "
+                f"version of the API services. "
+                f'Please upgrade to the latest "sparclclient".  '
+                f"The Client you are using expected version "
+                f"{SparclClient.KNOWN_GOOD_API_VERSION} but got "
+                f"{self.apiversion} from the SPARCL Server "
+                f"at {self.apiurl}."
+            )
             raise Exception(msg)
-        #self.session = requests.Session() #@@@
+        # self.session = requests.Session() #@@@
 
         self.clientversion = client_version
         self.fields = Fields(self.apiurl)
 
         ###
         ####################################################
         # END __init__()
 
     def __repr__(self):
         #!f' internal_names={self.internal_names},'
-        return(f'(sparclclient:{self.clientversion},'
-               f' api:{self.apiversion},'
-               f' {self.apiurl},'
-               f' verbose={self.verbose},'
-               f' connect_timeout={self.c_timeout},'
-               f' read_timeout={self.r_timeout})')
+        return (
+            f"(sparclclient:{self.clientversion},"
+            f" api:{self.apiversion},"
+            f" {self.apiurl},"
+            f" verbose={self.verbose},"
+            f" connect_timeout={self.c_timeout},"
+            f" read_timeout={self.r_timeout})"
+        )
 
     @property
     def all_datasets(self):
         return self.fields.all_drs
 
     def get_default_fields(self, *, dataset_list=None):
         """Get fields tagged as 'default' that are in DATASET_LIST.
@@ -215,22 +228,23 @@
 
         Returns:
             List of fields tagged as 'default' from DATASET_LIST.
 
         Example:
             >>> client = SparclClient()
             >>> client.get_default_fields()
-            ['flux', 'id', 'wavelength']
+            ['dec', 'flux', 'ra', 'sparcl_id', 'specid', 'wavelength']
         """
 
         if dataset_list is None:
             dataset_list = self.fields.all_drs
 
-        assert isinstance(dataset_list, (list, set)), (
-            f'DATASET_LIST must be a list. Found {dataset_list}')
+        assert isinstance(
+            dataset_list, (list, set)
+        ), f"DATASET_LIST must be a list. Found {dataset_list}"
 
         common = set(self.fields.common(dataset_list))
         union = self.fields.default_retrieve_fields(dataset_list=dataset_list)
         return sorted(common.intersection(union))
 
     def get_all_fields(self, *, dataset_list=None):
         """Get fields tagged as 'all' that are in DATASET_LIST.
@@ -245,39 +259,42 @@
 
         Returns:
             List of fields tagged as 'all' from DATASET_LIST.
 
         Example:
             >>> client = SparclClient()
             >>> client.get_all_fields()
-            ['data_release', 'datasetgroup', 'dateobs', 'dateobs_center', 'dec', 'exptime', 'fiberid', 'flux', 'id', 'instrument', 'ivar', 'mask', 'mjd', 'model', 'plate', 'ra', 'redshift', 'redshift_err', 'redshift_warning', 'run1d', 'run2d', 'site', 'sky', 'specid', 'specobjid', 'specprimary', 'spectype', 'targetid', 'telescope', 'wave_sigma', 'wavelength', 'wavemax', 'wavemin']
-    """  # noqa: E501
+            ['data_release', 'datasetgroup', 'dateobs', 'dateobs_center', 'dec', 'exptime', 'flux', 'instrument', 'ivar', 'mask', 'model', 'ra', 'redshift', 'redshift_err', 'redshift_warning', 'site', 'sparcl_id', 'specid', 'specprimary', 'spectype', 'survey', 'targetid', 'telescope', 'wave_sigma', 'wavelength', 'wavemax', 'wavemin']
+        """  # noqa: E501
 
         common = set(self.fields.common(dataset_list))
         union = self.fields.all_retrieve_fields(dataset_list=dataset_list)
         return sorted(common.intersection(union))
 
     def _validate_science_fields(self, science_fields, *, dataset_list=None):
         """Raise exception if any field name in SCIENCE_FIELDS is
         not registered in at least one of DATASET_LIST."""
         if dataset_list is None:
             dataset_list = self.fields.all_drs
         all = set(self.fields.common(dataset_list=dataset_list))
         unk = set(science_fields) - all
         if len(unk) > 0:
             drs = self.fields.all_drs if dataset_list is None else dataset_list
-            msg = (f'Unknown fields \"{",".join(unk)}\" given '
-                   f'for DataSets {",".join(drs)}. '
-                   f'Allowed fields are: {",".join(all)}. ')
+            msg = (
+                f'Unknown fields "{",".join(unk)}" given '
+                f'for DataSets {",".join(drs)}. '
+                f'Allowed fields are: {",".join(all)}. '
+            )
             raise ex.UnknownField(msg)
         return True
 
     def _common_internal(self, *, science_fields=None, dataset_list=None):
-        self._validate_science_fields(science_fields,
-                                      dataset_list=dataset_list)
+        self._validate_science_fields(
+            science_fields, dataset_list=dataset_list
+        )
 
         if dataset_list is None:
             dataset_list = self.fields.all_drs
         if science_fields is None:
             science_fields = self.fields.all_fields
         common = self.fields.common_internal(dataset_list)
         flds = set()
@@ -300,15 +317,15 @@
 
         Returns:
             Set of fields available from data sets in DATASET_LIST.
 
         Example:
             >>> client = SparclClient()
             >>> sorted(client.get_available_fields())
-            ['data_release', 'datasetgroup', 'dateobs', 'dateobs_center', 'dec', 'dirpath', 'exptime', 'extra_files', 'fiberid', 'filename', 'filesize', 'flux', 'id', 'instrument', 'ivar', 'mask', 'mjd', 'model', 'plate', 'ra', 'redshift', 'redshift_err', 'redshift_warning', 'run1d', 'run2d', 'site', 'sky', 'specid', 'specobjid', 'specprimary', 'spectype', 'targetid', 'telescope', 'updated', 'wave_sigma', 'wavelength', 'wavemax', 'wavemin']
+            ['data_release', 'datasetgroup', 'dateobs', 'dateobs_center', 'dec', 'dirpath', 'exptime', 'extra_files', 'filename', 'filesize', 'flux', 'instrument', 'ivar', 'mask', 'model', 'ra', 'redshift', 'redshift_err', 'redshift_warning', 'site', 'sparcl_id', 'specid', 'specprimary', 'spectype', 'survey', 'targetid', 'telescope', 'updated', 'wave_sigma', 'wavelength', 'wavemax', 'wavemin']
         """  # noqa: E501
 
         drs = self.fields.all_drs if dataset_list is None else dataset_list
         every = [set(self.fields.n2o[dr]) for dr in drs]
         return set.intersection(*every)
 
     @property
@@ -319,30 +336,34 @@
 
         Returns:
             API version (:obj:`float`).
 
         Example:
             >>> client = SparclClient()
             >>> client.version
-            8.0
+            9.0
         """
 
         if self.apiversion is None:
-            response = requests.get(f'{self.apiurl}/version',
-                                    timeout=self.timeout,
-                                    cache=True)
+            response = requests.get(
+                f"{self.apiurl}/version", timeout=self.timeout, cache=True
+            )
             self.apiversion = float(response.content)
         return self.apiversion
 
-    def find(self, outfields=None, *,
-             constraints={},  # dict(fname) = [op, param, ...]
-             #dataset_list=None,
-             limit=500,
-             sort=None,
-             verbose=None):
+    def find(
+        self,
+        outfields=None,
+        *,
+        constraints={},  # dict(fname) = [op, param, ...]
+        # dataset_list=None,
+        limit=500,
+        sort=None,
+        verbose=None,
+    ):
         """Find records in the SPARC database.
 
         Args:
             outfields (:obj:`list`, optional): List of fields to return.
                 Only CORE fields may be passed to this parameter.
                 Defaults to None, which will return only the sparcl_id
                 and _dr fields.
@@ -365,71 +386,79 @@
                 statement. Defaults to False.
 
         Returns:
             :class:`~sparcl.Results.Found`: Contains header and records.
 
         Example:
             >>> client = SparclClient()
-            >>> outs = ['id', 'ra', 'dec']
+            >>> outs = ['sparcl_id', 'ra', 'dec']
             >>> cons = {'spectype': ['GALAXY'], 'redshift': [0.5, 0.9]}
             >>> found = client.find(outfields=outs, constraints=cons)
             >>> sorted(list(found.records[0].keys()))
-            ['_dr', 'dec', 'id', 'ra']
+            ['_dr', 'dec', 'ra', 'sparcl_id']
         """
         # dataset_list (:obj:`list`, optional): List of data sets from
         #     which to find records. Defaults to None, which
         #     will find records in all data sets hosted on the SPARC
         #     database.
 
         verbose = self.verbose if verbose is None else verbose
 
         # Let "outfields" default to ['id']; but fld may have been renamed
         if outfields is None:
             dslist = list(self.fields.all_datasets)
-            idfld = self.fields._science_name('id', dslist[0])
+            idfld = self.fields._science_name("id", dslist[0])
             if idfld not in self.fields.common():
-                msg = (f'The "id" field ("{idfld}" is not common to all '
-                       f'current Data Sets ({(", ").join(dslist)}) '
-                       f'so we cannot use the default outfields="{idfld}".'
-                       )
+                msg = (
+                    f'The "id" field ("{idfld}" is not common to all '
+                    f'current Data Sets ({(", ").join(dslist)}) '
+                    f'so we cannot use the default outfields="{idfld}".'
+                )
                 raise ex.NoCommonIdField(msg)
             outfields = [idfld]
         dataset_list = self.fields.all_drs
-        #! self._validate_science_fields(outfields, dataset_list=dataset_list) # DLS-401
+        #! self._validate_science_fields(outfields,
+        #!                               dataset_list=dataset_list) # DLS-401
         dr = list(dataset_list)[0]
         if len(constraints) > 0:
-            self._validate_science_fields(constraints.keys(),
-                                          dataset_list=dataset_list)
-            constraints = {self.fields._internal_name(k, dr): v
-                           for k, v in constraints.items()}
-        uparams = dict(limit=limit,)
+            self._validate_science_fields(
+                constraints.keys(), dataset_list=dataset_list
+            )
+            constraints = {
+                self.fields._internal_name(k, dr): v
+                for k, v in constraints.items()
+            }
+        uparams = dict(
+            limit=limit,
+        )
         if sort is not None:
-            uparams['sort'] = sort
+            uparams["sort"] = sort
         qstr = urlencode(uparams)
-        url = f'{self.apiurl}/find/?{qstr}'
+        url = f"{self.apiurl}/find/?{qstr}"
 
         outfields = [self.fields._internal_name(s, dr) for s in outfields]
         search = [[k] + v for k, v in constraints.items()]
         sspec = dict(outfields=outfields, search=search)
         if verbose:
-            print(f'url={url} sspec={sspec}')
+            print(f"url={url} sspec={sspec}")
         res = requests.post(url, json=sspec, timeout=self.timeout)
 
         if res.status_code != 200:
-            if verbose and ('traceback' in res.json()):
+            if verbose and ("traceback" in res.json()):
                 print(f'DBG: Server traceback=\n{res.json()["traceback"]}')
             raise ex.genSparclException(res, verbose=self.verbose)
 
         found = Found(res.json(), client=self)
         if verbose:
-            print(f'Record key counts: {ut.count_values(found.records)}')
+            print(f"Record key counts: {ut.count_values(found.records)}")
         return found
 
-    def missing(self, uuid_list, *, dataset_list=None,
-                countOnly=False, verbose=False):
+    def missing(
+        self, uuid_list, *, dataset_list=None, countOnly=False, verbose=False
+    ):
         """Return the subset of sparcl_ids in the given uuid_list that are
         NOT stored in the SPARC database.
 
         Args:
             uuid_list (:obj:`list`): List of sparcl_ids.
 
             dataset_list (:obj:`list`, optional): List of data sets from
@@ -452,35 +481,37 @@
             >>> ids = ['ddbb57ee-8e90-4a0d-823b-0f5d97028076',]
             >>> client.missing(ids)
             ['ddbb57ee-8e90-4a0d-823b-0f5d97028076']
         """
 
         if dataset_list is None:
             dataset_list = self.fields.all_drs
-        assert isinstance(dataset_list, (list, set)), (
-            f'DATASET_LIST must be a list. Found {dataset_list}')
+        assert isinstance(
+            dataset_list, (list, set)
+        ), f"DATASET_LIST must be a list. Found {dataset_list}"
 
         verbose = verbose or self.verbose
-        uparams = dict(dataset_list=','.join(dataset_list))
+        uparams = dict(dataset_list=",".join(dataset_list))
         qstr = urlencode(uparams)
-        url = f'{self.apiurl}/missing/?{qstr}'
+        url = f"{self.apiurl}/missing/?{qstr}"
         uuids = list(uuid_list)
         if verbose:
             print(f'Using url="{url}"')
         res = requests.post(url, json=uuids, timeout=self.timeout)
 
         res.raise_for_status()
         if res.status_code != 200:
             raise Exception(res)
         ret = res.json()
         return ret
         # END missing()
 
-    def missing_specids(self, specid_list, *, dataset_list=None,
-                        countOnly=False, verbose=False):
+    def missing_specids(
+        self, specid_list, *, dataset_list=None, countOnly=False, verbose=False
+    ):
         """Return the subset of specids in the given specid_list that are
         NOT stored in the SPARC database.
 
         Args:
             specid_list (:obj:`list`): List of specids.
 
             dataset_list (:obj:`list`, optional): List of data sets from
@@ -495,77 +526,80 @@
                 statement. Defaults to False.
 
         Returns:
             A list of the subset of specids in the given specid_list that
             are NOT stored in the SPARC database.
 
         Example:
-            >>> client = SparclClient(url=_DEV)
-            >>> specids = ['1506454396622366720', '1506454671500273664']
+            >>> client = SparclClient(url=_PAT)
+            >>> specids = ['7972592460248666112', '3663710814482833408']
             >>> client.missing_specids(specids + ['bad_id'])
             ['bad_id']
         """
         if dataset_list is None:
             dataset_list = self.fields.all_drs
-        assert isinstance(dataset_list, (list, set)), (
-            f'DATASET_LIST must be a list. Found {dataset_list}')
+        assert isinstance(
+            dataset_list, (list, set)
+        ), f"DATASET_LIST must be a list. Found {dataset_list}"
 
         verbose = verbose or self.verbose
-        uparams = dict(dataset_list=','.join(dataset_list))
+        uparams = dict(dataset_list=",".join(dataset_list))
         qstr = urlencode(uparams)
-        url = f'{self.apiurl}/missing_specids/?{qstr}'
+        url = f"{self.apiurl}/missing_specids/?{qstr}"
         specids = list(specid_list)
         if verbose:
             print(f'Using url="{url}"')
         res = requests.post(url, json=specids, timeout=self.timeout)
 
         res.raise_for_status()
         if res.status_code != 200:
             raise Exception(res)
         ret = res.json()
         return ret
         # END missing_specids()
 
-
     # Include fields are Science (not internal) names. But the mapping
     # of Internal to Science name depends on DataSet.  Its possible
     # for a field (Science name) to be valid in one DataSet but not
     # another.  For the include_list to be valid, all fields must be
     # valid Science field names for all DS in given dataset_list.
     # (defaults to all DataSets ingested)
     def _validate_include(self, include_list, dataset_list):
         if not isinstance(include_list, (list, set)):
-            msg = f'Bad INCLUDE_LIST. Must be list. Got {include_list}'
+            msg = f"Bad INCLUDE_LIST. Must be list. Got {include_list}"
             raise ex.BadInclude(msg)
 
-        available_science = self.get_available_fields(
-            dataset_list=dataset_list)
+        avail_science = self.get_available_fields(dataset_list=dataset_list)
         inc_set = set(include_list)
-        unknown = inc_set.difference(available_science)
+        unknown = inc_set.difference(avail_science)
         if len(unknown) > 0:
-            msg = (f'The INCLUDE list ({",".join(sorted(include_list))}) '
-                   f'contains invalid data field names '
-                   f'for Data Sets ({",".join(sorted(dataset_list))}). '
-                   f'Unknown fields are: '
-                   f'{", ".join(sorted(list(unknown)))}. '
-                   f'Available fields are: '
-                   f'{", ".join(sorted(available_science))}.')
+            msg = (
+                f'The INCLUDE list ({",".join(sorted(include_list))}) '
+                f"contains invalid data field names "
+                f'for Data Sets ({",".join(sorted(dataset_list))}). '
+                f"Unknown fields are: "
+                f'{", ".join(sorted(list(unknown)))}. '
+                f"Available fields are: "
+                f'{", ".join(sorted(avail_science))}.'
+            )
             raise ex.BadInclude(msg)
         return True
 
-    def retrieve(self,  # noqa: C901
-                 uuid_list,
-                 *,
-                 svc='spectras',  # 'retrieve',
-                 format='pkl',    # 'json',
-                 include='DEFAULT',
-                 dataset_list=None,
-                 limit=500,
-                 chunk=500,
-                 verbose=None):
+    def retrieve(  # noqa: C901
+        self,
+        uuid_list,
+        *,
+        svc="spectras",  # 'retrieve',
+        format="pkl",  # 'json',
+        include="DEFAULT",
+        dataset_list=None,
+        limit=500,
+        chunk=500,
+        verbose=None,
+    ):
         """Retrieve spectra records from the SPARC database by list of
         sparcl_ids.
 
         Args:
             uuid_list (:obj:`list`): List of sparcl_ids.
 
             svc (:obj:`str`, optional): Defaults to 'spectras'.
@@ -577,38 +611,39 @@
                 the fields tagged as 'default'.
 
             dataset_list (:obj:`list`, optional): List of data sets from
                 which to retrieve spectra data. Defaults to None, meaning all
                 data sets hosted on the SPARC database.
 
             limit (:obj:`int`, optional): Maximum number of records to
-                return. Defaults to 500.
+                return. Defaults to 500. Maximum allowed is 24,000.
 
             chunk (:obj:`int`, optional): Size of chunks to break list into.
                 Defaults to 500.
 
             verbose (:obj:`bool`, optional): Set to True for in-depth return
                 statement. Defaults to False.
 
         Returns:
             :class:`~sparcl.Results.Retrieved`: Contains header and records.
 
         Example:
             >>> client = SparclClient()
-            >>> ids = ['000017b6-56a2-4f87-8828-3a3409ba1083',]
-            >>> inc = ['id', 'flux', 'wavelength', 'model']
+            >>> ids = ['00000f0b-07db-4234-892a-6e347db79c89',]
+            >>> inc = ['sparcl_id', 'flux', 'wavelength', 'model']
             >>> ret = client.retrieve(uuid_list=ids, include=inc)
             >>> type(ret.records[0].wavelength)
             <class 'numpy.ndarray'>
         """
 
         if dataset_list is None:
             dataset_list = self.fields.all_drs
-        assert isinstance(dataset_list, (list, set)), (
-            f'DATASET_LIST must be a list. Found {dataset_list}')
+        assert isinstance(
+            dataset_list, (list, set)
+        ), f"DATASET_LIST must be a list. Found {dataset_list}"
 
         verbose = self.verbose if verbose is None else verbose
 
         if (include == DEFAULT) or (include is None) or include == []:
             include_list = self.get_default_fields(dataset_list=dataset_list)
         elif include == ALL:
             include_list = self.get_all_fields(dataset_list=dataset_list)
@@ -618,125 +653,137 @@
         self._validate_include(include_list, dataset_list)
 
         req_num = min(len(uuid_list), (limit or len(uuid_list)))
         #! print(f'DBG: req_num = {req_num:,d}'
         #!       f'  len(uuid_list)={len(uuid_list):,d}'
         #!       f'  limit={limit}'
         #!       f'  MAX_NUM_RECORDS_RETRIEVED={MAX_NUM_RECORDS_RETRIEVED:,d}')
-        if (req_num > MAX_NUM_RECORDS_RETRIEVED):
-            msg = (f'Too many records asked for with client.retrieve().'
-                   f'  {len(uuid_list):,d} IDs provided,'
-                   f'  limit={limit}.'
-                   f'  But the maximum allowed is'
-                   f' {MAX_NUM_RECORDS_RETRIEVED:,d}.')
+        if req_num > MAX_NUM_RECORDS_RETRIEVED:
+            msg = (
+                f"Too many records asked for with client.retrieve()."
+                f"  {len(uuid_list):,d} IDs provided,"
+                f"  limit={limit}."
+                f"  But the maximum allowed is"
+                f" {MAX_NUM_RECORDS_RETRIEVED:,d}."
+            )
             raise ex.TooManyRecords(msg)
 
         com_include = self._common_internal(
-            science_fields=include_list,
-            dataset_list=dataset_list)
-        uparams = dict(include=','.join(com_include),
-                       # limit=limit,  # altered uuid_list to reflect limit
-                       chunk_len=chunk,
-                       format=format,
-                       dataset_list=','.join(dataset_list))
+            science_fields=include_list, dataset_list=dataset_list
+        )
+        uparams = dict(
+            include=",".join(com_include),
+            # limit=limit,  # altered uuid_list to reflect limit
+            chunk_len=chunk,
+            format=format,
+            dataset_list=",".join(dataset_list),
+        )
         qstr = urlencode(uparams)
 
         #!url = f'{self.apiurl}/retrieve/?{qstr}'
-        url = f'{self.apiurl}/{svc}/?{qstr}'
+        url = f"{self.apiurl}/{svc}/?{qstr}"
         if verbose:
             print(f'Using url="{url}"')
             ut.tic()
 
         try:
             ids = list(uuid_list) if limit is None else list(uuid_list)[:limit]
             res = requests.post(url, json=ids, timeout=self.timeout)
         except requests.exceptions.ConnectTimeout as reCT:
-            raise ex.UnknownSparcl(f'ConnectTimeout: {reCT}')
+            raise ex.UnknownSparcl(f"ConnectTimeout: {reCT}")
         except requests.exceptions.ReadTimeout as reRT:
-            msg = (f'Try increasing the value of the "read_timeout" parameter'
-                   f' to "SparclClient()".'
-                   f' The current values is: {self.r_timeout} (seconds)'
-                   f'{reRT}')
+            msg = (
+                f'Try increasing the value of the "read_timeout" parameter'
+                f' to "SparclClient()".'
+                f" The current values is: {self.r_timeout} (seconds)"
+                f"{reRT}"
+            )
             raise ex.ReadTimeout(msg) from None
         except requests.exceptions.ConnectionError as reCE:
-            raise ex.UnknownSparcl(f'ConnectionError: {reCE}')
+            raise ex.UnknownSparcl(f"ConnectionError: {reCE}")
         except requests.exceptions.TooManyRedirects as reTMR:
-            raise ex.UnknownSparcl(f'TooManyRedirects: {reTMR}')
+            raise ex.UnknownSparcl(f"TooManyRedirects: {reTMR}")
         except requests.exceptions.HTTPError as reHTTP:
-            raise ex.UnknownSparcl(f'HTTPError: {reHTTP}')
+            raise ex.UnknownSparcl(f"HTTPError: {reHTTP}")
         except requests.exceptions.URLRequired as reUR:
-            raise ex.UnknownSparcl(f'URLRequired: {reUR}')
+            raise ex.UnknownSparcl(f"URLRequired: {reUR}")
         except requests.exceptions.RequestException as reRE:
-            raise ex.UnknownSparcl(f'RequestException: {reRE}')
+            raise ex.UnknownSparcl(f"RequestException: {reRE}")
         except Exception as err:  # fall through
             raise ex.UnknownSparcl(err)
 
         if verbose:
             elapsed = ut.toc()
-            print(f'Got response to post in {elapsed} seconds')
+            print(f"Got response to post in {elapsed} seconds")
         if res.status_code != 200:
             if verbose:
-                print(f'DBG: Server response=\n{res.text}')
+                print(f"DBG: Server response=\n{res.text}")
             # @@@ FAILS on invalid JSON. Maybe not json at all !!!
-            if verbose and ('traceback' in res.json()):
+            if verbose and ("traceback" in res.json()):
                 print(f'DBG: Server traceback=\n{res.json()["traceback"]}')
             raise ex.genSparclException(res, verbose=verbose)
 
-        if format == 'json':
+        if format == "json":
             results = res.json()
-        elif format == 'pkl':
+        elif format == "pkl":
             # Read chunked binary file (representing pickle file) from
             # server response. Load pickle into python data structure.
             # Python structure is list of records where first element
             # is a header.
-            with tempfile.TemporaryFile(mode='w+b') as fp:
+            with tempfile.TemporaryFile(mode="w+b") as fp:
                 for idx, chunk in enumerate(res.iter_content(chunk_size=None)):
                     fp.write(chunk)
                 # Position to start of file for pickle reading (load)
                 fp.seek(0)
                 results = pickle.load(fp)
         else:
             results = res.json()
 
         meta = results[0]
         if verbose:
             count = len(results) - 1
-            print(f'Got {count} spectra in '
-                  f'{elapsed:.2f} seconds ({count/elapsed:.0f} '
-                  'spectra/sec)')
+            print(
+                f"Got {count} spectra in "
+                f"{elapsed:.2f} seconds ({count/elapsed:.0f} "
+                "spectra/sec)"
+            )
             print(f'{meta["status"]}')
 
-        if len(meta['status'].get('warnings', [])) > 0:
-            warn(f"{'; '.join(meta['status'].get('warnings'))}",
-                 stacklevel=2)
+        if len(meta["status"].get("warnings", [])) > 0:
+            warn(f"{'; '.join(meta['status'].get('warnings'))}", stacklevel=2)
 
         return Retrieved(results, client=self)
 
-    def retrieve_by_specid(self,
-                           specid_list,
-                           *,
-                           svc='spectras',  # 'retrieve',
-                           format='pkl',    # 'json',
-                           include='DEFAULT',
-                           dataset_list=None,
-                           limit=500,
-                           verbose=False):
+    def retrieve_by_specid(
+        self,
+        specid_list,
+        *,
+        svc="spectras",  # 'retrieve',
+        format="pkl",  # 'json',
+        include="DEFAULT",
+        dataset_list=None,
+        limit=500,
+        verbose=False,
+    ):
         """Retrieve spectra records from the SPARC database by list of specids.
 
         Args:
             specid_list (:obj:`list`): List of specids.
 
             include (:obj:`list`, optional): List of field names to include
                 in each record. Defaults to 'DEFAULT', which will return
                 the fields tagged as 'default'.
 
             dataset_list (:obj:`list`, optional): List of data sets from
                 which to retrieve spectra data. Defaults to None, meaning all
                 data sets hosted on the SPARC database.
 
+            limit (:obj:`int`, optional): Maximum number of records to
+                return. Defaults to 500. Maximum allowed is 24,000.
+
             verbose (:obj:`bool`, optional): Set to True for in-depth return
                 statement. Defaults to False.
 
         Returns:
             :class:`~sparcl.Results.Retrieved`: Contains header and records.
 
         Example:
@@ -747,42 +794,47 @@
             >>> len(ret.records[0].wavelength)
             4617
 
         """
         #!specid_list = list(specid_list)
         assert isinstance(specid_list, list), (
             f'The "specid_list" parameter must be a python list. '
-            f'You used a value of type {type(specid_list)}.')
-        assert len(specid_list) > 0, (
-            f'The "specid_list" parameter value must be a non-empty list')
+            f"You used a value of type {type(specid_list)}."
+        )
+        assert (
+            len(specid_list) > 0
+        ), f'The "specid_list" parameter value must be a non-empty list'
         assert isinstance(specid_list[0], int), (
             f'The "specid_list" parameter must be a python list of INTEGERS. '
-            f'You used an element value of type {type(specid_list[0])}.')
+            f"You used an element value of type {type(specid_list[0])}."
+        )
 
         if dataset_list is None:
-            constraints = {'specid': specid_list}
+            constraints = {"specid": specid_list}
         else:
-            constraints = {'specid': specid_list,
-                           'data_release': dataset_list}
+            constraints = {"specid": specid_list, "data_release": dataset_list}
 
         # Science Field Name for uuid.
         dr = list(self.fields.all_drs)[0]
-        idfld = self.fields._science_name('id', dr)
+        idfld = self.fields._science_name("id", dr)
 
         found = self.find([idfld], constraints=constraints, limit=limit)
         if verbose:
-            print(f'Found {found.count} matches.')
-        res = self.retrieve(found.ids,
-                            svc=svc,
-                            format=format,
-                            include=include,
-                            dataset_list=dataset_list,
-                            limit=limit,
-                            verbose=verbose)
+            print(f"Found {found.count} matches.")
+        res = self.retrieve(
+            found.ids,
+            svc=svc,
+            format=format,
+            include=include,
+            dataset_list=dataset_list,
+            limit=limit,
+            verbose=verbose,
+        )
         if verbose:
-            print(f'Got {res.count} records.')
+            print(f"Got {res.count} records.")
         return res
 
 
 if __name__ == "__main__":
     import doctest
+
     doctest.testmod()
```

## sparcl/conf.py

```diff
@@ -1,33 +1,34 @@
 # Python Standard Library
 import configparser
 import os.path
 
 
-class Conf():
+class Conf:
     """
     Configuration parameters for `ada_client`.
     """
 
     def __init__(self, conf_file=None):
         config = configparser.ConfigParser()
-        conf_files = ['~/sparc.ini', 'sparcl/sparc.ini']
+        conf_files = ["~/sparc.ini", "sparcl/sparc.ini"]
         if conf_file is None:
             for cf in conf_files:
                 if os.path.exists(os.path.expanduser(cf)):
                     config.read(os.path.expanduser(cf))
 
-        if 'ada.server' not in config:
-            raise Exception(f'Could not find conf file in any of: '
-                            f'{(",").join(conf_files)} '
-                            f'Create one and try again.'
-                            )
+        if "ada.server" not in config:
+            raise Exception(
+                f"Could not find conf file in any of: "
+                f'{(",").join(conf_files)} '
+                f"Create one and try again."
+            )
 
         self.config = config
 
     @property
     def server_baseurl(self):
-        return self.config['sparc.server']['ServerBaseUrl']
+        return self.config["sparc.server"]["ServerBaseUrl"]
 
     @property
     def server_timeout(self):
-        return self.config['sparc.server']['ServerTimout']
+        return self.config["sparc.server"]["ServerTimout"]
```

## sparcl/exceptions.py

```diff
@@ -3,127 +3,139 @@
 
 def genSparclException(response, verbose=False):
     """Given status from Server response.json(), which is a dict, generate
     a native SPARCL exception suitable for Science programs."""
 
     content = response.content
     if verbose:
-        print(f'Exception: response content={content}')
+        print(f"Exception: response content={content}")
     status = response.json()
 
     # As of Python 3.10.0.alpha6, python "match" statement could be used
     # instead of if-elif-else.
     # https://docs.python.org/3.10/whatsnew/3.10.html#pep-634-structural-pattern-matching
-    if status.get('errorCode') == 'BADPATH':
-        return BadPath(status.get('errorMessage'))
-    elif status.get('errorCode') == 'BADQUERY':
-        return BadQuery(status.get('errorMessage'))
-    elif status.get('errorCode') == 'UNKFIELD':
-        return UnknownField(status.get('errorMessage'))
-    elif status.get('errorCode') == 'BADCONST':
-        return BadSearchConstraint(status.get('errorMessage'))
+    if status.get("errorCode") == "BADPATH":
+        return BadPath(status.get("errorMessage"))
+    elif status.get("errorCode") == "BADQUERY":
+        return BadQuery(status.get("errorMessage"))
+    elif status.get("errorCode") == "UNKFIELD":
+        return UnknownField(status.get("errorMessage"))
+    elif status.get("errorCode") == "BADCONST":
+        return BadSearchConstraint(status.get("errorMessage"))
     else:
         return UnknownServerError(
-            f"{status.get('errorMessage')} "
-            f"[{status.get('errorCode')}]")
+            f"{status.get('errorMessage')} " f"[{status.get('errorCode')}]"
+        )
 
 
 class BaseSparclException(Exception):
-    """Base Class for all SPARCL exceptions. """
-    error_code = 'UNKNOWN'
-    error_message = '<NA>'
+    """Base Class for all SPARCL exceptions."""
+
+    error_code = "UNKNOWN"
+    error_message = "<NA>"
     traceback = None
 
     def get_subclass_name(self):
         return self.__class__.__name__
 
     def __init__(self, error_message, error_code=None):
         Exception.__init__(self)
         self.error_message = error_message
         if error_code:
             self.error_code = error_code
         self.traceback = traceback.format_exc()
 
     def __str__(self):
-        return f'[{self.error_code}] {self.error_message}'
+        return f"[{self.error_code}] {self.error_message}"
 
     def to_dict(self):
         """Convert a SPARCL exception to a python dictionary"""
-        dd = dict(errorMessage=self.error_message,
-                  errorCode=self.error_code)
+        dd = dict(errorMessage=self.error_message, errorCode=self.error_code)
         if self.traceback is not None:
-            dd['traceback'] = self.traceback
+            dd["traceback"] = self.traceback
         return dd
 
 
 class BadPath(BaseSparclException):
     """A field path starts with a non-core field."""
-    error_code = 'BADPATH'
+
+    error_code = "BADPATH"
 
 
 class BadQuery(BaseSparclException):
     """Bad find constraints."""
-    error_code = 'BADPATH'
+
+    error_code = "BADPATH"
 
 
 class BadInclude(BaseSparclException):
     """Include list contains invalid data field(s)."""
-    error_code = 'BADINCL'
+
+    error_code = "BADINCL"
 
 
 class UnknownServerError(BaseSparclException):
     """Client got a status response from the SPARC Server that we do not
     know how to decode."""
-    error_code = 'UNKNOWN'
+
+    error_code = "UNKNOWN"
 
 
 class UnkDr(BaseSparclException):
     """The Data Release is not known or not supported."""
-    error_code = 'UNKDR'
+
+    error_code = "UNKDR"
 
 
 class ReadTimeout(BaseSparclException):
     """The server did not send any data in the allotted amount of time."""
-    error_code = 'RTIMEOUT'
+
+    error_code = "RTIMEOUT"
 
 
 class UnknownSparcl(BaseSparclException):
     """Unknown SPARCL error.  If this is ever raised (seen in a log)
     create and use a new BaseSparcException exception that is more specific."""
-    error_code = 'UNKSPARC'
+
+    error_code = "UNKSPARC"
 
 
 class UnknownField(BaseSparclException):
     """Unknown field name for a record"""
-    error_code = 'UNKFIELD'
+
+    error_code = "UNKFIELD"
 
 
 class NoCommonIdField(BaseSparclException):
     """The field name for Science id field is not common to all Data Sets"""
-    error_code = 'IDNOTCOM'
+
+    error_code = "IDNOTCOM"
 
 
 class ServerConnectionError(BaseSparclException):
-    error_code = 'SRVCONER'
+    error_code = "SRVCONER"
 
 
 class BadSearchConstraint(BaseSparclException):
-    error_code = 'BADSCONS'
+    error_code = "BADSCONS"
 
 
 class NoRecords(BaseSparclException):
     """Results did not contain any records"""
-    error_code = 'NORECORD'
+
+    error_code = "NORECORD"
 
 
 class TooManyRecords(BaseSparclException):
     """Too many records asked for in RETRIEVE"""
-    error_code = 'TOOMANYR'
+
+    error_code = "TOOMANYR"
 
 
 class NoIDs(BaseSparclException):
     """The length of the list of original IDs passed to the reorder
-       method was zero"""
-    error_code = 'NOIDS'
+    method was zero"""
+
+    error_code = "NOIDS"
 
 
 # error_code values should be no bigger than 8 characters 12345678
```

## sparcl/fields.py

```diff
@@ -1,86 +1,108 @@
 """Get Field names associated with various SPARCL conditions.
 """
 # Python Standard Library
 from collections import defaultdict
+
 # External Packages
 import requests
 
 
 def validate_fields(datafields):
     # datafields is simply:
     #   DataField.objects.all().values(*atts)
 
-    drs = set([df['data_release'] for df in datafields])
-    core = {df['origdp']: df['newdp']
-            for df in datafields if df['storage'] == 'C'}
-
-    o2n = {dr: {df['origdp']: df['newdp']
-                for df in datafields
-                if df['data_release'] == dr}
-           for dr in drs}
+    drs = set([df["data_release"] for df in datafields])
+    core = {
+        df["origdp"]: df["newdp"] for df in datafields if df["storage"] == "C"
+    }
+
+    o2n = {
+        dr: {
+            df["origdp"]: df["newdp"]
+            for df in datafields
+            if df["data_release"] == dr
+        }
+        for dr in drs
+    }
 
     for dr, df in o2n.items():
         #  1-1 mapping origdp <-> newdp across all DR
         if len(set(df.values())) != len(df):
-            msg = (f'Data Release={dr} does not have a one-to-one mapping '
-                   f'between Original and Science field names.')
+            msg = (
+                f"Data Release={dr} does not have a one-to-one mapping "
+                f"between Original and Science field names."
+            )
             raise Exception(msg)
 
         acore = defaultdict(list)  # ambiguous core fields(more than one value)
         for k in core.keys():
             if df.get(k) != core.get(k):
                 acore[k].append(df.get(k))
         if len(acore) > 0:
-            msg = (f'DataFields do not have the same '
-                   f'Science field name for core values across all Data Sets. '
-                   f'{dict(acore)}'
-                   )
+            msg = (
+                f"DataFields do not have the same "
+                f"Science field name for core values across all Data Sets. "
+                f"{dict(acore)}"
+            )
             raise Exception(msg)
 
     return True
 
 
-class Fields():  # Derived from a single query
+class Fields:  # Derived from a single query
     """Lookup of Field Names"""
 
     def __init__(self, apiurl):
-
         # [rec, ...]
         # where rec is dict containing keys:
         # 'data_release', 'origdp', 'newdp', 'storage', 'default', 'all'
-        datafields = requests.get(f'{apiurl}/datafields/').json()
+        datafields = requests.get(f"{apiurl}/datafields/").json()
 
         validate_fields(datafields)
 
-        dr_list = set(df['data_release'] for df in datafields)
+        dr_list = set(df["data_release"] for df in datafields)
 
         self.datafields = datafields
         # o2n[DR][InternalName] => ScienceName
-        self.o2n = {dr: {df['origdp']: df['newdp']
-                         for df in datafields
-                         if df['data_release'] == dr}
-                    for dr in dr_list}
+        self.o2n = {
+            dr: {
+                df["origdp"]: df["newdp"]
+                for df in datafields
+                if df["data_release"] == dr
+            }
+            for dr in dr_list
+        }
         # n2o[DR][ScienceName] => InternalName
-        self.n2o = {dr: {df['newdp']: df['origdp']
-                         for df in datafields
-                         if df['data_release'] == dr}
-                    for dr in dr_list}
+        self.n2o = {
+            dr: {
+                df["newdp"]: df["origdp"]
+                for df in datafields
+                if df["data_release"] == dr
+            }
+            for dr in dr_list
+        }
         self.all_drs = dr_list
-        self.all_fields = set([df['newdp'] for df in datafields])
+        self.all_fields = set([df["newdp"] for df in datafields])
         self.datafields = datafields
 
         # Per DataRelease: get Storage, Default, All for each (user) fieldname
         # dr_attrs[DR][newdp] => dict[storage,default,all]
-        self.attrs = {dr: {df['newdp']: {'storage': df['storage'],
-                                         'default': df['default'],
-                                         'all': df['all']}
-                           for df in datafields
-                           if df['data_release'] == dr}
-                      for dr in dr_list}
+        self.attrs = {
+            dr: {
+                df["newdp"]: {
+                    "storage": df["storage"],
+                    "default": df["default"],
+                    "all": df["all"],
+                }
+                for df in datafields
+                if df["data_release"] == dr
+            }
+            for dr in dr_list
+        }
 
     @property
     def all_datasets(self):
         return self.all_drs
 
     def _science_name(self, internal_name, dataset):
         return self.o2n[dataset].get(internal_name)
@@ -96,34 +118,38 @@
                 if v.get(attr):
                     fields.add(k)
         return fields
 
     def default_retrieve_fields(self, dataset_list=None):
         if dataset_list is None:
             dataset_list = self.all_drs
-        return self.filter_fields('default', dataset_list)
+        return self.filter_fields("default", dataset_list)
 
     def all_retrieve_fields(self, dataset_list=None):
         if dataset_list is None:
             dataset_list = self.all_drs
-        return self.filter_fields('all', dataset_list)
+        return self.filter_fields("all", dataset_list)
 
     def common(self, dataset_list=None):
         """Fields common to DATASET_LIST (or All datasets if None)"""
         if dataset_list is None:
             dataset_list = self.all_drs
-        return sorted(set.intersection(*[set(self.n2o[dr].keys())
-                                         for dr in dataset_list]))
+        return sorted(
+            set.intersection(
+                *[set(self.n2o[dr].keys()) for dr in dataset_list]
+            )
+        )
 
     def common_internal(self, dataset_list=None):
         """Fields common to DATASET_LIST (or All datasets if None)"""
         if dataset_list is None:
             dataset_list = self.all_drs
-        return set.intersection(*[set(self.o2n[dr].keys())
-                                  for dr in dataset_list])
+        return set.intersection(
+            *[set(self.o2n[dr].keys()) for dr in dataset_list]
+        )
 
     # There is probably an algorithm to partition ELEMENTS into
     # the _minumum_ number of SETS such that the union of all SETS
     # contains all ELEMENTS. For now, parition by Data Set (when used).
     def field_partitions(self, fields):
         """Partition FIELDS into the DataSets that contain them"""
         dr_fields = defaultdict(list)
```

## sparcl/gather_2d.py

```diff
@@ -1,24 +1,23 @@
 """Align or resample spectra related fields across multiple records."""
 # See client.py for Doctest example
 #
 # For info about problems with floating point,
 #   See:  https://docs.python.org/3/tutorial/floatingpoint.html
 #   Also: https://docs.python.org/3/library/decimal.html#floating-point-notes
 #
-import math
 from decimal import Decimal
+
 #
 import numpy as np
+
 #
 import sparcl.client
 
 
-
-
 # Map every wavelength of every record to index (ri,wi)
 # where
 #   ri: Record Index
 #   wi: Window Index (offset of wavelength in WINDOW)
 #   window: ordered list of wavelengths that include ALL unique
 #           wavelengths in all records
 #! def rec_woffset(records, window):
@@ -32,19 +31,21 @@
 #!             ar[ri,wi] = wl
 #!     return ar
 
 
 def _wavelength_offsets(records):
     # sorted list of wavelengths from ALL records
     window = sorted(
-        set(records[0].wavelength).union(*[r.wavelength for r in records[1:]]))
+        set(records[0].wavelength).union(*[r.wavelength for r in records[1:]])
+    )
     # offsets[ri] = index into WINDOW
-    offsets = {ri: window.index(rec.wavelength[0])
-               for ri, rec in enumerate(records)}
-    return(window, offsets)
+    offsets = {
+        ri: window.index(rec.wavelength[0]) for ri, rec in enumerate(records)
+    }
+    return (window, offsets)
 
 
 def _validate_wavelength_alignment(records, window, offsets, precision=None):
     PLACES = Decimal(10) ** -precision if precision is not None else None
     #! print(f'DBG: PLACES={PLACES}')
     # Given an exact wavelength match between first wl (wavelength) in a rec
     # and the wl at its offset of WINDOW, ensure all the remaning wls
@@ -59,44 +60,45 @@
             #! msg = (f'Wavelength in '
             #!        f'Record[{ri}][{wi}] ({recwl}) does not match '
             #!        f'Window[{offsets[ri]+wi} = offset[{ri}]={offsets[ri]} '
             #!        f'+ {wi}]  ({wwl})'
             #!        )
             #! assert recwl == wwl, msg
             if recwl != wwl:
-                msg = (f'The spectra cannot be aligned with the given'
-                       f' "precision" parameter ({precision}).'
-                       f' Try lowering the precision value.')
+                msg = (
+                    f"The spectra cannot be aligned with the given"
+                    f' "precision" parameter ({precision}).'
+                    f" Try lowering the precision value."
+                )
                 raise Exception(msg)
 
 
 # We want to align a bunch of records by wavelength into a single
 # 2d numpy array (record vs wavelength).  In general, we
 # are not guaranteed that this is possible -- even if using only
 # records from a single DataSet. So validate it first.
 # (If not valid, allowing wavelength slop might help.)
 def _align_wavelengths(records):
-    window, offsets = wavelength_offsets(records)
+    window, offsets = _wavelength_offsets(records)
     _validate_wavelength_alignment(records, window, offsets)
     ar = np.ones([len(records), len(window)])
     for ri, r in enumerate(records):
         for wi, wl in enumerate(r.wavelength):
             ar[ri, offsets[ri + wi]] = wl  # @@@WRONG!!! We want FLUX
     return ar
 
 
-def _tt1(numrecs=20, dr='BOSS-DR16'):
+def _tt1(numrecs=20, dr="BOSS-DR16"):
     client = sparcl.client.SparclClient()
-    found = client.find(constraints=dict(data_release=[dr]),
-                        limit=numrecs)
+    found = client.find(constraints=dict(data_release=[dr]), limit=numrecs)
     got = client.retrieve(found.ids)
     records = got.records
-    window, offsets = wavelength_offsets(records)
-    print(f'Built window len={len(window)}; offsets={offsets}')
-    #return records, window, offsets
+    window, offsets = _wavelength_offsets(records)
+    print(f"Built window len={len(window)}; offsets={offsets}")
+    # return records, window, offsets
     ar = _align_wavelengths(records)
     return ar
 
 
 # precision:: number of decimal places
 # "records" must contain "wavelength" field.
 def _wavelength_grid_offsets(records, precision=11):
@@ -105,37 +107,41 @@
     # set of wavelengths from ALL records. Quantized to precision
     gset = set()  # Grid SET
     for r in records:
         gset.update([Decimal(w).quantize(PLACES) for w in r.wavelength])
     grid = sorted(gset)  # 1D sorted list of wavelengths (bigger than any rec)
     #! print(f'DBG grid({len(grid)})[:10]={grid[:10]}')
     # offsets[ri] = index into GRID
-    offsets = {ri: grid.index(Decimal(rec.wavelength[0]).quantize(PLACES))
-               for ri, rec in enumerate(records)}
-    return(grid, offsets)
+    offsets = {
+        ri: grid.index(Decimal(rec.wavelength[0]).quantize(PLACES))
+        for ri, rec in enumerate(records)
+    }
+    return (grid, offsets)
 
 
 # return 2D numpy array of FLUX values that is aligned to wavelength GRID.
 # GRID is generally wider than flux for single record. Pad with NaN.
 def _flux_grid(records, grid, offsets, precision=None):
     _validate_wavelength_alignment(records, grid, offsets, precision=precision)
     ar = np.full([len(records), len(grid)], np.nan)
     for ri, r in enumerate(records):
         for fi, flux in enumerate(r.flux):
             ar[ri, offsets[ri] + fi] = flux
     return ar
 
+
 # RETURN 2D nparray(records,wavelengthGrid) = fieldValue
 def _field_grid(records, fieldName, grid, offsets, precision=None):
     ar = np.full([len(records), len(grid)], np.nan)
     for ri, r in enumerate(records):
         for fi, fieldValue in enumerate(r[fieldName]):
             ar[ri, offsets[ri] + fi] = fieldValue
     return ar  # (wavelengthGrid, records)
 
+
 # RETURN 2D nparray(fields,wavelengthGrid) = fieldValue
 #! def rec_grid(rec, fields, grid, offsets, precision=None):
 #!     ar = np.full([len(fields), len(grid)], np.nan)
 #!     ri = 0
 #!     for fi, fieldValue in enumerate(r[fieldName]):
 #!         ar[ri, offsets[ri] + fi] = fieldValue
 #!     return ar  # (wavelengthGrid, fields)
@@ -143,27 +149,32 @@
 
 # Align flux from records into one array using quantization
 #! def flux_records(records, precision=None):
 #!     grid, offsets = wavelength_grid_offsets(records, precision=precision)
 #!     ar = _flux_grid(records, grid, offsets, precision=precision)
 #!     return ar, np.array([float(x) for x in grid])
 
+
 def _validate_spectra_fields(records, fields):
-    spectra_fields = [client.fields.n2o['BOSS-DR16'][k] for k,v in client.fields.attrs['BOSS-DR16'].items() if v['storage']=='S']
-    [k for k in records[0].keys() if not k.startswith('_')]
+    #! spectra_fields = [
+    #!     client.fields.n2o["BOSS-DR16"][k]
+    #!     for k, v in client.fields.attrs["BOSS-DR16"].items()
+    #!     if v["storage"] == "S"
+    #! ]
+    [k for k in records[0].keys() if not k.startswith("_")]
 
 
 # TOP level: Intended for access from Jupyter NOTEBOOK.
 # Align spectra related field from records into one array using quantization.
-def align_records(records, fields=['flux','wavelength'], precision=7):
+def align_records(records, fields=["flux", "wavelength"], precision=7):
     """Align given spectra-type fields to a common wavelength grid.
 
     Args:
-        records (list): List of dictionaries. The keys for all these dictionaries
-            are Science Field Names.
+        records (list): List of dictionaries.
+            The keys for all these dictionaries are Science Field Names.
 
         fields (:obj:`list`, optional): List of Science Field Names of
             spectra related fields to align and include in the results.
             DEFAULT=['flux', 'wavelength']
 
         precision (:obj:`int`, optional): Number of decimal points to use for
             quantizing wavelengths into a grid.
@@ -179,25 +190,29 @@
         >>> client = sparcl.client.SparclClient()
         >>> specflds = ['wavelength', 'model']
         >>> cons = {"data_release": ['BOSS-DR16']}
         >>> found = client.find(constraints=cons, limit=21)
         >>> got = client.retrieve(found.ids, include=specflds)
         >>> ar_dict, grid = align_records(got.records, fields=specflds)
         >>> ar_dict['model'].shape
-        (21, 4670)
+        (21, 4666)
 
     """
     # Report Garbage In
-    if 'wavelength' not in fields:
-        msg = (f'You must provide "wavelength" in the list provided'
-               f' in the "fields" paramter.  Got: {fields}')
+    if "wavelength" not in fields:
+        msg = (
+            f'You must provide "wavelength" in the list provided'
+            f' in the "fields" paramter.  Got: {fields}'
+        )
         raise Exception(msg)
-    if 'wavelength' not in records[0]:
-        msg = (f'Records must contain the "wavelength" field.'
-               f' The first record contains fields: {sorted(records[0].keys())}')
+    if "wavelength" not in records[0]:
+        msg = (
+            f'Records must contain the "wavelength" field.'
+            f" The first record contains fields: {sorted(records[0].keys())}"
+        )
         raise Exception(msg)
 
     #! _validate_spectra_fields(records, fields)
     grid, offsets = _wavelength_grid_offsets(records, precision=precision)
     _validate_wavelength_alignment(records, grid, offsets, precision=precision)
 
     # One slice for each field; each slice a 2darray(wavelength, record)=fldVal
@@ -205,26 +220,14 @@
     for fld in fields:
         ar = _field_grid(records, fld, grid, offsets, precision=None)
         adict[fld] = ar
 
     return adict, np.array([float(x) for x in grid])
 
 
-def _tt(numrecs=9, dr='BOSS-DR16', precision=7):
-    # Get sample of NUMRECS records from DR DataSet.
-    client = sparcl.client.SparclClient()
-    found = client.find(constraints=dict(data_release=[dr]),
-                        limit=numrecs)
-    got = client.retrieve(found.ids)
-    records = got.records
-
-    #! grid, offsets = _wavelength_grid_offsets(records, precision=precision)
-    #! print(f'Built grid len={len(grid)} '
-    #!       f'offsets({len(offsets)})[:5]={list(offsets.values())[:5]}')
-    #! ar = _flux_grid(records, grid, offsets, precision=precision)
-    ar, grid = flux_records(records, precision=precision)
-    return ar, grid  # ar (numRecs,len(grid))
-# with np.printoptions(threshold=np.inf, linewidth=210, formatter=dict(float=lambda v: f'{v: > 7.3f}')): print(ar.T)  # noqa: E501
+# with np.printoptions(threshold=np.inf, linewidth=210,
+#   formatter=dict(float=lambda v: f'{v: > 7.3f}')): print(ar.T)  # noqa: E501
 
 if __name__ == "__main__":
     import doctest
+
     doctest.testmod()
```

## sparcl/resample_spectra.py

```diff
@@ -1,36 +1,41 @@
 # NOT INTENDED FOR PUBLIC USE!
 #
 # See:
 #   https://spectres.readthedocs.io/en/latest/
-
+import math
 import spectres
+import numpy as np
+
+# Local
+import sparcl.client
+
 
 # Per paper, should be able to pass all flux in one call to spectres
 # https://arxiv.org/pdf/1705.05165.pdf
 # Perhaps users would rather the bins uniform (1,5,20 Angstroms?)
 def _resample_flux(records, wavstep=1):
     smallest = math.floor(min([min(r.wavelength) for r in records]))
     largest = math.ceil(max([max(r.wavelength) for r in records]))
 
     #!wrange = largest - smallest
-    #new_wavs = np.fromfunction(lambda i: i + smallest, (wrange,), dtype=int)
-    #flux_2d = np.ones([len(records), wrange])
+    # new_wavs = np.fro<mfunction(lambda i: i + smallest, (wrange,), dtype=int)
+    # flux_2d = np.ones([len(records), wrange])
 
     new_wavs = np.array(range(smallest, largest + 1, wavstep))
     flux_2d = np.full([len(records), len(new_wavs)], None, dtype=float)
 
     for idx, rec in enumerate(records):
-        flux_2d[idx] = spectres.spectres(new_wavs,
-                                         rec.wavelength,
-                                         rec.flux,
-                                         verbose=False)
+        flux_2d[idx] = spectres.spectres(
+            new_wavs, rec.wavelength, rec.flux, verbose=False
+        )
     return flux_2d, new_wavs
 
 
 def _tt0(numrecs=20):
     client = sparcl.client.SparclClient()
-    found = client.find(constraints=dict(data_release=['BOSS-DR16']),
-                        limit=numrecs)
+    found = client.find(
+        constraints=dict(data_release=["BOSS-DR16"]), limit=numrecs
+    )
     got = client.retrieve(found.ids)
     flux_2d, new_wavs = _resample_flux(got.records)
     return flux_2d, new_wavs
```

## sparcl/type_conversion.py

```diff
@@ -1,18 +1,22 @@
 # Python Standard Library
 from abc import ABC, abstractmethod
 import copy
+
 #!from pprint import pformat
 from enum import Enum, auto
+
 # External Packages
 import numpy as np
+
 #!import pandas as pd
 from specutils import Spectrum1D
 import astropy.units as u
 from astropy.nddata import InverseVariance
+
 # Local Packages
 import sparcl.exceptions as ex
 
 
 """It would be much better if this were abstracted and easier to
 update with new Data Types and new DataReleases.  Perhaps use
 something like the Server "Personalities".  I've rejected abstracting
@@ -49,101 +53,104 @@
     NUMPY = auto()
     PANDAS = auto()
     SPECTRUM1D = auto()
 
 
 class Convert(ABC):
     """Convert JSON record to mix of plain python
-       and selected data record type.
+    and selected data record type.
     """
 
     @abstractmethod
     def to_numpy(self, record, o2nLUT):
         newrec = copy.deepcopy(record)
-        return(newrec)
+        return newrec
 
     @abstractmethod
     def to_spectrum1d(self, record, o2nLUT):
         newrec = copy.deepcopy(record)
-        return(newrec)
+        return newrec
+
 
 #!    @abstractmethod
 #!    def to_pandas(self, record, o2nLUT):
 #!        newrec = copy.deepcopy(record)
 #!        return(newrec)
 
 
 class NoopConvert(Convert):
     def to_numpy(self, record, o2nLUT):
-        return(record)
+        return record
 
     def to_spectrum1d(self, record, o2nLUT):
-        return(record)
+        return record
 
     def to_pandas(self, record, o2nLUT):
-        return(record)
+        return record
 
 
 class SdssDr16(Convert):
     def to_numpy(self, record, o2nLUT):
         """Convert FitsFile record to a structure that uses Numpy"""
         arflds = [
-            'spectra.coadd.and_mask',
-            'spectra.coadd.flux',
-            'spectra.coadd.ivar',
-            'spectra.coadd.loglam',
-            'spectra.coadd.model',
-            'spectra.coadd.or_mask',
-            'spectra.coadd.sky',
-            'spectra.coadd.wdisp',
+            "spectra.coadd.and_mask",
+            "spectra.coadd.flux",
+            "spectra.coadd.ivar",
+            "spectra.coadd.loglam",
+            "spectra.coadd.model",
+            "spectra.coadd.or_mask",
+            "spectra.coadd.sky",
+            "spectra.coadd.wdisp",
         ]
         lofl = [record[o2nLUT[f]] for f in arflds if f in o2nLUT]
         newrec = dict(nparr=np.array(lofl))
         for orig, new in o2nLUT.items():
             if orig in arflds:
                 continue
             if new in record:
                 newrec[new] = record[new]
-        return(newrec)
+        return newrec
 
     # Sdss
     def to_spectrum1d(self, record, o2nLUT):
         arflds = [
-            'red_shift',
-            'spectra.coadd.flux',
-            'spectra.coadd.ivar',
-            'spectra.coadd.loglam',
-            'spectra.coadd.and_mask',
+            "red_shift",
+            "spectra.coadd.flux",
+            "spectra.coadd.ivar",
+            "spectra.coadd.loglam",
+            "spectra.coadd.and_mask",
         ]
 
-        loglam = record[o2nLUT['spectra.coadd.loglam']]
-        flux = record[o2nLUT['spectra.coadd.flux']]
-        ivar = record[o2nLUT['spectra.coadd.ivar']]
-        and_mask = record[o2nLUT['spectra.coadd.and_mask']]
+        loglam = record[o2nLUT["spectra.coadd.loglam"]]
+        flux = record[o2nLUT["spectra.coadd.flux"]]
+        ivar = record[o2nLUT["spectra.coadd.ivar"]]
+        and_mask = record[o2nLUT["spectra.coadd.and_mask"]]
 
-        wavelength = (10**np.array(loglam)) * u.AA
-        flux = np.array(flux) * 10 ** -17 * u.Unit('erg cm-2 s-1 AA-1')
+        wavelength = (10 ** np.array(loglam)) * u.AA
+        flux = np.array(flux) * 10**-17 * u.Unit("erg cm-2 s-1 AA-1")
         ivar = InverseVariance(np.array(ivar))
-        z = record.get('red_shift')
+        z = record.get("red_shift")
 
         newrec = dict(
             # flux, uncertainty, wavevelength, mask(and), redshift
-            spec1d=Spectrum1D(spectral_axis=wavelength,
-                              flux=flux,
-                              uncertainty=ivar,
-                              redshift=z,
-                              mask=and_mask),
+            spec1d=Spectrum1D(
+                spectral_axis=wavelength,
+                flux=flux,
+                uncertainty=ivar,
+                redshift=z,
+                mask=and_mask,
+            ),
         )
         for orig, new in o2nLUT.items():
             if orig in arflds:
                 continue
             if new in record:
                 newrec[new] = record[new]
 
-        return(newrec)
+        return newrec
 
 
 #!    def to_pandas(self, record, o2nLUT):
 #!        arflds = [
 #!            'spectra.coadd.and_mask',
 #!            'spectra.coadd.flux',
 #!            'spectra.coadd.ivar',
@@ -163,68 +170,71 @@
 #!                newrec[new] = record[new]
 #!        return(newrec)
 
 
 class BossDr16(Convert):
     def to_numpy(self, record, o2nLUT):
         arflds = [
-            'spectra.coadd.AND_MASK',
-            'spectra.coadd.FLUX',
-            'spectra.coadd.IVAR',
-            'spectra.coadd.LOGLAM',
-            'spectra.coadd.MODEL',
-            'spectra.coadd.OR_MASK',
-            'spectra.coadd.SKY',
-            'spectra.coadd.WDISP',
+            "spectra.coadd.AND_MASK",
+            "spectra.coadd.FLUX",
+            "spectra.coadd.IVAR",
+            "spectra.coadd.LOGLAM",
+            "spectra.coadd.MODEL",
+            "spectra.coadd.OR_MASK",
+            "spectra.coadd.SKY",
+            "spectra.coadd.WDISP",
         ]
         lofl = [record[o2nLUT[f]] for f in arflds if f in o2nLUT]
         newrec = dict(nparr=np.array(lofl))
         for orig, new in o2nLUT.items():
             # Don't carry over the fields used to build the new datatype.
             # This would be duplication since their content is already
             # in the new datatype.
             if orig in arflds:
                 continue
             if new in record:
                 newrec[new] = record[new]
-        return(newrec)
+        return newrec
 
     # BOSS
     def to_spectrum1d(self, record, o2nLUT):
         arflds = [
-            'red_shift',
-            'spectra.coadd.FLUX',
-            'spectra.coadd.IVAR',
-            'spectra.coadd.LOGLAM',
-            'spectra.coadd.AND_MASK',
+            "red_shift",
+            "spectra.coadd.FLUX",
+            "spectra.coadd.IVAR",
+            "spectra.coadd.LOGLAM",
+            "spectra.coadd.AND_MASK",
         ]
-        loglam = record[o2nLUT['spectra.coadd.LOGLAM']]
-        flux = record[o2nLUT['spectra.coadd.FLUX']]
-        ivar = record[o2nLUT['spectra.coadd.IVAR']]
-        and_mask = record[o2nLUT['spectra.coadd.AND_MASK']]
+        loglam = record[o2nLUT["spectra.coadd.LOGLAM"]]
+        flux = record[o2nLUT["spectra.coadd.FLUX"]]
+        ivar = record[o2nLUT["spectra.coadd.IVAR"]]
+        and_mask = record[o2nLUT["spectra.coadd.AND_MASK"]]
 
-        wavelength = (10**np.array(loglam)) * u.AA
-        flux = np.array(flux) * 10 ** -17 * u.Unit('erg cm-2 s-1 AA-1')
+        wavelength = (10 ** np.array(loglam)) * u.AA
+        flux = np.array(flux) * 10**-17 * u.Unit("erg cm-2 s-1 AA-1")
         ivar = InverseVariance(np.array(ivar))
-        z = record.get('red_shift')
+        z = record.get("red_shift")
 
         newrec = dict(
             # flux, uncertainty, wavelength, mask(and), redshift
-            spec1d=Spectrum1D(spectral_axis=wavelength,
-                              flux=flux,
-                              uncertainty=ivar,
-                              redshift=z,
-                              mask=and_mask),
+            spec1d=Spectrum1D(
+                spectral_axis=wavelength,
+                flux=flux,
+                uncertainty=ivar,
+                redshift=z,
+                mask=and_mask,
+            ),
         )
         for orig, new in o2nLUT.items():
             if orig in arflds:
                 continue
             if new in record:
                 newrec[new] = record[new]
-        return(newrec)
+        return newrec
+
 
 #!    def to_pandas(self, record, o2nLUT): # BOSS
 #!        arflds = [
 #!            'spectra.coadd.AND_MASK',
 #!            'spectra.coadd.FLUX',
 #!            'spectra.coadd.IVAR',
 #!            'spectra.coadd.LOGLAM',
@@ -243,158 +253,166 @@
 #!                newrec[new] = record[new]
 #!        return(newrec)
 
 
 class Desi(Convert):
     def to_numpy(self, record, o2nLUT):
         arflds = [
-            'spectra.b_flux',
-            'spectra.b_ivar',
-            'spectra.b_mask',
-            'spectra.b_wavelength',
-            'spectra.r_flux',
-            'spectra.r_ivar',
-            'spectra.r_mask',
-            'spectra.r_wavelength',
-            'spectra.z_flux',
-            'spectra.z_ivar',
-            'spectra.z_mask',
-            'spectra.z_wavelength',
+            "spectra.b_flux",
+            "spectra.b_ivar",
+            "spectra.b_mask",
+            "spectra.b_wavelength",
+            "spectra.r_flux",
+            "spectra.r_ivar",
+            "spectra.r_mask",
+            "spectra.r_wavelength",
+            "spectra.z_flux",
+            "spectra.z_ivar",
+            "spectra.z_mask",
+            "spectra.z_wavelength",
         ]
         lofl = [record[o2nLUT[f]] for f in arflds if f in o2nLUT]
         newrec = dict(nparr=np.array(lofl))
         for orig, new in o2nLUT.items():
             if orig in arflds:
                 continue
             if new in record:
                 newrec[new] = record[new]
-        return(newrec)
+        return newrec
 
     def to_spectrum1d(self, record, o2nLUT):  # Desi
         arflds = [
-            'red_shift',
-            'spectra.b_flux',
-            'spectra.b_ivar',
-            'spectra.b_mask',
-            'spectra.b_wavelength',
-            'spectra.r_flux',
-            'spectra.r_ivar',
-            'spectra.r_mask',
-            'spectra.r_wavelength',
-            'spectra.z_flux',
-            'spectra.z_ivar',
-            'spectra.z_mask',
-            'spectra.z_wavelength',
+            "red_shift",
+            "spectra.b_flux",
+            "spectra.b_ivar",
+            "spectra.b_mask",
+            "spectra.b_wavelength",
+            "spectra.r_flux",
+            "spectra.r_ivar",
+            "spectra.r_mask",
+            "spectra.r_wavelength",
+            "spectra.z_flux",
+            "spectra.z_ivar",
+            "spectra.z_mask",
+            "spectra.z_wavelength",
         ]
 
-        z = record.get('red_shift')
+        z = record.get("red_shift")
 
         # _b
-        wavelength_b = record[o2nLUT['spectra.b_wavelength']]
-        flux_b = record[o2nLUT['spectra.b_flux']]
-        ivar_b = record[o2nLUT['spectra.b_ivar']]
-        mask_b = record[o2nLUT['spectra.b_mask']]
+        wavelength_b = record[o2nLUT["spectra.b_wavelength"]]
+        flux_b = record[o2nLUT["spectra.b_flux"]]
+        ivar_b = record[o2nLUT["spectra.b_ivar"]]
+        mask_b = record[o2nLUT["spectra.b_mask"]]
         # Define units
         wavelength_b = np.array(wavelength_b) * u.AA
-        flux_b = np.array(flux_b) * 10 ** -17 * u.Unit('erg cm-2 s-1 AA-1')
+        flux_b = np.array(flux_b) * 10**-17 * u.Unit("erg cm-2 s-1 AA-1")
         ivar_b = InverseVariance(np.array(ivar_b))
 
         # _r
-        wavelength_r = record[o2nLUT['spectra.r_wavelength']]
-        flux_r = record[o2nLUT['spectra.r_flux']]
-        ivar_r = record[o2nLUT['spectra.r_ivar']]
-        mask_r = record[o2nLUT['spectra.r_mask']]
+        wavelength_r = record[o2nLUT["spectra.r_wavelength"]]
+        flux_r = record[o2nLUT["spectra.r_flux"]]
+        ivar_r = record[o2nLUT["spectra.r_ivar"]]
+        mask_r = record[o2nLUT["spectra.r_mask"]]
         # Define units
         wavelength_r = np.array(wavelength_r) * u.AA
-        flux_r = np.array(flux_r) * 10 ** -17 * u.Unit('erg cm-2 s-1 AA-1')
+        flux_r = np.array(flux_r) * 10**-17 * u.Unit("erg cm-2 s-1 AA-1")
         ivar_r = InverseVariance(np.array(ivar_r))
 
         # _z
-        wavelength_z = record[o2nLUT['spectra.z_wavelength']]
-        flux_z = record[o2nLUT['spectra.z_flux']]
-        ivar_z = record[o2nLUT['spectra.z_ivar']]
-        mask_z = record[o2nLUT['spectra.z_mask']]
+        wavelength_z = record[o2nLUT["spectra.z_wavelength"]]
+        flux_z = record[o2nLUT["spectra.z_flux"]]
+        ivar_z = record[o2nLUT["spectra.z_ivar"]]
+        mask_z = record[o2nLUT["spectra.z_mask"]]
         # Define units
         wavelength_z = np.array(wavelength_z) * u.AA
-        flux_z = np.array(flux_z) * 10 ** -17 * u.Unit('erg cm-2 s-1 AA-1')
+        flux_z = np.array(flux_z) * 10**-17 * u.Unit("erg cm-2 s-1 AA-1")
         ivar_z = InverseVariance(np.array(ivar_z))
 
         newrec = dict(
             # flux, uncertainty, wavevelength, mask, redshift
-            b_spec1d=Spectrum1D(spectral_axis=wavelength_b,
-                                flux=flux_b,
-                                uncertainty=ivar_b,
-                                redshift=z,
-                                mask=mask_b),
-            r_spec1d=Spectrum1D(spectral_axis=wavelength_r,
-                                flux=flux_r,
-                                uncertainty=ivar_r,
-                                redshift=z,
-                                mask=mask_r),
-            z_spec1d=Spectrum1D(spectral_axis=wavelength_z,
-                                flux=flux_z,
-                                uncertainty=ivar_z,
-                                redshift=z,
-                                mask=mask_z),
+            b_spec1d=Spectrum1D(
+                spectral_axis=wavelength_b,
+                flux=flux_b,
+                uncertainty=ivar_b,
+                redshift=z,
+                mask=mask_b,
+            ),
+            r_spec1d=Spectrum1D(
+                spectral_axis=wavelength_r,
+                flux=flux_r,
+                uncertainty=ivar_r,
+                redshift=z,
+                mask=mask_r,
+            ),
+            z_spec1d=Spectrum1D(
+                spectral_axis=wavelength_z,
+                flux=flux_z,
+                uncertainty=ivar_z,
+                redshift=z,
+                mask=mask_z,
+            ),
         )
         for orig, new in o2nLUT.items():
             if orig in arflds:
                 continue
             if new in record:
                 newrec[new] = record[new]
-        return(newrec)
+        return newrec
 
 
 class DesiDenali(Desi):
     pass
 
 
 class DesiEverest(Desi):
     pass
 
 
 # DR Instance LookUp Table
 diLUT = {
-    'SDSS-DR16': SdssDr16(),
-    'BOSS-DR16': BossDr16(),
-    'DESI-denali': DesiDenali(),
-    'DESI-everest': DesiEverest(),
+    "SDSS-DR16": SdssDr16(),
+    "BOSS-DR16": BossDr16(),
+    "DESI-denali": DesiDenali(),
+    "DESI-everest": DesiEverest(),
     #'Unknown': NoopConvert(),
 }
 
 
 def convert(record, rtype, client, include, verbose=False):
     if rtype is None:
         return record
 
-    dr = record['_dr']
+    dr = record["_dr"]
 
     # Validate parameters
     if dr not in diLUT:
-        allowed = ', '.join(list(diLUT.keys()))
-        msg = (f'The Data Set associated with a records, "{dr}",'
-               f' is not supported for Type Conversion.'
-               f' Available Data Sets are: {allowed}.')
+        allowed = ", ".join(list(diLUT.keys()))
+        msg = (
+            f'The Data Set associated with a records, "{dr}",'
+            f" is not supported for Type Conversion."
+            f" Available Data Sets are: {allowed}."
+        )
         raise ex.UnkDr(msg)
 
     drin = diLUT.get(dr, NoopConvert())
 
     o2nLUT = copy.copy(client.orig2newLUT[dr])  # orig2newLUT[dr][orig] = new
-    o2nLUT['_dr'] = '_dr'
+    o2nLUT["_dr"] = "_dr"
     #!n2oLUT = client.new2origLUT[dr]
     #!required = set(client.required[dr])
     #!if include is not None:
     #!    nuke = set(n2oLUT.keys()).difference(required.union(include))
     #!    for new in nuke:
     #!        del o2nLUT[n2oLUT[new]]
 
-    if rtype == 'json':
+    if rtype == "json":
         return record
-    elif rtype == 'numpy':
+    elif rtype == "numpy":
         return drin.to_numpy(record, o2nLUT)
-    elif rtype == 'pandas':
+    elif rtype == "pandas":
         return drin.to_pandas(record, o2nLUT)
-    elif rtype == 'spectrum1d':
+    elif rtype == "spectrum1d":
         return drin.to_spectrum1d(record, o2nLUT)
     else:
-        raise Exception(f'Unknown record type ({rtype})')
+        raise Exception(f"Unknown record type ({rtype})")
     return None
```

## sparcl/unsupported.py

```diff
@@ -1,55 +1,65 @@
 # End Users should not use anything from this file. All of it is
 # considered experimentatal and be broken or change without notice.
 ############################################
 # Python Standard Library
 from urllib.parse import urlencode
+
 #!from urllib.parse import urlparse
 #!from warnings import warn
 import pickle
 import tempfile
 import json
+
 ############################################
 # External Packages
 import requests
+
 ############################################
 # Local Packages
 #!from sparcl.fields import Fields
 import sparcl.exceptions as ex
 
-_STAGE = 'https://sparclstage.datalab.noirlab.edu'  # noqa: E221
-_PAT   = 'https://sparc1.datalab.noirlab.edu'       # noqa: E221
+_STAGE = "https://sparclstage.datalab.noirlab.edu"  # noqa: E221
+_PAT = "https://sparc1.datalab.noirlab.edu"  # noqa: E221
 
-drs = ['SDSS-DR16', 'BOSS-DR16', 'DESI-EDR']
+drs = ["SDSS-DR16", "BOSS-DR16", "DESI-EDR"]
 
 
-def retrieve(ids, include=['id'], dataset_list=['BOSS-DR16'],
-             server=_PAT,
-             svc='spectras',  # or 'retrieve',
-             limit=100, verbose=True):
-    uparams = dict(include=','.join(include),
-                   limit=limit,
-                   dataset_list=','.join(dataset_list))
+def retrieve(
+    ids,
+    include=["id"],
+    dataset_list=["BOSS-DR16"],
+    server=_PAT,
+    svc="spectras",  # or 'retrieve',
+    limit=100,
+    verbose=True,
+):
+    uparams = dict(
+        include=",".join(include),
+        limit=limit,
+        dataset_list=",".join(dataset_list),
+    )
     qstr = urlencode(uparams)
 
-    url = f'{server}/sparc/{svc}/?{qstr}'
+    url = f"{server}/sparc/{svc}/?{qstr}"
     if verbose:
-        print(f'Using ids={ids[:2]}')
+        print(f"Using ids={ids[:2]}")
         print(f'Using url="{url}"')
-        print(f'curl -X POST "{url}" -d \'{json.dumps(ids)}\' > retrieve.pkl')
+        print(f"curl -X POST \"{url}\" -d '{json.dumps(ids)}' > retrieve.pkl")
 
     res = requests.post(url, json=ids)
 
     if res.status_code != 200:
         #! if verbose and ('traceback' in res.json()):
         #!     print(f'DBG: Server traceback=\n{res.json()["traceback"]}')
         raise ex.genSparclException(res, verbose=verbose)
 
     # unpack pickle file from result
-    with tempfile.TemporaryFile(mode='w+b') as fp:
+    with tempfile.TemporaryFile(mode="w+b") as fp:
         for idx, chunk in enumerate(res.iter_content(chunk_size=None)):
             fp.write(chunk)
         # Position to start of file for pickle reading (load)
         fp.seek(0)
         results = pickle.load(fp)
 
     return results
```

## sparcl/utils.py

```diff
@@ -1,12 +1,13 @@
 # Python library
 import datetime
 import time
 import socket
 import itertools
+
 # External packages
 #   none
 # LOCAL packages
 #   none
 
 
 # data = {
@@ -21,25 +22,27 @@
 #         }
 #     }
 # }
 #
 # data1 = AttrDict(data)
 # print(data1.b.b1.b2a.b3b)  # -> b3bval
 class _AttrDict(dict):
-    """ Dictionary subclass whose entries can be accessed by attributes
+    """Dictionary subclass whose entries can be accessed by attributes
     (as well as normally).
     """
+
     def __init__(self, *args, **kwargs):
         def from_nested_dict(data):
-            """ Construct nested AttrDicts from nested dictionaries. """
+            """Construct nested AttrDicts from nested dictionaries."""
             if not isinstance(data, dict):
                 return data
             else:
-                return _AttrDict({key: from_nested_dict(data[key])
-                                 for key in data})
+                return _AttrDict(
+                    {key: from_nested_dict(data[key]) for key in data}
+                )
 
         super(_AttrDict, self).__init__(*args, **kwargs)
         self.__dict__ = self
 
         for key in self.keys():
             self[key] = from_nested_dict(self[key])
 
@@ -73,15 +76,15 @@
     Args:
        None.
     Returns:
        Time, date, and hostname.
     """
     hostname = socket.gethostname()
     now = str(datetime.datetime.now())
-    return(hostname, now)
+    return (hostname, now)
 
 
 def objform(obj):
     """Nested structure of python object. Avoid spewing big lists.
     See also: https://code.activestate.com/recipes/577504/
 
     Args:
@@ -95,58 +98,59 @@
        <class 'sparcl.client.AttrDict'>
     """
     # dict((k,len(v)) for (k,v) in qs[0]['spzline'].items())
     if obj is None:
         return None
     elif type(obj) is list:
         if len(obj) > 999:
-            return f'CNT={len(obj)}'
+            return f"CNT={len(obj)}"
         elif len(obj) < 9:
             return [objform(x) for x in obj]
         else:
-            return [objform(x) for x in obj[:10]] + ['...']
+            return [objform(x) for x in obj[:10]] + ["..."]
     elif type(obj) is dict:
         return dict((k, objform(v)) for (k, v) in obj.items())
     else:
         return str(type(obj))
 
 
-def dict2tree(obj, name=None, prefix=''):
+def dict2tree(obj, name=None, prefix=""):
     """Return abstracted nested tree. Terminals contain TYPE.
     As a special case, a list is given as a dict that represents a
     compound type.  E.G. {'<list(3835)[0]>': float} means a list of
     3835 elements where the first element is of type 'float'.  NB:
     Only the type of the first element in a list is given.  If the
     list has hetergeneous types, that fact is invisible in the
     structure!!
     """
-    nextpfx = '' if name is None else (prefix + name + '.')
+    nextpfx = "" if name is None else (prefix + name + ".")
     showname = prefix if name is None else (prefix + name)
     if isinstance(obj, dict):
         children = dict()
-        for (k, v) in obj.items():
+        for k, v in obj.items():
             if isinstance(v, dict) or isinstance(v, list):
                 val = dict2tree(v, name=k, prefix=nextpfx)
             else:
                 #!val = {k: type(v).__name__}
                 val = {nextpfx + k: type(v).__name__}
             children.update(val)
         tree = children if name is None else {showname: children}
     elif isinstance(obj, list):
-        children = f'<list({len(obj)})[0]>:{type(obj[0]).__name__}'
+        children = f"<list({len(obj)})[0]>:{type(obj[0]).__name__}"
         tree = {showname: children}
     else:
         tree = {showname: type(obj).__name__}
-    return(tree)
+    return tree
 
 
 def invLUT(lut):
     """Given dict[k]=v, Return dict[v]=k"""
     return {v: k for k, v in lut.items()}
 
+
 def count_values(recs):
     """Count number of non-None values in a list of dictionaries.
     A key that exists with a value of None is treated the same as a
     key that does not exist at all. i.e. It does not add to the count.
 
     Args:
        recs (:obj:`list`): ('records') List of dictionaries.
```

## sparcl/benchmarks/benchmarks.py

```diff
@@ -1,172 +1,194 @@
 #! /usr/bin/env python
-'''Benchmark speed of SPARC spectra retrieve with various parameters.
-'''
+"""Benchmark speed of SPARC spectra retrieve with various parameters.
+"""
 # EXAMPLES:
 # cd ~/sandbox/sparclclient
 # python3 -m sparcl.benchmarks.benchmarks ~/data/sparc/sids5.list
 # python3 -m sparcl.benchmarks.benchmarks ~/data/sparc/sids644.list
 
 # Alice reported 22 minutes on 64K retrieved from specClient (rate=48 spec/sec)
 #   slack.spectro: 3/31/2021
 
 # Standard Python library
 import argparse
 import logging
 import os
 from pprint import pformat
+
 # External packages
 import psutil
+
 # Local packages
 from ..client import SparclClient
 from ..utils import tic, toc, here_now
 
-#rooturl = 'http://localhost:8030/' #@@@
-rooturl = 'http://sparc1.datalab.noirlab.edu:8000/'
+# rooturl = 'http://localhost:8030/' #@@@
+rooturl = "http://sparc1.datalab.noirlab.edu:8000/"
 
 
-def human_size(num, units=['b', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB']):
+def human_size(num, units=["b", "KB", "MB", "GB", "TB", "PB", "EB"]):
     """Returns a human readable string representation of NUM."""
-    return (f'{num:.1f} {units[0]}'
-            if num < 1024 else human_size(num / 1000, units[1:]))
+    return (
+        f"{num:.1f} {units[0]}"
+        if num < 1024
+        else human_size(num / 1000, units[1:])
+    )
 
 
 # with open('/data/sparc/sids5.list') as f:
 #      specids = [int(line.strip()) for line in f if not line.startswith('#')]
-def run_retrieve(specids, columns=None, xfer='p', verbose=True):
+def run_retrieve(specids, columns=None, xfer="p", verbose=True):
     #!print(f'Retrieving {len(specids):,} spectra')
     psutil.cpu_percent()  # begin interval
     client = SparclClient(url=rooturl)
     result = dict(numcols=len(columns), numspecids=len(specids))
     if verbose:
-        print(f'Experiment: {pformat(result)}')
+        print(f"Experiment: {pformat(result)}")
     tic()
     data = client.retrieve(specids, columns=columns, xfer=xfer)
     elapsed = toc()
     #!cpu = psutil.cpu_percent(interval=1)
     if verbose:
-        print(f'len(specids)={len(specids)} len(data)={len(data)}')
-    assert len(specids) == len(data)   # @@@ but some of ingest may have failed
-    assert len(data[0]['spectra__coadd__flux']) > 1000
-    result.update(elapsed=elapsed,
-                  retrieved=len(data),
-                  rate=len(data) / elapsed,
-                  end_smrem=psutil.swap_memory().free,
-                  end_vmrem=psutil.virtual_memory().available,
-                  end_cpuload=os.getloadavg()[1],
-                  end_cpuperc=psutil.cpu_percent()  # end interval
-                  )
-    return(result)
+        print(f"len(specids)={len(specids)} len(data)={len(data)}")
+    assert len(specids) == len(data)  # @@@ but some of ingest may have failed
+    assert len(data[0]["spectra__coadd__flux"]) > 1000
+    result.update(
+        elapsed=elapsed,
+        retrieved=len(data),
+        rate=len(data) / elapsed,
+        end_smrem=psutil.swap_memory().free,
+        end_vmrem=psutil.virtual_memory().available,
+        end_cpuload=os.getloadavg()[1],
+        end_cpuperc=psutil.cpu_percent(),  # end interval
+    )
+    return result
 
 
-def run_paged_retrieve(specids, columns=None, xfer='p',
-                       page=5000, verbose=True, keepall=False):
+def run_paged_retrieve(
+    specids, columns=None, xfer="p", page=5000, verbose=True, keepall=False
+):
     """Do 1 more more PAGE size retrieves to get data for all specids"""
-    print(f'Paged Retrieve of {len(specids):,} spectra')
+    print(f"Paged Retrieve of {len(specids):,} spectra")
     psutil.cpu_percent()  # begin interval
     client = SparclClient(url=rooturl)
-    result = dict(numcols=len(columns),
-                  numspecids=len(specids),
-                  xfer=xfer,
-                  page=page)
+    result = dict(
+        numcols=len(columns), numspecids=len(specids), xfer=xfer, page=page
+    )
     if verbose:
-        print(f'Experiment: {pformat(result)}')
+        print(f"Experiment: {pformat(result)}")
 
     data = []
     datacnt = 0
     tic()
     for cnt in range(0, len(specids), page):
-        pdata = client.retrieve(specids[cnt:cnt + page],
-                                columns=columns,
-                                xfer=xfer)
+        pdata = client.retrieve(
+            specids[cnt : cnt + page], columns=columns, xfer=xfer
+        )
         datacnt += len(pdata)
         if keepall:
             data.extend(pdata)
     elapsed = toc()
 
     #! cpu = psutil.cpu_percent(interval=1)
     if verbose:
-        print(f'len(specids)={len(specids)} datacnt={datacnt}')
-    #assert len(specids) == len(data)   # @@@but some of ingest may have failed
+        print(f"len(specids)={len(specids)} datacnt={datacnt}")
+    # assert len(specids) == len(data)   # @@@but some ingests may have failed
     #!assert len(data[0]['spectra__coadd__flux']) > 1000 # @@@
-    result.update(elapsed=elapsed,
-                  retrieved=len(data),
-                  rate=datacnt / elapsed,
-                  end_smrem=psutil.swap_memory().free,
-                  end_vmrem=psutil.virtual_memory().available,
-                  end_cpuload=os.getloadavg()[1],
-                  end_cpuperc=psutil.cpu_percent()  # end interval
-                  )
-    return(result)
+    result.update(
+        elapsed=elapsed,
+        retrieved=len(data),
+        rate=datacnt / elapsed,
+        end_smrem=psutil.swap_memory().free,
+        end_vmrem=psutil.virtual_memory().available,
+        end_cpuload=os.getloadavg()[1],
+        end_cpuperc=psutil.cpu_percent(),  # end interval
+    )
+    return result
 
 
 # flux,loglam,ivar,and_mask,or_mask,wdisp,sky,model
-allcols = ['flux', 'loglam', 'ivar', 'and_mask', 'or_mask',
-           'wdisp', 'sky', 'model']
+allcols = [
+    "flux",
+    "loglam",
+    "ivar",
+    "and_mask",
+    "or_mask",
+    "wdisp",
+    "sky",
+    "model",
+]
 
 experiment_0 = dict(
-    xfers=['p'],
+    xfers=["p"],
     specidcnts=[600, 60],
     numcols=range(1, 3),
 )
 experiment_1 = dict(
-    xfers=['p'],
+    xfers=["p"],
     specidcnts=[6, 60, 600, 6000, 30000],
     numcols=range(1, 3),
-    #numcols=range(1,len(allcols)+1),
+    # numcols=range(1,len(allcols)+1),
 )
 experiment_2 = dict(
-    xfers=['p'],
+    xfers=["p"],
     specidcnts=[1000, 100, 10],
     numcols=range(1, len(allcols) + 1),
 )
 experiment_3 = dict(
-    xfers=['p'],
-    specidcnts=[1000, ],
+    xfers=["p"],
+    specidcnts=[
+        1000,
+    ],
     numcols=reversed(range(1, len(allcols) + 1)),
 )
 
 experiment_8 = dict(
-    xfers=['p', ],
-    specidcnts=[65000, ],
-    numcols=[1, 2, 8]
+    xfers=[
+        "p",
+    ],
+    specidcnts=[
+        65000,
+    ],
+    numcols=[1, 2, 8],
 )
 experiment_9 = dict(
-    xfers=['p', 'j'],
-    specidcnts=sorted(set([min(7 * 10 ** x, 65000) for x in range(6)])),
+    xfers=["p", "j"],
+    specidcnts=sorted(set([min(7 * 10**x, 65000) for x in range(6)])),
     numcols=range(1, len(allcols) + 1),
 )
 
 
 def run_trials(allspecids, verbose=True):
-    #ex = experiment_9 #@@@
+    # ex = experiment_9 #@@@
     ex = experiment_8  # @@@
 
-    xfers = ex['xfers']
-    specidcnts = ex['specidcnts']
-    numcols = ex['numcols']
+    xfers = ex["xfers"]
+    specidcnts = ex["specidcnts"]
+    numcols = ex["numcols"]
 
-    klist = ['elapsed', 'numcols', 'numspecids', 'page', 'rate', 'xfer']
+    klist = ["elapsed", "numcols", "numspecids", "page", "rate", "xfer"]
 
     all = []
     for xfer in xfers:
         for n in numcols:
             cols = allcols[:n]
             for specidcnt in specidcnts:
                 specids = allspecids[:specidcnt]
                 #!result = run_retrieve(specids, columns=cols, xfer='p')
-                result = run_paged_retrieve(specids, columns=cols, xfer='p')
+                result = run_paged_retrieve(specids, columns=cols, xfer="p")
                 if verbose:
-                    #print(f'Run-Result: {pformat(result)}')
-                    reduced = dict((k, result[k])
-                                   for k in result.keys() if k in klist)
-                    print(f'Run-Result: {reduced}')
+                    # print(f'Run-Result: {pformat(result)}')
+                    reduced = dict(
+                        (k, result[k]) for k in result.keys() if k in klist
+                    )
+                    print(f"Run-Result: {reduced}")
                 all.append(result)
     report(all, len(allspecids), xfer=xfer)
-    return(all)
+    return all
 
 
 def report(results, specidcnt, xfer=None, bandwidth=False):
     hostname, now = here_now()
     min1, min5, min15 = os.getloadavg()
     #!smrem = psutil.swap_memory().free
     #!vmrem = psutil.virtual_memory().available
@@ -178,112 +200,138 @@
         #! ul_speed = s.upload(threads=1)
         #! dl_speed = s.download(threads=1)
     else:
         #! ul_speed = 0
         dl_speed = 0
 
     #! Upload speed:    {human_size(ul_speed)}
-    print(f'\nBenchmark run on {hostname} at {now} with {specidcnt} specids.')
-    print(f'''
+    print(f"\nBenchmark run on {hostname} at {now} with {specidcnt} specids.")
+    print(
+        f"""
 Transfer Method: {"Pickle" if xfer=='p' else "JSON"}
 Download speed:  {human_size(dl_speed)}
-    ''')
+    """
+    )
     # Load Avg:        {min5:.1f}
     #         (avg num processes running over last 5 minutes)
     # CPU utilization: {cpuperc:.0f}%
     # Swap Mem Avail:    {human_size(smrem)}
     # Virtual Mem Avail: {human_size(vmrem)}
     # (Above statistics are for CLIENT.)
 
     #!print(f'Column\tSID\tRate \tAvg \tCPU \tSwap\tVirt')
     #!print(f' Count\tCnt\ts/sec\tLoad\tUtil\t Mem\t Mem')
     #!print(f'------\t---\t-----\t----\t----')
-    print(f'Column\tSID\tRate ')
-    print(f' Count\tCnt\ts/sec')
-    print(f'------\t---\t-----')
+    print(f"Column\tSID\tRate ")
+    print(f" Count\tCnt\ts/sec")
+    print(f"------\t---\t-----")
     for r in results:
-        print(("{numcols}\t"
-               "{numspecids}\t"
-               "{rate:.0f}\t"
-               #!"{end_cpuload:.02f}\t"
-               #!"{end_cpuperc:.0f}%\t"
-               #!"{smrem}\t"
-               #!"{vmrem}\t"
-               ).format(**r))
-        #smrem=human_size(r['end_smrem']),
-        #vmrem=human_size(r['end_vmrem']),
-    print('''
+        print(
+            (
+                "{numcols}\t"
+                "{numspecids}\t"
+                "{rate:.0f}\t"
+                #!"{end_cpuload:.02f}\t"
+                #!"{end_cpuperc:.0f}%\t"
+                #!"{smrem}\t"
+                #!"{vmrem}\t"
+            ).format(**r)
+        )
+        # smrem=human_size(r['end_smrem']),
+        # vmrem=human_size(r['end_vmrem']),
+    print(
+        """
 LEGEND:
   Rate:: spectra/second
   Transfer method:: Pickle, Json
   Load:: Number of processes in system run queue averaged over last 5 minutes.
-  ''')
-    return 'Done'
+  """
+    )
+    return "Done"
+
 
 ##############################################################################
 
 
 def my_parser():
     parser = argparse.ArgumentParser(
         #!version='1.0.1',
-        description='My shiny new python program',
-        epilog='EXAMPLE: %(prog)s a b"'
+        description="My shiny new python program",
+        epilog='EXAMPLE: %(prog)s a b"',
     )
-    allcols = ['flux', 'loglam', 'ivar', 'and_mask', 'or_mask',
-               'wdisp', 'sky', 'model']
+    allcols = [
+        "flux",
+        "loglam",
+        "ivar",
+        "and_mask",
+        "or_mask",
+        "wdisp",
+        "sky",
+        "model",
+    ]
     #!dftcols = 'flux,loglam'
-    dftcols = ','.join(allcols)
-    parser.add_argument('specids', type=argparse.FileType('r'),
-                        help=('File containing list of '
-                              'specobjids. One per line.')
-                        )
-    parser.add_argument('--cols',
-                        #choices=allcols,
-                        default=dftcols,
-                        help=(f'List of comma seperated columns to get. '
-                              f'Default="{dftcols}"')
-                        )
-    parser.add_argument('--xfer', default='p',
-                        help='Mode to use to transfer from Server to Client.'
-                        )
-
-    parser.add_argument('--loglevel',
-                        help='Kind of diagnostic output',
-                        choices=['CRTICAL', 'ERROR', 'WARNING',
-                                 'INFO', 'DEBUG'],
-                        default='WARNING',
-                        )
+    dftcols = ",".join(allcols)
+    parser.add_argument(
+        "specids",
+        type=argparse.FileType("r"),
+        help=("File containing list of " "specobjids. One per line."),
+    )
+    parser.add_argument(
+        "--cols",
+        # choices=allcols,
+        default=dftcols,
+        help=(
+            f"List of comma seperated columns to get. " f'Default="{dftcols}"'
+        ),
+    )
+    parser.add_argument(
+        "--xfer",
+        default="p",
+        help="Mode to use to transfer from Server to Client.",
+    )
+
+    parser.add_argument(
+        "--loglevel",
+        help="Kind of diagnostic output",
+        choices=["CRTICAL", "ERROR", "WARNING", "INFO", "DEBUG"],
+        default="WARNING",
+    )
     return parser
 
 
 def main():
     parser = my_parser()
     args = parser.parse_args()
     args.specids.close()
     args.specids = args.specids.name
 
     log_level = getattr(logging, args.loglevel.upper(), None)
     if not isinstance(log_level, int):
-        parser.error('Invalid log level: %s' % args.loglevel)
-    logging.basicConfig(level=log_level,
-                        format='%(levelname)s %(message)s',
-                        datefmt='%m-%d %H:%M'
-                        )
-    logging.debug('Debug output is enabled!!!')
+        parser.error("Invalid log level: %s" % args.loglevel)
+    logging.basicConfig(
+        level=log_level,
+        format="%(levelname)s %(message)s",
+        datefmt="%m-%d %H:%M",
+    )
+    logging.debug("Debug output is enabled!!!")
 
     specids = []
-    with open(args.specids, 'r') as fin:
+    with open(args.specids, "r") as fin:
         for line in fin:
-            if line.startswith('#'):
+            if line.startswith("#"):
                 continue
             specids.append(int(line.strip()))
     #! cols = args.cols.split(',')
-    #print(f'specids count={len(specids)} columns={cols}')
+    # print(f'specids count={len(specids)} columns={cols}')
 
-    #run_retrieve(specids, columns=cols, xfer='p')
-    print(f'Starting benchmark on {here_now()}')
+    # run_retrieve(specids, columns=cols, xfer='p')
+    print(f"Starting benchmark on {here_now()}")
     #! all = run_trials(specids)
-    print(f'Finished benchmark on {here_now()}')
+    print(f"Finished benchmark on {here_now()}")
+
+
+def foo(x):
+    pass
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

## Comparing `sparclclient-1.2.0b4.dev2.dist-info/LICENSE` & `sparclclient-1.2.1.dev0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `sparclclient-1.2.0b4.dev2.dist-info/METADATA` & `sparclclient-1.2.1.dev0.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 Metadata-Version: 2.1
 Name: sparclclient
-Version: 1.2.0b4.dev2
+Version: 1.2.1.dev0
 Summary: A client for getting spectra data from NOIRLab.
 Home-page: https://github.com/astro-datalab/sparclclient
 Author: NOIRLab DataLab
 Author-email: datalab-spectro@noirlab.edu
 Project-URL: Documentation, https://sparclclient.readthedocs.io/en/latest/
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: requests (==2.26.0)
+Requires-Dist: requests (==2.31.0)
 Requires-Dist: numpy
 Requires-Dist: spectres
 
 # sparclclient
 Python Client for SPARC (SPectra Analysis and Retrievable Catalog Lab)
```

## Comparing `sparclclient-1.2.0b4.dev2.dist-info/RECORD` & `sparclclient-1.2.1.dev0.dist-info/RECORD`

 * *Files 27% similar despite different names*

```diff
@@ -1,20 +1,18 @@
-sparcl/Results.py,sha256=S4jQcXnw6qfdIEL-NEsCqMK6LolFaERGxjNWIvNhACM,8039
-sparcl/__init__.py,sha256=wFUVztoapp5PN6H41TPgeUnX0l209vCnDScmG0xuF6Y,1035
-sparcl/big_retrieve.py,sha256=q0ScH87QqPL4bz4g0hB0AO3k4c_TiuQrWjBJHqHhE60,798
-sparcl/client.py,sha256=4WLo8MAK3HXpHCU2tsmVXnA0HL2RTxt51Gy8a9pBEPA,31709
-sparcl/conf.py,sha256=O9l4-vpWBZK0QjhHxjskGO8kHPxBj7mkWlchd2rot1c,953
-sparcl/dls_376.py,sha256=WvZjuZFRU0jgH3ELRrMQdslkMWiF2wFQrSag0cYii-I,887
-sparcl/exceptions.py,sha256=q7ONsLsop9OQJJCD4SEzfdsojv0yo3WQT0SluaxGOQ0,3813
-sparcl/fields.py,sha256=7MpaJQr2d1GktS7aeM4010jyLqDdKQ7BZIF9hM0IjII,5002
-sparcl/gather_2d.py,sha256=ZRr41vNHV4tnf63-QuTu04SlWv6TOzK-CeHpbt9YwOY,9254
-sparcl/resample_spectra.py,sha256=2MO-sDCCFg2eNiK6jQs2EJRu4bNnXycGV8WaOydssG4,1329
-sparcl/type_conversion.py,sha256=RX7OD1iGuuUrf-yAd0ISdiqBq4CP7QlCw0vvkAdHdsQ,13112
-sparcl/unsupported.py,sha256=vkSaK3Ppcxx6mMsqBktUjI0uS7RwBJYH2BkBABsnyIM,1867
-sparcl/utils.py,sha256=YlLUP0j4thUyEwTJAaqJ7zzsvbCxPe5EYTn9kvWGfBY,4682
+sparcl/Results.py,sha256=GGHLPUy5vnKbzZqNaE5aFUlTtj6NH4cOt4_QAt9ojQg,8107
+sparcl/__init__.py,sha256=zqbzbiXwR53jklREe24C6cRWJfsp0cQjCDoTwYDzOk8,1108
+sparcl/client.py,sha256=YHxKyndkDP8zVVQYv3Jg1fJoYPqzri0dy4kTTKxvTbY,31551
+sparcl/conf.py,sha256=GFNDelaiVIAkjNjvFlG7HAlPpU39nqZmTPohQGmOcgI,928
+sparcl/exceptions.py,sha256=QfK3aTl1HTQmbhUCRfW9eo-rzEA1yUawKGtv_ca4nvY,3801
+sparcl/fields.py,sha256=Oef3KNmWqsqi8BPXuQA7EazLtwgfkqprbrPxev8sCio,5126
+sparcl/gather_2d.py,sha256=mTM9YhsGf2CMOQ-CE4-JcAs1PhA0mVKlciy9dKnp67o,8685
+sparcl/resample_spectra.py,sha256=Z_Lkoq4LapaIQp7tqZ88LayRLE8o9c832Icc0jSr5ok,1282
+sparcl/type_conversion.py,sha256=QmXNX9j_7QHnBu83f2ZBfREoql9wuo98ZbhQtSjRRWc,12965
+sparcl/unsupported.py,sha256=bfkkZa-PuqwN-Bqo3vCIrLupbWMDTCiTHPMNfXnqmMc,1848
+sparcl/utils.py,sha256=O_vX9msSBEy7BgI0do0PAUatgl7cqHv0Z5n-uHnzOh8,4685
 sparcl/benchmarks/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-sparcl/benchmarks/benchmarks.py,sha256=FPZ2KExfVWHhGt3B4VyfgOhxxsemj7OeBWJO0dyDDC4,9667
-sparclclient-1.2.0b4.dev2.dist-info/LICENSE,sha256=y10EluGMCzGs9X4oYCYyix3l6u-lawB_vlGR8qe442Q,1576
-sparclclient-1.2.0b4.dev2.dist-info/METADATA,sha256=rnakmWYHqFnnhnJrXbhu1nMPWpc_9y3XbzfF1YIYoIo,872
-sparclclient-1.2.0b4.dev2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-sparclclient-1.2.0b4.dev2.dist-info/top_level.txt,sha256=d5CZ3Duxq3MyQTB2ZqOrdtSBv4GdVceF-pOZFmsuHZY,7
-sparclclient-1.2.0b4.dev2.dist-info/RECORD,,
+sparcl/benchmarks/benchmarks.py,sha256=OmlSdnAPLmcvGXsr-HzGyfAAcnoqlO0JQ4EIA7JGhZc,9424
+sparclclient-1.2.1.dev0.dist-info/LICENSE,sha256=y10EluGMCzGs9X4oYCYyix3l6u-lawB_vlGR8qe442Q,1576
+sparclclient-1.2.1.dev0.dist-info/METADATA,sha256=6i6RFXFfid5EnMIRf6raYnvGpEMzJWYYlG1zknyWryE,870
+sparclclient-1.2.1.dev0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+sparclclient-1.2.1.dev0.dist-info/top_level.txt,sha256=d5CZ3Duxq3MyQTB2ZqOrdtSBv4GdVceF-pOZFmsuHZY,7
+sparclclient-1.2.1.dev0.dist-info/RECORD,,
```

