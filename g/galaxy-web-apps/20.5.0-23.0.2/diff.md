# Comparing `tmp/galaxy-web-apps-20.5.0.tar.gz` & `tmp/galaxy-web-apps-23.0.2.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/galaxy-web-apps-20.5.0.tar", last modified: Sat Jul  4 15:44:37 2020, max compression
+gzip compressed data, was "/home/runner/work/galaxy/galaxy/packages/web_apps/dist/.tmp-jjupeqpe/galaxy-web-apps-23.0.2.tar", last modified: Tue Jun 13 17:13:40 2023, max compression
```

## Comparing `galaxy-web-apps-20.5.0.tar` & `galaxy-web-apps-23.0.2.tar`

### file list

```diff
@@ -1,574 +1,794 @@
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/
--rw-r--r--   0 john       (502) staff       (20)       66 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/dev-requirements.txt
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/
--rw-r--r--   0 john       (502) staff       (20)       65 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/__init__.py
--rw-r--r--   0 john       (502) staff       (20)      411 2020-07-04 15:44:35.000000 galaxy-web-apps-20.5.0/galaxy/project_galaxy_web_apps.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/webapps/
--rw-r--r--   0 john       (502) staff       (20)      123 2019-02-22 18:09:10.000000 galaxy-web-apps-20.5.0/galaxy/webapps/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/webapps/base/
--rw-r--r--   0 john       (502) staff       (20)        0 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/base/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    77440 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/base/controller.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/__init__.py
--rw-r--r--   0 john       (502) staff       (20)     9200 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/_fetch_util.py
--rw-r--r--   0 john       (502) staff       (20)     3049 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/annotations.py
--rw-r--r--   0 john       (502) staff       (20)     3649 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/authenticate.py
--rw-r--r--   0 john       (502) staff       (20)    12454 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/cloud.py
--rw-r--r--   0 john       (502) staff       (20)    12763 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/cloudauthz.py
--rw-r--r--   0 john       (502) staff       (20)     4934 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/configuration.py
--rw-r--r--   0 john       (502) staff       (20)     4505 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/container_resolution.py
--rw-r--r--   0 john       (502) staff       (20)     3299 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/dataset_collections.py
--rw-r--r--   0 john       (502) staff       (20)    21811 2020-03-13 22:37:26.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/datasets.py
--rw-r--r--   0 john       (502) staff       (20)     4503 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/datatypes.py
--rw-r--r--   0 john       (502) staff       (20)     2276 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/display_applications.py
--rw-r--r--   0 john       (502) staff       (20)     2954 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/dynamic_tools.py
--rw-r--r--   0 john       (502) staff       (20)     3403 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/extended_metadata.py
--rw-r--r--   0 john       (502) staff       (20)    16480 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/folder_contents.py
--rw-r--r--   0 john       (502) staff       (20)    14123 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/folders.py
--rw-r--r--   0 john       (502) staff       (20)     3119 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/forms.py
--rw-r--r--   0 john       (502) staff       (20)     2532 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/genomes.py
--rw-r--r--   0 john       (502) staff       (20)     5174 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/group_roles.py
--rw-r--r--   0 john       (502) staff       (20)     5190 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/group_users.py
--rw-r--r--   0 john       (502) staff       (20)     5197 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/groups.py
--rw-r--r--   0 john       (502) staff       (20)    22975 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/histories.py
--rw-r--r--   0 john       (502) staff       (20)    51487 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/history_contents.py
--rw-r--r--   0 john       (502) staff       (20)     2431 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/item_tags.py
--rw-r--r--   0 john       (502) staff       (20)     7416 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/job_files.py
--rw-r--r--   0 john       (502) staff       (20)     1593 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/job_ports.py
--rw-r--r--   0 john       (502) staff       (20)    19312 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/jobs.py
--rw-r--r--   0 john       (502) staff       (20)    18010 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/libraries.py
--rw-r--r--   0 john       (502) staff       (20)    23817 2020-01-29 19:05:09.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/library_contents.py
--rw-r--r--   0 john       (502) staff       (20)    39948 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/library_datasets.py
--rw-r--r--   0 john       (502) staff       (20)     3707 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/metrics.py
--rw-r--r--   0 john       (502) staff       (20)     2297 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/page_revisions.py
--rw-r--r--   0 john       (502) staff       (20)     5540 2020-01-29 19:05:09.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/pages.py
--rw-r--r--   0 john       (502) staff       (20)     1971 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/plugins.py
--rw-r--r--   0 john       (502) staff       (20)     3773 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/provenance.py
--rw-r--r--   0 john       (502) staff       (20)     5360 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/quotas.py
--rw-r--r--   0 john       (502) staff       (20)     8616 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/remote_files.py
--rw-r--r--   0 john       (502) staff       (20)     3686 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/roles.py
--rw-r--r--   0 john       (502) staff       (20)     3052 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/search.py
--rw-r--r--   0 john       (502) staff       (20)     1614 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tags.py
--rw-r--r--   0 john       (502) staff       (20)     4344 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tool_data.py
--rw-r--r--   0 john       (502) staff       (20)    12243 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tool_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)     3090 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tool_entry_points.py
--rw-r--r--   0 john       (502) staff       (20)    25167 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tool_shed_repositories.py
--rw-r--r--   0 john       (502) staff       (20)    26000 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tools.py
--rw-r--r--   0 john       (502) staff       (20)     1857 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/toolshed.py
--rw-r--r--   0 john       (502) staff       (20)     1356 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tours.py
--rw-r--r--   0 john       (502) staff       (20)     1960 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/uploads.py
--rw-r--r--   0 john       (502) staff       (20)    41875 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/users.py
--rw-r--r--   0 john       (502) staff       (20)     9229 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/visualizations.py
--rw-r--r--   0 john       (502) staff       (20)     1287 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/webhooks.py
--rw-r--r--   0 john       (502) staff       (20)    53465 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/workflows.py
--rw-r--r--   0 john       (502) staff       (20)    67162 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/buildapp.py
--rw-r--r--   0 john       (502) staff       (20)   121967 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/config_schema.yml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/
--rw-r--r--   0 john       (502) staff       (20)       32 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/__init__.py
--rw-r--r--   0 john       (502) staff       (20)     9711 2019-06-10 19:11:09.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/_create_history_template.py
--rw-r--r--   0 john       (502) staff       (20)    83725 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/admin.py
--rw-r--r--   0 john       (502) staff       (20)    92478 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/admin_toolshed.py
--rw-r--r--   0 john       (502) staff       (20)     7893 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/async.py
--rw-r--r--   0 john       (502) staff       (20)     6717 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/authnz.py
--rw-r--r--   0 john       (502) staff       (20)    11130 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/data_manager.py
--rw-r--r--   0 john       (502) staff       (20)    61568 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/dataset.py
--rw-r--r--   0 john       (502) staff       (20)      180 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/error.py
--rw-r--r--   0 john       (502) staff       (20)    14616 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/forms.py
--rw-r--r--   0 john       (502) staff       (20)    66538 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/history.py
--rw-r--r--   0 john       (502) staff       (20)     2092 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/interactive_environments.py
--rw-r--r--   0 john       (502) staff       (20)     2202 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/interactivetool.py
--rw-r--r--   0 john       (502) staff       (20)      628 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/library.py
--rw-r--r--   0 john       (502) staff       (20)     3718 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/openid.py
--rw-r--r--   0 john       (502) staff       (20)    31334 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/page.py
--rw-r--r--   0 john       (502) staff       (20)    19376 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/root.py
--rw-r--r--   0 john       (502) staff       (20)     2306 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/shed_tool_static.py
--rw-r--r--   0 john       (502) staff       (20)     9636 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/tag.py
--rw-r--r--   0 john       (502) staff       (20)     7192 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/tool_runner.py
--rw-r--r--   0 john       (502) staff       (20)    17565 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/user.py
--rw-r--r--   0 john       (502) staff       (20)     1702 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/userskeys.py
--rw-r--r--   0 john       (502) staff       (20)    43278 2020-01-29 19:05:09.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/visualization.py
--rw-r--r--   0 john       (502) staff       (20)    45913 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/workflow.py
--rw-r--r--   0 john       (502) staff       (20)     2601 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/job_config_schema.yml
--rw-r--r--   0 john       (502) staff       (20)   113171 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/uwsgi_schema.yml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/
--rw-r--r--   0 john       (502) staff       (20)      164 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/__init__.py
--rw-r--r--   0 john       (502) staff       (20)     1725 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/app.py
--rw-r--r--   0 john       (502) staff       (20)     5765 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/buildapp.py
--rw-r--r--   0 john       (502) staff       (20)     4482 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/config.py
--rw-r--r--   0 john       (502) staff       (20)     2527 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/config_schema.yml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/
--rw-r--r--   0 john       (502) staff       (20)       34 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/__init__.py
--rw-r--r--   0 john       (502) staff       (20)     8883 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/history.py
--rw-r--r--   0 john       (502) staff       (20)     4871 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/home.py
--rw-r--r--   0 john       (502) staff       (20)    45351 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/jobs.py
--rw-r--r--   0 john       (502) staff       (20)      962 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/query.py
--rw-r--r--   0 john       (502) staff       (20)      279 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/root.py
--rw-r--r--   0 john       (502) staff       (20)    10154 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/system.py
--rw-r--r--   0 john       (502) staff       (20)    15762 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/tools.py
--rw-r--r--   0 john       (502) staff       (20)    10130 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/users.py
--rw-r--r--   0 john       (502) staff       (20)    19620 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/workflows.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/framework/
--rw-r--r--   0 john       (502) staff       (20)        0 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/framework/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    38552 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/framework/grids.py
--rw-r--r--   0 john       (502) staff       (20)   113171 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/galaxy/webapps/reports/uwsgi_schema.yml
--rw-r--r--   0 john       (502) staff       (20)     2947 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/galaxy/webapps/util.py
--rw-r--r--   0 john       (502) staff       (20)   113171 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/galaxy/webapps/uwsgi_schema.yml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/galaxy_web_apps.egg-info/
--rw-r--r--   0 john       (502) staff       (20)        1 2020-07-04 15:44:35.000000 galaxy-web-apps-20.5.0/galaxy_web_apps.egg-info/dependency_links.txt
--rw-r--r--   0 john       (502) staff       (20)       27 2020-07-04 15:44:35.000000 galaxy-web-apps-20.5.0/galaxy_web_apps.egg-info/entry_points.txt
--rw-r--r--   0 john       (502) staff       (20)        1 2020-07-04 15:44:35.000000 galaxy-web-apps-20.5.0/galaxy_web_apps.egg-info/not-zip-safe
--rw-r--r--   0 john       (502) staff       (20)     1686 2020-07-04 15:44:35.000000 galaxy-web-apps-20.5.0/galaxy_web_apps.egg-info/PKG-INFO
--rw-r--r--   0 john       (502) staff       (20)       11 2020-07-04 15:44:35.000000 galaxy-web-apps-20.5.0/galaxy_web_apps.egg-info/requires.txt
--rw-r--r--   0 john       (502) staff       (20)    25304 2020-07-04 15:44:36.000000 galaxy-web-apps-20.5.0/galaxy_web_apps.egg-info/SOURCES.txt
--rw-r--r--   0 john       (502) staff       (20)       17 2020-07-04 15:44:35.000000 galaxy-web-apps-20.5.0/galaxy_web_apps.egg-info/top_level.txt
--rw-r--r--   0 john       (502) staff       (20)      158 2020-07-04 15:44:35.000000 galaxy-web-apps-20.5.0/HISTORY.rst
--rw-r--r--   0 john       (502) staff       (20)    11109 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/LICENSE
--rw-r--r--   0 john       (502) staff       (20)     2917 2020-07-04 13:55:08.000000 galaxy-web-apps-20.5.0/Makefile
--rw-r--r--   0 john       (502) staff       (20)       22 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/MANIFEST.in
--rw-r--r--   0 john       (502) staff       (20)     1686 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/PKG-INFO
--rw-r--r--   0 john       (502) staff       (20)      326 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/README.rst
--rw-r--r--   0 john       (502) staff       (20)       12 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/requirements.txt
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/scripts/
--rw-r--r--   0 john       (502) staff       (20)     1466 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/scripts/commit_version.py
--rw-r--r--   0 john       (502) staff       (20)     2182 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/scripts/new_version.py
--rw-r--r--   0 john       (502) staff       (20)      843 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/scripts/print_version_for_release.py
--rw-r--r--   0 john       (502) staff       (20)      372 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/setup.cfg
--rw-r--r--   0 john       (502) staff       (20)     3758 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/setup.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    10082 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/attribute_handlers.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/repository/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/repository/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    31760 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/repository/relation_builder.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/tool/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/tool/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    10133 2019-02-22 18:09:10.000000 galaxy-web-apps-20.5.0/tool_shed/dependencies/tool/tag_attribute_handler.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/galaxy_install/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/galaxy_install/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    42327 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/galaxy_install/dependency_display.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/galaxy_install/grids/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/galaxy_install/grids/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    11789 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/tool_shed/galaxy_install/grids/admin_toolshed_grids.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/galaxy_install/utility_containers/
--rw-r--r--   0 john       (502) staff       (20)     8724 2019-02-22 18:09:10.000000 galaxy-web-apps-20.5.0/tool_shed/galaxy_install/utility_containers/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/grids/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/grids/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    21350 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/grids/admin_grids.py
--rw-r--r--   0 john       (502) staff       (20)     2087 2019-02-22 18:09:10.000000 galaxy-web-apps-20.5.0/tool_shed/grids/repository_grid_filter_manager.py
--rw-r--r--   0 john       (502) staff       (20)    77691 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/grids/repository_grids.py
--rw-r--r--   0 john       (502) staff       (20)    23855 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/grids/repository_review_grids.py
--rw-r--r--   0 john       (502) staff       (20)     9526 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/grids/util.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/managers/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/managers/__init__.py
--rw-r--r--   0 john       (502) staff       (20)     4529 2019-02-26 19:28:27.000000 galaxy-web-apps-20.5.0/tool_shed/managers/groups.py
--rw-r--r--   0 john       (502) staff       (20)     2144 2019-02-22 18:09:10.000000 galaxy-web-apps-20.5.0/tool_shed/managers/repositories.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/metadata/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/metadata/__init__.py
--rw-r--r--   0 john       (502) staff       (20)      110 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/metadata/metadata_generator.py
--rw-r--r--   0 john       (502) staff       (20)    63884 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/metadata/repository_metadata_manager.py
--rw-r--r--   0 john       (502) staff       (20)    24336 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/repository_registry.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/repository_types/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/repository_types/__init__.py
--rw-r--r--   0 john       (502) staff       (20)      596 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/repository_types/metadata.py
--rw-r--r--   0 john       (502) staff       (20)      736 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/tool_shed/repository_types/registry.py
--rw-r--r--   0 john       (502) staff       (20)     1557 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/repository_types/repository_suite_definition.py
--rw-r--r--   0 john       (502) staff       (20)     1527 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/repository_types/tool_dependency_definition.py
--rw-r--r--   0 john       (502) staff       (20)      630 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/repository_types/unrestricted.py
--rw-r--r--   0 john       (502) staff       (20)     3113 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/repository_types/util.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/
--rw-r--r--   0 john       (502) staff       (20)       33 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/base/
--rw-r--r--   0 john       (502) staff       (20)        0 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/base/__init__.py
--rw-r--r--   0 john       (502) staff       (20)      997 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/base/common.py
--rw-r--r--   0 john       (502) staff       (20)     9724 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/base/test_db_util.py
--rw-r--r--   0 john       (502) staff       (20)    88846 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/test/base/twilltestcase.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/
--rw-r--r--   0 john       (502) staff       (20)        0 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    24866 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0000_basic_repository_features.py
--rw-r--r--   0 john       (502) staff       (20)    10378 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0010_repository_with_tool_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)     6923 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0020_basic_repository_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    12226 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0030_repository_dependency_revisions.py
--rw-r--r--   0 john       (502) staff       (20)     8533 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0040_repository_circular_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    19205 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0050_circular_dependencies_4_levels.py
--rw-r--r--   0 john       (502) staff       (20)     4434 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0070_invalid_tool.py
--rw-r--r--   0 john       (502) staff       (20)     6964 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0080_advanced_circular_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    13153 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0090_tool_search.py
--rw-r--r--   0 john       (502) staff       (20)    15761 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0100_complex_repository_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)     9305 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0110_invalid_simple_repository_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)     8275 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0120_simple_repository_dependency_multiple_owners.py
--rw-r--r--   0 john       (502) staff       (20)     4688 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0130_datatype_converters.py
--rw-r--r--   0 john       (502) staff       (20)     4328 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0140_tool_help_images.py
--rw-r--r--   0 john       (502) staff       (20)     6107 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0150_prior_installation_required.py
--rw-r--r--   0 john       (502) staff       (20)    10055 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0160_circular_prior_installation_required.py
--rw-r--r--   0 john       (502) staff       (20)     9069 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0170_complex_prior_installation_required.py
--rw-r--r--   0 john       (502) staff       (20)    41936 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0300_reset_all_metadata.py
--rw-r--r--   0 john       (502) staff       (20)     4589 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0310_hg_api_features.py
--rw-r--r--   0 john       (502) staff       (20)    33906 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0400_repository_component_reviews.py
--rw-r--r--   0 john       (502) staff       (20)    12317 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0410_repository_component_review_access_control.py
--rw-r--r--   0 john       (502) staff       (20)    14468 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0420_citable_urls_for_repositories.py
--rw-r--r--   0 john       (502) staff       (20)    10732 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0430_browse_utilities.py
--rw-r--r--   0 john       (502) staff       (20)    24858 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0440_deleting_dependency_definitions.py
--rw-r--r--   0 john       (502) staff       (20)    31906 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0460_upload_to_repository.py
--rw-r--r--   0 john       (502) staff       (20)    17514 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0470_tool_dependency_repository_type.py
--rw-r--r--   0 john       (502) staff       (20)     4185 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0480_tool_dependency_xml_verification.py
--rw-r--r--   0 john       (502) staff       (20)     9832 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0530_repository_admin_feature.py
--rw-r--r--   0 john       (502) staff       (20)     8805 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0540_get_all_metadata_from_api.py
--rw-r--r--   0 john       (502) staff       (20)     9848 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_0550_metadata_updated_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)     8151 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1000_install_basic_repository.py
--rw-r--r--   0 john       (502) staff       (20)     8800 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1010_install_repository_with_tool_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)     9330 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1020_install_repository_with_repository_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    11843 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1030_install_repository_with_dependency_revisions.py
--rw-r--r--   0 john       (502) staff       (20)    11255 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1040_install_repository_basic_circular_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    27756 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1050_circular_dependencies_4_levels.py
--rw-r--r--   0 john       (502) staff       (20)     5830 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1070_invalid_tool.py
--rw-r--r--   0 john       (502) staff       (20)    27551 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1080_advanced_circular_dependency_installation.py
--rw-r--r--   0 john       (502) staff       (20)    12809 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1090_repository_dependency_handling.py
--rw-r--r--   0 john       (502) staff       (20)     8098 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1100_install_updated_repository_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    20031 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1120_install_repository_with_complex_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    11599 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1130_install_repository_with_invalid_repository_dependency.py
--rw-r--r--   0 john       (502) staff       (20)    11908 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1140_simple_repository_dependency_multiple_owners.py
--rw-r--r--   0 john       (502) staff       (20)     5515 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1150_datatype_converters.py
--rw-r--r--   0 john       (502) staff       (20)     4255 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1160_tool_help_images.py
--rw-r--r--   0 john       (502) staff       (20)     8748 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1170_prior_installation_required.py
--rw-r--r--   0 john       (502) staff       (20)    16994 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1180_circular_prior_installation_required.py
--rw-r--r--   0 john       (502) staff       (20)    12117 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1190_complex_prior_installation_required.py
--rw-r--r--   0 john       (502) staff       (20)     8613 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1200_uninstall_and_reinstall_basic_repository.py
--rw-r--r--   0 john       (502) staff       (20)     9886 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1210_uninstall_reinstall_repository_with_tool_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    10258 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1220_uninstall_reinstall_repository_with_repository_dependencies.py
--rw-r--r--   0 john       (502) staff       (20)    13567 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1230_uninstall_reinstall_repository_with_dependency_revisions.py
--rw-r--r--   0 john       (502) staff       (20)    33860 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1300_reset_all_metadata.py
--rw-r--r--   0 john       (502) staff       (20)     6801 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1410_update_manager.py
--rw-r--r--   0 john       (502) staff       (20)    22942 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1420_tool_dependency_environment_inheritance.py
--rw-r--r--   0 john       (502) staff       (20)     8289 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1430_repair_installed_repository.py
--rw-r--r--   0 john       (502) staff       (20)     7605 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1440_missing_env_sh_files.py
--rw-r--r--   0 john       (502) staff       (20)    11140 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1450_installing_datatypes_sniffers.py
--rw-r--r--   0 john       (502) staff       (20)     5079 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1460_data_managers.py
--rw-r--r--   0 john       (502) staff       (20)     7600 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1470_updating_installed_repositories.py
--rw-r--r--   0 john       (502) staff       (20)     9376 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/test/functional_tests.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0460_files/
--rw-r--r--   0 john       (502) staff       (20)      129 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0460_files/repository_dependencies.xml
--rw-r--r--   0 john       (502) staff       (20)     2048 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0460_files/repository_dependencies_in_root.tar
--rw-r--r--   0 john       (502) staff       (20)     2560 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0460_files/repository_dependencies_in_subfolder.tar
--rw-r--r--   0 john       (502) staff       (20)      181 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0460_files/tool_dependencies.xml
--rw-r--r--   0 john       (502) staff       (20)     2048 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0460_files/tool_dependencies_in_root.tar
--rw-r--r--   0 john       (502) staff       (20)     2560 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0460_files/tool_dependencies_in_subfolder.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0480_files/
--rw-r--r--   0 john       (502) staff       (20)      519 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0480_files/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/column_maker/
--rw-r--r--   0 john       (502) staff       (20)    10240 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/column_maker/column_maker.tar
--rw-r--r--   0 john       (502) staff       (20)      161 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/column_maker/repository_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/convert_chars/
--rw-r--r--   0 john       (502) staff       (20)    10240 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/convert_chars/convert_chars.tar
--rw-r--r--   0 john       (502) staff       (20)      182 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/convert_chars/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/package_bwa/
--rw-r--r--   0 john       (502) staff       (20)      483 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0540_files/package_bwa/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0550_files/
--rw-r--r--   0 john       (502) staff       (20)     3650 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0550_files/filtering_1.0.tgz
--rw-r--r--   0 john       (502) staff       (20)      452 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0550_files/package_freebayes_1_0550.tgz
--rw-r--r--   0 john       (502) staff       (20)      461 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0550_files/package_freebayes_2_0550.tgz
--rw-r--r--   0 john       (502) staff       (20)      407 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0550_files/package_samtools_1_0550.tgz
--rw-r--r--   0 john       (502) staff       (20)      419 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0550_files/package_samtools_2_0550.tgz
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0550_files/temp/
--rw-r--r--   0 john       (502) staff       (20)      583 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/0550_files/temp/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/binary_tarballs/
--rw-r--r--   0 john       (502) staff       (20)     2560 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/binary_tarballs/atlas.tar
--rw-r--r--   0 john       (502) staff       (20)     2560 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/binary_tarballs/boost.tar
--rw-r--r--   0 john       (502) staff       (20)     2560 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/binary_tarballs/bzlib.tar
--rw-r--r--   0 john       (502) staff       (20)     2560 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/binary_tarballs/lapack.tar
--rw-r--r--   0 john       (502) staff       (20)     2560 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/binary_tarballs/numpy.tar
--rw-r--r--   0 john       (502) staff       (20)     2560 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/binary_tarballs/rdkit.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_atlas_3_10_1420/
--rw-r--r--   0 john       (502) staff       (20)     1320 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_atlas_3_10_1420/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_boost_1_53_1420/
--rw-r--r--   0 john       (502) staff       (20)     1496 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_boost_1_53_1420/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_bzlib_1_0_1420/
--rw-r--r--   0 john       (502) staff       (20)     1263 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_bzlib_1_0_1420/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_lapack_3_4_1420/
--rw-r--r--   0 john       (502) staff       (20)      815 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_lapack_3_4_1420/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_numpy_1_7_1420/
--rw-r--r--   0 john       (502) staff       (20)     2131 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_numpy_1_7_1420/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_rdkit_2012_12_1420/
--rw-r--r--   0 john       (502) staff       (20)     2397 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1420_files/package_rdkit_2012_12_1420/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1440_files/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1440_files/complex_dependency/
--rw-r--r--   0 john       (502) staff       (20)      239 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1440_files/complex_dependency/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1440_files/dependency_definition/
--rw-r--r--   0 john       (502) staff       (20)      503 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1440_files/dependency_definition/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1460_files/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1460_files/data_manager_files/
--rw-r--r--   0 john       (502) staff       (20)     1932 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/1460_files/data_manager_files/test_data_manager.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bed_to_gff_converter/
--rw-r--r--   0 john       (502) staff       (20)    10240 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bed_to_gff_converter/bed_to_gff_converter.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bismark/
--rw-r--r--   0 john       (502) staff       (20)   593920 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bismark/bismark.tar
--rw-r--r--   0 john       (502) staff       (20)    14801 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bismark/bismark_methylation_extractor.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/blast/
--rw-r--r--   0 john       (502) staff       (20)    30720 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/blast/blast_datatypes.tar
--rw-r--r--   0 john       (502) staff       (20)    20480 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/blast/blastxml_to_top_descr.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bowtie2_loc_sample/
--rwxr-xr-x   0 john       (502) staff       (20)     1712 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bowtie2_loc_sample/bowtie2_indices.loc.sample
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bwa/
--rw-r--r--   0 john       (502) staff       (20)    58880 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bwa/bwa_base.tar
--rw-r--r--   0 john       (502) staff       (20)    60416 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bwa/bwa_color.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bwa/complex/
--rw-r--r--   0 john       (502) staff       (20)    57344 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bwa/complex/bwa_base.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bwa/complex/readme/
--rw-r--r--   0 john       (502) staff       (20)      591 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bwa/complex/readme/tool_dependencies.xml
--rw-r--r--   0 john       (502) staff       (20)      483 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/bwa/complex/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/column_maker/
--rw-r--r--   0 john       (502) staff       (20)    10240 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/column_maker/column_maker.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/convert_chars/
--rw-r--r--   0 john       (502) staff       (20)    10240 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/convert_chars/convert_chars.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/0470_files/
--rw-r--r--   0 john       (502) staff       (20)    11776 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/0470_files/emboss_complex_dependency.tar
--rw-r--r--   0 john       (502) staff       (20)      252 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/0470_files/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/datatypes/
--rw-r--r--   0 john       (502) staff       (20)     9011 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/datatypes/datatypes_conf.xml
--rw-r--r--   0 john       (502) staff       (20)    20480 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/emboss.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/emboss_5_0_0/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/emboss_5_0_0/first_tool_dependency/
--rw-r--r--   0 john       (502) staff       (20)     2514 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/emboss_5_0_0/first_tool_dependency/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/emboss_5_0_0/second_tool_dependency/
--rw-r--r--   0 john       (502) staff       (20)     2562 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/emboss_5_0_0/second_tool_dependency/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/libx11_proto/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/libx11_proto/first_tool_dependency/
--rw-r--r--   0 john       (502) staff       (20)     1659 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/libx11_proto/first_tool_dependency/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/libx11_proto/second_tool_dependency/
--rw-r--r--   0 john       (502) staff       (20)     1702 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/emboss/libx11_proto/second_tool_dependency/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering/
--rw-r--r--   0 john       (502) staff       (20)       32 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering/filtering_0000.txt
--rw-r--r--   0 john       (502) staff       (20)    20480 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering/filtering_1.1.0.tar
--rw-r--r--   0 john       (502) staff       (20)    20480 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering/filtering_2.2.0.tar
--rw-r--r--   0 john       (502) staff       (20)    40960 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering/filtering_test_data.tar
--rw-r--r--   0 john       (502) staff       (20)       28 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering/README
--rw-r--r--   0 john       (502) staff       (20)       95 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering/readme.txt
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering_workflow/
--rw-r--r--   0 john       (502) staff       (20)     1863 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/filtering_workflow/Workflow_for_0060_filter_workflow_repository.ga
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/
--rw-r--r--   0 john       (502) staff       (20)    51200 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/freebayes.tar
--rw-r--r--   0 john       (502) staff       (20)    40806 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/freebayes.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/invalid_tool_dependencies/
--rw-r--r--   0 john       (502) staff       (20)     2205 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/invalid_tool_dependencies/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/malformed_tool_dependencies/
--rw-r--r--   0 john       (502) staff       (20)     2168 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/malformed_tool_dependencies/tool_dependencies.xml
--rw-r--r--   0 john       (502) staff       (20)     1209 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/sam_fa_indices.loc.sample
--rw-r--r--   0 john       (502) staff       (20)      386 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/tool_data_table_conf.xml.sample
--rw-r--r--   0 john       (502) staff       (20)     1210 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/freebayes/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/htseq_count/
--rw-r--r--   0 john       (502) staff       (20)   634880 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/htseq_count/htseq_count.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/package_matplotlib/
--rw-r--r--   0 john       (502) staff       (20)    10240 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/package_matplotlib/package_matplotlib_1_2.tar
--rw-r--r--   0 john       (502) staff       (20)     1658 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/package_matplotlib/tool_dependencies.xml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/package_numpy/
--rw-r--r--   0 john       (502) staff       (20)    10240 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/package_numpy/package_numpy_1_7.tar
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/proteomics_datatypes/
--rw-r--r--   0 john       (502) staff       (20)    20480 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/proteomics_datatypes/proteomics_datatypes.tar
--rw-r--r--   0 john       (502) staff       (20)       23 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/test/test_data/readme.txt
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/tools/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/tools/__init__.py
--rw-r--r--   0 john       (502) staff       (20)       75 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/tools/data_table_manager.py
--rw-r--r--   0 john       (502) staff       (20)    14158 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/tools/tool_validator.py
--rw-r--r--   0 john       (502) staff       (20)     3226 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/tools/tool_version_manager.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/util/
--rw-r--r--   0 john       (502) staff       (20)        0 2017-05-02 18:51:14.000000 galaxy-web-apps-20.5.0/tool_shed/util/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    53042 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/util/admin_util.py
--rw-r--r--   0 john       (502) staff       (20)       66 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/util/basic_util.py
--rw-r--r--   0 john       (502) staff       (20)    12002 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/util/commit_util.py
--rw-r--r--   0 john       (502) staff       (20)       67 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/util/common_util.py
--rw-r--r--   0 john       (502) staff       (20)      311 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/util/container_util.py
--rw-r--r--   0 john       (502) staff       (20)       69 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/util/encoding_util.py
--rw-r--r--   0 john       (502) staff       (20)    10575 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/util/hg_util.py
--rw-r--r--   0 john       (502) staff       (20)     4375 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/util/hgweb_config.py
--rw-r--r--   0 john       (502) staff       (20)    17014 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/util/metadata_util.py
--rw-r--r--   0 john       (502) staff       (20)     5913 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/util/readme_util.py
--rw-r--r--   0 john       (502) staff       (20)     3638 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/util/repository_content_util.py
--rw-r--r--   0 john       (502) staff       (20)    29488 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/util/repository_util.py
--rw-r--r--   0 john       (502) staff       (20)     5952 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/util/review_util.py
--rw-r--r--   0 john       (502) staff       (20)    11120 2019-02-22 18:09:10.000000 galaxy-web-apps-20.5.0/tool_shed/util/search_util.py
--rw-r--r--   0 john       (502) staff       (20)     7531 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/util/shed_index.py
--rw-r--r--   0 john       (502) staff       (20)    22402 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/util/shed_util_common.py
--rw-r--r--   0 john       (502) staff       (20)       76 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/util/tool_dependency_util.py
--rw-r--r--   0 john       (502) staff       (20)       65 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/util/tool_util.py
--rw-r--r--   0 john       (502) staff       (20)      882 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/tool_shed/util/web_util.py
--rw-r--r--   0 john       (502) staff       (20)       64 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/util/xml_util.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/utility_containers/
--rw-r--r--   0 john       (502) staff       (20)    19953 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/tool_shed/utility_containers/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    46986 2019-11-20 12:54:47.000000 galaxy-web-apps-20.5.0/tool_shed/utility_containers/utility_container_manager.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/
--rw-r--r--   0 john       (502) staff       (20)      166 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/
--rw-r--r--   0 john       (502) staff       (20)        0 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/__init__.py
--rw-r--r--   0 john       (502) staff       (20)      890 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/authenticate.py
--rw-r--r--   0 john       (502) staff       (20)     7279 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/categories.py
--rw-r--r--   0 john       (502) staff       (20)      886 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/configuration.py
--rw-r--r--   0 john       (502) staff       (20)     6952 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/groups.py
--rw-r--r--   0 john       (502) staff       (20)    55377 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/repositories.py
--rw-r--r--   0 john       (502) staff       (20)    12245 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/repository_revisions.py
--rw-r--r--   0 john       (502) staff       (20)     8585 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/tools.py
--rw-r--r--   0 john       (502) staff       (20)     5324 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/api/users.py
--rw-r--r--   0 john       (502) staff       (20)     4314 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/app.py
--rw-r--r--   0 john       (502) staff       (20)    14736 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/buildapp.py
--rw-r--r--   0 john       (502) staff       (20)     7328 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/config.py
--rw-r--r--   0 john       (502) staff       (20)    18408 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/config_schema.yml
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/
--rw-r--r--   0 john       (502) staff       (20)       36 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    27497 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/admin.py
--rw-r--r--   0 john       (502) staff       (20)      612 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/groups.py
--rw-r--r--   0 john       (502) staff       (20)     1426 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/hg.py
--rw-r--r--   0 john       (502) staff       (20)   172818 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/repository.py
--rw-r--r--   0 john       (502) staff       (20)    38676 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/repository_review.py
--rw-r--r--   0 john       (502) staff       (20)    24963 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/upload.py
--rw-r--r--   0 john       (502) staff       (20)    24573 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/user.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/framework/
--rw-r--r--   0 john       (502) staff       (20)       51 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/framework/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/framework/middleware/
--rw-r--r--   0 john       (502) staff       (20)       23 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/framework/middleware/__init__.py
--rw-r--r--   0 john       (502) staff       (20)     6091 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/framework/middleware/remoteuser.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/
--rw-r--r--   0 john       (502) staff       (20)    20526 2020-07-04 13:42:51.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    21408 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/mapping.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/
--rw-r--r--   0 john       (502) staff       (20)        0 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/__init__.py
--rw-r--r--   0 john       (502) staff       (20)     4248 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/check.py
--rw-r--r--   0 john       (502) staff       (20)      989 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/migrate.cfg
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/
--rw-r--r--   0 john       (502) staff       (20)     9052 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0001_initial_tables.py
--rw-r--r--   0 john       (502) staff       (20)     1604 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0002_add_tool_suite_column.py
--rw-r--r--   0 john       (502) staff       (20)     1799 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0003_review_and_review_association_tables.py
--rw-r--r--   0 john       (502) staff       (20)     3641 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0004_repository_tables.py
--rw-r--r--   0 john       (502) staff       (20)     9239 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0005_drop_tool_related_tables.py
--rw-r--r--   0 john       (502) staff       (20)     1479 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0006_add_email_alerts_column.py
--rw-r--r--   0 john       (502) staff       (20)     2028 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0007_add_long_description_times_downloaded_columns.py
--rw-r--r--   0 john       (502) staff       (20)     1817 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0008_add_repository_metadata_table.py
--rw-r--r--   0 john       (502) staff       (20)     1835 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0009_add_malicious_column.py
--rw-r--r--   0 john       (502) staff       (20)     1811 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0010_add_new_repo_alert_column.py
--rw-r--r--   0 john       (502) staff       (20)     1562 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0011_add_tool_versions_column.py
--rw-r--r--   0 john       (502) staff       (20)     1790 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0012_add_downloadable_column.py
--rw-r--r--   0 john       (502) staff       (20)     9474 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0013_add_review_tables.py
--rw-r--r--   0 john       (502) staff       (20)     1678 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0014_add_deprecated_column.py
--rw-r--r--   0 john       (502) staff       (20)     1457 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0015_add_api_keys_table.py
--rw-r--r--   0 john       (502) staff       (20)     4342 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0016_add_do_not_test_tools_functionally_correct_errors_columns.py
--rw-r--r--   0 john       (502) staff       (20)     5274 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0017_add_galaxy_utility_columns_to_repository_metadata_table.py
--rw-r--r--   0 john       (502) staff       (20)     3874 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0018_add_repository_metadata_flag_columns.py
--rw-r--r--   0 john       (502) staff       (20)     3250 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0019_add_skip_tool_test_table_and_test_install_error_column.py
--rw-r--r--   0 john       (502) staff       (20)     1565 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0020_add_repository_type_column.py
--rw-r--r--   0 john       (502) staff       (20)     1026 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0021_change_repository_type_value.py
--rw-r--r--   0 john       (502) staff       (20)     7189 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0022_add_repository_admin_roles.py
--rw-r--r--   0 john       (502) staff       (20)     1687 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0023_add_repository_url_and_hompeage_url.py
--rw-r--r--   0 john       (502) staff       (20)     1150 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0024_password_reset.py
--rw-r--r--   0 john       (502) staff       (20)     1169 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0025_session_timeout.py
--rw-r--r--   0 john       (502) staff       (20)     1613 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0026_add_numeric_revision_column.py
--rw-r--r--   0 john       (502) staff       (20)        0 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/search/
--rw-r--r--   0 john       (502) staff       (20)        0 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/search/__init__.py
--rw-r--r--   0 john       (502) staff       (20)    10105 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/search/repo_search.py
--rw-r--r--   0 john       (502) staff       (20)     3633 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/search/tool_search.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/security/
--rw-r--r--   0 john       (502) staff       (20)    11482 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/security/__init__.py
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/
--rw-r--r--   0 john       (502) staff       (20)     1106 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/browse_repository.mako
--rw-r--r--   0 john       (502) staff       (20)     2334 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/browse_tool_dependency.mako
--rw-r--r--   0 john       (502) staff       (20)    16514 2020-03-13 22:37:26.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/common.mako
--rw-r--r--   0 john       (502) staff       (20)     2995 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/manage_repository.mako
--rw-r--r--   0 john       (502) staff       (20)     3127 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/repository_actions_menu.mako
--rw-r--r--   0 john       (502) staff       (20)     2055 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/repository_installation_status.mako
--rw-r--r--   0 john       (502) staff       (20)     1832 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/reset_metadata_on_selected_repositories.mako
--rw-r--r--   0 john       (502) staff       (20)    10591 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/select_tool_panel_section.mako
--rw-r--r--   0 john       (502) staff       (20)     1201 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/tool_dependency_installation_status.mako
--rw-r--r--   0 john       (502) staff       (20)     8555 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/view_tool_metadata.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/base/
--rw-r--r--   0 john       (502) staff       (20)     7486 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/base/base_panels.mako
--rw-r--r--   0 john       (502) staff       (20)     2468 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/base.mako
--rw-r--r--   0 john       (502) staff       (20)     4760 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/display_common.mako
--rw-r--r--   0 john       (502) staff       (20)     4179 2020-01-01 17:09:49.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/galaxy_client_app.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/legacy/
--rw-r--r--   0 john       (502) staff       (20)     8430 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/legacy/grid_base.mako
--rw-r--r--   0 john       (502) staff       (20)      126 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/legacy/grid_base_async.mako
--rw-r--r--   0 john       (502) staff       (20)     1713 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/message.mako
--rw-r--r--   0 john       (502) staff       (20)     2930 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/refresh_frames.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/
--rw-r--r--   0 john       (502) staff       (20)     1740 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/center.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/
--rw-r--r--   0 john       (502) staff       (20)     3287 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group.mako
--rw-r--r--   0 john       (502) staff       (20)     3979 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group_create.mako
--rw-r--r--   0 john       (502) staff       (20)     1440 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group_rename.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/
--rw-r--r--   0 john       (502) staff       (20)     4844 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role.mako
--rw-r--r--   0 john       (502) staff       (20)     4277 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role_create.mako
--rw-r--r--   0 john       (502) staff       (20)     1801 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role_rename.mako
--rw-r--r--   0 john       (502) staff       (20)     4567 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/index.mako
--rw-r--r--   0 john       (502) staff       (20)     2544 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/statistics.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/user/
--rw-r--r--   0 john       (502) staff       (20)     1477 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/user/reset_password.mako
--rw-r--r--   0 john       (502) staff       (20)     3300 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/user/user.mako
--rw-r--r--   0 john       (502) staff       (20)     8782 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/base_panels.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/category/
--rw-r--r--   0 john       (502) staff       (20)     1137 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/category/create_category.mako
--rw-r--r--   0 john       (502) staff       (20)     1831 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/category/edit_category.mako
--rw-r--r--   0 john       (502) staff       (20)      432 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/category/grid.mako
--rw-r--r--   0 john       (502) staff       (20)      422 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/category/valid_grid.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/
--rw-r--r--   0 john       (502) staff       (20)     6347 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/common.mako
--rw-r--r--   0 john       (502) staff       (20)     9638 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/grid_common.mako
--rw-r--r--   0 john       (502) staff       (20)    12315 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/repository_actions_menu.mako
--rw-r--r--   0 john       (502) staff       (20)     3742 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/reset_metadata_on_selected_repositories.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/group/
--rw-r--r--   0 john       (502) staff       (20)      991 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/group/index.mako
--rw-r--r--   0 john       (502) staff       (20)    14217 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/index.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/
--rw-r--r--   0 john       (502) staff       (20)     4225 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/browse_repository.mako
--rw-r--r--   0 john       (502) staff       (20)    57615 2020-03-13 22:37:32.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/common.mako
--rw-r--r--   0 john       (502) staff       (20)     1481 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/contact_owner.mako
--rw-r--r--   0 john       (502) staff       (20)     3723 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/create_repository.mako
--rw-r--r--   0 john       (502) staff       (20)     2185 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/docker_image_repositories.mako
--rw-r--r--   0 john       (502) staff       (20)     1985 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/export_repository.mako
--rw-r--r--   0 john       (502) staff       (20)     2734 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/find_tools.mako
--rw-r--r--   0 john       (502) staff       (20)    17723 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/manage_repository.mako
--rw-r--r--   0 john       (502) staff       (20)     2208 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/preview_tools_in_changeset.mako
--rw-r--r--   0 john       (502) staff       (20)     6192 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/rate_repository.mako
--rw-r--r--   0 john       (502) staff       (20)     5602 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/tool_form.mako
--rw-r--r--   0 john       (502) staff       (20)     7597 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/upload.mako
--rw-r--r--   0 john       (502) staff       (20)     4779 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/view_changelog.mako
--rw-r--r--   0 john       (502) staff       (20)     6293 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/view_changeset.mako
--rw-r--r--   0 john       (502) staff       (20)    11469 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/view_repository.mako
--rw-r--r--   0 john       (502) staff       (20)    11868 2020-04-21 18:03:35.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/view_tool_metadata.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/
--rw-r--r--   0 john       (502) staff       (20)     5945 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/browse_review.mako
--rw-r--r--   0 john       (502) staff       (20)     1143 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/create_component.mako
--rw-r--r--   0 john       (502) staff       (20)     1513 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/edit_component.mako
--rw-r--r--   0 john       (502) staff       (20)     9060 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/edit_review.mako
--rw-r--r--   0 john       (502) staff       (20)       42 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/grid.mako
--rw-r--r--   0 john       (502) staff       (20)     5661 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/reviews_of_changeset_revision.mako
--rw-r--r--   0 john       (502) staff       (20)     4274 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/reviews_of_repository.mako
--rw-r--r--   0 john       (502) staff       (20)     4650 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/select_previous_review.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/role/
--rw-r--r--   0 john       (502) staff       (20)     5431 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/role/role.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/
--rw-r--r--   0 john       (502) staff       (20)     1737 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/api_keys.mako
--rw-r--r--   0 john       (502) staff       (20)     1543 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/change_password.mako
--rw-r--r--   0 john       (502) staff       (20)     1251 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/index.mako
--rw-r--r--   0 john       (502) staff       (20)     2934 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/login.mako
--rw-r--r--   0 john       (502) staff       (20)     1059 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/logout.mako
--rw-r--r--   0 john       (502) staff       (20)     2014 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/manage_email_alerts.mako
--rw-r--r--   0 john       (502) staff       (20)     6446 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/manage_info.mako
--rw-r--r--   0 john       (502) staff       (20)     8272 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/register.mako
--rw-r--r--   0 john       (502) staff       (20)      679 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/reset_password.mako
--rw-r--r--   0 john       (502) staff       (20)     1199 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/username.mako
-drwxr-xr-x   0 john       (502) staff       (20)        0 2020-07-04 15:44:37.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/util/
--rw-r--r--   0 john       (502) staff       (20)        0 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/util/__init__.py
--rw-r--r--   0 john       (502) staff       (20)     1274 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/util/ratings_util.py
--rw-r--r--   0 john       (502) staff       (20)     4580 2020-01-01 17:09:40.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/util/shed_statistics.py
--rw-r--r--   0 john       (502) staff       (20)   113171 2019-02-26 19:28:15.000000 galaxy-web-apps-20.5.0/tool_shed/webapp/uwsgi_schema.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/
+-rw-r--r--   0 runner    (1001) docker     (123)      999 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/HISTORY.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    12875 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)      493 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)     2353 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)      275 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/README.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       89 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/dev-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/
+-rw-r--r--   0 runner    (1001) docker     (123)       91 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/biom/
+-rw-r--r--   0 runner    (1001) docker     (123)      367 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/biom/biom_simple.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ensembl/
+-rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ensembl/ensembl_bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3929 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ensembl/ensembl_gff.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3992 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ensembl/ensembl_interval_as_bed.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/gbrowse/
+-rw-r--r--   0 runner    (1001) docker     (123)     1663 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_gff.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1863 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_interval_as_bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1675 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_wig.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/icn3d/
+-rw-r--r--   0 runner    (1001) docker     (123)      441 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/icn3d/icn3d_simple.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igb/
+-rw-r--r--   0 runner    (1001) docker     (123)      767 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igb/bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      720 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igb/bb.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      794 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igb/bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      843 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igb/bedgraph.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1108 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igb/bigwig.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      795 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igb/gtf.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1084 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igb/wig.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igv/
+-rw-r--r--   0 runner    (1001) docker     (123)     5123 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igv/bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3887 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igv/bigwig.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4999 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igv/genbank.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4747 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igv/genome_fasta.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4937 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igv/gff.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4965 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igv/interval_as_bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     5383 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/igv/vcf.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/image/
+-rw-r--r--   0 runner    (1001) docker     (123)      434 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/image/avivator.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/intermine/
+-rw-r--r--   0 runner    (1001) docker     (123)      420 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/intermine/intermine_simple.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/iobio/
+-rw-r--r--   0 runner    (1001) docker     (123)      442 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/iobio/bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      488 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/iobio/vcf.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/minerva/
+-rw-r--r--   0 runner    (1001) docker     (123)      429 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/minerva/tabular.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/rviewer/
+-rw-r--r--   0 runner    (1001) docker     (123)     1254 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/rviewer/bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1236 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/rviewer/vcf.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ucsc/
+-rw-r--r--   0 runner    (1001) docker     (123)     1456 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ucsc/bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1295 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ucsc/bigbed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1295 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ucsc/bigwig.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1528 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ucsc/interval_as_bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1274 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ucsc/maf_customtrack.xml
+-rwxr-xr-x   0 runner    (1001) docker     (123)      436 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ucsc/trackhub.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1432 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/datatypes/display_applications/configs/ucsc/vcf.xml
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/
+-rw-r--r--   0 runner    (1001) docker     (123)      169 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7745 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/api.py
+-rw-r--r--   0 runner    (1001) docker     (123)    66402 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/controller.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/
+-rw-r--r--   0 runner    (1001) docker     (123)     2253 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/erricon.ico
+-rw-r--r--   0 runner    (1001) docker     (123)    15086 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/favicon.ico
+-rw-r--r--   0 runner    (1001) docker     (123)     9716 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/favicon.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    20770 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/formatHelp.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      752 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/delete.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      130 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/delete_tag_icon_gray.png
+-rw-r--r--   0 runner    (1001) docker     (123)      917 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/dw.gif
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/
+-rw-r--r--   0 runner    (1001) docker     (123)   134808 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/FontAwesome.otf
+-rw-r--r--   0 runner    (1001) docker     (123)   165742 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/fontawesome-webfont.eot
+-rw-r--r--   0 runner    (1001) docker     (123)   444379 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/fontawesome-webfont.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   165548 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/fontawesome-webfont.ttf
+-rw-r--r--   0 runner    (1001) docker     (123)    98024 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/fontawesome-webfont.woff
+-rw-r--r--   0 runner    (1001) docker     (123)    77160 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/fontawesome-webfont.woff2
+-rw-r--r--   0 runner    (1001) docker     (123)    25883 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/iconic_stroke.eot
+-rw-r--r--   0 runner    (1001) docker     (123)    40600 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/iconic_stroke.otf
+-rw-r--r--   0 runner    (1001) docker     (123)    68076 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/iconic_stroke.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    18856 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fonts/iconic_stroke.ttf
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/
+-rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/application-dock-270-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      516 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/application-dock-270.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1041 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-000-small-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      553 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-090.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      770 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-circle.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1123 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-resize-090-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      403 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-resize-090.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1389 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-split-bw.png
+-rw-r--r--   0 runner    (1001) docker     (123)      707 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-split.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1334 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-transition-270-bw.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1314 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/arrow-transition-bw.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1346 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/block--plus-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      689 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/block--plus.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1353 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/bookmarks-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      596 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/bookmarks.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      682 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/bug.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      436 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/chart.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1214 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/chevron-expand-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      490 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/chevron-expand.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      493 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/chevron.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      375 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/control-270.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      555 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/cross-button.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1422 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/cross-circle-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      689 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/cross-circle.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1053 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/cross-small-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      476 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/cross.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1333 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/disk--arrow-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      603 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/disk--arrow.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      475 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/disk.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      613 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/exclamation.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      621 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/external.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      536 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/eye.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1424 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/gear-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      721 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/gear.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1575 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/globe-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      849 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/globe.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1273 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/hammer-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      575 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/hammer.png
+-rw-r--r--   0 runner    (1001) docker     (123)      651 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/information-white.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1299 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/layer-transparent-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      566 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/layer-transparent.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1392 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/layers-stack-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      664 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/layers-stack.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      681 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/magnifier-left.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      736 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/magnifier-zoom-out.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      758 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/magnifier-zoom.png
+-rw-r--r--   0 runner    (1001) docker     (123)      776 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/navigation.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      309 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/pencil-small.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      475 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/pencil.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1309 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/plus-button-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      544 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/plus-button.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      674 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/plus-circle.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      520 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/sticky-note-text.png
+-rw-r--r--   0 runner    (1001) docker     (123)      778 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/tag--plus.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      714 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/tag-label.png
+-rw-r--r--   0 runner    (1001) docker     (123)      661 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/tags.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1200 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/toggle-bw.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1269 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/toggle-expand-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      520 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/toggle-expand.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      448 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/toggle.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1243 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/toolbox-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      488 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/toolbox.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1155 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/ui-slider-050-bw.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      454 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/fugue/ui-slider-050.png
+-rw-r--r--   0 runner    (1001) docker     (123)      147 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/hatch-fade-023858.gif
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/
+-rw-r--r--   0 runner    (1001) docker     (123)     1381 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/delete_icon.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1621 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/delete_icon_dark.png
+-rw-r--r--   0 runner    (1001) docker     (123)      425 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/delete_icon_grey.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1512 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/eye_icon.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1631 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/eye_icon_dark.png
+-rw-r--r--   0 runner    (1001) docker     (123)      396 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/eye_icon_grey.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1413 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/pencil_icon.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1571 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/pencil_icon_dark.png
+-rw-r--r--   0 runner    (1001) docker     (123)      405 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-buttons/pencil_icon_grey.png
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-states/
+-rw-r--r--   0 runner    (1001) docker     (123)      497 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-states/data_empty.png
+-rw-r--r--   0 runner    (1001) docker     (123)      497 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-states/data_error.png
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-states/data_ok.png
+-rw-r--r--   0 runner    (1001) docker     (123)      562 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history-states/data_queued.png
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      204 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history_down_arrow.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      201 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/history_up_arrow.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     1531 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/icon_error_lrg.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     1010 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/icon_error_sml.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     1383 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/icon_info_lrg.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      606 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/icon_info_sml.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     1492 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/icon_success_lrg.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      990 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/icon_success_sml.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     1491 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/icon_warning_lrg.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      576 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/icon_warning_sml.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     3208 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/loading_large_white_bg.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     1849 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/loading_small_white_bg.gif
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/maf_icons/
+-rw-r--r--   0 runner    (1001) docker     (123)    11787 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/maf_icons/interval2maf.png
+-rw-r--r--   0 runner    (1001) docker     (123)    11270 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/maf_icons/stitchMaf.png
+-rw-r--r--   0 runner    (1001) docker     (123)      328 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/openid-16x16.gif
+-rw-r--r--   0 runner    (1001) docker     (123)    21641 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/rgWebLogo3_test.jpg
+-rw-r--r--   0 runner    (1001) docker     (123)       77 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/square_empty.gif
+-rw-r--r--   0 runner    (1001) docker     (123)       82 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/square_error.gif
+-rw-r--r--   0 runner    (1001) docker     (123)       80 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/square_ok.gif
+-rw-r--r--   0 runner    (1001) docker     (123)       79 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/square_queued.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      180 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/square_running.gif
+-rwxr-xr-x   0 runner    (1001) docker     (123)      815 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/star.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      308 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tool_menu_down_arrow.gif
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/collection_ops/
+-rw-r--r--   0 runner    (1001) docker     (123)   257713 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/collection_ops/build_list.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    81633 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/collection_ops/filter_empty.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    72821 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/collection_ops/filter_error.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    92949 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/collection_ops/flatten.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   108978 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/collection_ops/unzip.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   109459 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/collection_ops/zip.svg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/lda/
+-rw-r--r--   0 runner    (1001) docker     (123)    33141 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/lda/first_matrix_generator_example_file.png
+-rw-r--r--   0 runner    (1001) docker     (123)    23673 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tools/lda/second_matrix_generator_example_file.png
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/
+-rw-r--r--   0 runner    (1001) docker     (123)     1272 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/block.png
+-rw-r--r--   0 runner    (1001) docker     (123)      463 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/close_btn.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     4460 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/diag_bg.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      434 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/go_btn.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/handle-left.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      124 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/handle-right.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      339 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/pan_left.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      356 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/pan_right.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      712 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/show_history.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      111 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/zoom_in.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      151 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/zoom_in_full.gif
+-rw-r--r--   0 runner    (1001) docker     (123)       95 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/zoom_out.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      120 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/tracks/zoom_out_full.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      920 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/up.gif
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)       87 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/visualization/draggable_horizontal.png
+-rw-r--r--   0 runner    (1001) docker     (123)      103 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/visualization/draggable_vertical.png
+-rw-r--r--   0 runner    (1001) docker     (123)      116 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/visualization/strand_left.png
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/visualization/strand_left_inv.png
+-rw-r--r--   0 runner    (1001) docker     (123)      120 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/visualization/strand_right.png
+-rw-r--r--   0 runner    (1001) docker     (123)       86 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/visualization/strand_right_inv.png
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/yui/
+-rw-r--r--   0 runner    (1001) docker     (123)     6610 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/images/yui/rel_interstitial_loading.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     1083 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/incompatible-browser.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/patmat/
+-rw-r--r--   0 runner    (1001) docker     (123)    10834 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/patmat/findcluster.png
+-rw-r--r--   0 runner    (1001) docker     (123)       87 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/robots.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/
+-rw-r--r--   0 runner    (1001) docker     (123)     2501 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/autocomplete_tagging.css
+-rw-r--r--   0 runner    (1001) docker     (123)     2204 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/bootstrap-tour.css
+-rw-r--r--   0 runner    (1001) docker     (123)       64 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/circster.css
+-rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/data_running.gif
+-rw-r--r--   0 runner    (1001) docker     (123)      810 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/data_upload.gif
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/dynatree_skin/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4046 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/dynatree_skin/icons-rtl.gif
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4041 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/dynatree_skin/icons.gif
+-rwxr-xr-x   0 runner    (1001) docker     (123)      570 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/dynatree_skin/loading.gif
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9902 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/dynatree_skin/ui.dynatree.css
+-rwxr-xr-x   0 runner    (1001) docker     (123)      842 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/dynatree_skin/vline-rtl.gif
+-rwxr-xr-x   0 runner    (1001) docker     (123)      844 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/dynatree_skin/vline.gif
+-rw-r--r--   0 runner    (1001) docker     (123)     1620 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/embed_item.css
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/history.css
+-rw-r--r--   0 runner    (1001) docker     (123)      785 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/info_icon.svg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      180 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-bg_flat_0_aaaaaa_40x100.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      178 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-bg_flat_75_ffffff_40x100.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      144 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-bg_glass_55_fbf9ee_1x400.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      105 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-bg_glass_65_ffffff_1x400.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      111 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-bg_glass_75_dadada_1x400.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      110 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-bg_glass_75_e6e6e6_1x400.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      119 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-bg_glass_95_fef1ec_1x400.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)      101 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-bg_highlight-soft_75_cccccc_1x100.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4369 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-icons_222222_256x240.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5355 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-icons_2e83ff_256x240.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4369 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-icons_454545_256x240.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4369 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-icons_888888_256x240.png
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4369 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/images/ui-icons_cd0a0a_256x240.png
+-rw-r--r--   0 runner    (1001) docker     (123)    20147 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery-ui/smoothness/jquery-ui.css
+-rw-r--r--   0 runner    (1001) docker     (123)      845 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/jquery.rating.css
+-rw-r--r--   0 runner    (1001) docker     (123)     3571 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/library.css
+-rw-r--r--   0 runner    (1001) docker     (123)      942 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/masthead.css
+-rw-r--r--   0 runner    (1001) docker     (123)      378 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/ok_small.png
+-rw-r--r--   0 runner    (1001) docker     (123)      770 2023-06-13 17:04:36.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/question-octagon-frame.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3423 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/reports.css
+-rw-r--r--   0 runner    (1001) docker     (123)    31536 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/sprite-fugue.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3046 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/sprite-history-buttons.png
+-rw-r--r--   0 runner    (1001) docker     (123)     1643 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/sprite-history-states.png
+-rw-r--r--   0 runner    (1001) docker     (123)    11228 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/style/trackster.css
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/mvc/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/mvc/groups/
+-rw-r--r--   0 runner    (1001) docker     (123)     1719 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/mvc/groups/group-detail-view.js.map
+-rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/mvc/groups/group-list-view.js.map
+-rw-r--r--   0 runner    (1001) docker     (123)      587 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/mvc/groups/group-listrow-view.js.map
+-rw-r--r--   0 runner    (1001) docker     (123)      375 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/mvc/groups/group-model.js.map
+-rw-r--r--   0 runner    (1001) docker     (123)      122 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/test-file.js.map
+-rw-r--r--   0 runner    (1001) docker     (123)      878 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/maps/toolshed.groups.js.map
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/scripts/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/scripts/mvc/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/scripts/mvc/groups/
+-rw-r--r--   0 runner    (1001) docker     (123)     3266 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/scripts/mvc/groups/group-detail-view.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1776 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/scripts/mvc/groups/group-list-view.js
+-rw-r--r--   0 runner    (1001) docker     (123)      600 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/scripts/mvc/groups/group-listrow-view.js
+-rw-r--r--   0 runner    (1001) docker     (123)      246 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/scripts/mvc/groups/group-model.js
+-rw-r--r--   0 runner    (1001) docker     (123)      738 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/toolshed/scripts/toolshed.groups.js
+-rw-r--r--   0 runner    (1001) docker     (123)      901 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/user_disabled.html
+-rw-r--r--   0 runner    (1001) docker     (123)     2691 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/welcome.html.sample
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/
+-rw-r--r--   0 runner    (1001) docker     (123)      196 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-blockquote.png
+-rw-r--r--   0 runner    (1001) docker     (123)      166 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-h1.png
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-h2.png
+-rw-r--r--   0 runner    (1001) docker     (123)      170 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-h3.png
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-h4.png
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-h5.png
+-rw-r--r--   0 runner    (1001) docker     (123)      171 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-h6.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3607 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-p.png
+-rw-r--r--   0 runner    (1001) docker     (123)      177 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/lbl-pre.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3273 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/wymiframe.css
+-rw-r--r--   0 runner    (1001) docker     (123)      867 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/default/wymiframe.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/
+-rw-r--r--   0 runner    (1001) docker     (123)      196 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-blockquote.png
+-rw-r--r--   0 runner    (1001) docker     (123)      166 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-h1.png
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-h2.png
+-rw-r--r--   0 runner    (1001) docker     (123)      170 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-h3.png
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-h4.png
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-h5.png
+-rw-r--r--   0 runner    (1001) docker     (123)      171 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-h6.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3607 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-p.png
+-rw-r--r--   0 runner    (1001) docker     (123)      177 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/lbl-pre.png
+-rw-r--r--   0 runner    (1001) docker     (123)     3537 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/wymiframe.css
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/iframe/galaxy/wymiframe.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/
+-rw-r--r--   0 runner    (1001) docker     (123)     1973 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/bg.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1552 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/ca.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1683 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/cs.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1694 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/de.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1771 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/en.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1548 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/es.js
+-rw-r--r--   0 runner    (1001) docker     (123)     2253 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/fa.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1543 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/fi.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1540 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/fr.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1812 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/he.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1517 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/hr.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1553 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/hu.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1523 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/it.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1501 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/nb.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1537 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/nl.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1504 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/nn.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1563 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/pl.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1543 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/pt-br.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1547 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/pt.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1956 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/ru.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1484 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/sv.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1537 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/tr.js
+-rw-r--r--   0 runner    (1001) docker     (123)     1206 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/lang/zh_cn.js
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/default/
+-rw-r--r--   0 runner    (1001) docker     (123)     3651 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/default/icons.png
+-rw-r--r--   0 runner    (1001) docker     (123)     7890 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/default/skin.css
+-rw-r--r--   0 runner    (1001) docker     (123)     1394 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/default/skin.js
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/galaxy/
+-rw-r--r--   0 runner    (1001) docker     (123)     3041 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/galaxy/icons.png
+-rw-r--r--   0 runner    (1001) docker     (123)     8099 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/galaxy/skin.css
+-rw-r--r--   0 runner    (1001) docker     (123)     1154 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/static/wymeditor/skins/galaxy/skin.js
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/base/
+-rw-r--r--   0 runner    (1001) docker     (123)     6892 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/base/base_panels.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3230 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     5809 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/display_base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4641 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/display_common.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2746 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/embed_base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     6529 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/galaxy_client_app.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3629 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/js-app.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/legacy/
+-rw-r--r--   0 runner    (1001) docker     (123)     8033 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/legacy/grid_base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      126 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/legacy/grid_base_async.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1343 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/message.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      520 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/no_access.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     8698 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/page_base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2930 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/refresh_frames.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1646 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/slug_editing_js.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      809 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/sorting_base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2460 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/spark_base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     6228 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/tagging_common.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1142 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/tool_shed_rating.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/dataset/
+-rw-r--r--   0 runner    (1001) docker     (123)     7695 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/dataset/copy_view.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     6330 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/dataset/display.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/dataset/display_application/
+-rw-r--r--   0 runner    (1001) docker     (123)     1082 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/dataset/display_application/display.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      615 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/dataset/large_file.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      840 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/dataset/tabular_chunked.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/history/
+-rw-r--r--   0 runner    (1001) docker     (123)      715 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/history/as_xml.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      315 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/history/list_as_xml.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/root/
+-rw-r--r--   0 runner    (1001) docker     (123)     2015 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/root/tool_runner.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)    10235 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/visualization/phyloviz.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3085 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/visualization/sweepster.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     5551 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/visualization/trackster.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/workflow/
+-rw-r--r--   0 runner    (1001) docker     (123)     5804 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/workflow/build_from_current_history.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      550 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/galaxy/workflow/myexp_export.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/
+-rw-r--r--   0 runner    (1001) docker     (123)     1021 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/base_panels.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4276 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/dataset_info.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/history_and_dataset_per_user.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2529 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/history_and_dataset_type.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2218 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/history_per_user.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     8217 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/index.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3272 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/job_info.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4586 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_errors_per_tool.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3783 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_per_month_all.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4641 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_per_month_by_user_and_destination.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3598 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_per_month_in_error.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4036 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_per_tool.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3593 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_per_user.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4145 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_per_user_by_destination.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4121 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_specified_month_all.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4074 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_specified_month_in_error.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2712 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_tool_per_month.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2582 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_user_per_month.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3311 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/jobs_user_per_month_by_destination.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1117 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/registered_users.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1972 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/registered_users_per_month.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1293 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/registered_users_specified_date.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1729 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/registered_users_specified_month.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1763 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/run_stats.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4201 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/system.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3529 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/tool_error_messages.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2867 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/tool_execution_time.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2739 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/tool_execution_time_per_month.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2801 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/tools_and_job_state.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1270 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/tools_and_job_state_per_month.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/users_last_access_date.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2158 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/users_user_disk_usage.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3067 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/workflows_per_month_all.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3987 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/workflows_per_user.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3757 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/workflows_per_workflow.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2351 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/workflows_user_per_month.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/tool_shed/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/tool_shed/common/
+-rw-r--r--   0 runner    (1001) docker     (123)     6318 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/tool_shed/common/common.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/tool_shed/repository/
+-rw-r--r--   0 runner    (1001) docker     (123)    57616 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/tool_shed/repository/common.mako
+-rw-r--r--   0 runner    (1001) docker     (123)    51377 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/base/webapp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/
+-rw-r--r--   0 runner    (1001) docker     (123)    16415 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3358 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/annotations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/authenticate.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12123 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/cloud.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12815 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/cloudauthz.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6327 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4444 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4671 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/container_resolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5097 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/dataset_collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15388 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/datasets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5572 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/datatypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1101 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/display_applications.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4008 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/drs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2848 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/dynamic_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3186 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/extended_metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4562 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/folder_contents.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5696 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/folders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3239 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/forms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3545 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/genomes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3074 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/group_roles.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3688 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/group_users.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1625 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/groups.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20345 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/histories.py
+-rw-r--r--   0 runner    (1001) docker     (123)    40178 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/history_contents.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2410 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/item_tags.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6854 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/job_files.py
+-rw-r--r--   0 runner    (1001) docker     (123)      607 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/job_lock.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1668 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/job_ports.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20766 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/jobs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7412 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/libraries.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23118 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/library_contents.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38658 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/library_datasets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1413 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/licenses.py
+-rw-r--r--   0 runner    (1001) docker     (123)      952 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2052 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/page_revisions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9658 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/pages.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2114 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/plugins.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3658 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/provenance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4196 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/quotas.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3215 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/remote_files.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1805 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/roles.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4089 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/sanitize_allow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3268 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/search.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2286 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/short_term_storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tags.py
+-rw-r--r--   0 runner    (1001) docker     (123)      661 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tasks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4776 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tool_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12623 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tool_dependencies.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4079 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tool_entry_points.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22648 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tool_shed_repositories.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22646 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1840 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/toolshed.py
+-rw-r--r--   0 runner    (1001) docker     (123)      970 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tours.py
+-rw-r--r--   0 runner    (1001) docker     (123)      990 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/trs_consumer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1055 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/trs_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2305 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/uploads.py
+-rw-r--r--   0 runner    (1001) docker     (123)    47567 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/users.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13594 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/visualizations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/webhooks.py
+-rw-r--r--   0 runner    (1001) docker     (123)    64203 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/workflows.py
+-rw-r--r--   0 runner    (1001) docker     (123)    49241 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/buildapp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/
+-rw-r--r--   0 runner    (1001) docker     (123)       32 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    72329 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/admin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8446 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/admin_toolshed.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8045 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/async.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9320 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/authnz.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10616 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/data_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    55836 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)      193 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/error.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13698 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/forms.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34810 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/history.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24989 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/page.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5064 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/root.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2318 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/shed_tool_static.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9316 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/tag.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6814 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/tool_runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17619 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/user.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1695 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/userskeys.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36706 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/visualization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29874 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/workflow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5917 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/fast_app.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2215 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/fast_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8801 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/_fetch_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3517 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/authenticate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6326 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11719 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/dataset_collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35100 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/datasets.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26286 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/histories.py
+-rw-r--r--   0 runner    (1001) docker     (123)    64586 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/history_contents.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9069 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/invocations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2260 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/jobs.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15625 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/libraries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9726 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/library_folder_contents.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12748 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/library_folders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5760 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/pages.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6143 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/quotas.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5453 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/sharable.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/tool_shed_repositories.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12516 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3163 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/users.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1054 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/visualizations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6049 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/workflows.py
+-rw-r--r--   0 runner    (1001) docker     (123)      313 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/workers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/openapi/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/openapi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22422 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/openapi/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/
+-rw-r--r--   0 runner    (1001) docker     (123)      164 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1875 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/app.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4304 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/buildapp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4386 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/
+-rw-r--r--   0 runner    (1001) docker     (123)       34 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8782 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/history.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4766 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/home.py
+-rw-r--r--   0 runner    (1001) docker     (123)    47256 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/jobs.py
+-rw-r--r--   0 runner    (1001) docker     (123)      953 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/query.py
+-rw-r--r--   0 runner    (1001) docker     (123)      280 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/root.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9322 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/system.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15009 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9409 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/users.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18067 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/workflows.py
+-rw-r--r--   0 runner    (1001) docker     (123)      767 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/fast_app.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1747 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/reports/fast_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2935 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/galaxy/webapps/util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy_web_apps.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     2353 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy_web_apps.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    36835 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy_web_apps.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy_web_apps.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      446 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy_web_apps.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/galaxy_web_apps.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       81 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)     1699 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)       28 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/test-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12306 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/attribute_handlers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/repository/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/repository/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31451 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/repository/relation_builder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/tool/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/tool/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8901 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/dependencies/tool/tag_attribute_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/galaxy_install/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/galaxy_install/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/grids/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/grids/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18705 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/grids/admin_grids.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2020 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/grids/repository_grid_filter_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    69837 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/grids/repository_grids.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7121 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/grids/util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/managers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/managers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4421 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/managers/groups.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2254 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/managers/repositories.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/metadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      109 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/metadata/metadata_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    61963 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/metadata/repository_metadata_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24009 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/repository_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/repository_types/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/repository_types/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/repository_types/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      725 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/repository_types/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1556 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/repository_types/repository_suite_definition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/repository_types/tool_dependency_definition.py
+-rw-r--r--   0 runner    (1001) docker     (123)      629 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/repository_types/unrestricted.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3206 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/repository_types/util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      373 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/structured_app.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/tools/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      189 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/tools/data_table_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13129 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/tools/tool_validator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3114 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/tools/tool_version_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/util/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    47956 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/admin_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      662 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/basic_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11173 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/commit_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1461 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/common_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      270 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/container_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)       69 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/encoding_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10868 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/hg_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4371 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/hgweb_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15499 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/metadata_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6363 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/readme_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3280 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/repository_content_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24519 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/repository_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11395 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/search_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7836 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/shed_index.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21509 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/shed_util_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)       76 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/tool_dependency_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      271 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/tool_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      819 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/web_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      158 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/util/xml_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/utility_containers/
+-rw-r--r--   0 runner    (1001) docker     (123)    16949 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/utility_containers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/
+-rw-r--r--   0 runner    (1001) docker     (123)      166 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/
+-rw-r--r--   0 runner    (1001) docker     (123)      254 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1073 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/authenticate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6330 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/categories.py
+-rw-r--r--   0 runner    (1001) docker     (123)      856 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6634 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/groups.py
+-rw-r--r--   0 runner    (1001) docker     (123)    51115 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/repositories.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11714 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/repository_revisions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4817 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4641 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/api/users.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5421 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/app.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11715 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/buildapp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6632 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/
+-rw-r--r--   0 runner    (1001) docker     (123)       36 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24724 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/admin.py
+-rw-r--r--   0 runner    (1001) docker     (123)      490 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/groups.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1854 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/hg.py
+-rw-r--r--   0 runner    (1001) docker     (123)   140810 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/repository.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24349 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/upload.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24150 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/user.py
+-rw-r--r--   0 runner    (1001) docker     (123)      818 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/fast_app.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1845 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/fast_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/framework/
+-rw-r--r--   0 runner    (1001) docker     (123)       51 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      718 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/framework/decorators.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/framework/middleware/
+-rw-r--r--   0 runner    (1001) docker     (123)       23 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/framework/middleware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6093 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/framework/middleware/remoteuser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/
+-rw-r--r--   0 runner    (1001) docker     (123)    28410 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1393 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/mapping.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4361 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/check.py
+-rw-r--r--   0 runner    (1001) docker     (123)      989 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/migrate.cfg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/
+-rw-r--r--   0 runner    (1001) docker     (123)     7235 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0001_initial_tables.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1585 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0002_add_tool_suite_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0003_review_and_review_association_tables.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3119 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0004_repository_tables.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8343 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0005_drop_tool_related_tables.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1458 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0006_add_email_alerts_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2015 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0007_add_long_description_times_downloaded_columns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1658 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0008_add_repository_metadata_table.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1816 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0009_add_malicious_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1792 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0010_add_new_repo_alert_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1541 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0011_add_tool_versions_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1771 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0012_add_downloadable_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8879 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0013_add_review_tables.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1659 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0014_add_deprecated_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1387 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0015_add_api_keys_table.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4325 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0016_add_do_not_test_tools_functionally_correct_errors_columns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5247 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0017_add_galaxy_utility_columns_to_repository_metadata_table.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3855 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0018_add_repository_metadata_flag_columns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3120 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0019_add_skip_tool_test_table_and_test_install_error_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1544 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0020_add_repository_type_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)      988 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0021_change_repository_type_value.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6955 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0022_add_repository_admin_roles.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1666 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0023_add_repository_url_and_hompeage_url.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1069 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0024_password_reset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1152 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0025_session_timeout.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1596 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0026_add_numeric_revision_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1164 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0027_add_api_keys_deleted_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/search/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/search/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9714 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/search/repo_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3624 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/search/tool_search.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/security/
+-rw-r--r--   0 runner    (1001) docker     (123)    10387 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/security/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/base/
+-rw-r--r--   0 runner    (1001) docker     (123)     7065 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/base/base_panels.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2666 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     5725 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/galaxy_client_app.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/legacy/
+-rw-r--r--   0 runner    (1001) docker     (123)     7978 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/legacy/grid_base.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      126 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/legacy/grid_base_async.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1710 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/message.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2930 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/refresh_frames.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/
+-rw-r--r--   0 runner    (1001) docker     (123)     1740 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/center.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/
+-rw-r--r--   0 runner    (1001) docker     (123)     3287 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3979 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group_create.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1440 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group_rename.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/
+-rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4277 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role_create.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role_rename.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4463 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/index.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2544 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/statistics.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/user/
+-rw-r--r--   0 runner    (1001) docker     (123)     1477 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/user/reset_password.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3300 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/user/user.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     8291 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/base_panels.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/category/
+-rw-r--r--   0 runner    (1001) docker     (123)     1137 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/category/create_category.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1831 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/category/edit_category.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      432 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/category/grid.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      422 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/category/valid_grid.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/common/
+-rw-r--r--   0 runner    (1001) docker     (123)     5819 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/common/common.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     9633 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/common/grid_common.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     9554 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/common/repository_actions_menu.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3742 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/common/reset_metadata_on_selected_repositories.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/group/
+-rw-r--r--   0 runner    (1001) docker     (123)      992 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/group/index.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     9272 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/index.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/
+-rw-r--r--   0 runner    (1001) docker     (123)     2139 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/browse_repository.mako
+-rw-r--r--   0 runner    (1001) docker     (123)    57299 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/common.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1481 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/contact_owner.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3723 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/create_repository.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2185 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/docker_image_repositories.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2734 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/find_tools.mako
+-rw-r--r--   0 runner    (1001) docker     (123)    15294 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/manage_repository.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2208 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/preview_tools_in_changeset.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     3655 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/rate_repository.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     5602 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/tool_form.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     7495 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/upload.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     4779 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/view_changelog.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     6293 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/view_changeset.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     9475 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/view_repository.mako
+-rw-r--r--   0 runner    (1001) docker     (123)    11658 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/view_tool_metadata.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/role/
+-rw-r--r--   0 runner    (1001) docker     (123)     5431 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/role/role.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/
+-rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/api_keys.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1060 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/change_password.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1170 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/index.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2791 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/login.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      954 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/logout.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     2014 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/manage_email_alerts.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     6446 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/manage_info.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     7737 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/register.mako
+-rw-r--r--   0 runner    (1001) docker     (123)      679 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/reset_password.mako
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/username.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-13 17:13:40.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/util/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/util/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1272 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/util/ratings_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4569 2023-06-13 17:04:37.000000 galaxy-web-apps-23.0.2/tool_shed/webapp/util/shed_statistics.py
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/base/controller.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/base/controller.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,63 +1,67 @@
 """
 Contains functionality needed in every web interface
 """
 import logging
+from typing import (
+    Any,
+    Callable,
+    Optional,
+)
 
-from six import string_types
 from sqlalchemy import true
 from webob.exc import (
     HTTPBadRequest,
     HTTPInternalServerError,
-    HTTPNotImplemented
+    HTTPNotImplemented,
 )
 
 from galaxy import (
     exceptions,
     model,
     security,
     util,
-    web
+    web,
 )
 from galaxy.datatypes.interval import ChromatinInteractions
 from galaxy.managers import (
-    api_keys,
     base as managers_base,
     configuration,
     users,
-    workflows
+    workflows,
 )
+from galaxy.managers.sharable import SlugBuilder
 from galaxy.model import (
     ExtendedMetadata,
     ExtendedMetadataIndex,
     HistoryDatasetAssociation,
     LibraryDatasetDatasetAssociation,
     tags,
 )
 from galaxy.model.item_attrs import UsesAnnotations
 from galaxy.util.dictifiable import Dictifiable
 from galaxy.util.sanitize_html import sanitize_html
 from galaxy.web import (
     error,
-    url_for
+    url_for,
 )
 from galaxy.web.form_builder import (
     AddressField,
     CheckboxField,
-    PasswordField
+    PasswordField,
 )
 from galaxy.workflow.modules import WorkflowModuleInjector
 
 log = logging.getLogger(__name__)
 
 # States for passing messages
 SUCCESS, INFO, WARNING, ERROR = "done", "info", "warning", "error"
 
 
-class BaseController(object):
+class BaseController:
     """
     Base class for Galaxy web application controllers.
     """
 
     def __init__(self, app):
         """Initialize an interface for application 'app'"""
         self.app = app
@@ -65,56 +69,58 @@
         self.user_manager = users.UserManager(app)
 
     def get_toolbox(self):
         """Returns the application toolbox"""
         return self.app.toolbox
 
     def get_class(self, class_name):
-        """ Returns the class object that a string denotes. Without this method, we'd have to do eval(<class_name>). """
+        """Returns the class object that a string denotes. Without this method, we'd have to do eval(<class_name>)."""
         return managers_base.get_class(class_name)
 
     def get_object(self, trans, id, class_name, check_ownership=False, check_accessible=False, deleted=None):
         """
         Convenience method to get a model object with the specified checks.
         """
-        return managers_base.get_object(trans, id, class_name, check_ownership=check_ownership, check_accessible=check_accessible, deleted=deleted)
+        return managers_base.get_object(
+            trans, id, class_name, check_ownership=check_ownership, check_accessible=check_accessible, deleted=deleted
+        )
 
     # this should be here - but catching errors from sharable item controllers that *should* have SharableItemMixin
     #   but *don't* then becomes difficult
     # def security_check( self, trans, item, check_ownership=False, check_accessible=False ):
     #    log.warning( 'BaseController.security_check: %s, %b, %b', str( item ), check_ownership, check_accessible )
     #    # meant to be overridden in SharableSecurityMixin
     #    return item
 
     def get_user(self, trans, id, check_ownership=False, check_accessible=False, deleted=None):
-        return self.get_object(trans, id, 'User', check_ownership=False, check_accessible=False, deleted=deleted)
+        return self.get_object(trans, id, "User", check_ownership=False, check_accessible=False, deleted=deleted)
 
     def get_group(self, trans, id, check_ownership=False, check_accessible=False, deleted=None):
-        return self.get_object(trans, id, 'Group', check_ownership=False, check_accessible=False, deleted=deleted)
+        return self.get_object(trans, id, "Group", check_ownership=False, check_accessible=False, deleted=deleted)
 
     def get_role(self, trans, id, check_ownership=False, check_accessible=False, deleted=None):
-        return self.get_object(trans, id, 'Role', check_ownership=False, check_accessible=False, deleted=deleted)
+        return self.get_object(trans, id, "Role", check_ownership=False, check_accessible=False, deleted=deleted)
 
     # ---- parsing query params
     def decode_id(self, id):
         return managers_base.decode_id(self.app, id)
 
     def encode_all_ids(self, trans, rval, recursive=False):
         """
         Encodes all integer values in the dict rval whose keys are 'id' or end with '_id'
 
         It might be useful to turn this in to a decorator
         """
         return trans.security.encode_all_ids(rval, recursive=recursive)
 
-    def parse_filter_params(self, qdict, filter_attr_key='q', filter_value_key='qv', attr_op_split_char='-'):
-        """
-        """
+    # TODO this will be replaced by lib.galaxy.managers.base.ModelFilterParser.build_filter_params
+    def parse_filter_params(self, qdict, filter_attr_key="q", filter_value_key="qv", attr_op_split_char="-"):
+        """ """
         # TODO: import DEFAULT_OP from FilterParser
-        DEFAULT_OP = 'eq'
+        DEFAULT_OP = "eq"
         if filter_attr_key not in qdict:
             return []
         # precondition: attrs/value pairs are in-order in the qstring
         attrs = qdict.get(filter_attr_key)
         if not isinstance(attrs, list):
             attrs = [attrs]
         # ops are strings placed after the attr strings and separated by a split char (e.g. 'create_time-lt')
@@ -134,108 +140,88 @@
         if not isinstance(values, list):
             values = [values]
         # TODO: it may be more helpful to the consumer if we error on incomplete 3-tuples
         #   (instead of relying on zip to shorten)
         return list(zip(attrs, ops, values))
 
     def parse_limit_offset(self, qdict):
-        """
-        """
+        """ """
+
         def _parse_pos_int(i):
             try:
                 new_val = int(i)
                 if new_val >= 0:
                     return new_val
             except (TypeError, ValueError):
                 pass
             return None
 
-        limit = _parse_pos_int(qdict.get('limit', None))
-        offset = _parse_pos_int(qdict.get('offset', None))
+        limit = _parse_pos_int(qdict.get("limit", None))
+        offset = _parse_pos_int(qdict.get("offset", None))
         return (limit, offset)
 
 
 Root = BaseController
 
 
 class BaseUIController(BaseController):
-
     def get_object(self, trans, id, class_name, check_ownership=False, check_accessible=False, deleted=None):
         try:
-            return BaseController.get_object(self, trans, id, class_name,
-                                             check_ownership=check_ownership, check_accessible=check_accessible, deleted=deleted)
+            return BaseController.get_object(
+                self,
+                trans,
+                id,
+                class_name,
+                check_ownership=check_ownership,
+                check_accessible=check_accessible,
+                deleted=deleted,
+            )
         except exceptions.MessageException:
-            raise       # handled in the caller
+            raise  # handled in the caller
         except Exception:
             log.exception("Exception in get_object check for %s %s:", class_name, str(id))
-            raise Exception('Server error retrieving %s id ( %s ).' % (class_name, str(id)))
+            raise Exception(f"Server error retrieving {class_name} id ( {str(id)} ).")
 
     def message_exception(self, trans, message, sanitize=True):
         trans.response.status = 400
-        return {'err_msg': util.sanitize_text(message) if sanitize else message}
+        return {"err_msg": util.sanitize_text(message) if sanitize else message}
 
 
 class BaseAPIController(BaseController):
-
     def get_object(self, trans, id, class_name, check_ownership=False, check_accessible=False, deleted=None):
         try:
-            return BaseController.get_object(self, trans, id, class_name,
-                                             check_ownership=check_ownership, check_accessible=check_accessible, deleted=deleted)
+            return BaseController.get_object(
+                self,
+                trans,
+                id,
+                class_name,
+                check_ownership=check_ownership,
+                check_accessible=check_accessible,
+                deleted=deleted,
+            )
 
-        except exceptions.ItemDeletionException as e:
-            raise HTTPBadRequest(detail="Invalid %s id ( %s ) specified: %s" % (class_name, str(id), util.unicodify(e)))
-        except exceptions.MessageException as e:
-            raise HTTPBadRequest(detail=e.err_msg)
+        except exceptions.MessageException:
+            raise
         except Exception as e:
             log.exception("Exception in get_object check for %s %s.", class_name, str(id))
             raise HTTPInternalServerError(comment=util.unicodify(e))
 
-    def validate_in_users_and_groups(self, trans, payload):
-        """
-        For convenience, in_users and in_groups can be encoded IDs or emails/group names in the API.
-        """
-        def get_id(item, model_class, column):
-            try:
-                return trans.security.decode_id(item)
-            except Exception:
-                pass  # maybe an email/group name
-            # this will raise if the item is invalid
-            return trans.sa_session.query(model_class).filter(column == item).first().id
-        new_in_users = []
-        new_in_groups = []
-        invalid = []
-        for item in util.listify(payload.get('in_users', [])):
-            try:
-                new_in_users.append(get_id(item, trans.app.model.User, trans.app.model.User.table.c.email))
-            except Exception:
-                invalid.append(item)
-        for item in util.listify(payload.get('in_groups', [])):
-            try:
-                new_in_groups.append(get_id(item, trans.app.model.Group, trans.app.model.Group.table.c.name))
-            except Exception:
-                invalid.append(item)
-        if invalid:
-            msg = "The following value(s) for associated users and/or groups could not be parsed: %s." % ', '.join(invalid)
-            msg += "  Valid values are email addresses of users, names of groups, or IDs of both."
-            raise Exception(msg)
-        payload['in_users'] = list(map(str, new_in_users))
-        payload['in_groups'] = list(map(str, new_in_groups))
-
     def not_implemented(self, trans, **kwd):
         raise HTTPNotImplemented()
 
     def _parse_serialization_params(self, kwd, default_view):
-        view = kwd.get('view', None)
-        keys = kwd.get('keys')
-        if isinstance(keys, string_types):
-            keys = keys.split(',')
+        view = kwd.get("view", None)
+        keys = kwd.get("keys")
+        if isinstance(keys, str):
+            keys = keys.split(",")
         return dict(view=view, keys=keys, default_view=default_view)
 
+    # TODO: this will be replaced by lib.galaxy.schema.FilterQueryParams.build_order_by
     def _parse_order_by(self, manager, order_by_string):
-        ORDER_BY_SEP_CHAR = ','
+        ORDER_BY_SEP_CHAR = ","
         if ORDER_BY_SEP_CHAR in order_by_string:
             return [manager.parse_order_by(o) for o in order_by_string.split(ORDER_BY_SEP_CHAR)]
         return manager.parse_order_by(order_by_string)
 
 
 class JSAppLauncher(BaseUIController):
     """
@@ -243,19 +229,28 @@
     """
 
     #: path to js app template
     JS_APP_MAKO_FILEPATH = "/js-app.mako"
     #: window-scoped js function to call to start the app (will be passed options, bootstrapped)
     DEFAULT_ENTRY_FN = "app"
     #: keys used when serializing current user for bootstrapped data
-    USER_BOOTSTRAP_KEYS = ('id', 'email', 'username', 'is_admin', 'tags_used',
-                           'total_disk_usage', 'nice_total_disk_usage', 'quota_percent', 'preferences')
+    USER_BOOTSTRAP_KEYS = (
+        "id",
+        "email",
+        "username",
+        "is_admin",
+        "tags_used",
+        "total_disk_usage",
+        "nice_total_disk_usage",
+        "quota_percent",
+        "preferences",
+    )
 
     def __init__(self, app):
-        super(JSAppLauncher, self).__init__(app)
+        super().__init__(app)
         self.user_manager = users.UserManager(app)
         self.user_serializer = users.CurrentUserSerializer(app)
         self.config_serializer = configuration.ConfigSerializer(app)
         self.admin_config_serializer = configuration.AdminConfigSerializer(app)
 
     def _check_require_login(self, trans):
         if self.app.config.require_login and self.user_manager.is_anonymous(trans.user):
@@ -268,309 +263,277 @@
     def client(self, trans, **kwd):
         """
         Endpoint for clientside routes.  This ships the primary SPA client.
 
         Should not be used with url_for -- see
         (https://github.com/galaxyproject/galaxy/issues/1878) for why.
         """
-        self._check_require_login(trans)
         return self._bootstrapped_client(trans, **kwd)
 
     # This includes contextualized user options in the bootstrapped data; we
     # don't want to cache it.
     @web.do_not_cache
-    def _bootstrapped_client(self, trans, app_name='analysis', **kwd):
+    def _bootstrapped_client(self, trans, app_name="analysis", **kwd):
         js_options = self._get_js_options(trans)
-        js_options['config'].update(self._get_extended_config(trans))
+        js_options["config"].update(self._get_extended_config(trans))
         return self.template(trans, app_name, options=js_options, **kwd)
 
     def _get_js_options(self, trans, root=None):
         """
         Return a dictionary of session/site configuration/options to jsonify
         and pass onto the js app.
 
         Defaults to `config`, `user`, and the root url. Pass kwargs to update further.
         """
-        root = root or web.url_for('/')
+        root = root or web.url_for("/")
         js_options = {
-            'root'               : root,
-            'user'               : self.user_serializer.serialize(trans.user, self.USER_BOOTSTRAP_KEYS, trans=trans),
-            'config'             : self._get_site_configuration(trans),
-            'params'             : dict(trans.request.params),
-            'session_csrf_token' : trans.session_csrf_token,
+            "root": root,
+            "user": self.user_serializer.serialize(trans.user, self.USER_BOOTSTRAP_KEYS, trans=trans),
+            "config": self._get_site_configuration(trans),
+            "params": dict(trans.request.params),
+            "session_csrf_token": trans.session_csrf_token,
         }
         return js_options
 
     def _get_extended_config(self, trans):
         config = {
-            'active_view'                   : 'analysis',
-            'enable_cloud_launch'           : trans.app.config.get_bool('enable_cloud_launch', False),
-            'enable_webhooks'               : True if trans.app.webhooks_registry.webhooks else False,
-            'toolbox'                       : trans.app.toolbox.to_dict(trans),
-            'message_box_visible'           : trans.app.config.message_box_visible,
-            'show_inactivity_warning'       : trans.app.config.user_activation_on and trans.user and not trans.user.active,
-            'tool_shed_urls'                : list(trans.app.tool_shed_registry.tool_sheds.values()) if trans.app.tool_shed_registry else [],
-            'tool_dynamic_configs'          : list(trans.app.toolbox.dynamic_conf_filenames())
+            "active_view": "analysis",
+            "enable_webhooks": True if trans.app.webhooks_registry.webhooks else False,
+            "message_box_visible": trans.app.config.message_box_visible,
+            "show_inactivity_warning": trans.app.config.user_activation_on and trans.user and not trans.user.active,
+            "tool_shed_urls": list(trans.app.tool_shed_registry.tool_sheds.values())
+            if trans.app.tool_shed_registry
+            else [],
+            "tool_dynamic_configs": list(trans.app.toolbox.dynamic_conf_filenames()),
         }
 
         # TODO: move to user
         stored_workflow_menu_index = {}
         stored_workflow_menu_entries = []
-        for menu_item in getattr(trans.user, 'stored_workflow_menu_entries', []):
+        for menu_item in getattr(trans.user, "stored_workflow_menu_entries", []):
             encoded_stored_workflow_id = trans.security.encode_id(menu_item.stored_workflow_id)
             if encoded_stored_workflow_id not in stored_workflow_menu_index:
                 stored_workflow_menu_index[encoded_stored_workflow_id] = True
-                stored_workflow_menu_entries.append({
-                    'id': encoded_stored_workflow_id,
-                    'name': util.unicodify(menu_item.stored_workflow.name)
-                })
-        config['stored_workflow_menu_entries'] = stored_workflow_menu_entries
+                stored_workflow_menu_entries.append(
+                    {"id": encoded_stored_workflow_id, "name": util.unicodify(menu_item.stored_workflow.name)}
+                )
+        config["stored_workflow_menu_entries"] = stored_workflow_menu_entries
         return config
 
     def _get_site_configuration(self, trans):
         """
         Return a dictionary representing Galaxy's current configuration.
         """
         try:
             serializer = self.config_serializer
             if self.user_manager.is_admin(trans.user, trans=trans):
                 serializer = self.admin_config_serializer
-            return serializer.serialize_to_view(self.app.config, view='all')
+            return serializer.serialize_to_view(self.app.config, view="all", host=trans.host)
         except Exception as exc:
             log.exception(exc)
             return {}
 
-    def template(self, trans, app_name, entry_fn='app', options=None, bootstrapped_data=None, masthead=True, **additional_options):
+    def template(
+        self,
+        trans,
+        app_name: str,
+        entry_fn: str = "app",
+        options=None,
+        bootstrapped_data: Optional[dict] = None,
+        masthead: Optional[bool] = True,
+        **additional_options,
+    ):
         """
         Render and return the single page mako template that starts the app.
 
-        `app_name` (string): the first portion of the webpack bundle to as the app.
-        `entry_fn` (string): the name of the window-scope function that starts the
-            app. Defaults to 'app'.
-        `bootstrapped_data` (dict): (optional) update containing any more data
-            the app may need.
-        `masthead` (boolean): (optional, default=True) include masthead elements in
-            the initial page dom.
-        `additional_options` (kwargs): update to the options sent to the app.
+        :param app_name: the first portion of the webpack bundle to as the app.
+        :param entry_fn: the name of the window-scope function that starts the
+                         app. Defaults to 'app'.
+        :param bootstrapped_data: update containing any more data
+                                  the app may need.
+        :param masthead: include masthead elements in the initial page dom.
+        :param additional_options: update to the options sent to the app.
         """
         options = options or self._get_js_options(trans)
         options.update(additional_options)
         return trans.fill_template(
             self.JS_APP_MAKO_FILEPATH,
             js_app_name=app_name,
             js_app_entry_fn=(entry_fn or self.DEFAULT_ENTRY_FN),
             options=options,
             bootstrapped=(bootstrapped_data or {}),
-            masthead=masthead
+            masthead=masthead,
         )
 
 
-class Datatype(object):
+class Datatype:
     """Used for storing in-memory list of datatypes currently in the datatypes registry."""
 
     def __init__(self, extension, dtype, type_extension, mimetype, display_in_upload):
         self.extension = extension
         self.dtype = dtype
         self.type_extension = type_extension
         self.mimetype = mimetype
         self.display_in_upload = display_in_upload
 
+
 #
 # -- Mixins for working with Galaxy objects. --
 #
 
 
-class CreatesApiKeysMixin(object):
-    """
-    Mixing centralizing logic for creating API keys for user objects.
-
-    Deprecated - please use api_keys.ApiKeyManager for new development.
-    """
-
-    def create_api_key(self, trans, user):
-        return api_keys.ApiKeyManager(trans.app).create_api_key(user)
-
-
-class SharableItemSecurityMixin(object):
-    """ Mixin for handling security for sharable items. """
+class SharableItemSecurityMixin:
+    """Mixin for handling security for sharable items."""
 
     def security_check(self, trans, item, check_ownership=False, check_accessible=False):
-        """ Security checks for an item: checks if (a) user owns item or (b) item is accessible to user. """
-        return managers_base.security_check(trans, item, check_ownership=check_ownership, check_accessible=check_accessible)
-
-
-class ExportsHistoryMixin(object):
-
-    def serve_ready_history_export(self, trans, jeha):
-        assert jeha.ready
-        if jeha.compressed:
-            trans.response.set_content_type('application/x-gzip')
-        else:
-            trans.response.set_content_type('application/x-tar')
-        disposition = 'attachment; filename="%s"' % jeha.export_name
-        trans.response.headers["Content-Disposition"] = disposition
-        archive = trans.app.object_store.get_filename(jeha.dataset)
-        return open(archive, mode='rb')
-
-    def queue_history_export(self, trans, history, gzip=True, include_hidden=False, include_deleted=False):
-        # Convert options to booleans.
-        if isinstance(gzip, string_types):
-            gzip = (gzip in ['True', 'true', 'T', 't'])
-        if isinstance(include_hidden, string_types):
-            include_hidden = (include_hidden in ['True', 'true', 'T', 't'])
-        if isinstance(include_deleted, string_types):
-            include_deleted = (include_deleted in ['True', 'true', 'T', 't'])
-
-        # Run job to do export.
-        history_exp_tool = trans.app.toolbox.get_tool('__EXPORT_HISTORY__')
-        params = {
-            'history_to_export': history,
-            'compress': gzip,
-            'include_hidden': include_hidden,
-            'include_deleted': include_deleted
-        }
-
-        history_exp_tool.execute(trans, incoming=params, history=history, set_output_hid=True)
-
-
-class ImportsHistoryMixin(object):
-
-    def queue_history_import(self, trans, archive_type, archive_source):
-        # Run job to do import.
-        history_imp_tool = trans.app.toolbox.get_tool('__IMPORT_HISTORY__')
-        incoming = {'__ARCHIVE_SOURCE__' : archive_source, '__ARCHIVE_TYPE__' : archive_type}
-        history_imp_tool.execute(trans, incoming=incoming)
-
-
-class UsesLibraryMixin(object):
-
-    def get_library(self, trans, id, check_ownership=False, check_accessible=True):
-        l = self.get_object(trans, id, 'Library')
-        if check_accessible and not (trans.user_is_admin or trans.app.security_agent.can_access_library(trans.get_current_user_roles(), l)):
-            error("LibraryFolder is not accessible to the current user")
-        return l
+        """Security checks for an item: checks if (a) user owns item or (b) item is accessible to user."""
+        return managers_base.security_check(
+            trans, item, check_ownership=check_ownership, check_accessible=check_accessible
+        )
 
 
 class UsesLibraryMixinItems(SharableItemSecurityMixin):
+    get_object: Callable
 
-    def get_library_folder(self, trans, id, check_ownership=False, check_accessible=True):
-        return self.get_object(trans, id, 'LibraryFolder',
-                               check_ownership=False, check_accessible=check_accessible)
+    def get_library_folder(self, trans, id: int, check_ownership=False, check_accessible=True):
+        return self.get_object(trans, id, "LibraryFolder", check_ownership=False, check_accessible=check_accessible)
 
     def get_library_dataset_dataset_association(self, trans, id, check_ownership=False, check_accessible=True):
         # Deprecated in lieu to galaxy.managers.lddas.LDDAManager.get() but not
         # reusing that exactly because of subtle differences in exception handling
         # logic (API controller override get_object to be slightly different).
-        return self.get_object(trans, id, 'LibraryDatasetDatasetAssociation',
-                               check_ownership=False, check_accessible=check_accessible)
+        return self.get_object(
+            trans, id, "LibraryDatasetDatasetAssociation", check_ownership=False, check_accessible=check_accessible
+        )
 
     def get_library_dataset(self, trans, id, check_ownership=False, check_accessible=True):
-        return self.get_object(trans, id, 'LibraryDataset',
-                               check_ownership=False, check_accessible=check_accessible)
+        return self.get_object(trans, id, "LibraryDataset", check_ownership=False, check_accessible=check_accessible)
 
     # TODO: it makes no sense that I can get roles from a user but not user.is_admin()
     # def can_user_add_to_library_item( self, trans, user, item ):
     #    if not user: return False
     #    return (  ( user.is_admin() )
     #           or ( trans.app.security_agent.can_add_library_item( user.all_roles(), item ) ) )
 
     def can_current_user_add_to_library_item(self, trans, item):
         if not trans.user:
             return False
-        return (trans.user_is_admin or
-                trans.app.security_agent.can_add_library_item(trans.get_current_user_roles(), item))
+        return trans.user_is_admin or trans.app.security_agent.can_add_library_item(
+            trans.get_current_user_roles(), item
+        )
 
     def check_user_can_add_to_library_item(self, trans, item, check_accessible=True):
         """
         Raise exception if user cannot add to the specified library item (i.e.
         Folder). Can set check_accessible to False if folder was loaded with
         this check.
         """
         if not trans.user:
-            return False
+            raise exceptions.ItemAccessibilityException("Anonymous users cannot add to library items")
 
         current_user_roles = trans.get_current_user_roles()
         if trans.user_is_admin:
             return True
 
         if check_accessible:
             if not trans.app.security_agent.can_access_library_item(current_user_roles, item, trans.user):
-                raise exceptions.ItemAccessibilityException('You do not have access to the requested item')
+                raise exceptions.ItemAccessibilityException("You do not have access to the requested item")
 
         if not trans.app.security_agent.can_add_library_item(trans.get_current_user_roles(), item):
             # Slight misuse of ItemOwnershipException?
             raise exceptions.ItemOwnershipException("User cannot add to library item.")
 
-    def _copy_hdca_to_library_folder(self, trans, hda_manager, from_hdca_id, folder_id, ldda_message=''):
+    def _copy_hdca_to_library_folder(self, trans, hda_manager, from_hdca_id: int, folder_id: int, ldda_message=""):
         """
         Fetches the collection identified by `from_hcda_id` and dispatches individual collection elements to
         _copy_hda_to_library_folder
         """
         hdca = trans.sa_session.query(trans.app.model.HistoryDatasetCollectionAssociation).get(from_hdca_id)
-        if hdca.collection.collection_type != 'list':
-            raise exceptions.NotImplemented('Cannot add nested collections to library. Please flatten your collection first.')
+        if hdca.collection.collection_type != "list":
+            raise exceptions.NotImplemented(
+                "Cannot add nested collections to library. Please flatten your collection first."
+            )
         hdas = []
         for element in hdca.collection.elements:
             hdas.append((element.element_identifier, element.dataset_instance.id))
-        return [self._copy_hda_to_library_folder(trans,
-                                                 hda_manager=hda_manager,
-                                                 from_hda_id=hda_id,
-                                                 folder_id=folder_id,
-                                                 ldda_message=ldda_message,
-                                                 element_identifier=element_identifier) for (element_identifier, hda_id) in hdas]
-
-    def _copy_hda_to_library_folder(self, trans, hda_manager, from_hda_id, folder_id, ldda_message='', element_identifier=None):
+        return [
+            self._copy_hda_to_library_folder(
+                trans,
+                hda_manager=hda_manager,
+                from_hda_id=hda_id,
+                folder_id=folder_id,
+                ldda_message=ldda_message,
+                element_identifier=element_identifier,
+            )
+            for (element_identifier, hda_id) in hdas
+        ]
+
+    def _copy_hda_to_library_folder(
+        self, trans, hda_manager, from_hda_id: int, folder_id: int, ldda_message="", element_identifier=None
+    ):
         """
         Copies hda ``from_hda_id`` to library folder ``folder_id``, optionally
         adding ``ldda_message`` to the new ldda's ``message``.
 
         ``library_contents.create`` will branch to this if called with 'from_hda_id'
         in its payload.
         """
-        log.debug('_copy_hda_to_library_folder: %s' % (str((from_hda_id, folder_id, ldda_message))))
-        # PRECONDITION: folder_id has already been altered to remove the folder prefix ('F')
+        log.debug(f"_copy_hda_to_library_folder: {str((from_hda_id, folder_id, ldda_message))}")
         # TODO: allow name and other, editable ldda attrs?
         if ldda_message:
             ldda_message = sanitize_html(ldda_message)
 
         # check permissions on (all three?) resources: hda, library, folder
         # TODO: do we really need the library??
         hda = hda_manager.get_owned(from_hda_id, trans.user, current_history=trans.history)
         hda = hda_manager.error_if_uploading(hda)
         folder = self.get_library_folder(trans, folder_id, check_accessible=True)
 
         # TOOD: refactor to use check_user_can_add_to_library_item, eliminate boolean
         # can_current_user_add_to_library_item.
         if folder.parent_library.deleted:
-            raise exceptions.ObjectAttributeInvalidException('You cannot add datasets into deleted library. Undelete it first.')
+            raise exceptions.ObjectAttributeInvalidException(
+                "You cannot add datasets into deleted library. Undelete it first."
+            )
         if not self.can_current_user_add_to_library_item(trans, folder):
-            raise exceptions.InsufficientPermissionsException('You do not have proper permissions to add a dataset to this folder,')
+            raise exceptions.InsufficientPermissionsException(
+                "You do not have proper permissions to add a dataset to this folder,"
+            )
 
-        ldda = self.copy_hda_to_library_folder(trans, hda, folder, ldda_message=ldda_message, element_identifier=element_identifier)
+        ldda = self.copy_hda_to_library_folder(
+            trans, hda, folder, ldda_message=ldda_message, element_identifier=element_identifier
+        )
         # I don't see a reason why hdas copied into libraries should not be visible.
         # If there is, refactor `ldda.visible = True` to do this only when adding HDCAs.
         ldda.visible = True
         ldda.update_parent_folder_update_times()
         trans.sa_session.flush()
         ldda_dict = ldda.to_dict()
         rval = trans.security.encode_dict_ids(ldda_dict)
-        update_time = ldda.update_time.strftime("%Y-%m-%d %I:%M %p")
-        rval['update_time'] = update_time
+        update_time = ldda.update_time.isoformat()
+        rval["update_time"] = update_time
         return rval
 
-    def copy_hda_to_library_folder(self, trans, hda, library_folder, roles=None, ldda_message='', element_identifier=None):
+    def copy_hda_to_library_folder(
+        self, trans, hda, library_folder, roles=None, ldda_message="", element_identifier=None
+    ):
         # PRECONDITION: permissions for this action on hda and library_folder have been checked
         roles = roles or []
 
         # this code was extracted from library_common.add_history_datasets_to_library
         # TODO: refactor library_common.add_history_datasets_to_library to use this for each hda to copy
 
         # create the new ldda and apply the folder perms to it
-        ldda = hda.to_library_dataset_dataset_association(trans, target_folder=library_folder,
-                                                          roles=roles, ldda_message=ldda_message, element_identifier=element_identifier)
+        ldda = hda.to_library_dataset_dataset_association(
+            trans,
+            target_folder=library_folder,
+            roles=roles,
+            ldda_message=ldda_message,
+            element_identifier=element_identifier,
+        )
         self._apply_library_folder_permissions_to_ldda(trans, library_folder, ldda)
         self._apply_hda_permissions_to_ldda(trans, hda, ldda)
         # TODO:?? not really clear on how permissions are being traded here
         #   seems like hda -> ldda permissions should be set in to_library_dataset_dataset_association
         #   then they get reset in _apply_library_folder_permissions_to_ldda
         #   then finally, re-applies hda -> ldda for missing actions in _apply_hda_permissions_to_ldda??
         return ldda
@@ -596,16 +559,16 @@
         security_agent = trans.app.security_agent
         dataset_permissions_dict = security_agent.get_permissions(hda.dataset)
         library_dataset = ldda.library_dataset
         library_dataset_actions = [permission.action for permission in library_dataset.actions]
 
         # except that: if DATASET_MANAGE_PERMISSIONS exists in the hda.dataset permissions,
         #   we need to instead apply those roles to the LIBRARY_MANAGE permission to the library dataset
-        dataset_manage_permissions_action = security_agent.get_action('DATASET_MANAGE_PERMISSIONS').action
-        library_manage_permissions_action = security_agent.get_action('LIBRARY_MANAGE').action
+        dataset_manage_permissions_action = security_agent.get_action("DATASET_MANAGE_PERMISSIONS").action
+        library_manage_permissions_action = security_agent.get_action("LIBRARY_MANAGE").action
         # TODO: test this and remove if in loop below
         # TODO: doesn't handle action.action
         # if dataset_manage_permissions_action in dataset_permissions_dict:
         #    managing_roles = dataset_permissions_dict.pop( dataset_manage_permissions_action )
         #    dataset_permissions_dict[ library_manage_permissions_action ] = managing_roles
 
         flush_needed = False
@@ -637,14 +600,16 @@
 
 
 class UsesVisualizationMixin(UsesLibraryMixinItems):
     """
     Mixin for controllers that use Visualization objects.
     """
 
+    slug_builder = SlugBuilder()
+
     def get_visualization(self, trans, id, check_ownership=True, check_accessible=False):
         """
         Get a Visualization from the database by id, verifying ownership.
         """
         # Load workflow from database
         try:
             visualization = trans.sa_session.query(trans.model.Visualization).get(trans.security.decode_id(id))
@@ -732,51 +697,51 @@
         """
         Return a set of summary attributes for a visualization in dictionary form.
         NOTE: that encoding ids isn't done here should happen at the caller level.
         """
         # TODO: deleted
         # TODO: importable
         return {
-            'id'        : visualization.id,
-            'title'     : visualization.title,
-            'type'      : visualization.type,
-            'dbkey'     : visualization.dbkey,
+            "id": visualization.id,
+            "title": visualization.title,
+            "type": visualization.type,
+            "dbkey": visualization.dbkey,
         }
 
     def get_visualization_dict(self, visualization):
         """
         Return a set of detailed attributes for a visualization in dictionary form.
         The visualization's latest_revision is returned in its own sub-dictionary.
         NOTE: that encoding ids isn't done here should happen at the caller level.
         """
         return {
-            'model_class': 'Visualization',
-            'id'         : visualization.id,
-            'title'      : visualization.title,
-            'type'       : visualization.type,
-            'user_id'    : visualization.user.id,
-            'dbkey'      : visualization.dbkey,
-            'slug'       : visualization.slug,
+            "model_class": "Visualization",
+            "id": visualization.id,
+            "title": visualization.title,
+            "type": visualization.type,
+            "user_id": visualization.user.id,
+            "dbkey": visualization.dbkey,
+            "slug": visualization.slug,
             # to_dict only the latest revision (allow older to be fetched elsewhere)
-            'latest_revision' : self.get_visualization_revision_dict(visualization.latest_revision),
-            'revisions' : [r.id for r in visualization.revisions],
+            "latest_revision": self.get_visualization_revision_dict(visualization.latest_revision),
+            "revisions": [r.id for r in visualization.revisions],
         }
 
     def get_visualization_revision_dict(self, revision):
         """
         Return a set of detailed attributes for a visualization in dictionary form.
         NOTE: that encoding ids isn't done here should happen at the caller level.
         """
         return {
-            'model_class'      : 'VisualizationRevision',
-            'id'               : revision.id,
-            'visualization_id' : revision.visualization.id,
-            'title'            : revision.title,
-            'dbkey'            : revision.dbkey,
-            'config'           : revision.config,
+            "model_class": "VisualizationRevision",
+            "id": revision.id,
+            "visualization_id": revision.visualization.id,
+            "title": revision.title,
+            "dbkey": revision.dbkey,
+            "config": revision.config,
         }
 
     def import_visualization(self, trans, id, user=None):
         """
         Copy the visualization with the given id and associate the copy
         with the given user (defaults to trans.user).
 
@@ -789,39 +754,52 @@
             if not trans.user:
                 raise exceptions.ItemAccessibilityException("You must be logged in to import Galaxy visualizations")
             user = trans.user
 
         # check accessibility
         visualization = self.get_visualization(trans, id, check_ownership=False)
         if not visualization.importable:
-            raise exceptions.ItemAccessibilityException("The owner of this visualization has disabled imports via this link.")
+            raise exceptions.ItemAccessibilityException(
+                "The owner of this visualization has disabled imports via this link."
+            )
         if visualization.deleted:
             raise exceptions.ItemDeletionException("You can't import this visualization because it has been deleted.")
 
         # copy vis and alter title
         # TODO: need to handle custom db keys.
-        imported_visualization = visualization.copy(user=user, title="imported: " + visualization.title)
+        imported_visualization = visualization.copy(user=user, title=f"imported: {visualization.title}")
         trans.sa_session.add(imported_visualization)
         trans.sa_session.flush()
         return imported_visualization
 
-    def create_visualization(self, trans, type, title="Untitled Visualization", slug=None,
-                             dbkey=None, annotation=None, config={}, save=True):
+    def create_visualization(
+        self,
+        trans,
+        type,
+        title="Untitled Visualization",
+        slug=None,
+        dbkey=None,
+        annotation=None,
+        config=None,
+        save=True,
+    ):
         """
         Create visualiation and first revision.
         """
+        config = config or {}
         visualization = self._create_visualization(trans, title, type, dbkey, slug, annotation, save)
         # TODO: handle this error structure better either in _create or here
         if isinstance(visualization, dict):
             err_dict = visualization
-            raise ValueError(err_dict['title_err'] or err_dict['slug_err'])
+            raise ValueError(err_dict["title_err"] or err_dict["slug_err"])
 
         # Create and save first visualization revision
-        revision = trans.model.VisualizationRevision(visualization=visualization, title=title,
-                                                     config=config, dbkey=dbkey)
+        revision = trans.model.VisualizationRevision(
+            visualization=visualization, title=title, config=config, dbkey=dbkey
+        )
         visualization.latest_revision = revision
 
         if save:
             session = trans.sa_session
             session.add(revision)
             session.flush()
 
@@ -831,15 +809,18 @@
         """
         Adds a new `VisualizationRevision` to the given `visualization` with
         the given parameters and set its parent visualization's `latest_revision`
         to the new revision.
         """
         # precondition: only add new revision on owned vis's
         # TODO:?? should we default title, dbkey, config? to which: visualization or latest_revision?
-        revision = trans.model.VisualizationRevision(visualization, title, dbkey, config)
+        revision = trans.model.VisualizationRevision(
+            visualization=visualization, title=title, dbkey=dbkey, config=config
+        )
+
         visualization.latest_revision = revision
         # TODO:?? does this automatically add revision to visualzation.revisions?
         trans.sa_session.add(revision)
         trans.sa_session.flush()
         return revision
 
     def save_visualization(self, trans, config, type, id=None, title=None, dbkey=None, slug=None, annotation=None):
@@ -860,73 +841,74 @@
         # do NOT alter the dbkey
         vis_rev.dbkey = vis.dbkey
         # do alter the title and config
         vis_rev.title = title
 
         # -- Validate config. --
 
-        if vis.type == 'trackster':
+        if vis.type == "trackster":
+
             def unpack_track(track_dict):
-                """ Unpack a track from its json. """
-                dataset_dict = track_dict['dataset']
+                """Unpack a track from its json."""
+                dataset_dict = track_dict["dataset"]
                 return {
-                    "dataset_id": trans.security.decode_id(dataset_dict['id']),
-                    "hda_ldda": dataset_dict.get('hda_ldda', 'hda'),
-                    "track_type": track_dict['track_type'],
-                    "prefs": track_dict['prefs'],
-                    "mode": track_dict['mode'],
-                    "filters": track_dict['filters'],
-                    "tool_state": track_dict['tool_state']
+                    "dataset_id": trans.security.decode_id(dataset_dict["id"]),
+                    "hda_ldda": dataset_dict.get("hda_ldda", "hda"),
+                    "track_type": track_dict["track_type"],
+                    "prefs": track_dict["prefs"],
+                    "mode": track_dict["mode"],
+                    "filters": track_dict["filters"],
+                    "tool_state": track_dict["tool_state"],
                 }
 
             def unpack_collection(collection_json):
-                """ Unpack a collection from its json. """
+                """Unpack a collection from its json."""
                 unpacked_drawables = []
-                drawables = collection_json['drawables']
+                drawables = collection_json["drawables"]
                 for drawable_json in drawables:
-                    if 'track_type' in drawable_json:
+                    if "track_type" in drawable_json:
                         drawable = unpack_track(drawable_json)
                     else:
                         drawable = unpack_collection(drawable_json)
                     unpacked_drawables.append(drawable)
                 return {
-                    "obj_type": collection_json['obj_type'],
+                    "obj_type": collection_json["obj_type"],
                     "drawables": unpacked_drawables,
-                    "prefs": collection_json.get('prefs', []),
-                    "filters": collection_json.get('filters', None)
+                    "prefs": collection_json.get("prefs", []),
+                    "filters": collection_json.get("filters", None),
                 }
 
             # TODO: unpack and validate bookmarks:
             def unpack_bookmarks(bookmarks_json):
                 return bookmarks_json
 
             # Unpack and validate view content.
-            view_content = unpack_collection(config['view'])
-            bookmarks = unpack_bookmarks(config['bookmarks'])
+            view_content = unpack_collection(config["view"])
+            bookmarks = unpack_bookmarks(config["bookmarks"])
             vis_rev.config = {"view": view_content, "bookmarks": bookmarks}
             # Viewport from payload
-            viewport = config.get('viewport')
+            viewport = config.get("viewport")
             if viewport:
-                chrom = viewport['chrom']
-                start = viewport['start']
-                end = viewport['end']
-                overview = viewport['overview']
-                vis_rev.config["viewport"] = {'chrom': chrom, 'start': start, 'end': end, 'overview': overview}
+                chrom = viewport["chrom"]
+                start = viewport["start"]
+                end = viewport["end"]
+                overview = viewport["overview"]
+                vis_rev.config["viewport"] = {"chrom": chrom, "start": start, "end": end, "overview": overview}
         else:
             # Default action is to save the config as is with no validation.
             vis_rev.config = config
 
         vis.latest_revision = vis_rev
         session.add(vis_rev)
         session.flush()
         encoded_id = trans.security.encode_id(vis.id)
-        return {"vis_id": encoded_id, "url": url_for(controller='visualization', action=vis.type, id=encoded_id)}
+        return {"vis_id": encoded_id, "url": url_for(controller="visualization", action=vis.type, id=encoded_id)}
 
     def get_tool_def(self, trans, hda):
-        """ Returns definition of an interactive tool for an HDA. """
+        """Returns definition of an interactive tool for an HDA."""
 
         # Get dataset's job.
         job = None
         for job_output_assoc in hda.creating_job_associations:
             job = job_output_assoc.job
             break
         if not job:
@@ -938,108 +920,110 @@
 
         # Tool must have a Trackster configuration.
         if not tool.trackster_conf:
             return None
 
         # -- Get tool definition and add input values from job. --
         tool_dict = tool.to_dict(trans, io_details=True)
-        tool_param_values = dict([(p.name, p.value) for p in job.parameters])
+        tool_param_values = {p.name: p.value for p in job.parameters}
         tool_param_values = tool.params_from_strings(tool_param_values, trans.app, ignore_errors=True)
 
         # Only get values for simple inputs for now.
-        inputs_dict = [i for i in tool_dict['inputs'] if i['type'] not in ['data', 'hidden_data', 'conditional']]
+        inputs_dict = [i for i in tool_dict["inputs"] if i["type"] not in ["data", "hidden_data", "conditional"]]
         for t_input in inputs_dict:
             # Add value to tool.
-            if 'name' in t_input:
-                name = t_input['name']
+            if "name" in t_input:
+                name = t_input["name"]
                 if name in tool_param_values:
                     value = tool_param_values[name]
                     if isinstance(value, Dictifiable):
                         value = value.to_dict()
-                    t_input['value'] = value
+                    t_input["value"] = value
 
         return tool_dict
 
     def get_visualization_config(self, trans, visualization):
-        """ Returns a visualization's configuration. Only works for trackster visualizations right now. """
+        """Returns a visualization's configuration. Only works for trackster visualizations right now."""
         config = None
-        if visualization.type in ['trackster', 'genome']:
+        if visualization.type in ["trackster", "genome"]:
             # Unpack Trackster config.
             latest_revision = visualization.latest_revision
-            bookmarks = latest_revision.config.get('bookmarks', [])
+            bookmarks = latest_revision.config.get("bookmarks", [])
 
             def pack_track(track_dict):
-                dataset_id = track_dict['dataset_id']
-                hda_ldda = track_dict.get('hda_ldda', 'hda')
+                dataset_id = track_dict["dataset_id"]
+                hda_ldda = track_dict.get("hda_ldda", "hda")
                 dataset_id = trans.security.encode_id(dataset_id)
                 dataset = self.get_hda_or_ldda(trans, hda_ldda, dataset_id)
                 try:
-                    prefs = track_dict['prefs']
+                    prefs = track_dict["prefs"]
                 except KeyError:
                     prefs = {}
-                track_data_provider = trans.app.data_provider_registry.get_data_provider(trans,
-                                                                                         original_dataset=dataset,
-                                                                                         source='data')
+                track_data_provider = trans.app.data_provider_registry.get_data_provider(
+                    trans, original_dataset=dataset, source="data"
+                )
                 return {
                     "track_type": dataset.datatype.track_type,
                     "dataset": trans.security.encode_dict_ids(dataset.to_dict()),
                     "prefs": prefs,
-                    "mode": track_dict.get('mode', 'Auto'),
-                    "filters": track_dict.get('filters', {'filters' : track_data_provider.get_filters()}),
+                    "mode": track_dict.get("mode", "Auto"),
+                    "filters": track_dict.get("filters", {"filters": track_data_provider.get_filters()}),
                     "tool": self.get_tool_def(trans, dataset),
-                    "tool_state": track_dict.get('tool_state', {})
+                    "tool_state": track_dict.get("tool_state", {}),
                 }
 
             def pack_collection(collection_dict):
                 drawables = []
-                for drawable_dict in collection_dict['drawables']:
-                    if 'track_type' in drawable_dict:
+                for drawable_dict in collection_dict["drawables"]:
+                    if "track_type" in drawable_dict:
                         drawables.append(pack_track(drawable_dict))
                     else:
                         drawables.append(pack_collection(drawable_dict))
                 return {
-                    'obj_type': collection_dict['obj_type'],
-                    'drawables': drawables,
-                    'prefs': collection_dict.get('prefs', []),
-                    'filters': collection_dict.get('filters', {})
+                    "obj_type": collection_dict["obj_type"],
+                    "drawables": drawables,
+                    "prefs": collection_dict.get("prefs", []),
+                    "filters": collection_dict.get("filters", {}),
                 }
 
             def encode_dbkey(dbkey):
                 """
                 Encodes dbkey as needed. For now, prepends user's public name
                 to custom dbkey keys.
                 """
                 encoded_dbkey = dbkey
                 user = visualization.user
-                if 'dbkeys' in user.preferences and str(dbkey) in user.preferences['dbkeys']:
-                    encoded_dbkey = "%s:%s" % (user.username, dbkey)
+                if "dbkeys" in user.preferences and str(dbkey) in user.preferences["dbkeys"]:
+                    encoded_dbkey = f"{user.username}:{dbkey}"
                 return encoded_dbkey
 
             # Set tracks.
             tracks = []
-            if 'tracks' in latest_revision.config:
+            if "tracks" in latest_revision.config:
                 # Legacy code.
-                for track_dict in visualization.latest_revision.config['tracks']:
+                for track_dict in visualization.latest_revision.config["tracks"]:
                     tracks.append(pack_track(track_dict))
-            elif 'view' in latest_revision.config:
-                for drawable_dict in visualization.latest_revision.config['view']['drawables']:
-                    if 'track_type' in drawable_dict:
+            elif "view" in latest_revision.config:
+                for drawable_dict in visualization.latest_revision.config["view"]["drawables"]:
+                    if "track_type" in drawable_dict:
                         tracks.append(pack_track(drawable_dict))
                     else:
                         tracks.append(pack_collection(drawable_dict))
 
-            config = {"title": visualization.title,
-                      "vis_id": trans.security.encode_id(visualization.id) if visualization.id is not None else None,
-                      "tracks": tracks,
-                      "bookmarks": bookmarks,
-                      "chrom": "",
-                      "dbkey": encode_dbkey(visualization.dbkey)}
+            config = {
+                "title": visualization.title,
+                "vis_id": trans.security.encode_id(visualization.id) if visualization.id is not None else None,
+                "tracks": tracks,
+                "bookmarks": bookmarks,
+                "chrom": "",
+                "dbkey": encode_dbkey(visualization.dbkey),
+            }
 
-            if 'viewport' in latest_revision.config:
-                config['viewport'] = latest_revision.config['viewport']
+            if "viewport" in latest_revision.config:
+                config["viewport"] = latest_revision.config["viewport"]
         else:
             # Default action is to return config unaltered.
             latest_revision = visualization.latest_revision
             config = latest_revision.config
 
         return config
 
@@ -1052,21 +1036,21 @@
 
         # Get track definition.
         return {
             "track_type": dataset.datatype.track_type,
             "name": dataset.name,
             "dataset": trans.security.encode_dict_ids(dataset.to_dict()),
             "prefs": {},
-            "filters": {'filters' : track_data_provider.get_filters()},
+            "filters": {"filters": track_data_provider.get_filters()},
             "tool": self.get_tool_def(trans, dataset),
-            "tool_state": {}
+            "tool_state": {},
         }
 
     def get_hda_or_ldda(self, trans, hda_ldda, dataset_id):
-        """ Returns either HDA or LDDA for hda/ldda and id combination. """
+        """Returns either HDA or LDDA for hda/ldda and id combination."""
         if hda_ldda == "hda":
             return self.get_hda(trans, dataset_id, check_ownership=False, check_accessible=True)
         else:
             return self.get_library_dataset_dataset_association(trans, dataset_id)
 
     def get_hda(self, trans, dataset_id, check_ownership=True, check_accessible=False, check_state=True):
         """
@@ -1074,67 +1058,71 @@
         the current transaction.
 
         Deprecated in lieu to galaxy.managers.hdas.HDAManager.get_accessible(decoded_id, user)
         """
         try:
             dataset_id = trans.security.decode_id(dataset_id)
         except (AttributeError, TypeError):
-            raise HTTPBadRequest("Invalid dataset id: %s." % str(dataset_id))
+            raise HTTPBadRequest(f"Invalid dataset id: {str(dataset_id)}.")
 
         try:
             data = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(int(dataset_id))
         except Exception:
-            raise HTTPBadRequest("Invalid dataset id: %s." % str(dataset_id))
+            raise HTTPBadRequest(f"Invalid dataset id: {str(dataset_id)}.")
 
         if not data:
-            raise HTTPBadRequest("Invalid dataset id: %s." % str(dataset_id))
+            raise HTTPBadRequest(f"Invalid dataset id: {str(dataset_id)}.")
 
         if check_ownership:
             # Verify ownership.
             user = trans.get_user()
             if not user:
                 error("Must be logged in to manage Galaxy items")
-            if data.history.user != user:
-                error("%s is not owned by current user" % data.__class__.__name__)
+            if data.user != user:
+                error(f"{data.__class__.__name__} is not owned by current user")
 
         if check_accessible:
             current_user_roles = trans.get_current_user_roles()
 
             if not trans.app.security_agent.can_access_dataset(current_user_roles, data.dataset):
                 error("You are not allowed to access this dataset")
 
             if check_state and data.state == trans.model.Dataset.states.UPLOAD:
-                return trans.show_error_message("Please wait until this dataset finishes uploading " +
-                                                "before attempting to view it.")
+                return trans.show_error_message(
+                    "Please wait until this dataset finishes uploading " + "before attempting to view it."
+                )
         return data
 
     # -- Helper functions --
 
     def _create_visualization(self, trans, title, type, dbkey=None, slug=None, annotation=None, save=True):
-        """ Create visualization but not first revision. Returns Visualization object. """
+        """Create visualization but not first revision. Returns Visualization object."""
         user = trans.get_user()
 
         # Error checking.
         title_err = slug_err = ""
         if not title:
             title_err = "visualization name is required"
         elif slug and not managers_base.is_valid_slug(slug):
             slug_err = "visualization identifier must consist of only lowercase letters, numbers, and the '-' character"
-        elif slug and trans.sa_session.query(trans.model.Visualization).filter_by(user=user, slug=slug, deleted=False).first():
+        elif (
+            slug
+            and trans.sa_session.query(trans.model.Visualization).filter_by(user=user, slug=slug, deleted=False).first()
+        ):
             slug_err = "visualization identifier must be unique"
 
         if title_err or slug_err:
-            return {'title_err': title_err, 'slug_err': slug_err}
+            return {"title_err": title_err, "slug_err": slug_err}
 
         # Create visualization
         visualization = trans.model.Visualization(user=user, title=title, dbkey=dbkey, type=type)
         if slug:
             visualization.slug = slug
         else:
-            self.create_item_slug(trans.sa_session, visualization)
+            self.slug_builder.create_item_slug(trans.sa_session, visualization)
         if annotation:
             annotation = sanitize_html(annotation)
             # TODO: if this is to stay in the mixin, UsesAnnotations should be added to the superclasses
             #   right now this is depending on the classes that include this mixin to have UsesAnnotations
             self.add_item_annotation(trans.sa_session, trans.user, visualization, annotation)
 
         if save:
@@ -1154,34 +1142,33 @@
         data_sources = dataset.get_datasources(trans)
         query_dbkey = dataset.dbkey
         if query_dbkey == "?":
             query_dbkey = dbkey
         chroms_info = self.app.genomes.chroms(trans, dbkey=query_dbkey)
 
         # If there are no messages (messages indicate data is not ready/available), get data.
-        messages_list = [data_source_dict['message'] for data_source_dict in data_sources.values()]
+        messages_list = [data_source_dict["message"] for data_source_dict in data_sources.values()]
         message = self._get_highest_priority_msg(messages_list)
         if message:
             rval = message
         else:
             # HACK: chromatin interactions tracks use data as source.
-            source = 'index'
+            source = "index"
             if isinstance(dataset.datatype, ChromatinInteractions):
-                source = 'data'
+                source = "data"
 
-            data_provider = trans.app.data_provider_registry.get_data_provider(trans,
-                                                                               original_dataset=dataset,
-                                                                               source=source)
+            data_provider = trans.app.data_provider_registry.get_data_provider(
+                trans, original_dataset=dataset, source=source
+            )
             # HACK: pass in additional params which are used for only some
             # types of data providers; level, cutoffs used for summary tree,
             # num_samples for BBI, and interchromosomal used for chromatin interactions.
-            rval = data_provider.get_genome_data(chroms_info,
-                                                 level=4, detail_cutoff=0, draw_cutoff=0,
-                                                 num_samples=150,
-                                                 interchromosomal=True)
+            rval = data_provider.get_genome_data(
+                chroms_info, level=4, detail_cutoff=0, draw_cutoff=0, num_samples=150, interchromosomal=True
+            )
 
         return rval
 
     # FIXME: this method probably belongs down in the model.Dataset class.
     def _get_highest_priority_msg(self, message_list):
         """
         Returns highest priority message from a list of messages.
@@ -1198,173 +1185,161 @@
                     return_message = message
                 elif return_message is None and message == "pending":
                     return_message = message
         return return_message
 
 
 class UsesStoredWorkflowMixin(SharableItemSecurityMixin, UsesAnnotations):
-    """ Mixin for controllers that use StoredWorkflow objects. """
+    """Mixin for controllers that use StoredWorkflow objects."""
+
+    slug_builder = SlugBuilder()
 
     def get_stored_workflow(self, trans, id, check_ownership=True, check_accessible=False):
-        """ Get a StoredWorkflow from the database by id, verifying ownership. """
+        """Get a StoredWorkflow from the database by id, verifying ownership."""
         # Load workflow from database
         workflow_contents_manager = workflows.WorkflowsManager(self.app)
         workflow = workflow_contents_manager.get_stored_workflow(trans=trans, workflow_id=id)
 
         if not workflow:
             error("Workflow not found")
         else:
             self.security_check(trans, workflow, check_ownership, check_accessible)
 
             # Older workflows may be missing slugs, so set them here.
             if not workflow.slug:
-                self.create_item_slug(trans.sa_session, workflow)
+                self.slug_builder.create_item_slug(trans.sa_session, workflow)
                 trans.sa_session.flush()
 
         return workflow
 
-    def get_stored_workflow_steps(self, trans, stored_workflow):
-        """ Restores states for a stored workflow's steps. """
+    def get_stored_workflow_steps(self, trans, stored_workflow: model.StoredWorkflow):
+        """Restores states for a stored workflow's steps."""
         module_injector = WorkflowModuleInjector(trans)
-        for step in stored_workflow.latest_workflow.steps:
+        workflow = stored_workflow.latest_workflow
+        module_injector.inject_all(workflow, exact_tools=False, ignore_tool_missing_exception=True)
+        for step in workflow.steps:
             try:
-                module_injector.inject(step, exact_tools=False)
+                module_injector.compute_runtime_state(step)
             except exceptions.ToolMissingException:
                 pass
 
     def _import_shared_workflow(self, trans, stored):
-        """ Imports a shared workflow """
+        """Imports a shared workflow"""
         # Copy workflow.
         imported_stored = model.StoredWorkflow()
-        imported_stored.name = "imported: " + stored.name
-        workflow = stored.latest_workflow.copy()
+        imported_stored.name = f"imported: {stored.name}"
+        workflow = stored.latest_workflow.copy(user=trans.user)
         workflow.stored_workflow = imported_stored
         imported_stored.latest_workflow = workflow
         imported_stored.user = trans.user
+        imported_stored.copy_tags_from(stored.user, stored)
         # Save new workflow.
         session = trans.sa_session
         session.add(imported_stored)
         session.flush()
 
         # Copy annotations.
         self.copy_item_annotation(session, stored.user, stored, imported_stored.user, imported_stored)
         for order_index, step in enumerate(stored.latest_workflow.steps):
-            self.copy_item_annotation(session, stored.user, step,
-                                      imported_stored.user, imported_stored.latest_workflow.steps[order_index])
+            self.copy_item_annotation(
+                session, stored.user, step, imported_stored.user, imported_stored.latest_workflow.steps[order_index]
+            )
         session.flush()
         return imported_stored
 
-    def _workflow_from_dict(self, trans, data, source=None, add_to_menu=False, publish=False, exact_tools=True, fill_defaults=False, from_tool_form=False):
-        """
-        Creates a workflow from a dict. Created workflow is stored in the database and returned.
-        """
-        # TODO: replace this method with direct access to manager.
-        workflow_contents_manager = workflows.WorkflowContentsManager(self.app)
-        raw_workflow_description = workflow_contents_manager.ensure_raw_description(data)
-        created_workflow = workflow_contents_manager.build_workflow_from_raw_description(
-            trans,
-            raw_workflow_description,
-            source=source,
-            add_to_menu=add_to_menu,
-            publish=publish,
-            exact_tools=exact_tools,
-            fill_defaults=fill_defaults,
-            from_tool_form=from_tool_form,
-        )
-        return created_workflow.stored_workflow, created_workflow.missing_tools
-
     def _workflow_to_dict(self, trans, stored):
         """
         Converts a workflow to a dict of attributes suitable for exporting.
         """
         workflow_contents_manager = workflows.WorkflowContentsManager(self.app)
         return workflow_contents_manager.workflow_to_dict(
             trans,
             stored,
         )
 
 
-class UsesFormDefinitionsMixin(object):
+class UsesFormDefinitionsMixin:
     """Mixin for controllers that use Galaxy form objects."""
 
-    def get_all_forms(self, trans, all_versions=False, filter=None, form_type='All'):
+    def get_all_forms(self, trans, all_versions=False, filter=None, form_type="All"):
         """
         Return all the latest forms from the form_definition_current table
         if all_versions is set to True. Otherwise return all the versions
         of all the forms from the form_definition table.
         """
         if all_versions:
             return trans.sa_session.query(trans.app.model.FormDefinition)
         if filter:
             fdc_list = trans.sa_session.query(trans.app.model.FormDefinitionCurrent).filter_by(**filter)
         else:
             fdc_list = trans.sa_session.query(trans.app.model.FormDefinitionCurrent)
-        if form_type == 'All':
+        if form_type == "All":
             return [fdc.latest_form for fdc in fdc_list]
         else:
             return [fdc.latest_form for fdc in fdc_list if fdc.latest_form.type == form_type]
 
     def save_widget_field(self, trans, field_obj, widget_name, **kwd):
         # Save a form_builder field object
         params = util.Params(kwd)
         if isinstance(field_obj, trans.model.UserAddress):
-            field_obj.desc = util.restore_text(params.get('%s_short_desc' % widget_name, ''))
-            field_obj.name = util.restore_text(params.get('%s_name' % widget_name, ''))
-            field_obj.institution = util.restore_text(params.get('%s_institution' % widget_name, ''))
-            field_obj.address = util.restore_text(params.get('%s_address' % widget_name, ''))
-            field_obj.city = util.restore_text(params.get('%s_city' % widget_name, ''))
-            field_obj.state = util.restore_text(params.get('%s_state' % widget_name, ''))
-            field_obj.postal_code = util.restore_text(params.get('%s_postal_code' % widget_name, ''))
-            field_obj.country = util.restore_text(params.get('%s_country' % widget_name, ''))
-            field_obj.phone = util.restore_text(params.get('%s_phone' % widget_name, ''))
+            field_obj.desc = util.restore_text(params.get(f"{widget_name}_short_desc", ""))
+            field_obj.name = util.restore_text(params.get(f"{widget_name}_name", ""))
+            field_obj.institution = util.restore_text(params.get(f"{widget_name}_institution", ""))
+            field_obj.address = util.restore_text(params.get(f"{widget_name}_address", ""))
+            field_obj.city = util.restore_text(params.get(f"{widget_name}_city", ""))
+            field_obj.state = util.restore_text(params.get(f"{widget_name}_state", ""))
+            field_obj.postal_code = util.restore_text(params.get(f"{widget_name}_postal_code", ""))
+            field_obj.country = util.restore_text(params.get(f"{widget_name}_country", ""))
+            field_obj.phone = util.restore_text(params.get(f"{widget_name}_phone", ""))
             trans.sa_session.add(field_obj)
             trans.sa_session.flush()
 
     def get_form_values(self, trans, user, form_definition, **kwd):
-        '''
+        """
         Returns the name:value dictionary containing all the form values
-        '''
+        """
         params = util.Params(kwd)
         values = {}
         for field in form_definition.fields:
-            field_type = field['type']
-            field_name = field['name']
-            input_value = params.get(field_name, '')
+            field_type = field["type"]
+            field_name = field["name"]
+            input_value = params.get(field_name, "")
             if field_type == AddressField.__name__:
                 input_text_value = util.restore_text(input_value)
-                if input_text_value == 'new':
+                if input_text_value == "new":
                     # Save this new address in the list of this user's addresses
                     user_address = trans.model.UserAddress(user=user)
                     self.save_widget_field(trans, user_address, field_name, **kwd)
                     trans.sa_session.refresh(user)
                     field_value = int(user_address.id)
-                elif input_text_value in ['', 'none', 'None', None]:
-                    field_value = ''
+                elif input_text_value in ["", "none", "None", None]:
+                    field_value = ""
                 else:
                     field_value = int(input_text_value)
             elif field_type == CheckboxField.__name__:
                 field_value = CheckboxField.is_checked(input_value)
             elif field_type == PasswordField.__name__:
-                field_value = kwd.get(field_name, '')
+                field_value = kwd.get(field_name, "")
             else:
                 field_value = util.restore_text(input_value)
             values[field_name] = field_value
         return values
 
 
-class SharableMixin(object):
-    """ Mixin for a controller that manages an item that can be shared. """
+class SharableMixin:
+    """Mixin for a controller that manages an item that can be shared."""
 
-    manager = None
-    serializer = None
+    manager: Any = None
+    serializer: Any = None
+    slug_builder = SlugBuilder()
 
     # -- Implemented methods. --
 
     def _is_valid_slug(self, slug):
-        """ Returns true if slug is valid. """
+        """Returns true if slug is valid."""
         return managers_base.is_valid_slug(slug)
 
     @web.expose
     @web.require_login("modify Galaxy items")
     def set_slug_async(self, trans, id, new_slug):
         item = self.get_item(trans, id)
         if item:
@@ -1372,171 +1347,57 @@
             if trans.sa_session.query(item.__class__).filter_by(user=item.user, slug=new_slug).count() == 0:
                 item.slug = new_slug
                 trans.sa_session.flush()
 
         return item.slug
 
     def _make_item_accessible(self, sa_session, item):
-        """ Makes item accessible--viewable and importable--and sets item's slug.
-            Does not flush/commit changes, however. Item must have name, user,
-            importable, and slug attributes. """
+        """Makes item accessible--viewable and importable--and sets item's slug.
+        Does not flush/commit changes, however. Item must have name, user,
+        importable, and slug attributes."""
         item.importable = True
-        self.create_item_slug(sa_session, item)
-
-    def create_item_slug(self, sa_session, item):
-        """ Create/set item slug. Slug is unique among user's importable items
-            for item's class. Returns true if item's slug was set/changed; false
-            otherwise.
-        """
-        cur_slug = item.slug
-
-        # Setup slug base.
-        if cur_slug is None or cur_slug == "":
-            # Item can have either a name or a title.
-            if hasattr(item, 'name'):
-                item_name = item.name
-            elif hasattr(item, 'title'):
-                item_name = item.title
-            slug_base = util.ready_name_for_url(item_name.lower())
-        else:
-            slug_base = cur_slug
-
-        # Using slug base, find a slug that is not taken. If slug is taken,
-        # add integer to end.
-        new_slug = slug_base
-        count = 1
-        # Ensure unique across model class and user and don't include this item
-        # in the check in case it has previously been assigned a valid slug.
-        while sa_session.query(item.__class__).filter(item.__class__.user == item.user, item.__class__.slug == new_slug, item.__class__.id != item.id).count() != 0:
-            # Slug taken; choose a new slug based on count. This approach can
-            # handle numerous items with the same name gracefully.
-            new_slug = '%s-%i' % (slug_base, count)
-            count += 1
-
-        # Set slug and return.
-        item.slug = new_slug
-        return item.slug == cur_slug
-
-    @web.legacy_expose_api
-    def sharing(self, trans, id, payload=None, **kwd):
-        skipped = False
-        class_name = self.manager.model_class.__name__
-        item = self.get_object(trans, id, class_name, check_ownership=True, check_accessible=True, deleted=False)
-        if payload and payload.get("action"):
-            action = payload.get("action")
-            if action == "make_accessible_via_link":
-                self._make_item_accessible(trans.sa_session, item)
-                if hasattr(item, "has_possible_members") and item.has_possible_members and payload.get("make_members_public", False):
-                    shared, skipped = self._make_members_public(trans, item)
-            elif action == "make_accessible_and_publish":
-                self._make_item_accessible(trans.sa_session, item)
-                if hasattr(item, "has_possible_members") and item.has_possible_members and payload.get("make_members_public", False):
-                    shared, skipped = self._make_members_public(trans, item)
-                item.published = True
-            elif action == "publish":
-                if item.importable:
-                    item.published = True
-                    if hasattr(item, "has_possible_members") and item.has_possible_members and payload.get("make_members_public", False):
-                        shared, skipped = self._make_members_public(trans, item)
-                else:
-                    raise exceptions.MessageException("%s not importable." % class_name)
-            elif action == "disable_link_access":
-                item.importable = False
-            elif action == "unpublish":
-                item.published = False
-            elif action == "disable_link_access_and_unpublish":
-                item.importable = item.published = False
-            elif action == "unshare_user":
-                user = trans.sa_session.query(trans.app.model.User).get(self.decode_id(payload.get("user_id")))
-                class_name_lc = class_name.lower()
-                ShareAssociation = getattr(trans.app.model, "%sUserShareAssociation" % class_name)
-                usas = trans.sa_session.query(ShareAssociation).filter_by(**{"user": user, class_name_lc: item}).all()
-                if not usas:
-                    raise exceptions.MessageException("%s was not shared with user." % class_name)
-                for usa in usas:
-                    trans.sa_session.delete(usa)
-            trans.sa_session.add(item)
-            trans.sa_session.flush()
-        if item.importable and not item.slug:
-            self._make_item_accessible(trans.sa_session, item)
-        item_dict = self.serializer.serialize_to_view(item,
-            user=trans.user, trans=trans, default_view="sharing")
-        item_dict["users_shared_with"] = [{"id": self.app.security.encode_id(a.user.id), "email": a.user.email} for a in item.users_shared_with]
-        if skipped:
-            item_dict["skipped"] = True
-        return item_dict
-
-    def _make_members_public(self, trans, item):
-        """ Make the non-purged datasets in history public
-        Performs pemissions check.
-        """
-        # TODO eventually we should handle more classes than just History
-        skipped = False
-        for hda in item.activatable_datasets:
-            dataset = hda.dataset
-            if not trans.app.security_agent.dataset_is_public(dataset):
-                if trans.app.security_agent.can_manage_dataset(trans.user.all_roles(), dataset):
-                    try:
-                        trans.app.security_agent.make_dataset_public(hda.dataset)
-                    except Exception:
-                        log.warning("Unable to make dataset with id: %s public", dataset.id)
-                        skipped = True
-                else:
-                    log.warning("User without permissions tried to make dataset with id: %s public", dataset.id)
-                    skipped = True
-        return item, skipped
+        self.slug_builder.create_item_slug(sa_session, item)
 
     # -- Abstract methods. --
 
     @web.expose
     @web.require_login("share Galaxy items")
     def share(self, trans, id=None, email="", **kwd):
-        """ Handle sharing an item with a particular user. """
+        """Handle sharing an item with a particular user."""
         raise NotImplementedError()
 
     @web.expose
-    def display_by_username_and_slug(self, trans, username, slug):
-        """ Display item by username and slug. """
+    def display_by_username_and_slug(self, trans, username, slug, **kwargs):
+        """Display item by username and slug."""
         raise NotImplementedError()
 
     @web.json
     @web.require_login("get item name and link")
     def get_name_and_link_async(self, trans, id=None):
-        """ Returns item's name and link. """
-        raise NotImplementedError()
-
-    @web.expose
-    @web.require_login("get item content asynchronously")
-    def get_item_content_async(self, trans, id):
-        """ Returns item content in HTML format. """
+        """Returns item's name and link."""
         raise NotImplementedError()
 
     def get_item(self, trans, id):
-        """ Return item based on id. """
+        """Return item based on id."""
         raise NotImplementedError()
 
 
-class UsesQuotaMixin(object):
-
-    def get_quota(self, trans, id, check_ownership=False, check_accessible=False, deleted=None):
-        return self.get_object(trans, id, 'Quota', check_ownership=False, check_accessible=False, deleted=deleted)
-
-
 class UsesTagsMixin(SharableItemSecurityMixin):
-
-    def get_tag_handler(self, trans):
+    def get_tag_handler(self, trans) -> tags.GalaxyTagHandler:
         return trans.app.tag_handler
 
     def _get_user_tags(self, trans, item_class_name, id):
         user = trans.user
         tagged_item = self._get_tagged_item(trans, item_class_name, id)
         return [tag for tag in tagged_item.tags if tag.user == user]
 
     def _get_tagged_item(self, trans, item_class_name, id, check_ownership=True):
-        tagged_item = self.get_object(trans, id, item_class_name, check_ownership=check_ownership, check_accessible=True)
+        tagged_item = self.get_object(
+            trans, id, item_class_name, check_ownership=check_ownership, check_accessible=True
+        )
         return tagged_item
 
     def _remove_items_tag(self, trans, item_class_name, id, tag_name):
         """Remove a tag from an item."""
         user = trans.user
         tagged_item = self._get_tagged_item(trans, item_class_name, id)
         deleted = tagged_item and self.get_tag_handler(trans).remove_item_tag(trans, user, tagged_item, tag_name)
@@ -1549,15 +1410,15 @@
         tag_assoc = self.get_tag_handler(trans).apply_item_tag(user, tagged_item, tag_name, tag_value)
         trans.sa_session.flush()
         return tag_assoc
 
     def _get_item_tag_assoc(self, trans, item_class_name, id, tag_name):
         user = trans.user
         tagged_item = self._get_tagged_item(trans, item_class_name, id)
-        log.debug("In get_item_tag_assoc with tagged_item %s" % tagged_item)
+        log.debug(f"In get_item_tag_assoc with tagged_item {tagged_item}")
         return self.get_tag_handler(trans)._get_item_tag_assoc(user, tagged_item, tag_name)
 
     def set_tags_from_list(self, trans, item, new_tags_list, user=None):
         tag_handler = tags.GalaxyTagHandler(trans.app.model.context)
         return tag_handler.set_tags_from_list(user, item, new_tags_list)
 
     def get_user_tags_used(self, trans, user=None):
@@ -1574,58 +1435,63 @@
             return []
 
         # get all the taggable model TagAssociations
         tag_models = [v.tag_assoc_class for v in trans.app.tag_handler.item_tag_assoc_info.values()]
         # create a union of subqueries for each for this user - getting only the tname and user_value
         all_tags_query = None
         for tag_model in tag_models:
-            subq = (trans.sa_session.query(tag_model.user_tname, tag_model.user_value)
-                    .filter(tag_model.user == trans.user))
+            subq = trans.sa_session.query(tag_model.user_tname, tag_model.user_value).filter(
+                tag_model.user == trans.user
+            )
             all_tags_query = subq if all_tags_query is None else all_tags_query.union(subq)
 
         # if nothing init'd the query, bail
         if all_tags_query is None:
             return []
 
         # boil the tag tuples down into a sorted list of DISTINCT name:val strings
         tags = all_tags_query.distinct().all()
-        tags = [((name + ':' + val) if val else name) for name, val in tags]
+        tags = [(f"{name}:{val}" if val else name) for name, val in tags]
         return sorted(tags)
 
 
 class UsesExtendedMetadataMixin(SharableItemSecurityMixin):
-    """ Mixin for getting and setting item extended metadata. """
+    """Mixin for getting and setting item extended metadata."""
 
     def get_item_extended_metadata_obj(self, trans, item):
         """
         Given an item object (such as a LibraryDatasetDatasetAssociation), find the object
         of the associated extended metadata
         """
         if item.extended_metadata:
             return item.extended_metadata
         return None
 
     def set_item_extended_metadata_obj(self, trans, item, extmeta_obj, check_writable=False):
         if item.__class__ == LibraryDatasetDatasetAssociation:
-            if not check_writable or trans.app.security_agent.can_modify_library_item(trans.get_current_user_roles(), item, trans.user):
+            if not check_writable or trans.app.security_agent.can_modify_library_item(
+                trans.get_current_user_roles(), item, trans.user
+            ):
                 item.extended_metadata = extmeta_obj
                 trans.sa_session.flush()
         if item.__class__ == HistoryDatasetAssociation:
             history = None
             if check_writable:
                 history = self.security_check(trans, item, check_ownership=True, check_accessible=True)
             else:
                 history = self.security_check(trans, item, check_ownership=False, check_accessible=True)
             if history:
                 item.extended_metadata = extmeta_obj
                 trans.sa_session.flush()
 
     def unset_item_extended_metadata_obj(self, trans, item, check_writable=False):
         if item.__class__ == LibraryDatasetDatasetAssociation:
-            if not check_writable or trans.app.security_agent.can_modify_library_item(trans.get_current_user_roles(), item, trans.user):
+            if not check_writable or trans.app.security_agent.can_modify_library_item(
+                trans.get_current_user_roles(), item, trans.user
+            ):
                 item.extended_metadata = None
                 trans.sa_session.flush()
         if item.__class__ == HistoryDatasetAssociation:
             history = None
             if check_writable:
                 history = self.security_check(trans, item, check_ownership=True, check_accessible=True)
             else:
@@ -1665,33 +1531,22 @@
         /data == [1,2,3]
 
         /data/[0] == 1
 
         """
         if isinstance(meta, dict):
             for a in meta:
-                for path, value in self._scan_json_block(meta[a], prefix + "/" + a):
-                    yield path, value
+                yield from self._scan_json_block(meta[a], f"{prefix}/{a}")
         elif isinstance(meta, list):
             for i, a in enumerate(meta):
-                for path, value in self._scan_json_block(a, prefix + "[%d]" % (i)):
-                    yield path, value
+                yield from self._scan_json_block(a, prefix + "[%d]" % (i))
         else:
             # BUG: Everything is cast to string, which can lead to false positives
             # for cross type comparisions, ie "True" == True
-            yield prefix, ("%s" % (meta)).encode("utf8", errors='replace')
-
-
-class ControllerUnavailable(Exception):
-    """
-    Deprecated: `BaseController` used to be available under the name `Root`
-    """
-    pass
-
-# ---- Utility methods -------------------------------------------------------
+            yield prefix, (f"{meta}").encode()
 
 
 def sort_by_attr(seq, attr):
     """
     Sort the sequence of objects by object's attribute
     Arguments:
     seq  - the list or any sequence (including immutable one) of objects to sort.
@@ -1701,7 +1556,19 @@
     # Create the auxiliary list of tuples where every i-th tuple has form
     # (seq[i].attr, i, seq[i]) and sort it. The second item of tuple is needed not
     # only to provide stable sorting, but mainly to eliminate comparison of objects
     # (which can be expensive or prohibited) in case of equal attribute values.
     intermed = [(getattr(v, attr), i, v) for i, v in enumerate(seq)]
     intermed.sort()
     return [_[-1] for _ in intermed]
+
+
+__all__ = (
+    "HTTPBadRequest",
+    "SharableMixin",
+    "sort_by_attr",
+    "url_for",
+    "UsesExtendedMetadataMixin",
+    "UsesFormDefinitionsMixin",
+    "UsesTagsMixin",
+    "web",
+)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/_fetch_util.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/_fetch_util.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,33 +1,27 @@
 import logging
 import os
 
 from galaxy.actions.library import (
     validate_path_upload,
     validate_server_directory_upload,
 )
-from galaxy.exceptions import (
-    RequestParameterInvalidException
-)
+from galaxy.exceptions import RequestParameterInvalidException
+from galaxy.files.uris import validate_non_local
 from galaxy.model.store.discover import (
     get_required_item,
     replace_request_syntax_sugar,
 )
-from galaxy.tools.actions.upload_common import (
-    validate_datatype_extension,
-    validate_url,
-)
-from galaxy.util import (
-    relpath,
-)
+from galaxy.schema.fields import DecodedDatabaseIdField
+from galaxy.tools.actions.upload_common import validate_datatype_extension
+from galaxy.util import relpath
 
 log = logging.getLogger(__name__)
 
 VALID_DESTINATION_TYPES = ["library", "library_folder", "hdca", "hdas"]
-ELEMENTS_FROM_TYPE = ["archive", "bagit", "bagit_archive", "directory"]
 # These elements_from cannot be sym linked to because they only exist during upload.
 ELEMENTS_FROM_TRANSIENT_TYPES = ["archive", "bagit_archive"]
 
 
 def validate_and_normalize_targets(trans, payload):
     """Validate and normalize all src references in fetch targets.
 
@@ -40,66 +34,63 @@
       as needed for each upload.
     """
     targets = payload.get("targets", [])
 
     for target in targets:
         destination = get_required_item(target, "destination", "Each target must specify a 'destination'")
         destination_type = get_required_item(destination, "type", "Each target destination must specify a 'type'")
-        if "object_id" in destination:
-            raise RequestParameterInvalidException("object_id not allowed to appear in the request.")
 
-        if destination_type not in VALID_DESTINATION_TYPES:
-            template = "Invalid target destination type [%s] encountered, must be one of %s"
-            msg = template % (destination_type, VALID_DESTINATION_TYPES)
-            raise RequestParameterInvalidException(msg)
         if destination_type == "library":
             library_name = get_required_item(destination, "name", "Must specify a library name")
             description = destination.get("description", "")
             synopsis = destination.get("synopsis", "")
-            library = trans.app.library_manager.create(
-                trans, library_name, description=description, synopsis=synopsis
-            )
+            library = trans.app.library_manager.create(trans, library_name, description=description, synopsis=synopsis)
             destination["type"] = "library_folder"
             for key in ["name", "description", "synopsis"]:
                 if key in destination:
                     del destination[key]
-            destination["library_folder_id"] = trans.app.security.encode_id(library.root_folder.id)
+            destination["library_folder_id"] = DecodedDatabaseIdField.encode(library.root_folder.id)
 
     # Unlike upload.py we don't transmit or use run_as_real_user in the job - we just make sure
     # in_place and purge_source are set on the individual upload fetch sources as needed based
     # on this.
     run_as_real_user = trans.app.config.external_chown_script is not None  # See comment in upload.py
-    purge_ftp_source = getattr(trans.app.config, 'ftp_upload_purge', True) and not run_as_real_user
+    purge_ftp_source = getattr(trans.app.config, "ftp_upload_purge", True) and not run_as_real_user
 
     payload["check_content"] = trans.app.config.check_upload_content
 
     def check_src(item):
-        if "object_id" in item:
-            raise RequestParameterInvalidException("object_id not allowed to appear in the request.")
-
-        validate_datatype_extension(datatypes_registry=trans.app.datatypes_registry, ext=item.get('ext'))
+        validate_datatype_extension(datatypes_registry=trans.app.datatypes_registry, ext=item.get("ext"))
 
         # Normalize file:// URLs into paths.
-        if item["src"] == "url" and item["url"].startswith("file://"):
-            item["src"] = "path"
-            item["path"] = item["url"][len("file://"):]
-            del item["path"]
+        if item["src"] == "url":
+            if "url" not in item:
+                raise RequestParameterInvalidException("src specified as 'url' but 'url' not specified")
 
-        if "in_place" in item:
-            raise RequestParameterInvalidException("in_place cannot be set in the upload request")
+            url = item["url"]
+            if url.startswith("file://"):
+                item["src"] = "path"
+                item["path"] = url[len("file://") :]
+                del item["url"]
 
         src = item["src"]
+        if src == "pasted":
+            if item.get("url") is not None:
+                raise RequestParameterInvalidException(
+                    "Cannot specify a 'url' when fetching data with pasted content 'src'"
+                )
+            if "paste_content" not in item:
+                raise RequestParameterInvalidException("src of type 'pasted' requires paste_content field")
+        else:
+            if "paste_content" in item:
+                raise RequestParameterInvalidException(
+                    "'paste_content' field can only be specified with src of type 'pasted'"
+                )
 
-        # Check link_data_only can only be set for certain src types and certain elements_from types.
         _handle_invalid_link_data_only_elements_type(item)
-        if src not in ["path", "server_dir"]:
-            _handle_invalid_link_data_only_type(item)
-        elements_from = item.get("elements_from", None)
-        if elements_from and elements_from not in ELEMENTS_FROM_TYPE:
-            raise RequestParameterInvalidException("Invalid elements_from/items_from found in request")
 
         if src == "path" or (src == "url" and item["url"].startswith("file:")):
             # Validate is admin, leave alone.
             validate_path_upload(trans)
         elif src == "server_dir":
             # Validate and replace with path definition.
             server_dir = item["server_dir"]
@@ -111,15 +102,15 @@
             full_path = None
 
             # It'd be nice if this can be de-duplicated with what is in parameters/grouping.py.
             user_ftp_dir = trans.user_ftp_dir
             is_directory = False
 
             assert not os.path.islink(user_ftp_dir), "User FTP directory cannot be a symbolic link"
-            for (dirpath, dirnames, filenames) in os.walk(user_ftp_dir):
+            for dirpath, dirnames, filenames in os.walk(user_ftp_dir):
                 for filename in filenames:
                     if ftp_path == filename:
                         path = relpath(os.path.join(dirpath, filename), user_ftp_dir)
                         if not os.path.islink(os.path.join(dirpath, filename)):
                             full_path = os.path.abspath(os.path.join(user_ftp_dir, path))
                             break
 
@@ -129,15 +120,15 @@
                         if not os.path.islink(os.path.join(dirpath, dirname)):
                             full_path = os.path.abspath(os.path.join(user_ftp_dir, path))
                             is_directory = True
                             break
 
             if is_directory:
                 # If the target is a directory - make sure no files under it are symbolic links
-                for (dirpath, dirnames, filenames) in os.walk(full_path):
+                for dirpath, dirnames, filenames in os.walk(full_path):
                     for filename in filenames:
                         if ftp_path == filename:
                             path = relpath(os.path.join(dirpath, filename), full_path)
                             if not os.path.islink(os.path.join(dirpath, filename)):
                                 full_path = False
                                 break
 
@@ -145,59 +136,61 @@
                         if ftp_path == dirname:
                             path = relpath(os.path.join(dirpath, filename), full_path)
                             if not os.path.islink(os.path.join(dirpath, filename)):
                                 full_path = False
                                 break
 
             if not full_path:
-                raise RequestParameterInvalidException("Failed to find referenced ftp_path or symbolic link was enountered")
+                raise RequestParameterInvalidException(
+                    "Failed to find referenced ftp_path or symbolic link was enountered"
+                )
 
             item["src"] = "path"
             item["path"] = full_path
             item["purge_source"] = purge_ftp_source
         elif src == "url":
             url = item["url"]
             looks_like_url = False
             for url_prefix in ["http://", "https://", "ftp://", "ftps://"]:
                 if url.startswith(url_prefix):
                     looks_like_url = True
                     break
 
+            if not looks_like_url and trans.app.file_sources.looks_like_uri(url):
+                looks_like_url = True
+
             if not looks_like_url:
-                raise RequestParameterInvalidException("Invalid URL [%s] found in src definition." % url)
+                raise RequestParameterInvalidException(f"Invalid URL [{url}] found in src definition.")
 
-            validate_url(url, trans.app.config.fetch_url_whitelist_ips)
+            validate_non_local(url, trans.app.config.fetch_url_allowlist_ips)
             item["in_place"] = run_as_real_user
         elif src == "files":
             item["in_place"] = run_as_real_user
+            item["purge_source"] = True
 
         # Small disagreement with traditional uploads - we purge less by default since whether purging
         # happens varies based on upload options in non-obvious ways.
         # https://github.com/galaxyproject/galaxy/issues/5361
         if "purge_source" not in item:
             item["purge_source"] = False
 
     replace_request_syntax_sugar(targets)
     _for_each_src(check_src, targets)
 
 
-def _handle_invalid_link_data_only_type(item):
-    link_data_only = item.get("link_data_only", False)
-    if link_data_only:
-        raise RequestParameterInvalidException("link_data_only is invalid for src type [%s]" % item.get("src"))
-
-
 def _handle_invalid_link_data_only_elements_type(item):
     link_data_only = item.get("link_data_only", False)
     if link_data_only and item.get("elements_from", False) in ELEMENTS_FROM_TRANSIENT_TYPES:
-        raise RequestParameterInvalidException("link_data_only is invalid for derived elements from [%s]" % item.get("elements_from"))
+        raise RequestParameterInvalidException(
+            f"link_data_only is invalid for derived elements from [{item.get('elements_from')}]"
+        )
 
 
 def _for_each_src(f, obj):
     if isinstance(obj, list):
         for item in obj:
             _for_each_src(f, item)
     if isinstance(obj, dict):
         if "src" in obj:
             f(obj)
-        for key, value in obj.items():
+        for value in obj.values():
             _for_each_src(f, value)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/annotations.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/annotations.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,93 +1,95 @@
 """
 API operations on annotations.
 """
 import logging
+from abc import abstractmethod
 
 from galaxy import (
     exceptions,
-    managers
+    managers,
 )
+from galaxy.managers.context import ProvidesHistoryContext
 from galaxy.model.item_attrs import UsesAnnotations
 from galaxy.util.sanitize_html import sanitize_html
 from galaxy.web import expose_api
-from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    UsesStoredWorkflowMixin
+from galaxy.webapps.base.controller import UsesStoredWorkflowMixin
+from . import (
+    BaseGalaxyAPIController,
+    depends,
 )
 
 log = logging.getLogger(__name__)
 
 
-class BaseAnnotationsController(BaseAPIController, UsesStoredWorkflowMixin, UsesAnnotations):
+class BaseAnnotationsController(BaseGalaxyAPIController, UsesStoredWorkflowMixin, UsesAnnotations):
+    tagged_item_id: str
 
     @expose_api
-    def index(self, trans, **kwd):
+    def index(self, trans: ProvidesHistoryContext, **kwd):
         idnum = kwd[self.tagged_item_id]
         item = self._get_item_from_id(trans, idnum)
         if item is not None:
-            return self.get_item_annotation_str(trans.sa_session, trans.get_user(), item)
+            return self.get_item_annotation_str(trans.sa_session, trans.user, item)
 
     @expose_api
-    def create(self, trans, payload, **kwd):
+    def create(self, trans: ProvidesHistoryContext, payload: dict, **kwd):
         if "text" not in payload:
             return ""
         idnum = kwd[self.tagged_item_id]
         item = self._get_item_from_id(trans, idnum)
         if item is not None:
             new_annotation = payload.get("text")
             # TODO: sanitize on display not entry
             new_annotation = sanitize_html(new_annotation)
 
-            self.add_item_annotation(trans.sa_session, trans.get_user(), item, new_annotation)
+            self.add_item_annotation(trans.sa_session, trans.user, item, new_annotation)
             trans.sa_session.flush()
             return new_annotation
         return ""
 
     @expose_api
-    def delete(self, trans, **kwd):
+    def delete(self, trans: ProvidesHistoryContext, **kwd):
         idnum = kwd[self.tagged_item_id]
         item = self._get_item_from_id(trans, idnum)
         if item is not None:
-            return self.delete_item_annotation(trans.sa_session, trans.get_user(), item)
+            return self.delete_item_annotation(trans.sa_session, trans.user, item)
 
     @expose_api
-    def undelete(self, trans, **kwd):
+    def undelete(self, trans: ProvidesHistoryContext, **kwd):
         raise exceptions.NotImplemented()
 
+    @abstractmethod
+    def _get_item_from_id(self, trans: ProvidesHistoryContext, idstr):
+        """Return item with annotation association."""
+
 
 class HistoryAnnotationsController(BaseAnnotationsController):
     controller_name = "history_annotations"
     tagged_item_id = "history_id"
+    history_manager: managers.histories.HistoryManager = depends(managers.histories.HistoryManager)
 
-    def __init__(self, app):
-        super(HistoryAnnotationsController, self).__init__(app)
-        self.history_manager = managers.histories.HistoryManager(app)
-
-    def _get_item_from_id(self, trans, idstr):
+    def _get_item_from_id(self, trans: ProvidesHistoryContext, idstr):
         decoded_idstr = self.decode_id(idstr)
         history = self.history_manager.get_accessible(decoded_idstr, trans.user, current_history=trans.history)
         return history
 
 
 class HistoryContentAnnotationsController(BaseAnnotationsController):
     controller_name = "history_content_annotations"
     tagged_item_id = "history_content_id"
+    hda_manager: managers.hdas.HDAManager = depends(managers.hdas.HDAManager)
 
-    def __init__(self, app):
-        super(HistoryContentAnnotationsController, self).__init__(app)
-        self.hda_manager = managers.hdas.HDAManager(app)
-
-    def _get_item_from_id(self, trans, idstr):
+    def _get_item_from_id(self, trans: ProvidesHistoryContext, idstr):
         decoded_idstr = self.decode_id(idstr)
         hda = self.hda_manager.get_accessible(decoded_idstr, trans.user)
         hda = self.hda_manager.error_if_uploading(hda)
         return hda
 
 
 class WorkflowAnnotationsController(BaseAnnotationsController):
     controller_name = "workflow_annotations"
     tagged_item_id = "workflow_id"
 
-    def _get_item_from_id(self, trans, idstr):
+    def _get_item_from_id(self, trans: ProvidesHistoryContext, idstr):
         hda = self.get_stored_workflow(trans, idstr)
         return hda
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/cloud.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/cloud.py`

 * *Files 13% similar despite different names*

```diff
@@ -4,55 +4,61 @@
 
 import logging
 
 from galaxy import exceptions
 from galaxy.exceptions import ActionInputError
 from galaxy.managers import (
     cloud,
-    datasets
+    datasets,
 )
+from galaxy.structured_app import StructuredApp
 from galaxy.web import expose_api
-from galaxy.webapps.base.controller import BaseAPIController
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class CloudController(BaseAPIController):
+class CloudController(BaseGalaxyAPIController):
     """
     RESTfull controller for interaction with Amazon S3.
     """
 
-    def __init__(self, app):
-        super(CloudController, self).__init__(app)
-        self.cloud_manager = cloud.CloudManager(app)
-        self.datasets_serializer = datasets.DatasetSerializer(app)
+    def __init__(
+        self, app: StructuredApp, cloud_manager: cloud.CloudManager, datasets_serializer: datasets.DatasetSerializer
+    ):
+        super().__init__(app)
+        self.cloud_manager = cloud_manager
+        self.datasets_serializer = datasets_serializer
 
     @expose_api
     def index(self, trans, **kwargs):
         """
-        * GET /api/cloud/storage
-            Lists cloud-based buckets (e.g., S3 bucket, Azure blob) user has defined.
-        :param trans:
-        :param kwargs:
+        GET /api/cloud/storage
+
+        Lists cloud-based buckets (e.g., S3 bucket, Azure blob) user has defined.
+
         :return: A list of cloud-based buckets user has defined.
         """
         # TODO: This can be implemented leveraging PluggedMedia objects (part of the user-based object store project)
         trans.response.status = 501
-        return 'Not Implemented'
+        return "Not Implemented"
 
     @expose_api
     def get(self, trans, payload, **kwargs):
         """
-        * POST /api/cloud/storage/get
-            gets given objects from a given cloud-based bucket to a Galaxy history.
-        :type  trans: galaxy.web.framework.webapp.GalaxyWebTransaction
+        POST /api/cloud/storage/get
+
+        gets given objects from a given cloud-based bucket to a Galaxy history.
+
+        :type  trans: galaxy.webapps.base.webapp.GalaxyWebTransaction
         :param trans: Galaxy web transaction
 
         :type  payload: dict
         :param payload: A dictionary structure containing the following keys:
+
             *   history_id:    the (encoded) id of history to which the object should be received to.
             *   bucket:        the name of a bucket from which data should be fetched from (e.g., a bucket name on AWS S3).
             *   objects:       a list of the names of objects to be fetched.
             *   authz_id:      the encoded ID of CloudAuthz to be used for authorizing access to the resource
                                provider. You may get a list of the defined authorizations via
                                `/api/cloud/authz`. Also, you can use `/api/cloud/authz/create` to define a
                                new authorization.
@@ -80,18 +86,21 @@
                                                         "False" if you upload a gzip, bz2 or zip archive
                                                         containing a binary file.
 
         :param kwargs:
 
         :rtype:  dictionary
         :return: a dictionary containing a `summary` view of the datasets copied from the given cloud-based storage.
+
         """
         if not isinstance(payload, dict):
-            raise ActionInputError('Invalid payload data type. The payload is expected to be a dictionary, '
-                                   'but received data of type `{}`.'.format(str(type(payload))))
+            raise ActionInputError(
+                "Invalid payload data type. The payload is expected to be a dictionary, "
+                "but received data of type `{}`.".format(str(type(payload)))
+            )
 
         missing_arguments = []
         encoded_history_id = payload.get("history_id", None)
         if encoded_history_id is None:
             missing_arguments.append("history_id")
 
         bucket = payload.get("bucket", None)
@@ -103,54 +112,60 @@
             missing_arguments.append("objects")
 
         encoded_authz_id = payload.get("authz_id", None)
         if encoded_authz_id is None:
             missing_arguments.append("authz_id")
 
         if len(missing_arguments) > 0:
-            raise ActionInputError("The following required arguments are missing in the payload: {}".format(missing_arguments))
+            raise ActionInputError(f"The following required arguments are missing in the payload: {missing_arguments}")
 
         try:
             history_id = self.decode_id(encoded_history_id)
         except exceptions.MalformedId as e:
-            raise ActionInputError('Invalid history ID. {}'.format(e))
+            raise ActionInputError(f"Invalid history ID. {e}")
 
         try:
             authz_id = self.decode_id(encoded_authz_id)
         except exceptions.MalformedId as e:
-            raise ActionInputError('Invalid authz ID. {}'.format(e))
+            raise ActionInputError(f"Invalid authz ID. {e}")
 
         if not isinstance(objects, list):
-            raise ActionInputError('The `objects` should be a list, but received an object of type {} instead.'.format(
-                type(objects)))
-
-        datasets = self.cloud_manager.get(trans=trans,
-                                          history_id=history_id,
-                                          bucket_name=bucket,
-                                          objects=objects,
-                                          authz_id=authz_id,
-                                          input_args=payload.get("input_args", None))
+            raise ActionInputError(
+                f"The `objects` should be a list, but received an object of type {type(objects)} instead."
+            )
+
+        datasets = self.cloud_manager.get(
+            trans=trans,
+            history_id=history_id,
+            bucket_name=bucket,
+            objects=objects,
+            authz_id=authz_id,
+            input_args=payload.get("input_args", None),
+        )
         rtv = []
         for dataset in datasets:
-            rtv.append(self.datasets_serializer.serialize_to_view(dataset, view='summary'))
+            rtv.append(self.datasets_serializer.serialize_to_view(dataset, view="summary"))
         return rtv
 
     @expose_api
     def send(self, trans, payload, **kwargs):
         """
-        * POST /api/cloud/storage/send
-            Sends given dataset(s) in a given history to a given cloud-based bucket. Each dataset is named
-            using the label assigned to the dataset in the given history (see `HistoryDatasetAssociation.name`).
-            If no dataset ID is given, this API sends all the datasets belonging to a given history to a given
-            cloud-based bucket.
-        :type  trans: galaxy.web.framework.webapp.GalaxyWebTransaction
+        POST /api/cloud/storage/send
+
+        Sends given dataset(s) in a given history to a given cloud-based bucket. Each dataset is named
+        using the label assigned to the dataset in the given history (see `HistoryDatasetAssociation.name`).
+        If no dataset ID is given, this API sends all the datasets belonging to a given history to a given
+        cloud-based bucket.
+
+        :type  trans: galaxy.webapps.base.webapp.GalaxyWebTransaction
         :param trans: Galaxy web transaction
 
         :type  payload: dictionary
         :param payload: A dictionary structure containing the following keys:
+
             *   history_id              the (encoded) id of history from which the object should be downloaed.
             *   bucket:                 the name of a bucket to which data should be sent (e.g., a bucket name on AWS S3).
             *   authz_id:               the encoded ID of CloudAuthz to be used for authorizing access to the resource
                                         provider. You may get a list of the defined authorizations via
                                         `/api/cloud/authz`. Also, you can use `/api/cloud/authz/create` to define a
                                         new authorization.
             *   dataset_ids:            [Optional; default: None]
@@ -159,28 +174,28 @@
                                         all the datasets belonging the specified history.
             *   overwrite_existing:     [Optional; default: False]
                                         A boolean value. If set to "True", and an object with same name of the dataset
                                         to be sent already exist in the bucket, Galaxy replaces the existing object
                                         with the dataset to be sent. If set to "False", Galaxy appends datetime
                                         to the dataset name to prevent overwriting an existing object.
 
-        :param kwargs:
-
         :rtype:     dictionary
         :return:    Information about the (un)successfully submitted dataset send jobs,
                     containing the following keys:
+
                         *   `bucket_name`:                  The name of bucket to which the listed datasets are queued
                                                             to be sent.
                         *   `sent_dataset_labels`:          A list of JSON objects with the following key-value pair:
                             **  `object`:                   The name of object is queued to be created.
                             **  `job_id`:                   The id of the queued send job.
 
                         *   `failed_dataset_labels`:        A list of JSON objects with the following key-value pair
                                                             representing the datasets Galaxy failed to create
                                                             (and queue) send job for:
+
                             **  `object`:                   The name of object is queued to be created.
                             **  `error`:                    A descriptive error message.
 
         """
         missing_arguments = []
         encoded_history_id = payload.get("history_id", None)
         if encoded_history_id is None:
@@ -191,48 +206,52 @@
             missing_arguments.append("bucket")
 
         encoded_authz_id = payload.get("authz_id", None)
         if encoded_authz_id is None:
             missing_arguments.append("authz_id")
 
         if len(missing_arguments) > 0:
-            raise ActionInputError("The following required arguments are missing in the payload: {}".format(missing_arguments))
+            raise ActionInputError(f"The following required arguments are missing in the payload: {missing_arguments}")
 
         try:
             history_id = self.decode_id(encoded_history_id)
         except exceptions.MalformedId as e:
-            raise ActionInputError('Invalid history ID. {}'.format(e))
+            raise ActionInputError(f"Invalid history ID. {e}")
 
         try:
             authz_id = self.decode_id(encoded_authz_id)
         except exceptions.MalformedId as e:
-            raise ActionInputError('Invalid authz ID. {}'.format(e))
+            raise ActionInputError(f"Invalid authz ID. {e}")
 
         encoded_dataset_ids = payload.get("dataset_ids", None)
         if encoded_dataset_ids is None:
             dataset_ids = None
         else:
             dataset_ids = set()
             invalid_dataset_ids = []
             for encoded_id in encoded_dataset_ids:
                 try:
                     dataset_ids.add(self.decode_id(encoded_id))
                 except exceptions.MalformedId:
                     invalid_dataset_ids.append(encoded_id)
             if len(invalid_dataset_ids) > 0:
-                raise ActionInputError("The following provided dataset IDs are invalid, please correct them and retry. "
-                                       "{}".format(invalid_dataset_ids))
-
-        log.info(msg="Received api/send request for `{}` datasets using authnz with id `{}`, and history `{}`."
-                     "".format("all the dataset in the given history" if not dataset_ids else len(dataset_ids),
-                               authz_id,
-                               history_id))
-
-        sent, failed = self.cloud_manager.send(trans=trans,
-                                               history_id=history_id,
-                                               bucket_name=bucket,
-                                               authz_id=authz_id,
-                                               dataset_ids=dataset_ids,
-                                               overwrite_existing=payload.get("overwrite_existing", False))
-        return {'sent_dataset_labels': sent,
-                'failed_dataset_labels': failed,
-                'bucket_name': bucket}
+                raise ActionInputError(
+                    "The following provided dataset IDs are invalid, please correct them and retry. "
+                    "{}".format(invalid_dataset_ids)
+                )
+
+        log.info(
+            msg="Received api/send request for `{}` datasets using authnz with id `{}`, and history `{}`."
+            "".format(
+                "all the dataset in the given history" if not dataset_ids else len(dataset_ids), authz_id, history_id
+            )
+        )
+
+        sent, failed = self.cloud_manager.send(
+            trans=trans,
+            history_id=history_id,
+            bucket_name=bucket,
+            authz_id=authz_id,
+            dataset_ids=dataset_ids,
+            overwrite_existing=payload.get("overwrite_existing", False),
+        )
+        return {"sent_dataset_labels": sent, "failed_dataset_labels": failed, "bucket_name": bucket}
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/cloudauthz.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/cloudauthz.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,243 +11,269 @@
 import logging
 
 from galaxy.exceptions import (
     ActionInputError,
     InternalServerError,
     MalformedId,
     RequestParameterInvalidException,
-    RequestParameterMissingException
+    RequestParameterMissingException,
 )
 from galaxy.managers import cloudauthzs
+from galaxy.structured_app import StructuredApp
 from galaxy.util import unicodify
-from galaxy.web import (
-    expose_api
-)
-from galaxy.webapps.base.controller import BaseAPIController
+from galaxy.web import expose_api
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class CloudAuthzController(BaseAPIController):
+class CloudAuthzController(BaseGalaxyAPIController):
     """
     RESTfull controller for defining cloud authorizations.
     """
 
-    def __init__(self, app):
-        super(CloudAuthzController, self).__init__(app)
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
         self.cloudauthz_manager = cloudauthzs.CloudAuthzManager(app)
         self.cloudauthz_serializer = cloudauthzs.CloudAuthzsSerializer(app)
         self.cloudauthz_deserializer = cloudauthzs.CloudAuthzsDeserializer(app)
 
     @expose_api
     def index(self, trans, **kwargs):
         """
-        * GET /api/cloud/authz
-            Lists all the cloud authorizations user has defined.
+        GET /api/cloud/authz
+
+        Lists all the cloud authorizations user has defined.
 
-        :type  trans: galaxy.web.framework.webapp.GalaxyWebTransaction
+        :type  trans: galaxy.webapps.base.webapp.GalaxyWebTransaction
         :param trans: Galaxy web transaction
 
         :param kwargs: empty dict
 
         :rtype: list of dict
         :return: a list of cloud authorizations (each represented in key-value pair format) defined for the user.
         """
         rtv = []
         for cloudauthz in trans.user.cloudauthz:
-            rtv.append(self.cloudauthz_serializer.serialize_to_view(
-                cloudauthz, user=trans.user, trans=trans, **self._parse_serialization_params(kwargs, 'summary')))
+            rtv.append(
+                self.cloudauthz_serializer.serialize_to_view(
+                    cloudauthz, user=trans.user, trans=trans, **self._parse_serialization_params(kwargs, "summary")
+                )
+            )
         return rtv
 
     @expose_api
     def create(self, trans, payload, **kwargs):
         """
         * POST /api/cloud/authz
             Request to store the payload as a cloudauthz (cloud authorization) configuration for a user.
 
-        :type  trans: galaxy.web.framework.webapp.GalaxyWebTransaction
+        :type  trans: galaxy.webapps.base.webapp.GalaxyWebTransaction
         :param trans: Galaxy web transaction
 
         :type payload: dict
         :param payload: A dictionary structure containing the following keys:
             *   provider:       the cloud-based resource provider to which this configuration belongs to.
 
             *   config:         a dictionary containing all the configuration required to request temporary credentials
                                 from the provider. See the following page for details:
-                                https://galaxyproject.org/cloud/authnz/
+                                https://galaxyproject.org/authnz/
 
             *   authn_id:       the (encoded) ID of a third-party authentication of a user. To have this ID, user must
                                 have logged-in to this Galaxy server using third-party identity (e.g., Google), or has
                                 associated his/her Galaxy account with a third-party OIDC-based identity. See this page:
-                                https://galaxyproject.org/admin/authentication/
+                                https://galaxyproject.org/authnz/config/
 
             *   description:    [Optional] a brief description for this configuration.
 
         :param kwargs: empty dict
 
         :rtype: dict
         :return: a dictionary with the following kvp:
             *   status:     HTTP response code
             *   message:    A message complementary to the response code.
         """
-        msg_template = "Rejected user `" + str(trans.user.id) + "`'s request to create cloudauthz config because of {}."
+        msg_template = f"Rejected user `{str(trans.user.id)}`'s request to create cloudauthz config because of {{}}."
         if not isinstance(payload, dict):
-            raise ActionInputError('Invalid payload data type. The payload is expected to be a dictionary, but '
-                                   'received data of type `{}`.'.format(str(type(payload))))
+            raise ActionInputError(
+                "Invalid payload data type. The payload is expected to be a dictionary, but "
+                "received data of type `{}`.".format(str(type(payload)))
+            )
 
         missing_arguments = []
-        provider = payload.get('provider', None)
+        provider = payload.get("provider", None)
         if provider is None:
-            missing_arguments.append('provider')
+            missing_arguments.append("provider")
 
-        config = payload.get('config', None)
+        config = payload.get("config", None)
         if config is None:
-            missing_arguments.append('config')
+            missing_arguments.append("config")
 
-        authn_id = payload.get('authn_id', None)
-        if authn_id is None:
-            missing_arguments.append('authn_id')
+        authn_id = payload.get("authn_id", None)
+        if authn_id is None and provider.lower() not in ["azure", "gcp"]:
+            missing_arguments.append("authn_id")
 
         if len(missing_arguments) > 0:
-            log.debug(msg_template.format("missing required config {}".format(missing_arguments)))
-            raise RequestParameterMissingException('The following required arguments are missing in the payload: '
-                                                   '{}'.format(missing_arguments))
+            log.debug(msg_template.format(f"missing required config {missing_arguments}"))
+            raise RequestParameterMissingException(
+                "The following required arguments are missing in the payload: " "{}".format(missing_arguments)
+            )
 
         description = payload.get("description", "")
 
         if not isinstance(config, dict):
-            log.debug(msg_template.format("invalid config type `{}`, expect `dict`".format(type(config))))
-            raise RequestParameterInvalidException('Invalid type for the required `config` variable; expect `dict` '
-                                                   'but received `{}`.'.format(type(config)))
-        try:
-            authn_id = self.decode_id(authn_id)
-        except Exception:
-            log.debug(msg_template.format("cannot decode authn_id `" + str(authn_id) + "`"))
-            raise MalformedId('Invalid `authn_id`!')
-
-        try:
-            trans.app.authnz_manager.can_user_assume_authn(trans, authn_id)
-        except Exception as e:
-            raise e
+            log.debug(msg_template.format(f"invalid config type `{type(config)}`, expect `dict`"))
+            raise RequestParameterInvalidException(
+                "Invalid type for the required `config` variable; expect `dict` "
+                "but received `{}`.".format(type(config))
+            )
+        if authn_id:
+            try:
+                decoded_authn_id = self.decode_id(authn_id)
+            except MalformedId as e:
+                log.debug(msg_template.format(f"cannot decode authz_id `{authn_id}`"))
+                raise e
+
+            try:
+                trans.app.authnz_manager.can_user_assume_authn(trans, decoded_authn_id)
+            except Exception as e:
+                raise e
 
         # No two authorization configuration with
         # exact same key/value should exist.
-        for ca in trans.user.cloudauthzs:
+        for ca in trans.user.cloudauthz:
             if ca.equals(trans.user.id, provider, authn_id, config):
-                log.debug("Rejected user `{}`'s request to create cloud authorization because a similar config "
-                          "already exists.".format(trans.user.id))
+                log.debug(
+                    "Rejected user `{}`'s request to create cloud authorization because a similar config "
+                    "already exists.".format(trans.user.id)
+                )
                 raise ActionInputError("A similar cloud authorization configuration is already defined.")
 
         try:
             new_cloudauthz = self.cloudauthz_manager.create(
-                user_id=trans.user.id,
-                provider=provider,
-                config=config,
-                authn_id=authn_id,
-                description=description
-            )
-            view = self.cloudauthz_serializer.serialize_to_view(new_cloudauthz, trans=trans, **self._parse_serialization_params(kwargs, 'summary'))
-            log.debug('Created a new cloudauthz record for the user id `{}` '.format(str(trans.user.id)))
-            trans.response.status = '200'
+                user_id=trans.user.id, provider=provider, config=config, authn_id=authn_id, description=description
+            )
+            view = self.cloudauthz_serializer.serialize_to_view(
+                new_cloudauthz, trans=trans, **self._parse_serialization_params(kwargs, "summary")
+            )
+            log.debug(f"Created a new cloudauthz record for the user id `{str(trans.user.id)}` ")
             return view
         except Exception as e:
             log.exception(msg_template.format("exception while creating the new cloudauthz record"))
-            raise InternalServerError('An unexpected error has occurred while responding to the create request of the '
-                                      'cloudauthz API.' + unicodify(e))
+            raise InternalServerError(
+                "An unexpected error has occurred while responding to the create request of the "
+                "cloudauthz API." + unicodify(e)
+            )
 
     @expose_api
     def delete(self, trans, encoded_authz_id, **kwargs):
         """
         * DELETE /api/cloud/authz/{encoded_authz_id}
             Deletes the CloudAuthz record with the given ``encoded_authz_id`` from database.
 
-        :type  trans: galaxy.web.framework.webapp.GalaxyWebTransaction
+        :type  trans: galaxy.webapps.base.webapp.GalaxyWebTransaction
         :param trans: Galaxy web transaction
 
         :type  encoded_authz_id:    string
         :param encoded_authz_id:    The encoded ID of the CloudAuthz record to be marked deleted.
 
         :rtype  JSON
         :return The cloudauthz record marked as deleted, serialized as a JSON object.
         """
 
-        msg_template = "Rejected user `" + str(trans.user.id) + "`'s request to delete cloudauthz config because of {}."
+        msg_template = f"Rejected user `{str(trans.user.id)}`'s request to delete cloudauthz config because of {{}}."
         try:
             authz_id = self.decode_id(encoded_authz_id)
-        except Exception:
-            log.debug(msg_template.format("cannot decode authz_id `" + str(encoded_authz_id) + "`"))
-            raise MalformedId('Invalid `authz_id`!')
+        except MalformedId as e:
+            log.debug(msg_template.format(f"cannot decode authz_id `{encoded_authz_id}`"))
+            raise e
 
         try:
             cloudauthz = trans.app.authnz_manager.try_get_authz_config(trans.sa_session, trans.user.id, authz_id)
             trans.sa_session.delete(cloudauthz)
             trans.sa_session.flush()
-            log.debug('Deleted a cloudauthz record with id `{}` for the user id `{}` '.format(authz_id, str(trans.user.id)))
-            view = self.cloudauthz_serializer.serialize_to_view(cloudauthz, trans=trans, **self._parse_serialization_params(kwargs, 'summary'))
-            trans.response.status = '200'
+            log.debug(f"Deleted a cloudauthz record with id `{authz_id}` for the user id `{str(trans.user.id)}` ")
+            view = self.cloudauthz_serializer.serialize_to_view(
+                cloudauthz, trans=trans, **self._parse_serialization_params(kwargs, "summary")
+            )
+            trans.response.status = "200"
             return view
         except Exception as e:
-            log.exception(msg_template.format("exception while deleting the cloudauthz record with "
-                                              "ID: `{}`.".format(encoded_authz_id)))
-            raise InternalServerError('An unexpected error has occurred while responding to the DELETE request of the '
-                                      'cloudauthz API.' + unicodify(e))
+            log.exception(
+                msg_template.format(
+                    "exception while deleting the cloudauthz record with " "ID: `{}`.".format(encoded_authz_id)
+                )
+            )
+            raise InternalServerError(
+                "An unexpected error has occurred while responding to the DELETE request of the "
+                "cloudauthz API." + unicodify(e)
+            )
 
     @expose_api
     def update(self, trans, encoded_authz_id, payload, **kwargs):
         """
-        * PUT /api/cloud/authz/{encoded_authz_id}
-            Updates the values for the cloudauthz configuration with the given ``encoded_authz_id``.
+        PUT /api/cloud/authz/{encoded_authz_id}
+
+        Updates the values for the cloudauthz configuration with the given ``encoded_authz_id``.
 
-            With this API only the following attributes of a cloudauthz configuration
-            can be updated: `authn_id`, `provider`, `config`, `deleted`.
+        With this API only the following attributes of a cloudauthz configuration
+        can be updated: `authn_id`, `provider`, `config`, `deleted`.
 
-        :type  trans:               galaxy.web.framework.webapp.GalaxyWebTransaction
+        :type  trans:               galaxy.webapps.base.webapp.GalaxyWebTransaction
         :param trans:               Galaxy web transaction
 
         :type  encoded_authz_id:    string
         :param encoded_authz_id:    The encoded ID of the CloudAuthz record to be updated.
 
         :type payload:              dict
         :param payload:             A dictionary structure containing the attributes to modified with their new values.
                                     It can contain any number of the following attributes:
+
                                         *   provider:   the cloud-based resource provider
                                                         to which this configuration belongs to.
 
                                         *   authn_id:   the (encoded) ID of a third-party authentication of a user.
                                                         To have this ID, user must have logged-in to this Galaxy server
                                                         using third-party identity (e.g., Google), or has associated
                                                         their Galaxy account with a third-party OIDC-based identity.
-                                                        See this page: https://galaxyproject.org/admin/authentication/
+                                                        See this page: https://galaxyproject.org/authnz/config/
 
                                                         Note: A user can associate a cloudauthz record with their own
                                                         authentications only. If the given authentication with authn_id
                                                         belongs to a different user, Galaxy will throw the
                                                         ItemAccessibilityException exception.
 
                                         *   config:     a dictionary containing all the configuration required to
                                                         request temporary credentials from the provider.
                                                         See the following page for details:
-                                                        https://galaxyproject.org/cloud/authnz/
+                                                        https://galaxyproject.org/authnz/
 
                                         *   deleted:    a boolean type marking the specified cloudauthz as (un)deleted.
 
         """
 
-        msg_template = "Rejected user `" + str(trans.user.id) + "`'s request to delete cloudauthz config because of {}."
+        msg_template = f"Rejected user `{str(trans.user.id)}`'s request to delete cloudauthz config because of {{}}."
         try:
             authz_id = self.decode_id(encoded_authz_id)
-        except Exception:
-            log.debug(msg_template.format("cannot decode authz_id `" + str(encoded_authz_id) + "`"))
-            raise MalformedId('Invalid `authz_id`!')
+        except MalformedId as e:
+            log.debug(msg_template.format(f"cannot decode authz_id `{encoded_authz_id}`"))
+            raise e
 
         try:
-            cloudauthz_to_update = trans.app.authnz_manager.try_get_authz_config(trans.sa_session, trans.user.id, authz_id)
+            cloudauthz_to_update = trans.app.authnz_manager.try_get_authz_config(
+                trans.sa_session, trans.user.id, authz_id
+            )
             self.cloudauthz_deserializer.deserialize(cloudauthz_to_update, payload, trans=trans)
-            self.cloudauthz_serializer.serialize_to_view(cloudauthz_to_update, view='summary')
-            return self.cloudauthz_serializer.serialize_to_view(cloudauthz_to_update, view='summary')
+            self.cloudauthz_serializer.serialize_to_view(cloudauthz_to_update, view="summary")
+            return self.cloudauthz_serializer.serialize_to_view(cloudauthz_to_update, view="summary")
         except MalformedId as e:
             raise e
         except Exception as e:
-            log.exception(msg_template.format("exception while updating the cloudauthz record with "
-                                              "ID: `{}`.".format(encoded_authz_id)))
-            raise InternalServerError('An unexpected error has occurred while responding to the PUT request of the '
-                                      'cloudauthz API.' + unicodify(e))
+            log.exception(
+                msg_template.format(
+                    "exception while updating the cloudauthz record with " "ID: `{}`.".format(encoded_authz_id)
+                )
+            )
+            raise InternalServerError(
+                "An unexpected error has occurred while responding to the PUT request of the "
+                "cloudauthz API." + unicodify(e)
+            )
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/container_resolution.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/container_resolution.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,43 +1,45 @@
 """
 API operations allowing clients to manage container resolution.
 """
 import logging
 
+import requests
+
+from galaxy.structured_app import StructuredApp
 from galaxy.tool_util.deps import views
 from galaxy.web import (
     expose_api,
-    require_admin
+    require_admin,
 )
-from galaxy.webapps.base.controller import BaseAPIController
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class ContainerResolutionAPIController(BaseAPIController):
-
-    def __init__(self, app):
-        super(ContainerResolutionAPIController, self).__init__(app)
+class ContainerResolutionAPIController(BaseGalaxyAPIController):
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
         self._view = views.ContainerResolutionView(app)
 
     @expose_api
     @require_admin
     def index(self, trans, **kwd):
         """
         GET /api/container_resolvers
         """
         return self._view.index()
 
     @expose_api
     @require_admin
-    def show(self, trans, id):
+    def show(self, trans, index):
         """
         GET /api/container_resolvers/<id>
         """
-        return self._view.show(id)
+        return self._view.show(index)
 
     @expose_api
     @require_admin
     def resolve(self, trans, index=None, **kwds):
         """
         GET /api/container_resolvers/resolve
         GET /api/container_resolvers/{index}/resolve
@@ -58,14 +60,15 @@
         :type   install:            boolean
         :param  install:            allow installation of new containers (for build_mulled* containers) the way job resolution
                                     will operate, defaults to False
         :rtype:     dict
         :returns:   a dictified description of the container dependency, with attribute
                     ``dependency_type: None`` if no match was found.
         """
+        kwds["session"] = requests.session()
         return self._view.resolve(index=index, **kwds)
 
     @expose_api
     @require_admin
     def resolve_toolbox(self, trans, **kwds):
         """
         GET /api/container_resolvers/toolbox
@@ -77,14 +80,15 @@
 
         :type   tool_ids:            str
         :param  tool_ids:            tool_ids to filter toolbox on
 
         :rtype:     list
         :returns:   list of items returned from resolve()
         """
+        kwds["session"] = requests.session()
         return self._view.resolve_toolbox(**kwds)
 
     @expose_api
     @require_admin
     def resolve_toolbox_with_install(self, trans, payload, **kwds):
         """
         POST /api/container_resolvers/toolbox/install
@@ -95,14 +99,15 @@
         as resolve_toolbox query parameters.
 
         :rtype:     list
         :returns:   list of items returned from resolve()
         """
         kwds.update(payload)
         kwds["install"] = True
+        kwds["session"] = requests.session()
         return self._view.resolve_toolbox(**kwds)
 
     @expose_api
     @require_admin
     def resolve_with_install(self, trans, payload, **kwds):
         """
         POST /api/container_resolvers/resolve/install
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/datasets.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/library_contents.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,488 +1,514 @@
 """
-API operations on the contents of a history dataset.
+API operations on the contents of a data library.
 """
 import logging
-import os
+from typing import Optional
 
-from six import string_types
+from sqlalchemy.orm.exc import (
+    MultipleResultsFound,
+    NoResultFound,
+)
 
 from galaxy import (
-    exceptions as galaxy_exceptions,
+    exceptions,
     managers,
-    model,
     util,
-    web
 )
-from galaxy.datatypes import dataproviders
-from galaxy.util.path import (
-    safe_walk
+from galaxy.actions.library import (
+    LibraryActions,
+    validate_path_upload,
+)
+from galaxy.managers.collections_util import (
+    api_payload_to_create_params,
+    dictify_dataset_collection_instance,
 )
-from galaxy.visualization.data_providers.genome import (
-    BamDataProvider,
-    FeatureLocationIndexDataProvider,
-    SamDataProvider
+from galaxy.model import (
+    ExtendedMetadata,
+    ExtendedMetadataIndex,
+    LibraryDataset,
+    tags,
 )
-from galaxy.web.framework.helpers import is_true
+from galaxy.structured_app import StructuredApp
+from galaxy.web import expose_api
 from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    UsesVisualizationMixin
+    HTTPBadRequest,
+    url_for,
+    UsesFormDefinitionsMixin,
+    UsesLibraryMixinItems,
 )
+from galaxy.webapps.galaxy.api import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class DatasetsController(BaseAPIController, UsesVisualizationMixin):
+class LibraryContentsController(
+    BaseGalaxyAPIController, UsesLibraryMixinItems, UsesFormDefinitionsMixin, LibraryActions
+):
+    def __init__(self, app: StructuredApp, hda_manager: managers.hdas.HDAManager):
+        super().__init__(app)
+        self.hda_manager = hda_manager
 
-    def __init__(self, app):
-        super(DatasetsController, self).__init__(app)
-        self.history_manager = managers.histories.HistoryManager(app)
-        self.hda_manager = managers.hdas.HDAManager(app)
-        self.hda_serializer = managers.hdas.HDASerializer(app)
-        self.hdca_serializer = managers.hdcas.HDCASerializer(app)
-        self.serializer_by_type = {'dataset': self.hda_serializer, 'dataset_collection': self.hdca_serializer}
-        self.ldda_manager = managers.lddas.LDDAManager(app)
-        self.history_contents_manager = managers.history_contents.HistoryContentsManager(app)
-        self.history_contents_filters = managers.history_contents.HistoryContentsFilters(app)
-
-    def _parse_serialization_params(self, kwd, default_view):
-        view = kwd.get('view', None)
-        keys = kwd.get('keys')
-        if isinstance(keys, string_types):
-            keys = keys.split(',')
-        return dict(view=view, keys=keys, default_view=default_view)
-
-    @web.expose_api
-    def index(self,
-              trans,
-              limit=500,
-              offset=0,
-              history_id=None,
-              **kwd):
+    @expose_api
+    def index(self, trans, library_id, **kwd):
         """
-        GET /api/datasets/
+        GET /api/libraries/{library_id}/contents:
 
-        Search datasets or collections using a query system
+        Return a list of library files and folders.
 
-        :rtype:     list
-        :returns:   dictionaries containing summary of dataset or dataset_collection information
+        .. note:: This endpoint is slow for large libraries. Returns all content traversing recursively through all folders.
+        .. seealso:: :class:`galaxy.webapps.galaxy.api.FolderContentsController.index` for a faster non-recursive solution
 
-        The list returned can be filtered by using two optional parameters:
-            q:      string, generally a property name to filter by followed
-                    by an (often optional) hyphen and operator string.
-            qv:     string, the value to filter by
-
-        ..example:
-            To filter the list to only those created after 2015-01-29,
-            the query string would look like:
-                '?q=create_time-gt&qv=2015-01-29'
-
-            Multiple filters can be sent in using multiple q/qv pairs:
-                '?q=create_time-gt&qv=2015-01-29&q=name-contains&qv=experiment-1'
-
-        The list returned can be paginated using two optional parameters:
-            limit:  integer, defaults to no value and no limit (return all)
-                    how many items to return
-            offset: integer, defaults to 0 and starts at the beginning
-                    skip the first ( offset - 1 ) items and begin returning
-                    at the Nth item
-
-        ..example:
-            limit and offset can be combined. Skip the first two and return five:
-                '?limit=5&offset=3'
-
-        The list returned can be ordered using the optional parameter:
-            order:  string containing one of the valid ordering attributes followed
-                    (optionally) by '-asc' or '-dsc' for ascending and descending
-                    order respectively. Orders can be stacked as a comma-
-                    separated list of values.
-
-        ..example:
-            To sort by name descending then create time descending:
-                '?order=name-dsc,create_time'
-
-        The ordering attributes and their default orders are:
-            hid defaults to 'hid-asc'
-            create_time defaults to 'create_time-dsc'
-            update_time defaults to 'update_time-dsc'
-            name    defaults to 'name-asc'
-
-        'order' defaults to 'create_time'
-        """
-        filter_params = self.parse_filter_params(kwd)
-        filters = self.history_contents_filters.parse_filters(filter_params)
-        view = kwd.get('view', 'summary')
-        order_by = self._parse_order_by(manager=self.history_contents_manager, order_by_string=kwd.get('order', 'create_time-dsc'))
-        container = None
-        if history_id:
-            container = self.history_manager.get_accessible(self.decode_id(history_id), trans.user)
-        contents = self.history_contents_manager.contents(
-            container=container, filters=filters, limit=limit, offset=offset, order_by=order_by, user_id=trans.user.id,
-        )
-        return [self.serializer_by_type[content.history_content_type].serialize_to_view(content, user=trans.user, trans=trans, view=view) for content in contents]
+        :param  library_id: the encoded id of the library
+        :type   library_id: str
 
-    @web.legacy_expose_api_anonymous
-    def show(self, trans, id, hda_ldda='hda', data_type=None, provider=None, **kwd):
-        """
-        GET /api/datasets/{encoded_dataset_id}
-        Displays information about and/or content of a dataset.
-        """
-        # Get dataset.
-        dataset = self.get_hda_or_ldda(trans, hda_ldda=hda_ldda, dataset_id=id)
-
-        # Use data type to return particular type of data.
-        if data_type == 'state':
-            rval = self._dataset_state(trans, dataset)
-        elif data_type == 'converted_datasets_state':
-            rval = self._converted_datasets_state(trans, dataset, kwd.get('chrom', None),
-                                                  is_true(kwd.get('retry', False)))
-        elif data_type == 'data':
-            rval = self._data(trans, dataset, **kwd)
-        elif data_type == 'features':
-            rval = self._search_features(trans, dataset, kwd.get('query'))
-        elif data_type == 'raw_data':
-            rval = self._raw_data(trans, dataset, provider, **kwd)
-        elif data_type == 'track_config':
-            rval = self.get_new_track_config(trans, dataset)
-        elif data_type == 'genome_data':
-            rval = self._get_genome_data(trans, dataset, kwd.get('dbkey', None))
-        else:
-            # Default: return dataset as dict.
-            if hda_ldda == 'hda':
-                return self.hda_serializer.serialize_to_view(dataset,
-                                                             view=kwd.get('view', 'detailed'), user=trans.user, trans=trans)
-            else:
-                rval = dataset.to_dict()
-        return rval
+        :returns:   list of dictionaries of the form:
 
-    @web.expose_api
-    def update_permissions(self, trans, dataset_id, payload, **kwd):
-        """
-        PUT /api/datasets/{encoded_dataset_id}/permissions
-        Updates permissions of a dataset.
+            * id:   the encoded id of the library item
+            * name: the 'library path'
+                or relationship of the library item to the root
+            * type: 'file' or 'folder'
+            * url:  the url to get detailed information on the library item
 
-        :rtype:     dict
-        :returns:   dictionary containing new permissions
-        """
-        if payload:
-            kwd.update(payload)
-        hda_ldda = kwd.get('hda_ldda', 'hda')
-        dataset_assoc = self.get_hda_or_ldda(trans, hda_ldda=hda_ldda, dataset_id=dataset_id)
-        if hda_ldda == "hda":
-            self.hda_manager.update_permissions(trans, dataset_assoc, **kwd)
-            return self.hda_manager.serialize_dataset_association_roles(trans, dataset_assoc)
-        else:
-            self.ldda_manager.update_permissions(trans, dataset_assoc, **kwd)
-            return self.ldda_manager.serialize_dataset_association_roles(trans, dataset_assoc)
+        :rtype:     list
 
-    def _dataset_state(self, trans, dataset, **kwargs):
+        :raises:  MalformedId, InconsistentDatabase, RequestParameterInvalidException, InternalServerError
         """
-        Returns state of dataset.
-        """
-        msg = self.hda_manager.data_conversion_status(dataset)
-        if not msg:
-            msg = dataset.conversion_messages.DATA
+        rval = []
+        current_user_roles = trans.get_current_user_roles()
 
-        return msg
+        def traverse(folder):
+            admin = trans.user_is_admin
+            rval = []
+            for subfolder in folder.active_folders:
+                if not admin:
+                    can_access, folder_ids = trans.app.security_agent.check_folder_contents(
+                        trans.user, current_user_roles, subfolder
+                    )
+                if (admin or can_access) and not subfolder.deleted:
+                    subfolder.api_path = f"{folder.api_path}/{subfolder.name}"
+                    subfolder.api_type = "folder"
+                    rval.append(subfolder)
+                    rval.extend(traverse(subfolder))
+            for ld in folder.datasets:
+                if not admin:
+                    can_access = trans.app.security_agent.can_access_dataset(
+                        current_user_roles, ld.library_dataset_dataset_association.dataset
+                    )
+                if (admin or can_access) and not ld.deleted:
+                    ld.api_path = f"{folder.api_path}/{ld.name}"
+                    ld.api_type = "file"
+                    rval.append(ld)
+            return rval
 
-    def _converted_datasets_state(self, trans, dataset, chrom=None, retry=False):
-        """
-        Init-like method that returns state of dataset's converted datasets.
-        Returns valid chroms for that dataset as well.
+        decoded_library_id = self.decode_id(library_id)
+        try:
+            library = (
+                trans.sa_session.query(trans.app.model.Library)
+                .filter(trans.app.model.Library.table.c.id == decoded_library_id)
+                .one()
+            )
+        except MultipleResultsFound:
+            raise exceptions.InconsistentDatabase("Multiple libraries found with the same id.")
+        except NoResultFound:
+            raise exceptions.RequestParameterInvalidException("No library found with the id provided.")
+        except Exception as e:
+            raise exceptions.InternalServerError(f"Error loading from the database.{util.unicodify(e)}")
+        if not (trans.user_is_admin or trans.app.security_agent.can_access_library(current_user_roles, library)):
+            raise exceptions.RequestParameterInvalidException("No library found with the id provided.")
+        encoded_id = f"F{trans.security.encode_id(library.root_folder.id)}"
+        # appending root folder
+        rval.append(
+            dict(
+                id=encoded_id,
+                type="folder",
+                name="/",
+                url=url_for("library_content", library_id=library_id, id=encoded_id),
+            )
+        )
+        library.root_folder.api_path = ""
+        # appending all other items in the library recursively
+        for content in traverse(library.root_folder):
+            encoded_id = trans.security.encode_id(content.id)
+            if content.api_type == "folder":
+                encoded_id = f"F{encoded_id}"
+            rval.append(
+                dict(
+                    id=encoded_id,
+                    type=content.api_type,
+                    name=content.api_path,
+                    url=url_for(
+                        "library_content",
+                        library_id=library_id,
+                        id=encoded_id,
+                    ),
+                )
+            )
+        return rval
+
+    @expose_api
+    def show(self, trans, id, library_id, **kwd):
         """
-        msg = self.hda_manager.data_conversion_status(dataset)
-        if msg:
-            return msg
+        GET /api/libraries/{library_id}/contents/{id}
 
-        # Get datasources and check for messages (which indicate errors). Retry if flag is set.
-        data_sources = dataset.get_datasources(trans)
-        messages_list = [data_source_dict['message'] for data_source_dict in data_sources.values()]
-        msg = self._get_highest_priority_msg(messages_list)
-        if msg:
-            if retry:
-                # Clear datasources and then try again.
-                dataset.clear_associated_files()
-                return self._converted_datasets_state(trans, dataset, chrom)
-            else:
-                return msg
+        Returns information about library file or folder.
 
-        # If there is a chrom, check for data on the chrom.
-        if chrom:
-            data_provider = trans.app.data_provider_registry.get_data_provider(trans,
-                                                                               original_dataset=dataset, source='index')
-            if not data_provider.has_data(chrom):
-                return dataset.conversion_messages.NO_DATA
-
-        # Have data if we get here
-        return {"status": dataset.conversion_messages.DATA, "valid_chroms": None}
-
-    def _search_features(self, trans, dataset, query):
-        """
-        Returns features, locations in dataset that match query. Format is a
-        list of features; each feature is a list itself: [name, location]
-        """
-        if dataset.can_convert_to("fli"):
-            converted_dataset = dataset.get_converted_dataset(trans, "fli")
-            if converted_dataset:
-                data_provider = FeatureLocationIndexDataProvider(converted_dataset=converted_dataset)
-                if data_provider:
-                    return data_provider.get_data(query)
-
-        return []
-
-    def _data(self, trans, dataset, chrom, low, high, start_val=0, max_vals=None, **kwargs):
-        """
-        Provides a block of data from a dataset.
-        """
-        # Parameter check.
-        if not chrom:
-            return dataset.conversion_messages.NO_DATA
-
-        # Dataset check.
-        msg = self.hda_manager.data_conversion_status(dataset)
-        if msg:
-            return msg
-
-        # Get datasources and check for messages.
-        data_sources = dataset.get_datasources(trans)
-        messages_list = [data_source_dict['message'] for data_source_dict in data_sources.values()]
-        return_message = self._get_highest_priority_msg(messages_list)
-        if return_message:
-            return return_message
-
-        extra_info = None
-        mode = kwargs.get("mode", "Auto")
-        data_provider_registry = trans.app.data_provider_registry
-        indexer = None
-
-        # Coverage mode uses index data.
-        if mode == "Coverage":
-            # Get summary using minimal cutoffs.
-            indexer = data_provider_registry.get_data_provider(trans, original_dataset=dataset, source='index')
-            return indexer.get_data(chrom, low, high, **kwargs)
-
-        # TODO:
-        # (1) add logic back in for no_detail
-        # (2) handle scenario where mode is Squish/Pack but data requested is large, so reduced data needed to be returned.
-
-        # If mode is Auto, need to determine what type of data to return.
-        if mode == "Auto":
-            # Get stats from indexer.
-            indexer = data_provider_registry.get_data_provider(trans, original_dataset=dataset, source='index')
-            stats = indexer.get_data(chrom, low, high, stats=True)
-
-            # If stats were requested, return them.
-            if 'stats' in kwargs:
-                if stats['data']['max'] == 0:
-                    return {'dataset_type': indexer.dataset_type, 'data': None}
-                else:
-                    return stats
-
-            # Stats provides features/base and resolution is bases/pixel, so
-            # multiplying them yields features/pixel.
-            features_per_pixel = stats['data']['max'] * float(kwargs['resolution'])
-
-            # Use heuristic based on features/pixel and region size to determine whether to
-            # return coverage data. When zoomed out and region is large, features/pixel
-            # is determining factor. However, when sufficiently zoomed in and region is
-            # small, coverage data is no longer provided.
-            if int(high) - int(low) > 50000 and features_per_pixel > 1000:
-                return indexer.get_data(chrom, low, high)
-
-        #
-        # Provide individual data points.
-        #
-
-        # Get data provider.
-        data_provider = data_provider_registry.get_data_provider(trans, original_dataset=dataset, source='data')
-
-        # Allow max_vals top be data provider set if not passed
-        if max_vals is None:
-            max_vals = data_provider.get_default_max_vals()
-
-        # Get reference sequence and mean depth for region; these is used by providers for aligned reads.
-        region = None
-        mean_depth = None
-        if isinstance(data_provider, (SamDataProvider, BamDataProvider)):
-            # Get reference sequence.
-            if dataset.dbkey:
-                # FIXME: increase region 1M each way to provide sequence for
-                # spliced/gapped reads. Probably should provide refseq object
-                # directly to data provider.
-                region = self.app.genomes.reference(trans, dbkey=dataset.dbkey, chrom=chrom,
-                                                    low=(max(0, int(low) - 1000000)),
-                                                    high=(int(high) + 1000000))
-
-            # Get mean depth.
-            if not indexer:
-                indexer = data_provider_registry.get_data_provider(trans, original_dataset=dataset, source='index')
-            stats = indexer.get_data(chrom, low, high, stats=True)
-            mean_depth = stats['data']['mean']
-
-        # Get and return data from data_provider.
-        result = data_provider.get_data(chrom, int(low), int(high), int(start_val), int(max_vals),
-                                        ref_seq=region, mean_depth=mean_depth, **kwargs)
-        result.update({'dataset_type': data_provider.dataset_type, 'extra_info': extra_info})
-        return result
-
-    def _raw_data(self, trans, dataset, provider=None, **kwargs):
-        """
-        Uses original (raw) dataset to return data. This method is useful
-        when the dataset is not yet indexed and hence using data would
-        be slow because indexes need to be created.
-        """
-        # Dataset check.
-        msg = self.hda_manager.data_conversion_status(dataset)
-        if msg:
-            return msg
-
-        registry = trans.app.data_provider_registry
-
-        # allow the caller to specify which provider is used
-        #   pulling from the original providers if possible, then the new providers
-        if provider:
-            if provider in registry.dataset_type_name_to_data_provider:
-                data_provider = registry.dataset_type_name_to_data_provider[provider](dataset)
-
-            elif dataset.datatype.has_dataprovider(provider):
-                kwargs = dataset.datatype.dataproviders[provider].parse_query_string_settings(kwargs)
-                # use dictionary to allow more than the data itself to be returned (data totals, other meta, etc.)
-                return {
-                    'data': list(dataset.datatype.dataprovider(dataset, provider, **kwargs))
-                }
+        :param  id:         the encoded id of the library item to return
+        :type   id:         str
 
-            else:
-                raise dataproviders.exceptions.NoProviderAvailable(dataset.datatype, provider)
+        :param  library_id: the encoded id of the library that contains this item
+        :type   library_id: str
 
-        # no provider name: look up by datatype
-        else:
-            data_provider = registry.get_data_provider(trans, raw=True, original_dataset=dataset)
+        :returns:   detailed library item information
+        :rtype:     dict
 
-        # Return data.
-        data = data_provider.get_data(**kwargs)
+        .. seealso::
+            :func:`galaxy.model.LibraryDataset.to_dict` and
+            :attr:`galaxy.model.LibraryFolder.dict_element_visible_keys`
+        """
+        class_name, content_id = self._decode_library_content_id(id)
+        if class_name == "LibraryFolder":
+            content = self.get_library_folder(trans, content_id, check_ownership=False, check_accessible=True)
+            rval = content.to_dict(view="element", value_mapper={"id": trans.security.encode_id})
+            rval["id"] = f"F{str(rval['id'])}"
+            if rval["parent_id"] is not None:  # This can happen for root folders.
+                rval["parent_id"] = f"F{str(trans.security.encode_id(rval['parent_id']))}"
+            rval["parent_library_id"] = trans.security.encode_id(rval["parent_library_id"])
+        else:
+            content = self.get_library_dataset(trans, content_id, check_ownership=False, check_accessible=True)
+            rval = content.to_dict(view="element")
+            rval["id"] = trans.security.encode_id(rval["id"])
+            rval["ldda_id"] = trans.security.encode_id(rval["ldda_id"])
+            rval["folder_id"] = f"F{str(trans.security.encode_id(rval['folder_id']))}"
+            rval["parent_library_id"] = trans.security.encode_id(rval["parent_library_id"])
 
-        return data
+            tag_manager = tags.GalaxyTagHandler(trans.sa_session)
+            rval["tags"] = tag_manager.get_tags_str(content.library_dataset_dataset_association.tags)
+        return rval
 
-    @web.legacy_expose_api_anonymous
-    def extra_files(self, trans, history_content_id, history_id, **kwd):
+    @expose_api
+    def create(self, trans, library_id, payload, **kwd):
         """
-        GET /api/histories/{encoded_history_id}/contents/{encoded_content_id}/extra_files
-        Generate list of extra files.
-        """
-        decoded_content_id = self.decode_id(history_content_id)
+        POST /api/libraries/{library_id}/contents:
 
-        hda = self.hda_manager.get_accessible(decoded_content_id, trans.user)
-        extra_files_path = hda.extra_files_path
-        rval = []
-        for root, directories, files in safe_walk(extra_files_path):
-            for directory in directories:
-                rval.append({"class": "Directory", "path": os.path.relpath(os.path.join(root, directory), extra_files_path)})
-            for file in files:
-                rval.append({"class": "File", "path": os.path.relpath(os.path.join(root, file), extra_files_path)})
+        Create a new library file or folder.
 
-        return rval
+        To copy an HDA into a library send ``create_type`` of 'file' and
+        the HDA's encoded id in ``from_hda_id`` (and optionally ``ldda_message``).
+
+        To copy an HDCA into a library send ``create_type`` of 'file' and
+        the HDCA's encoded id in ``from_hdca_id`` (and optionally ``ldda_message``).
+
+        :type   library_id: str
+        :param  library_id: the encoded id of the library where to create the new item
+        :type   payload:    dict
+        :param  payload:    dictionary structure containing:
+
+            * folder_id:    the encoded id of the parent folder of the new item
+            * create_type:  the type of item to create ('file', 'folder' or 'collection')
+            * from_hda_id:  (optional, only if create_type is 'file') the
+                encoded id of an accessible HDA to copy into the library
+            * ldda_message: (optional) the new message attribute of the LDDA created
+            * extended_metadata: (optional) sub-dictionary containing any extended
+                metadata to associate with the item
+            * upload_option: (optional) one of 'upload_file' (default), 'upload_directory' or 'upload_paths'
+            * server_dir: (optional, only if upload_option is
+                'upload_directory') relative path of the subdirectory of Galaxy
+                ``library_import_dir`` (if admin) or ``user_library_import_dir``
+                (if non-admin) to upload. All and only the files (i.e.
+                no subdirectories) contained in the specified directory will be
+                uploaded.
+            * filesystem_paths: (optional, only if upload_option is
+                'upload_paths' and the user is an admin) file paths on the
+                Galaxy server to upload to the library, one file per line
+            * link_data_only: (optional, only when upload_option is
+                'upload_directory' or 'upload_paths') either 'copy_files'
+                (default) or 'link_to_files'. Setting to 'link_to_files'
+                symlinks instead of copying the files
+            * name: (optional, only if create_type is 'folder') name of the
+                folder to create
+            * description: (optional, only if create_type is 'folder')
+                description of the folder to create
+            * tag_using_filenames: (optional)
+                create tags on datasets using the file's original name
+            * tags: (optional)
+                create the given list of tags on datasets
+
+        :returns:   a dictionary describing the new item unless ``from_hdca_id`` is supplied,
+                    in that case a list of such dictionaries is returned.
+        :rtype:     object
+        """
+        if trans.user_is_bootstrap_admin:
+            raise exceptions.RealUserRequiredException("Only real users can create a new library file or folder.")
+        if "create_type" not in payload:
+            raise exceptions.RequestParameterMissingException("Missing required 'create_type' parameter.")
+        create_type = payload.pop("create_type")
+        if create_type not in ("file", "folder", "collection"):
+            raise exceptions.RequestParameterInvalidException(
+                f"Invalid value for 'create_type' parameter ( {create_type} ) specified."
+            )
+        if "upload_option" in payload and payload["upload_option"] not in (
+            "upload_file",
+            "upload_directory",
+            "upload_paths",
+        ):
+            raise exceptions.RequestParameterInvalidException(
+                f"Invalid value for 'upload_option' parameter ( {payload['upload_option']} ) specified."
+            )
+        if "folder_id" not in payload:
+            raise exceptions.RequestParameterMissingException("Missing required 'folder_id' parameter.")
+        folder_id = payload.pop("folder_id")
+        _, folder_id = self._decode_library_content_id(folder_id)
+        folder_id = trans.security.decode_id(folder_id)
+        # security is checked in the downstream controller
+        parent = self.get_library_folder(trans, folder_id, check_ownership=False, check_accessible=False)
+        # The rest of the security happens in the library_common controller.
+
+        payload["tag_using_filenames"] = util.string_as_bool(payload.get("tag_using_filenames", None))
+        payload["tags"] = util.listify(payload.get("tags", None))
+
+        # are we copying an HDA to the library folder?
+        #   we'll need the id and any message to attach, then branch to that private function
+        from_hda_id, from_hdca_id, ldda_message = (
+            payload.pop("from_hda_id", None),
+            payload.pop("from_hdca_id", None),
+            payload.pop("ldda_message", ""),
+        )
+        if create_type == "file":
+            if from_hda_id:
+                return self._copy_hda_to_library_folder(
+                    trans, self.hda_manager, self.decode_id(from_hda_id), folder_id, ldda_message
+                )
+            if from_hdca_id:
+                return self._copy_hdca_to_library_folder(
+                    trans, self.hda_manager, self.decode_id(from_hdca_id), folder_id, ldda_message
+                )
+
+        # check for extended metadata, store it and pop it out of the param
+        # otherwise sanitize_param will have a fit
+        ex_meta_payload = payload.pop("extended_metadata", None)
+
+        # Now create the desired content object, either file or folder.
+        if create_type == "file":
+            status, output = self._upload_library_dataset(trans, folder_id, **payload)
+        elif create_type == "folder":
+            status, output = self._create_folder(trans, folder_id, **payload)
+        elif create_type == "collection":
+            # Not delegating to library_common, so need to check access to parent
+            # folder here.
+            self.check_user_can_add_to_library_item(trans, parent, check_accessible=True)
+            create_params = api_payload_to_create_params(payload)
+            create_params["parent"] = parent
+            dataset_collection_manager = trans.app.dataset_collection_manager
+            dataset_collection_instance = dataset_collection_manager.create(**create_params)
+            return [
+                dictify_dataset_collection_instance(
+                    dataset_collection_instance, security=trans.security, url_builder=trans.url_builder, parent=parent
+                )
+            ]
+        if status != 200:
+            trans.response.status = status
+            return output
+        else:
+            rval = []
+            for v in output.values():
+                if ex_meta_payload is not None:
+                    # If there is extended metadata, store it, attach it to the dataset, and index it
+                    ex_meta = ExtendedMetadata(ex_meta_payload)
+                    trans.sa_session.add(ex_meta)
+                    v.extended_metadata = ex_meta
+                    trans.sa_session.add(v)
+                    trans.sa_session.flush()
+                    for path, value in self._scan_json_block(ex_meta_payload):
+                        meta_i = ExtendedMetadataIndex(ex_meta, path, value)
+                        trans.sa_session.add(meta_i)
+                    trans.sa_session.flush()
+                if type(v) == trans.app.model.LibraryDatasetDatasetAssociation:
+                    v = v.library_dataset
+                encoded_id = trans.security.encode_id(v.id)
+                if create_type == "folder":
+                    encoded_id = f"F{encoded_id}"
+                rval.append(
+                    dict(
+                        id=encoded_id, name=v.name, url=url_for("library_content", library_id=library_id, id=encoded_id)
+                    )
+                )
+            return rval
+
+    def _upload_library_dataset(self, trans, folder_id: int, **kwd):
+        replace_dataset: Optional[LibraryDataset] = None
+        upload_option = kwd.get("upload_option", "upload_file")
+        dbkey = kwd.get("dbkey", "?")
+        if isinstance(dbkey, list):
+            last_used_build = dbkey[0]
+        else:
+            last_used_build = dbkey
+        roles = kwd.get("roles", "")
+        is_admin = trans.user_is_admin
+        current_user_roles = trans.get_current_user_roles()
+        folder = trans.sa_session.query(trans.app.model.LibraryFolder).get(folder_id)
+        self._check_access(trans, is_admin, folder, current_user_roles)
+        self._check_add(trans, is_admin, folder, current_user_roles)
+        library = folder.parent_library
+        if folder and last_used_build in ["None", None, "?"]:
+            last_used_build = folder.genome_build
+        error = False
+        if upload_option == "upload_paths":
+            validate_path_upload(trans)  # Duplicate check made in _upload_dataset.
+        elif roles:
+            # Check to see if the user selected roles to associate with the DATASET_ACCESS permission
+            # on the dataset that would cause accessibility issues.
+            vars = dict(DATASET_ACCESS_in=roles)
+            permissions, in_roles, error, message = trans.app.security_agent.derive_roles_from_access(
+                trans, library.id, "api", library=True, **vars
+            )
+        if error:
+            return 400, message
+        else:
+            created_outputs_dict = self._upload_dataset(
+                trans, folder_id=folder.id, replace_dataset=replace_dataset, **kwd
+            )
+            if created_outputs_dict:
+                if type(created_outputs_dict) == str:
+                    return 400, created_outputs_dict
+                elif type(created_outputs_dict) == tuple:
+                    return created_outputs_dict[0], created_outputs_dict[1]
+                return 200, created_outputs_dict
+            else:
+                return 400, "Upload failed"
 
-    @web.legacy_expose_api_raw_anonymous
-    def display(self, trans, history_content_id, history_id,
-                preview=False, filename=None, to_ext=None, raw=False, **kwd):
+    def _scan_json_block(self, meta, prefix=""):
         """
-        GET /api/histories/{encoded_history_id}/contents/{encoded_content_id}/display
-        Displays history content (dataset).
+        Scan a json style data structure, and emit all fields and their values.
+        Example paths
 
-        The query parameter 'raw' should be considered experimental and may be dropped at
-        some point in the future without warning. Generally, data should be processed by its
-        datatype prior to display (the defult if raw is unspecified or explicitly false.
-        """
-        decoded_content_id = self.decode_id(history_content_id)
-        raw = util.string_as_bool_or_none(raw)
+        Data
+        { "data" : [ 1, 2, 3 ] }
 
-        rval = ''
-        try:
-            hda = self.hda_manager.get_accessible(decoded_content_id, trans.user)
-            if raw:
-                if filename and filename != 'index':
-                    object_store = trans.app.object_store
-                    dir_name = hda.dataset.extra_files_path_name
-                    file_path = object_store.get_filename(hda.dataset,
-                                                          extra_dir=dir_name,
-                                                          alt_name=filename)
-                else:
-                    file_path = hda.file_name
-                rval = open(file_path, 'rb')
-            else:
-                display_kwd = kwd.copy()
-                if 'key' in display_kwd:
-                    del display_kwd["key"]
-                rval = hda.datatype.display_data(trans, hda, preview, filename, to_ext, **display_kwd)
-        except Exception as e:
-            log.exception("Error getting display data for dataset (%s) from history (%s)",
-                          history_content_id, history_id)
-            trans.response.status = 500
-            rval = "Could not get display data for dataset: %s" % util.unicodify(e)
-        return rval
+        Path:
+        /data == [1,2,3]
+
+        /data/[0] == 1
 
-    @web.legacy_expose_api_raw_anonymous
-    def get_metadata_file(self, trans, history_content_id, history_id, metadata_file=None, **kwd):
-        """
-        GET /api/histories/{history_id}/contents/{history_content_id}/metadata_file
         """
-        decoded_content_id = self.decode_id(history_content_id)
-        rval = ''
-        try:
-            hda = self.hda_manager.get_accessible(decoded_content_id, trans.user)
-            file_ext = hda.metadata.spec.get(metadata_file).get("file_ext", metadata_file)
-            fname = ''.join(c in util.FILENAME_VALID_CHARS and c or '_' for c in hda.name)[0:150]
-            trans.response.headers["Content-Type"] = "application/octet-stream"
-            trans.response.headers["Content-Disposition"] = 'attachment; filename="Galaxy%s-[%s].%s"' % (hda.hid, fname, file_ext)
-            return open(hda.metadata.get(metadata_file).file_name, 'rb')
-        except Exception as e:
-            log.exception("Error getting metadata_file (%s) for dataset (%s) from history (%s)",
-                          metadata_file, history_content_id, history_id)
-            trans.response.status = 500
-            rval = "Could not get metadata for dataset: %s" % util.unicodify(e)
-        return rval
+        if isinstance(meta, dict):
+            for a in meta:
+                yield from self._scan_json_block(meta[a], f"{prefix}/{a}")
+        elif isinstance(meta, list):
+            for i, a in enumerate(meta):
+                yield from self._scan_json_block(a, prefix + "[%d]" % (i))
+        else:
+            # BUG: Everything is cast to string, which can lead to false positives
+            # for cross type comparisions, ie "True" == True
+            yield prefix, (f"{meta}").encode()
+
+    @expose_api
+    def update(self, trans, id, library_id, payload, **kwd):
+        """
+        PUT /api/libraries/{library_id}/contents/{id}
+
+        Create an ImplicitlyConvertedDatasetAssociation.
+
+        .. seealso:: :class:`galaxy.model.ImplicitlyConvertedDatasetAssociation`
+
+        :type   id:         str
+        :param  id:         the encoded id of the library item to return
+        :type   library_id: str
+        :param  library_id: the encoded id of the library that contains this item
+        :type   payload:    dict
+        :param  payload:    dictionary structure containing::
+            'converted_dataset_id':
+
+        :rtype:     None
+        :returns:   None
+        """
+        if "converted_dataset_id" in payload:
+            converted_id = payload.pop("converted_dataset_id")
+            content = self.get_library_dataset(trans, id, check_ownership=False, check_accessible=False)
+            content_conv = self.get_library_dataset(trans, converted_id, check_ownership=False, check_accessible=False)
+            assoc = trans.app.model.ImplicitlyConvertedDatasetAssociation(
+                parent=content.library_dataset_dataset_association,
+                dataset=content_conv.library_dataset_dataset_association,
+                file_type=content_conv.library_dataset_dataset_association.extension,
+                metadata_safe=True,
+            )
+            trans.sa_session.add(assoc)
+            trans.sa_session.flush()
+
+    def _decode_library_content_id(self, content_id):
+        if len(content_id) % 16 == 0:
+            return "LibraryDataset", content_id
+        elif content_id.startswith("F"):
+            return "LibraryFolder", content_id[1:]
+        else:
+            raise HTTPBadRequest(f"Malformed library content id ( {str(content_id)} ) specified, unable to decode.")
 
-    @web.expose_api_anonymous
-    def converted(self, trans, dataset_id, ext, **kwargs):
+    @expose_api
+    def delete(self, trans, library_id, id, **kwd):
         """
-        converted( self, trans, dataset_id, ext, **kwargs )
-        * GET /api/datasets/{dataset_id}/converted/{ext}
-            return information about datasets made by converting this dataset
-            to a new format
-
-        :type   dataset_id: str
-        :param  dataset_id: the encoded id of the original HDA to check
-        :type   ext:        str
-        :param  ext:        file extension of the target format or None.
-
-        If there is no existing converted dataset for the format in `ext`,
-        one will be created.
-
-        If `ext` is None, a dictionary will be returned of the form
-        { <converted extension> : <converted id>, ... } containing all the
-        *existing* converted datasets.
+        DELETE /api/libraries/{library_id}/contents/{id}
 
-        ..note: `view` and `keys` are also available to control the serialization
-            of individual datasets. They have no effect when `ext` is None.
+        Delete the LibraryDataset with the given ``id``.
 
-        :rtype:     dict
-        :returns:   dictionary containing detailed HDA information
-                    or (if `ext` is None) an extension->dataset_id map
-        """
-        decoded_id = self.decode_id(dataset_id)
-        hda = self.hda_manager.get_accessible(decoded_id, trans.user)
-        if ext:
-            converted = self._get_or_create_converted(trans, hda, ext, **kwargs)
-            return self.hda_serializer.serialize_to_view(converted,
-                user=trans.user, trans=trans, **self._parse_serialization_params(kwargs, 'detailed'))
+        :type   id:     str
+        :param  id:     the encoded id of the library dataset to delete
+        :type   kwd:    dict
+        :param  kwd:    (optional) dictionary structure containing:
 
-        return self.hda_serializer.serialize_converted_datasets(hda, 'converted')
+            * payload:     a dictionary itself containing:
+                * purge:   if True, purge the LD
 
-    def _get_or_create_converted(self, trans, original, target_ext, **kwargs):
+        :rtype:     dict
+        :returns:   an error object if an error occurred or a dictionary containing:
+            * id:         the encoded id of the library dataset,
+            * deleted:    if the library dataset was marked as deleted,
+            * purged:     if the library dataset was purged
+        """
+        purge = False
+        if kwd.get("payload", None):
+            purge = util.string_as_bool(kwd["payload"].get("purge", False))
+
+        rval = {"id": id}
         try:
-            original.get_converted_dataset(trans, target_ext)
-            converted = original.get_converted_files_by_type(target_ext)
-            return converted
-
-        except model.NoConverterException:
-            exc_data = dict(source=original.ext, target=target_ext, available=list(original.get_converter_types().keys()))
-            raise galaxy_exceptions.RequestParameterInvalidException('Conversion not possible', **exc_data)
+            ld = self.get_library_dataset(trans, id, check_ownership=False, check_accessible=True)
+            user_is_admin = trans.user_is_admin
+            can_modify = trans.app.security_agent.can_modify_library_item(trans.user.all_roles(), ld)
+            log.debug("is_admin: %s, can_modify: %s", user_is_admin, can_modify)
+            if not (user_is_admin or can_modify):
+                trans.response.status = 403
+                rval.update({"error": "Unauthorized to delete or purge this library dataset"})
+                return rval
+
+            ld.deleted = True
+            if purge:
+                ld.purged = True
+                trans.sa_session.add(ld)
+                trans.sa_session.flush()
+
+                # TODO: had to change this up a bit from Dataset.user_can_purge
+                dataset = ld.library_dataset_dataset_association.dataset
+                no_history_assoc = len(dataset.history_associations) == len(dataset.purged_history_associations)
+                no_library_assoc = dataset.library_associations == [ld.library_dataset_dataset_association]
+                can_purge_dataset = not dataset.purged and no_history_assoc and no_library_assoc
+
+                if can_purge_dataset:
+                    try:
+                        ld.library_dataset_dataset_association.dataset.full_delete()
+                        trans.sa_session.add(ld.dataset)
+                    except Exception:
+                        pass
+                    # flush now to preserve deleted state in case of later interruption
+                    trans.sa_session.flush()
+                rval["purged"] = True
+            trans.sa_session.flush()
+            rval["deleted"] = True
+
+        except exceptions.httpexceptions.HTTPInternalServerError:
+            log.exception("Library_contents API, delete: uncaught HTTPInternalServerError: %s, %s", id, str(kwd))
+            raise
+        except exceptions.httpexceptions.HTTPException:
+            raise
+        except Exception as exc:
+            log.exception("library_contents API, delete: uncaught exception: %s, %s", id, str(kwd))
+            trans.response.status = 500
+            rval.update({"error": util.unicodify(exc)})
+        return rval
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/dynamic_tools.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/dynamic_tools.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,43 +1,43 @@
 import logging
 
-from galaxy import util, web
+from galaxy import (
+    util,
+    web,
+)
 from galaxy.exceptions import ObjectNotFound
-from galaxy.web import expose_api
-from galaxy.web import expose_api_anonymous_and_sessionless
+from galaxy.web import (
+    expose_api,
+    expose_api_anonymous_and_sessionless,
+)
 from galaxy.webapps.base.controller import BaseAPIController
 
 log = logging.getLogger(__name__)
 
 
 class DynamicToolsController(BaseAPIController):
     """
     RESTful controller for interactions with dynamic tools.
 
     Dynamic tools are tools defined in the database. Use the tools controller
     to run these tools and view functional information.
     """
 
-    def __init__(self, app):
-        super(DynamicToolsController, self).__init__(app)
-
     @expose_api_anonymous_and_sessionless
     def index(self, trans, **kwds):
         """
         GET /api/dynamic_tools
 
         This returns meta-information about the dynamic tool, such as
         tool_uuid. To use the tool or view funtional information such as
         inputs and outputs, use the standard tools API indexed by the
         ID (and optionally version) returned from this endpoint.
         """
         manager = self.app.dynamic_tools_manager
-        return list(
-            map(lambda t: t.to_dict(), manager.list_tools())
-        )
+        return list(map(lambda t: t.to_dict(), manager.list_tools()))
 
     @expose_api_anonymous_and_sessionless
     def show(self, trans, id, **kwd):
         """
         GET /api/dynamic_tools/{encoded_dynamic_tool_id|tool_uuid}
         """
         self._get_dynamic_tool(trans, id).to_dict()
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/extended_metadata.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/extended_metadata.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,89 +1,88 @@
 """
 API operations on annotations.
 """
 import logging
+from typing import (
+    Generic,
+    Optional,
+    TypeVar,
+)
 
 from galaxy import (
     managers,
-    web
+    model,
+    web,
 )
 from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    HTTPNotImplemented,
     UsesExtendedMetadataMixin,
     UsesLibraryMixinItems,
-    UsesStoredWorkflowMixin
+    UsesStoredWorkflowMixin,
+)
+from . import (
+    BaseGalaxyAPIController,
+    depends,
 )
 
 log = logging.getLogger(__name__)
 
+T = TypeVar("T")
+
+
+class BaseExtendedMetadataController(
+    BaseGalaxyAPIController, UsesExtendedMetadataMixin, UsesLibraryMixinItems, UsesStoredWorkflowMixin, Generic[T]
+):
+    exmeta_item_id: str
 
-class BaseExtendedMetadataController(BaseAPIController, UsesExtendedMetadataMixin, UsesLibraryMixinItems, UsesStoredWorkflowMixin):
+    def _get_item_from_id(self, trans, idstr, check_writable=True) -> Optional[T]:
+        ...
 
-    @web.legacy_expose_api
+    @web.expose_api
     def index(self, trans, **kwd):
         idnum = kwd[self.exmeta_item_id]
         item = self._get_item_from_id(trans, idnum, check_writable=False)
         if item is not None:
             ex_meta = self.get_item_extended_metadata_obj(trans, item)
             if ex_meta is not None:
                 return ex_meta.data
 
-    @web.legacy_expose_api
+    @web.expose_api
     def create(self, trans, payload, **kwd):
         idnum = kwd[self.exmeta_item_id]
         item = self._get_item_from_id(trans, idnum, check_writable=True)
         if item is not None:
             ex_obj = self.get_item_extended_metadata_obj(trans, item)
             if ex_obj is not None:
                 self.unset_item_extended_metadata_obj(trans, item)
                 self.delete_extended_metadata(trans, ex_obj)
             ex_obj = self.create_extended_metadata(trans, payload)
             self.set_item_extended_metadata_obj(trans, item, ex_obj)
 
-    @web.legacy_expose_api
-    def delete(self, trans, **kwd):
-        idnum = kwd[self.tagged_item_id]
-        item = self._get_item_from_id(trans, idnum, check_writable=True)
-        if item is not None:
-            ex_obj = self.get_item_extended_metadata_obj(trans, item)
-            if ex_obj is not None:
-                self.unset_item_extended_metadata_obj(trans, item)
-                self.delete_extended_metadata(trans, ex_obj)
 
-    @web.legacy_expose_api
-    def undelete(self, trans, **kwd):
-        raise HTTPNotImplemented()
-
-
-class LibraryDatasetExtendMetadataController(BaseExtendedMetadataController):
+class LibraryDatasetExtendMetadataController(BaseExtendedMetadataController[model.LibraryDatasetDatasetAssociation]):
     controller_name = "library_dataset_extended_metadata"
     exmeta_item_id = "library_content_id"
 
-    def _get_item_from_id(self, trans, idstr, check_writable=True):
+    def _get_item_from_id(self, trans, idstr, check_writable=True) -> Optional[model.LibraryDatasetDatasetAssociation]:
         if check_writable:
             item = self.get_library_dataset_dataset_association(trans, idstr)
             if trans.app.security_agent.can_modify_library_item(trans.get_current_user_roles(), item):
                 return item
         else:
             item = self.get_library_dataset_dataset_association(trans, idstr)
             if trans.app.security_agent.can_access_library_item(trans.get_current_user_roles(), item, trans.user):
                 return item
         return None
 
 
-class HistoryDatasetExtendMetadataController(BaseExtendedMetadataController):
+class HistoryDatasetExtendMetadataController(BaseExtendedMetadataController[model.HistoryDatasetAssociation]):
     controller_name = "history_dataset_extended_metadata"
     exmeta_item_id = "history_content_id"
+    hda_manager: managers.hdas.HDAManager = depends(managers.hdas.HDAManager)
 
-    def __init__(self, app):
-        super(HistoryDatasetExtendMetadataController, self).__init__(app)
-        self.hda_manager = managers.hdas.HDAManager(app)
-
-    def _get_item_from_id(self, trans, idstr, check_writable=True):
+    def _get_item_from_id(self, trans, idstr, check_writable=True) -> Optional[model.HistoryDatasetAssociation]:
         decoded_idstr = self.decode_id(idstr)
         if check_writable:
             return self.hda_manager.get_owned(decoded_idstr, trans.user, current_history=trans.history)
         else:
             hda = self.hda_manager.get_accessible(decoded_idstr, trans.user)
             return self.hda_manager.error_if_uploading(hda)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/folder_contents.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/libraries.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,340 +1,336 @@
-"""
-API operations on the contents of a library folder.
-"""
 import logging
+from typing import (
+    Any,
+    Dict,
+    List,
+    Optional,
+    Union,
+)
 
 from galaxy import (
     exceptions,
-    managers,
-    util
+    util,
+)
+from galaxy.managers.context import ProvidesAppContext
+from galaxy.managers.folders import FolderManager
+from galaxy.managers.libraries import LibraryManager
+from galaxy.managers.roles import RoleManager
+from galaxy.schema.fields import DecodedDatabaseIdField
+from galaxy.schema.schema import (
+    CreateLibrariesFromStore,
+    CreateLibraryPayload,
+    LibraryAvailablePermissions,
+    LibraryCurrentPermissions,
+    LibraryLegacySummary,
+    LibraryPermissionScope,
+    LibrarySummary,
+    LibrarySummaryList,
+    UpdateLibraryPayload,
 )
-from galaxy.managers import folders
-from galaxy.model import tags
-from galaxy.web import (
-    expose_api,
-    expose_api_anonymous
+from galaxy.security.idencoding import IdEncodingHelper
+from galaxy.webapps.galaxy.services.base import (
+    ConsumesModelStores,
+    ServiceBase,
 )
-from galaxy.webapps.base.controller import BaseAPIController, UsesLibraryMixin, UsesLibraryMixinItems
 
 log = logging.getLogger(__name__)
 
 
-class FolderContentsController(BaseAPIController, UsesLibraryMixin, UsesLibraryMixinItems):
-    """
-    Class controls retrieval, creation and updating of folder contents.
+class LibrariesService(ServiceBase, ConsumesModelStores):
     """
+    Common interface/service logic for interactions with libraries (top level) in the context of the API.
 
-    def __init__(self, app):
-        super(FolderContentsController, self).__init__(app)
-        self.folder_manager = folders.FolderManager()
-        self.hda_manager = managers.hdas.HDAManager(app)
-
-    @expose_api_anonymous
-    def index(self, trans, folder_id, **kwd):
-        """
-        GET /api/folders/{encoded_folder_id}/contents
+    Provides the logic of the actions invoked by API controllers and uses type definitions
+    and pydantic models to declare its parameters and return types.
+    """
 
-        Displays a collection (list) of a folder's contents
-        (files and folders). Encoded folder ID is prepended
-        with 'F' if it is a folder as opposed to a data set
-        which does not have it. Full path is provided in
-        response as a separate object providing data for
-        breadcrumb path building.
-
-        :param  folder_id: encoded ID of the folder which
-            contents should be library_dataset_dict
-        :type   folder_id: encoded string
+    def __init__(
+        self,
+        security: IdEncodingHelper,
+        folder_manager: FolderManager,
+        library_manager: LibraryManager,
+        role_manager: RoleManager,
+    ):
+        super().__init__(security)
+        self.folder_manager = folder_manager
+        self.library_manager = library_manager
+        self.role_manager = role_manager
+
+    def index(self, trans: ProvidesAppContext, deleted: Optional[bool] = False) -> LibrarySummaryList:
+        """Returns a list of summary data for all libraries.
+
+        :param  deleted: if True, show only ``deleted`` libraries, if False show only ``non-deleted``
+        :type   deleted: boolean (optional)
+
+        :returns:   list of dictionaries containing library information
+        :rtype:     list
+
+        .. seealso:: :attr:`galaxy.model.Library.dict_collection_visible_keys`
+
+        """
+        query, prefetched_ids = self.library_manager.list(trans, deleted)
+        libraries = []
+        for library in query:
+            libraries.append(self.library_manager.get_library_dict(trans, library, prefetched_ids))
+        return LibrarySummaryList.construct(__root__=libraries)
+
+    def show(self, trans, id: DecodedDatabaseIdField) -> LibrarySummary:
+        """Returns detailed information about a library."""
+        library = self.library_manager.get(trans, id)
+        library_dict = self.library_manager.get_library_dict(trans, library)
+        return LibrarySummary.construct(**library_dict)
+
+    def create(self, trans, payload: CreateLibraryPayload) -> LibrarySummary:
+        """Creates a new library.
+
+        .. note:: Currently, only admin users can create libraries.
+        """
+        library = self.library_manager.create(trans, payload.name, payload.description, payload.synopsis)
+        return self._to_summary(trans, library)
+
+    def create_from_store(self, trans, payload: CreateLibrariesFromStore) -> List[LibrarySummary]:
+        object_tracker = self.create_objects_from_store(
+            trans,
+            payload,
+            for_library=True,
+        )
+        rval = []
+        for library in object_tracker.libraries_by_key.values():
+            rval.append(self._to_summary(trans, library))
+        return rval
+
+    def update(self, trans, id: DecodedDatabaseIdField, payload: UpdateLibraryPayload) -> LibrarySummary:
+        """Updates the library with given ``id`` with the data in the payload."""
+        library = self.library_manager.get(trans, id)
+        name = payload.name
+        if name == "":
+            raise exceptions.RequestParameterMissingException(
+                "Parameter 'name' of library is required. You cannot remove it."
+            )
+        updated_library = self.library_manager.update(trans, library, name, payload.description, payload.synopsis)
+        return self._to_summary(trans, updated_library)
+
+    def delete(self, trans, id: DecodedDatabaseIdField, undelete: Optional[bool] = False) -> LibrarySummary:
+        """Marks the library with the given ``id`` as `deleted` (or removes the `deleted` mark if the `undelete` param is true)
+
+        .. note:: Currently, only admin users can un/delete libraries.
+
+        :param  undelete:    (optional) flag specifying whether the item should be deleted or undeleted, defaults to false:
+        :type   undelete:    bool
+
+        .. seealso:: :attr:`galaxy.model.Library.dict_element_visible_keys`
+        """
+        library = self.library_manager.get(trans, id)
+        library = self.library_manager.delete(trans, library, undelete)
+        return self._to_summary(trans, library)
+
+    def get_permissions(
+        self,
+        trans,
+        id: DecodedDatabaseIdField,
+        scope: Optional[LibraryPermissionScope] = LibraryPermissionScope.current,
+        is_library_access: Optional[bool] = False,
+        page: Optional[int] = 1,
+        page_limit: Optional[int] = 10,
+        query: Optional[str] = None,
+    ) -> Union[LibraryCurrentPermissions, LibraryAvailablePermissions]:
+        """Load all permissions for the given library id and return it.
+
+        :param  id:     the encoded id of the library
+        :type   id:     an encoded id string
+
+        :param  scope:      either 'current' or 'available'
+        :type   scope:      string
 
-        :param kwd: keyword dictionary with other params
-        :type  kwd: dict
+        :param  is_library_access:      indicates whether the roles available for the library access are requested
+        :type   is_library_access:      bool
 
-        :returns: dictionary containing all items and metadata
-        :type:    dict
+        :returns:   dictionary with all applicable permissions' values
+        :rtype:     dictionary
 
-        :raises: MalformedId, InconsistentDatabase, ObjectNotFound,
-             InternalServerError
+        :raises: InsufficientPermissionsException
         """
-        is_admin = trans.user_is_admin
-        deleted = kwd.get('include_deleted', 'missing')
         current_user_roles = trans.get_current_user_roles()
-        try:
-            deleted = util.asbool(deleted)
-        except ValueError:
-            deleted = False
-
-        decoded_folder_id = self.folder_manager.cut_and_decode(trans, folder_id)
-        folder = self.folder_manager.get(trans, decoded_folder_id)
-
-        # Special level of security on top of libraries.
-        if trans.app.security_agent.can_access_library(current_user_roles, folder.parent_library) or is_admin:
-            pass
-        else:
-            if trans.user:
-                log.warning("SECURITY: User (id: %s) without proper access rights is trying to load folder with ID of %s" % (trans.user.id, decoded_folder_id))
-            else:
-                log.warning("SECURITY: Anonymous user is trying to load restricted folder with ID of %s" % (decoded_folder_id))
-            raise exceptions.ObjectNotFound('Folder with the id provided ( %s ) was not found' % str(folder_id))
-
-        folder_contents = []
-        update_time = ''
-        create_time = ''
-        #  Go through every accessible item (folders, datasets) in the folder and include its metadata.
-        for content_item in self._load_folder_contents(trans, folder, deleted):
-            return_item = {}
-            encoded_id = trans.security.encode_id(content_item.id)
-            create_time = content_item.create_time.strftime("%Y-%m-%d %I:%M %p")
-
-            if content_item.api_type == 'folder':
-                encoded_id = 'F' + encoded_id
-                can_modify = is_admin or (trans.user and trans.app.security_agent.can_modify_library_item(current_user_roles, folder))
-                can_manage = is_admin or (trans.user and trans.app.security_agent.can_manage_library_item(current_user_roles, folder))
-                update_time = content_item.update_time.strftime("%Y-%m-%d %I:%M %p")
-                return_item.update(dict(can_modify=can_modify, can_manage=can_manage))
-                if content_item.description:
-                    return_item.update(dict(description=content_item.description))
-
-            elif content_item.api_type == 'file':
-                #  Is the dataset public or private?
-                #  When both are False the dataset is 'restricted'
-                #  Access rights are checked on the dataset level, not on the ld or ldda level to maintain consistency
-                dataset = content_item.library_dataset_dataset_association.dataset
-                is_unrestricted = trans.app.security_agent.dataset_is_public(dataset)
-                if not is_unrestricted and trans.user and trans.app.security_agent.dataset_is_private_to_user(trans, dataset):
-                    is_private = True
-                else:
-                    is_private = False
-
-                # Can user manage the permissions on the dataset?
-                can_manage = is_admin or (trans.user and trans.app.security_agent.can_manage_dataset(current_user_roles, content_item.library_dataset_dataset_association.dataset))
-                raw_size = int(content_item.library_dataset_dataset_association.get_size())
-                nice_size = util.nice_size(raw_size)
-                update_time = content_item.library_dataset_dataset_association.update_time.strftime("%Y-%m-%d %I:%M %p")
-
-                library_dataset_dict = content_item.to_dict()
-                encoded_ldda_id = trans.security.encode_id(content_item.library_dataset_dataset_association.id)
-
-                tag_manager = tags.GalaxyTagHandler(trans.sa_session)
-                ldda_tags = tag_manager.get_tags_str(content_item.library_dataset_dataset_association.tags)
-
-                return_item.update(dict(file_ext=library_dataset_dict['file_ext'],
-                                        date_uploaded=library_dataset_dict['date_uploaded'],
-                                        update_time=update_time,
-                                        is_unrestricted=is_unrestricted,
-                                        is_private=is_private,
-                                        can_manage=can_manage,
-                                        state=library_dataset_dict['state'],
-                                        file_size=nice_size,
-                                        raw_size=raw_size,
-                                        ldda_id=encoded_ldda_id,
-                                        tags=ldda_tags))
-                if content_item.library_dataset_dataset_association.message:
-                    return_item.update(dict(message=content_item.library_dataset_dataset_association.message))
-                elif content_item.library_dataset_dataset_association.info:
-                    # There is no message but ldda info contains something so we display that instead.
-                    return_item.update(dict(message=content_item.library_dataset_dataset_association.info))
-
-            # For every item include the default metadata
-            return_item.update(dict(id=encoded_id,
-                                    type=content_item.api_type,
-                                    name=content_item.name,
-                                    update_time=update_time,
-                                    create_time=create_time,
-                                    deleted=content_item.deleted))
-            folder_contents.append(return_item)
-
-        # Return the reversed path so it starts with the library node.
-        full_path = self.build_path(trans, folder)[::-1]
-
-        # Check whether user can add items to the current folder
-        can_add_library_item = is_admin or trans.app.security_agent.can_add_library_item(current_user_roles, folder)
-
-        # Check whether user can modify the current folder
-        can_modify_folder = is_admin or trans.app.security_agent.can_modify_library_item(current_user_roles, folder)
-
-        parent_library_id = None
-        if folder.parent_library is not None:
-            parent_library_id = trans.security.encode_id(folder.parent_library.id)
-
-        metadata = dict(full_path=full_path,
-                        can_add_library_item=can_add_library_item,
-                        can_modify_folder=can_modify_folder,
-                        folder_name=folder.name,
-                        folder_description=folder.description,
-                        parent_library_id=parent_library_id)
-        folder_container = dict(metadata=metadata, folder_contents=folder_contents)
-        return folder_container
-
-    def build_path(self, trans, folder):
-        """
-        Search the path upwards recursively and load the whole route of
-        names and ids for breadcrumb building purposes.
-
-        :param folder: current folder for navigating up
-        :param type:   Galaxy LibraryFolder
-
-        :returns:   list consisting of full path to the library
-        :type:      list
-        """
-        path_to_root = []
-        # We are almost in root
-        if folder.parent_id is None:
-            path_to_root.append(('F' + trans.security.encode_id(folder.id), folder.name))
+        is_admin = trans.user_is_admin
+        library = self.library_manager.get(trans, id)
+        if not (is_admin or trans.app.security_agent.can_manage_library_item(current_user_roles, library)):
+            raise exceptions.InsufficientPermissionsException(
+                "You do not have proper permission to access permissions of this library."
+            )
+
+        if scope == LibraryPermissionScope.current or scope is None:
+            roles = self.library_manager.get_current_roles(trans, library)
+            return LibraryCurrentPermissions.construct(**roles)
+
+        #  Return roles that are available to select.
+        elif scope == LibraryPermissionScope.available:
+            roles, total_roles = trans.app.security_agent.get_valid_roles(
+                trans, library, query, page, page_limit, is_library_access
+            )
+
+            return_roles = []
+            for role in roles:
+                role_id = DecodedDatabaseIdField.encode(role.id)
+                return_roles.append(dict(id=role_id, name=role.name, type=role.type))
+            return LibraryAvailablePermissions.construct(
+                roles=return_roles, page=page, page_limit=page_limit, total=total_roles
+            )
         else:
-            # We add the current folder and traverse up one folder.
-            path_to_root.append(('F' + trans.security.encode_id(folder.id), folder.name))
-            upper_folder = trans.sa_session.query(trans.app.model.LibraryFolder).get(folder.parent_id)
-            path_to_root.extend(self.build_path(trans, upper_folder))
-        return path_to_root
-
-    def _load_folder_contents(self, trans, folder, include_deleted):
-        """
-        Loads all contents of the folder (folders and data sets) but only
-        in the first level. Include deleted if the flag is set and if the
-        user has access to undelete it.
-
-        :param  folder:          the folder which contents are being loaded
-        :type   folder:          Galaxy LibraryFolder
-
-        :param  include_deleted: flag, when true the items that are deleted
-            and can be undeleted by current user are shown
-        :type   include_deleted: boolean
-
-        :returns:   a list containing the requested items
-        :type:      list
+            raise exceptions.RequestParameterInvalidException(
+                "The value of 'scope' parameter is invalid. Alllowed values: current, available"
+            )
+
+    def set_permissions(
+        self, trans, id: DecodedDatabaseIdField, payload: Dict[str, Any]
+    ) -> Union[LibraryLegacySummary, LibraryCurrentPermissions]:  # Old legacy response
+        """Set permissions of the given library to the given role ids.
+
+        :param  id:      the encoded id of the library to set the permissions of
+        :type   id:      an encoded id string
+        :param   payload: dictionary structure containing:
+
+            :param  action:            (required) describes what action should be performed
+                                       available actions: remove_restrictions, set_permissions
+            :type   action:            str
+            :param  access_ids[]:      list of Role.id defining roles that should have access permission on the library
+            :type   access_ids[]:      string or list
+            :param  add_ids[]:         list of Role.id defining roles that should have add item permission on the library
+            :type   add_ids[]:         string or list
+            :param  manage_ids[]:      list of Role.id defining roles that should have manage permission on the library
+            :type   manage_ids[]:      string or list
+            :param  modify_ids[]:      list of Role.id defining roles that should have modify permission on the library
+            :type   modify_ids[]:      string or list
+
+        :type:      dictionary
+        :returns:   dict of current roles for all available permission types
+        :rtype:     dictionary
+        :raises: RequestParameterInvalidException, InsufficientPermissionsException, InternalServerError
+                    RequestParameterMissingException
         """
-        current_user_roles = trans.get_current_user_roles()
         is_admin = trans.user_is_admin
-        content_items = []
-        for subfolder in folder.folders:
-            if subfolder.deleted:
-                if include_deleted:
-                    if is_admin:
-                        # Admins can see all deleted folders.
-                        subfolder.api_type = 'folder'
-                        content_items.append(subfolder)
-                    else:
-                        # Users with MODIFY permissions can see deleted folders.
-                        can_modify = trans.app.security_agent.can_modify_library_item(current_user_roles, subfolder)
-                        if can_modify:
-                            subfolder.api_type = 'folder'
-                            content_items.append(subfolder)
-            else:
-                # Undeleted folders are non-restricted for now. The contents are not.
-                # TODO decide on restrictions
-                subfolder.api_type = 'folder'
-                content_items.append(subfolder)
-                # if is_admin:
-                #     subfolder.api_type = 'folder'
-                #     content_items.append( subfolder )
-                # else:
-                #     can_access, folder_ids = trans.app.security_agent.check_folder_contents( trans.user, current_user_roles, subfolder )
-                #     if can_access:
-                #         subfolder.api_type = 'folder'
-                #         content_items.append( subfolder )
-
-        for dataset in folder.datasets:
-            if dataset.deleted:
-                if include_deleted:
-                    if is_admin:
-                        # Admins can see all deleted datasets.
-                        dataset.api_type = 'file'
-                        content_items.append(dataset)
-                    else:
-                        # Users with MODIFY permissions on the item can see the deleted item.
-                        can_modify = trans.app.security_agent.can_modify_library_item(current_user_roles, dataset)
-                        if can_modify:
-                            dataset.api_type = 'file'
-                            content_items.append(dataset)
-            else:
-                if is_admin:
-                    dataset.api_type = 'file'
-                    content_items.append(dataset)
-                else:
-                    can_access = trans.app.security_agent.can_access_dataset(current_user_roles, dataset.library_dataset_dataset_association.dataset)
-                    if can_access:
-                        dataset.api_type = 'file'
-                        content_items.append(dataset)
-
-        return content_items
-
-    @expose_api
-    def create(self, trans, encoded_folder_id, payload, **kwd):
-        """
-        POST /api/folders/{encoded_id}/contents
-
-        Create a new library file from an HDA.
+        current_user_roles = trans.get_current_user_roles()
+        library = self.library_manager.get(trans, id)
 
-        :param  encoded_folder_id:      the encoded id of the folder to import dataset(s) to
-        :type   encoded_folder_id:      an encoded id string
-        :param  payload:    dictionary structure containing:
-            :param from_hda_id:         (optional) the id of an accessible HDA to copy into the library
-            :type  from_hda_id:         encoded id
-            :param from_hdca_id:         (optional) the id of an accessible HDCA to copy into the library
-            :type  from_hdca_id:         encoded id
-            :param ldda_message:        (optional) the new message attribute of the LDDA created
-            :type   ldda_message:       str
-            :param extended_metadata:   (optional) dub-dictionary containing any extended metadata to associate with the item
-            :type  extended_metadata:   dict
-        :type   payload:    dict
-
-        :returns:   a dictionary describing the new item if ``from_hda_id`` is supplied or a list of
-                    such dictionaries describing the new items if ``from_hdca_id`` is supplied.
-        :rtype:     object
-
-        :raises:    ObjectAttributeInvalidException,
-            InsufficientPermissionsException, ItemAccessibilityException,
-            InternalServerError
-        """
-        encoded_folder_id_16 = self.__decode_library_content_id(trans, encoded_folder_id)
-        from_hda_id = payload.pop('from_hda_id', None)
-        from_hdca_id = payload.pop('from_hdca_id', None)
-        ldda_message = payload.pop('ldda_message', '')
-        if ldda_message:
-            ldda_message = util.sanitize_html.sanitize_html(ldda_message)
-        try:
-            if from_hda_id:
-                decoded_hda_id = self.decode_id(from_hda_id)
-                return self._copy_hda_to_library_folder(trans, self.hda_manager, decoded_hda_id, encoded_folder_id_16, ldda_message)
-            if from_hdca_id:
-                decoded_hdca_id = self.decode_id(from_hdca_id)
-                return self._copy_hdca_to_library_folder(trans, self.hda_manager, decoded_hdca_id, encoded_folder_id_16, ldda_message)
-        except Exception as exc:
-            # TODO handle exceptions better within the mixins
-            exc_message = util.unicodify(exc)
-            if 'not accessible to the current user' in exc_message or 'You are not allowed to access this dataset' in exc_message:
-                raise exceptions.ItemAccessibilityException('You do not have access to the requested item')
+        if not (is_admin or trans.app.security_agent.can_manage_library_item(current_user_roles, library)):
+            raise exceptions.InsufficientPermissionsException(
+                "You do not have proper permission to modify permissions of this library."
+            )
+
+        new_access_roles_ids = util.listify(payload.get("access_ids[]", None))
+        new_add_roles_ids = util.listify(payload.get("add_ids[]", None))
+        new_manage_roles_ids = util.listify(payload.get("manage_ids[]", None))
+        new_modify_roles_ids = util.listify(payload.get("modify_ids[]", None))
+
+        action = payload.get("action", None)
+        if action is None:
+            if payload is not None:
+                return self.set_permissions_old(trans, library, payload)
             else:
-                log.exception(exc)
-                raise exc
-
-    def __decode_library_content_id(self, trans, encoded_folder_id):
-        """
-        Identify whether the id provided is properly encoded
-        LibraryFolder.
-
-        :param  encoded_folder_id:  encoded id of Galaxy LibraryFolder
-        :type   encoded_folder_id:  encoded string
-
-        :returns:   encoded id of Folder (had 'F' prepended)
-        :type:  string
-
-        :raises:    MalformedId
-        """
-        if ((len(encoded_folder_id) % 16 == 1) and encoded_folder_id.startswith('F')):
-            return encoded_folder_id[1:]
+                raise exceptions.RequestParameterMissingException('The mandatory parameter "action" is missing.')
+        elif action == "remove_restrictions":
+            is_public = self.library_manager.make_public(trans, library)
+            if not is_public:
+                raise exceptions.InternalServerError("An error occurred while making library public.")
+        elif action == "set_permissions":
+            # ACCESS LIBRARY ROLES
+            valid_access_roles = []
+            invalid_access_roles_names = []
+            for role_id in new_access_roles_ids:
+                role = self.role_manager.get(trans, role_id)
+                valid_roles, total_roles = trans.app.security_agent.get_valid_roles(
+                    trans, library, is_library_access=True
+                )
+                if role in valid_roles:
+                    valid_access_roles.append(role)
+                else:
+                    invalid_access_roles_names.append(role_id)
+            if len(invalid_access_roles_names) > 0:
+                log.warning(
+                    f"The following roles could not be added to the library access permission: {str(invalid_access_roles_names)}"
+                )
+
+            # ADD TO LIBRARY ROLES
+            valid_add_roles = []
+            invalid_add_roles_names = []
+            for role_id in new_add_roles_ids:
+                role = self.role_manager.get(trans, role_id)
+                valid_roles, total_roles = trans.app.security_agent.get_valid_roles(trans, library)
+                if role in valid_roles:
+                    valid_add_roles.append(role)
+                else:
+                    invalid_add_roles_names.append(role_id)
+            if len(invalid_add_roles_names) > 0:
+                log.warning(
+                    f"The following roles could not be added to the add library item permission: {str(invalid_add_roles_names)}"
+                )
+
+            # MANAGE LIBRARY ROLES
+            valid_manage_roles = []
+            invalid_manage_roles_names = []
+            for role_id in new_manage_roles_ids:
+                role = self.role_manager.get(trans, role_id)
+                valid_roles, total_roles = trans.app.security_agent.get_valid_roles(trans, library)
+                if role in valid_roles:
+                    valid_manage_roles.append(role)
+                else:
+                    invalid_manage_roles_names.append(role_id)
+            if len(invalid_manage_roles_names) > 0:
+                log.warning(
+                    f"The following roles could not be added to the manage library permission: {str(invalid_manage_roles_names)}"
+                )
+
+            # MODIFY LIBRARY ROLES
+            valid_modify_roles = []
+            invalid_modify_roles_names = []
+            for role_id in new_modify_roles_ids:
+                role = self.role_manager.get(trans, role_id)
+                valid_roles, total_roles = trans.app.security_agent.get_valid_roles(trans, library)
+                if role in valid_roles:
+                    valid_modify_roles.append(role)
+                else:
+                    invalid_modify_roles_names.append(role_id)
+            if len(invalid_modify_roles_names) > 0:
+                log.warning(
+                    f"The following roles could not be added to the modify library permission: {str(invalid_modify_roles_names)}"
+                )
+
+            permissions = {trans.app.security_agent.permitted_actions.LIBRARY_ACCESS: valid_access_roles}
+            permissions.update({trans.app.security_agent.permitted_actions.LIBRARY_ADD: valid_add_roles})
+            permissions.update({trans.app.security_agent.permitted_actions.LIBRARY_MANAGE: valid_manage_roles})
+            permissions.update({trans.app.security_agent.permitted_actions.LIBRARY_MODIFY: valid_modify_roles})
+
+            trans.app.security_agent.set_all_library_permissions(trans, library, permissions)
+            trans.sa_session.refresh(library)
+            # Copy the permissions to the root folder
+            trans.app.security_agent.copy_library_permissions(trans, library, library.root_folder)
         else:
-            raise exceptions.MalformedId('Malformed folder id ( %s ) specified, unable to decode.' % str(encoded_folder_id))
-
-    @expose_api
-    def show(self, trans, id, library_id, **kwd):
-        """
-        GET /api/folders/{encoded_folder_id}/
-        """
-        raise exceptions.NotImplemented('Showing the library folder content is not implemented here.')
-
-    @expose_api
-    def update(self, trans, id, library_id, payload, **kwd):
-        """
-        PUT /api/folders/{encoded_folder_id}/contents
-        """
-        raise exceptions.NotImplemented('Updating the library folder content is not implemented here.')
+            raise exceptions.RequestParameterInvalidException(
+                'The mandatory parameter "action" has an invalid value.'
+                'Allowed values are: "remove_restrictions", set_permissions"'
+            )
+        roles = self.library_manager.get_current_roles(trans, library)
+        return LibraryCurrentPermissions.construct(**roles)
+
+    def set_permissions_old(self, trans, library, payload: Dict[str, Any]) -> LibraryLegacySummary:
+        """
+        *** old implementation for backward compatibility ***
+
+        Updates the library permissions.
+        """
+        permissions = {}
+        for k, v in trans.app.model.Library.permitted_actions.items():
+            role_params = payload.get(f"{k}_in", [])
+            in_roles = [trans.sa_session.query(trans.app.model.Role).get(x) for x in util.listify(role_params)]
+            permissions[trans.app.security_agent.get_action(v.action)] = in_roles
+        trans.app.security_agent.set_all_library_permissions(trans, library, permissions)
+        trans.sa_session.refresh(library)
+        # Copy the permissions to the root folder
+        trans.app.security_agent.copy_library_permissions(trans, library, library.root_folder)
+        item = library.to_dict(
+            view="element", value_mapper={"id": trans.security.encode_id, "root_folder_id": trans.security.encode_id}
+        )
+        return LibraryLegacySummary.construct(**item)
+
+    def _to_summary(self, trans, library) -> LibrarySummary:
+        library_dict = self.library_manager.get_library_dict(trans, library)
+        return LibrarySummary.construct(**library_dict)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/folders.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/library_folders.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,299 +1,277 @@
-"""
-API operations on library folders.
-"""
 import logging
+from typing import (
+    Optional,
+    Union,
+)
 
-from galaxy import (
-    exceptions,
-    util
+from galaxy import util
+from galaxy.exceptions import (
+    InsufficientPermissionsException,
+    RequestParameterInvalidException,
+    RequestParameterMissingException,
+)
+from galaxy.managers.folders import FolderManager
+from galaxy.managers.roles import RoleManager
+from galaxy.schema.fields import (
+    DecodedDatabaseIdField,
+    LibraryFolderDatabaseIdField,
 )
-from galaxy.managers import folders, roles
-from galaxy.web import expose_api
-from galaxy.webapps.base.controller import BaseAPIController, UsesLibraryMixin, UsesLibraryMixinItems
+from galaxy.schema.schema import (
+    CreateLibraryFolderPayload,
+    LibraryAvailablePermissions,
+    LibraryFolderCurrentPermissions,
+    LibraryFolderDetails,
+    LibraryPermissionScope,
+    UpdateLibraryFolderPayload,
+)
+from galaxy.security.idencoding import IdEncodingHelper
+from galaxy.webapps.galaxy.services.base import ServiceBase
 
 log = logging.getLogger(__name__)
 
 
-class FoldersController(BaseAPIController, UsesLibraryMixin, UsesLibraryMixinItems):
-
-    def __init__(self, app):
-        super(FoldersController, self).__init__(app)
-        self.folder_manager = folders.FolderManager()
-        self.role_manager = roles.RoleManager(app)
-
-    @expose_api
-    def index(self, trans, **kwd):
-        """
-        GET /api/folders/
-
-        This would normally display a list of folders. However, that would
-        be across multiple libraries, so it's not implemented.
-        """
-        raise exceptions.NotImplemented('Listing all accessible library folders is not implemented.')
+class LibraryFoldersService(ServiceBase):
+    """Common interface/service logic for interactions with library folders in the context of the API.
+    Provides the logic of the actions invoked by API controllers and uses type definitions
+    and pydantic models to declare its parameters and return types.
+    """
+
+    def __init__(self, security: IdEncodingHelper, folder_manager: FolderManager, role_manager: RoleManager):
+        super().__init__(security)
+        self.folder_manager = folder_manager
+        self.role_manager = role_manager
 
-    @expose_api
-    def show(self, trans, id, **kwd):
+    def show(self, trans, folder_id: LibraryFolderDatabaseIdField) -> LibraryFolderDetails:
         """
-        GET /api/folders/{encoded_folder_id}
-
         Displays information about a folder.
 
         :param  id:      the folder's encoded id (required)
         :type   id:      an encoded id string (has to be prefixed by 'F')
 
         :returns:   dictionary including details of the folder
         :rtype:     dict
         """
-        folder_id = self.folder_manager.cut_and_decode(trans, id)
         folder = self.folder_manager.get(trans, folder_id, check_manageable=False, check_accessible=True)
         return_dict = self.folder_manager.get_folder_dict(trans, folder)
-        return return_dict
+        return LibraryFolderDetails.construct(**return_dict)
 
-    @expose_api
-    def create(self, trans, encoded_parent_folder_id, payload=None, **kwd):
+    def create(
+        self, trans, parent_folder_id: LibraryFolderDatabaseIdField, payload: CreateLibraryFolderPayload
+    ) -> LibraryFolderDetails:
         """
-        POST /api/folders/{encoded_parent_folder_id}
-
         Create a new folder object underneath the one specified in the parameters.
 
-        :param  encoded_parent_folder_id:      (required) the parent folder's id
-        :type   encoded_parent_folder_id:      an encoded id string (should be prefixed by 'F')
+        :param  parent_folder_id:      (required) the parent folder's id
         :param   payload: dictionary structure containing:
+
             :param  name:                          (required) the name of the new folder
             :type   name:                          str
             :param  description:                   the description of the new folder
             :type   description:                   str
+
         :type       dictionary
         :returns:   information about newly created folder, notably including ID
         :rtype:     dictionary
         :raises: RequestParameterMissingException
         """
-        if payload:
-            kwd.update(payload)
-        name = kwd.get('name', None)
-        if name is None:
-            raise exceptions.RequestParameterMissingException("Missing required parameter 'name'.")
-        description = kwd.get('description', '')
-        decoded_parent_folder_id = self.folder_manager.cut_and_decode(trans, encoded_parent_folder_id)
-        parent_folder = self.folder_manager.get(trans, decoded_parent_folder_id)
-        new_folder = self.folder_manager.create(trans, parent_folder.id, name, description)
-        return self.folder_manager.get_folder_dict(trans, new_folder)
-
-    @expose_api
-    def get_permissions(self, trans, encoded_folder_id, **kwd):
+        parent_folder = self.folder_manager.get(trans, parent_folder_id)
+        new_folder = self.folder_manager.create(trans, parent_folder.id, payload.name, payload.description)
+        return_dict = self.folder_manager.get_folder_dict(trans, new_folder)
+        return LibraryFolderDetails.construct(**return_dict)
+
+    def get_permissions(
+        self,
+        trans,
+        folder_id: LibraryFolderDatabaseIdField,
+        scope: Optional[LibraryPermissionScope] = LibraryPermissionScope.current,
+        page: Optional[int] = 1,
+        page_limit: Optional[int] = 10,
+        query: Optional[str] = None,
+    ) -> Union[LibraryFolderCurrentPermissions, LibraryAvailablePermissions]:
         """
-        GET /api/folders/{id}/permissions
-
         Load all permissions for the given folder id and return it.
 
-        :param  encoded_folder_id:     the encoded id of the folder
-        :type   encoded_folder_id:     an encoded id string
+        :param  folder_id:     the encoded id of the folder
 
         :param  scope:      either 'current' or 'available'
         :type   scope:      string
 
         :returns:   dictionary with all applicable permissions' values
         :rtype:     dictionary
 
         :raises: InsufficientPermissionsException
         """
         current_user_roles = trans.get_current_user_roles()
         is_admin = trans.user_is_admin
-        decoded_folder_id = self.folder_manager.cut_and_decode(trans, encoded_folder_id)
-        folder = self.folder_manager.get(trans, decoded_folder_id)
+        folder = self.folder_manager.get(trans, folder_id)
 
         if not (is_admin or trans.app.security_agent.can_manage_library_item(current_user_roles, folder)):
-            raise exceptions.InsufficientPermissionsException('You do not have proper permission to access permissions of this folder.')
-
-        scope = kwd.get('scope', None)
-        if scope == 'current' or scope is None:
-            return self.folder_manager.get_current_roles(trans, folder)
+            raise InsufficientPermissionsException(
+                "You do not have proper permission to access permissions of this folder."
+            )
+
+        if scope is None or scope == LibraryPermissionScope.current:
+            current_permissions = self.folder_manager.get_current_roles(trans, folder)
+            return LibraryFolderCurrentPermissions.construct(**current_permissions)
         #  Return roles that are available to select.
-        elif scope == 'available':
-            page = kwd.get('page', None)
-            if page is not None:
-                page = int(page)
-            else:
-                page = 1
-            page_limit = kwd.get('page_limit', None)
-            if page_limit is not None:
-                page_limit = int(page_limit)
-            else:
-                page_limit = 10
-            query = kwd.get('q', None)
+        elif scope == LibraryPermissionScope.available:
             roles, total_roles = trans.app.security_agent.get_valid_roles(trans, folder, query, page, page_limit)
             return_roles = []
             for role in roles:
-                role_id = trans.security.encode_id(role.id)
+                role_id = DecodedDatabaseIdField.encode(role.id)
                 return_roles.append(dict(id=role_id, name=role.name, type=role.type))
-            return dict(roles=return_roles, page=page, page_limit=page_limit, total=total_roles)
+            return LibraryAvailablePermissions.construct(
+                roles=return_roles, page=page, page_limit=page_limit, total=total_roles
+            )
         else:
-            raise exceptions.RequestParameterInvalidException("The value of 'scope' parameter is invalid. Alllowed values: current, available")
-
-    @expose_api
-    def set_permissions(self, trans, encoded_folder_id, payload=None, **kwd):
+            raise RequestParameterInvalidException(
+                "The value of 'scope' parameter is invalid. Allowed values: current, available"
+            )
+
+    def set_permissions(
+        self, trans, folder_id: LibraryFolderDatabaseIdField, payload: dict
+    ) -> LibraryFolderCurrentPermissions:
         """
-        POST /api/folders/{encoded_folder_id}/permissions
-
         Set permissions of the given folder to the given role ids.
 
-        :param  encoded_folder_id:      the encoded id of the folder to set the permissions of
-        :type   encoded_folder_id:      an encoded id string
+        :param  folder_id:      the encoded id of the folder to set the permissions of
         :param   payload: dictionary structure containing:
+
             :param  action:            (required) describes what action should be performed
             :type   action:            string
             :param  add_ids[]:         list of Role.id defining roles that should have add item permission on the folder
             :type   add_ids[]:         string or list
             :param  manage_ids[]:      list of Role.id defining roles that should have manage permission on the folder
             :type   manage_ids[]:      string or list
             :param  modify_ids[]:      list of Role.id defining roles that should have modify permission on the folder
             :type   modify_ids[]:      string or list
+
         :type       dictionary
         :returns:   dict of current roles for all available permission types.
         :rtype:     dictionary
         :raises: RequestParameterInvalidException, InsufficientPermissionsException, RequestParameterMissingException
         """
-        if payload:
-            kwd.update(payload)
+
         is_admin = trans.user_is_admin
         current_user_roles = trans.get_current_user_roles()
-        decoded_folder_id = self.folder_manager.cut_and_decode(trans, encoded_folder_id)
-        folder = self.folder_manager.get(trans, decoded_folder_id)
+        folder = self.folder_manager.get(trans, folder_id)
         if not (is_admin or trans.app.security_agent.can_manage_library_item(current_user_roles, folder)):
-            raise exceptions.InsufficientPermissionsException('You do not have proper permission to modify permissions of this folder.')
+            raise InsufficientPermissionsException(
+                "You do not have proper permission to modify permissions of this folder."
+            )
+
+        new_add_roles_ids = util.listify(payload.get("add_ids[]", None))
+        new_manage_roles_ids = util.listify(payload.get("manage_ids[]", None))
+        new_modify_roles_ids = util.listify(payload.get("modify_ids[]", None))
 
-        new_add_roles_ids = util.listify(kwd.get('add_ids[]', None))
-        new_manage_roles_ids = util.listify(kwd.get('manage_ids[]', None))
-        new_modify_roles_ids = util.listify(kwd.get('modify_ids[]', None))
-
-        action = kwd.get('action', None)
+        action = payload.get("action", None)
         if action is None:
-            raise exceptions.RequestParameterMissingException('The mandatory parameter "action" is missing.')
-        elif action == 'set_permissions':
-
+            raise RequestParameterMissingException('The mandatory parameter "action" is missing.')
+        elif action == "set_permissions":
             # ADD TO LIBRARY ROLES
             valid_add_roles = []
             invalid_add_roles_names = []
             for role_id in new_add_roles_ids:
-                role = self.role_manager.get(trans, self.__decode_id(trans, role_id, 'role'))
+                role = self.role_manager.get(trans, role_id)
                 #  Check whether role is in the set of allowed roles
                 valid_roles, total_roles = trans.app.security_agent.get_valid_roles(trans, folder)
                 if role in valid_roles:
                     valid_add_roles.append(role)
                 else:
                     invalid_add_roles_names.append(role_id)
             if len(invalid_add_roles_names) > 0:
-                log.warning("The following roles could not be added to the add library item permission: " + str(invalid_add_roles_names))
+                log.warning(
+                    f"The following roles could not be added to the add library item permission: {str(invalid_add_roles_names)}"
+                )
 
             # MANAGE FOLDER ROLES
             valid_manage_roles = []
             invalid_manage_roles_names = []
             for role_id in new_manage_roles_ids:
-                role = self.role_manager.get(trans, self.__decode_id(trans, role_id, 'role'))
+                role = self.role_manager.get(trans, role_id)
                 #  Check whether role is in the set of allowed roles
                 valid_roles, total_roles = trans.app.security_agent.get_valid_roles(trans, folder)
                 if role in valid_roles:
                     valid_manage_roles.append(role)
                 else:
                     invalid_manage_roles_names.append(role_id)
             if len(invalid_manage_roles_names) > 0:
-                log.warning("The following roles could not be added to the manage folder permission: " + str(invalid_manage_roles_names))
+                log.warning(
+                    f"The following roles could not be added to the manage folder permission: {str(invalid_manage_roles_names)}"
+                )
 
             # MODIFY FOLDER ROLES
             valid_modify_roles = []
             invalid_modify_roles_names = []
             for role_id in new_modify_roles_ids:
-                role = self.role_manager.get(trans, self.__decode_id(trans, role_id, 'role'))
+                role = self.role_manager.get(trans, role_id)
                 #  Check whether role is in the set of allowed roles
                 valid_roles, total_roles = trans.app.security_agent.get_valid_roles(trans, folder)
                 if role in valid_roles:
                     valid_modify_roles.append(role)
                 else:
                     invalid_modify_roles_names.append(role_id)
             if len(invalid_modify_roles_names) > 0:
-                log.warning("The following roles could not be added to the modify folder permission: " + str(invalid_modify_roles_names))
+                log.warning(
+                    f"The following roles could not be added to the modify folder permission: {str(invalid_modify_roles_names)}"
+                )
 
             permissions = {trans.app.security_agent.permitted_actions.LIBRARY_ADD: valid_add_roles}
             permissions.update({trans.app.security_agent.permitted_actions.LIBRARY_MANAGE: valid_manage_roles})
             permissions.update({trans.app.security_agent.permitted_actions.LIBRARY_MODIFY: valid_modify_roles})
 
             trans.app.security_agent.set_all_library_permissions(trans, folder, permissions)
         else:
-            raise exceptions.RequestParameterInvalidException('The mandatory parameter "action" has an invalid value.'
-                                                              'Allowed values are: "set_permissions"')
-        return self.folder_manager.get_current_roles(trans, folder)
-
-    @expose_api
-    def delete(self, trans, encoded_folder_id, **kwd):
+            raise RequestParameterInvalidException(
+                'The mandatory parameter "action" has an invalid value.' 'Allowed values are: "set_permissions"'
+            )
+        current_permissions = self.folder_manager.get_current_roles(trans, folder)
+        return LibraryFolderCurrentPermissions.construct(**current_permissions)
+
+    def delete(
+        self, trans, folder_id: LibraryFolderDatabaseIdField, undelete: Optional[bool] = False
+    ) -> LibraryFolderDetails:
         """
-        DELETE /api/folders/{encoded_folder_id}
-
         Mark the folder with the given ``encoded_folder_id`` as `deleted`
         (or remove the `deleted` mark if the `undelete` param is true).
 
         .. note:: Currently, only admin users can un/delete folders.
 
-        :param  encoded_folder_id:     the encoded id of the folder to un/delete
-        :type   encoded_folder_id:     an encoded id string
+        :param  folder_id:     the encoded id of the folder to un/delete
 
         :param  undelete:    (optional) flag specifying whether the item should be deleted or undeleted, defaults to false:
         :type   undelete:    bool
 
         :returns:   detailed folder information
         :rtype:     dictionary
 
         """
-        folder = self.folder_manager.get(trans, self.folder_manager.cut_and_decode(trans, encoded_folder_id), True)
-        undelete = util.string_as_bool(kwd.get('undelete', False))
+        folder = self.folder_manager.get(trans, folder_id, True)
         folder = self.folder_manager.delete(trans, folder, undelete)
         folder_dict = self.folder_manager.get_folder_dict(trans, folder)
-        return folder_dict
+        return LibraryFolderDetails.construct(**folder_dict)
 
-    @expose_api
-    def update(self, trans, encoded_folder_id, payload=None, **kwd):
+    def update(
+        self, trans, folder_id: LibraryFolderDatabaseIdField, payload: UpdateLibraryFolderPayload
+    ) -> LibraryFolderDetails:
         """
-        PATCH /api/folders/{encoded_folder_id}
+         Update the folder with id ``folder_id`` with the data in the payload.
 
-        Update the folder defined by an ``encoded_folder_id``
-        with the data in the payload.
+        .. note:: Currently, only administrators can update library folders. Also the folder must not be `deleted`.
 
-       .. note:: Currently, only admin users can update library folders. Also the folder must not be `deleted`.
+         :param  folder_id:      the encoded id of the folder
 
-        :param  id:      the encoded id of the folder
-        :type   id:      an encoded id string
+         :param  payload: (required) dictionary structure containing::
+             'name':         new folder's name, cannot be empty
+             'description':  new folder's description
+         :type   payload: dict
 
-        :param  payload: (required) dictionary structure containing::
-            'name':         new folder's name, cannot be empty
-            'description':  new folder's description
-        :type   payload: dict
+         :returns:   detailed folder information
+         :rtype:     dict
 
-        :returns:   detailed folder information
-        :rtype:     dict
-
-        :raises: RequestParameterMissingException
+         :raises: RequestParameterMissingException
         """
-        decoded_folder_id = self.folder_manager.cut_and_decode(trans, encoded_folder_id)
-        folder = self.folder_manager.get(trans, decoded_folder_id)
-        if payload:
-            kwd.update(payload)
-        name = kwd.get('name', None)
-        if not name:
-            raise exceptions.RequestParameterMissingException("Parameter 'name' of library folder is required. You cannot remove it.")
-        description = kwd.get('description', None)
-        updated_folder = self.folder_manager.update(trans, folder, name, description)
+        folder = self.folder_manager.get(trans, folder_id)
+        updated_folder = self.folder_manager.update(trans, folder, payload.name, payload.description)
         folder_dict = self.folder_manager.get_folder_dict(trans, updated_folder)
-        return folder_dict
-
-    def __decode_id(self, trans, encoded_id, object_name=None):
-        """
-        Try to decode the id.
-
-        :param  object_name:      Name of the object the id belongs to. (optional)
-        :type   object_name:      str
-        """
-        try:
-            return trans.security.decode_id(encoded_id)
-        except TypeError:
-            raise exceptions.MalformedId('Malformed %s id specified, unable to decode.' % object_name if object_name is not None else '')
-        except ValueError:
-            raise exceptions.MalformedId('Wrong %s id specified, unable to decode.' % object_name if object_name is not None else '')
+        return LibraryFolderDetails.construct(**folder_dict)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/forms.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/forms.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,75 +2,83 @@
 API operations on FormDefinition objects.
 """
 import logging
 
 from galaxy import web
 from galaxy.forms.forms import form_factory
 from galaxy.util import XML
-from galaxy.webapps.base.controller import BaseAPIController, url_for
+from galaxy.webapps.base.controller import url_for
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class FormDefinitionAPIController(BaseAPIController):
-
+class FormDefinitionAPIController(BaseGalaxyAPIController):
     @web.legacy_expose_api
     def index(self, trans, **kwd):
         """
         GET /api/forms
         Displays a collection (list) of forms.
         """
         if not trans.user_is_admin:
             trans.response.status = 403
             return "You are not authorized to view the list of forms."
         query = trans.sa_session.query(trans.app.model.FormDefinition)
         rval = []
         for form_definition in query:
-            item = form_definition.to_dict(value_mapper={'id': trans.security.encode_id, 'form_definition_current_id': trans.security.encode_id})
-            item['url'] = url_for('form', id=trans.security.encode_id(form_definition.id))
+            item = form_definition.to_dict(
+                value_mapper={"id": trans.security.encode_id, "form_definition_current_id": trans.security.encode_id}
+            )
+            item["url"] = url_for("form", id=trans.security.encode_id(form_definition.id))
             rval.append(item)
         return rval
 
     @web.legacy_expose_api
     def show(self, trans, id, **kwd):
         """
         GET /api/forms/{encoded_form_id}
         Displays information about a form.
         """
         form_definition_id = id
         try:
             decoded_form_definition_id = trans.security.decode_id(form_definition_id)
         except TypeError:
             trans.response.status = 400
-            return "Malformed form definition id ( %s ) specified, unable to decode." % str(form_definition_id)
+            return f"Malformed form definition id ( {str(form_definition_id)} ) specified, unable to decode."
         try:
             form_definition = trans.sa_session.query(trans.app.model.FormDefinition).get(decoded_form_definition_id)
         except Exception:
             form_definition = None
         if not form_definition or not trans.user_is_admin:
             trans.response.status = 400
-            return "Invalid form definition id ( %s ) specified." % str(form_definition_id)
-        item = form_definition.to_dict(view='element', value_mapper={'id': trans.security.encode_id, 'form_definition_current_id': trans.security.encode_id})
-        item['url'] = url_for('form', id=form_definition_id)
+            return f"Invalid form definition id ( {str(form_definition_id)} ) specified."
+        item = form_definition.to_dict(
+            view="element",
+            value_mapper={"id": trans.security.encode_id, "form_definition_current_id": trans.security.encode_id},
+        )
+        item["url"] = url_for("form", id=form_definition_id)
         return item
 
     @web.legacy_expose_api
     def create(self, trans, payload, **kwd):
         """
         POST /api/forms
         Creates a new form.
         """
         if not trans.user_is_admin:
             trans.response.status = 403
             return "You are not authorized to create a new form."
-        xml_text = payload.get('xml_text', None)
+        xml_text = payload.get("xml_text", None)
         if xml_text is None:
             trans.response.status = 400
             return "Missing required parameter 'xml_text'."
             # enhance to allow creating from more than just xml
         form_definition = form_factory.from_elem(XML(xml_text))
         trans.sa_session.add(form_definition)
         trans.sa_session.flush()
         encoded_id = trans.security.encode_id(form_definition.id)
-        item = form_definition.to_dict(view='element', value_mapper={'id': trans.security.encode_id, 'form_definition_current_id': trans.security.encode_id})
-        item['url'] = url_for('form', id=encoded_id)
+        item = form_definition.to_dict(
+            view="element",
+            value_mapper={"id": trans.security.encode_id, "form_definition_current_id": trans.security.encode_id},
+        )
+        item["url"] = url_for("form", id=encoded_id)
         return [item]
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/histories.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/histories.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,535 +1,651 @@
-"""
-API operations on a history.
-
-.. seealso:: :class:`galaxy.model.History`
-"""
 import glob
 import logging
 import os
+import shutil
+from pathlib import Path
+from tempfile import (
+    NamedTemporaryFile,
+    SpooledTemporaryFile,
+)
+from typing import (
+    List,
+    Optional,
+    Tuple,
+    Union,
+)
 
 from sqlalchemy import (
     false,
-    true
+    true,
 )
 
 from galaxy import (
-    exceptions,
+    exceptions as glx_exceptions,
     model,
-    util
 )
-from galaxy.managers import (
-    citations,
-    histories,
-    users,
-    workflows,
+from galaxy.celery.tasks import (
+    import_model_store,
+    prepare_history_download,
+    write_history_to,
+)
+from galaxy.files.uris import validate_uri_access
+from galaxy.managers.citations import CitationsManager
+from galaxy.managers.context import ProvidesHistoryContext
+from galaxy.managers.histories import (
+    HistoryDeserializer,
+    HistoryExportManager,
+    HistoryFilters,
+    HistoryManager,
+    HistorySerializer,
 )
-from galaxy.util import (
-    restore_text,
-    string_as_bool
+from galaxy.managers.users import UserManager
+from galaxy.model.store import payload_to_source_uri
+from galaxy.schema import (
+    FilterQueryParams,
+    SerializationParams,
 )
-from galaxy.web import (
-    expose_api,
-    expose_api_anonymous,
-    expose_api_anonymous_and_sessionless,
-    expose_api_raw,
-    url_for
+from galaxy.schema.fields import DecodedDatabaseIdField
+from galaxy.schema.schema import (
+    AnyHistoryView,
+    AsyncFile,
+    AsyncTaskResultSummary,
+    CreateHistoryFromStore,
+    CreateHistoryPayload,
+    CustomBuildsMetadataResponse,
+    ExportHistoryArchivePayload,
+    HistoryArchiveExportResult,
+    HistoryImportArchiveSourceType,
+    JobExportHistoryArchiveModel,
+    JobIdResponse,
+    JobImportHistoryResponse,
+    LabelValuePair,
+    StoreExportPayload,
+    WriteStoreToPayload,
 )
-from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    ExportsHistoryMixin,
-    ImportsHistoryMixin,
-    SharableMixin
+from galaxy.schema.tasks import (
+    GenerateHistoryDownload,
+    ImportModelStoreTaskRequest,
+    WriteHistoryTo,
 )
+from galaxy.schema.types import LatestLiteral
+from galaxy.security.idencoding import IdEncodingHelper
+from galaxy.util import restore_text
+from galaxy.web.short_term_storage import ShortTermStorageAllocator
+from galaxy.webapps.galaxy.services.base import (
+    async_task_summary,
+    ConsumesModelStores,
+    model_store_storage_target,
+    ServesExportStores,
+    ServiceBase,
+)
+from galaxy.webapps.galaxy.services.sharable import ShareableService
 
 log = logging.getLogger(__name__)
 
+DEFAULT_ORDER_BY = "create_time-dsc"
 
-class HistoriesController(BaseAPIController, ExportsHistoryMixin, ImportsHistoryMixin, SharableMixin):
-
-    def __init__(self, app):
-        super(HistoriesController, self).__init__(app)
-        self.citations_manager = citations.CitationsManager(app)
-        self.user_manager = users.UserManager(app)
-        self.workflow_manager = workflows.WorkflowsManager(app)
-        self.manager = histories.HistoryManager(app)
-        self.serializer = histories.HistorySerializer(app)
-        self.deserializer = histories.HistoryDeserializer(app)
-        self.filters = histories.HistoryFilters(app)
-
-    @expose_api_anonymous
-    def index(self, trans, deleted='False', **kwd):
-        """
-        index( trans, deleted='False' )
-        * GET /api/histories:
-            return undeleted histories for the current user
-        * GET /api/histories/deleted:
-            return deleted histories for the current user
-        .. note:: Anonymous users are allowed to get their current history
 
-        :type   deleted: boolean
-        :param  deleted: if True, show only deleted histories, if False, non-deleted
+class HistoriesService(ServiceBase, ConsumesModelStores, ServesExportStores):
+    """Common interface/service logic for interactions with histories in the context of the API.
 
-        :rtype:     list
-        :returns:   list of dictionaries containing summary history information
+    Provides the logic of the actions invoked by API controllers and uses type definitions
+    and pydantic models to declare its parameters and return types.
+    """
+
+    def __init__(
+        self,
+        security: IdEncodingHelper,
+        manager: HistoryManager,
+        user_manager: UserManager,
+        serializer: HistorySerializer,
+        deserializer: HistoryDeserializer,
+        citations_manager: CitationsManager,
+        history_export_manager: HistoryExportManager,
+        filters: HistoryFilters,
+        short_term_storage_allocator: ShortTermStorageAllocator,
+    ):
+        super().__init__(security)
+        self.manager = manager
+        self.user_manager = user_manager
+        self.serializer = serializer
+        self.deserializer = deserializer
+        self.citations_manager = citations_manager
+        self.history_export_manager = history_export_manager
+        self.filters = filters
+        self.shareable_service = ShareableService(self.manager, self.serializer)
+        self.short_term_storage_allocator = short_term_storage_allocator
+
+    def index(
+        self,
+        trans: ProvidesHistoryContext,
+        serialization_params: SerializationParams,
+        filter_query_params: FilterQueryParams,
+        deleted_only: Optional[bool] = False,
+        all_histories: Optional[bool] = False,
+    ):
+        """
+        Return a collection of histories for the current user. Additional filters can be applied.
 
-        The following are optional parameters:
-            view:   string, one of ('summary','detailed'), defaults to 'summary'
-                    controls which set of properties to return
-            keys:   comma separated strings, unused by default
-                    keys/names of individual properties to return
-            all:    boolean, defaults to 'false', admin-only
-                    returns all histories, not just current user's
-
-        If neither keys or views are sent, the default view (set of keys) is returned.
-        If both a view and keys are sent, the key list and the view's keys are
-        combined.
-        If keys are send and no view, only those properties in keys are returned.
-
-        For which properties are available see:
-            galaxy/managers/histories/HistorySerializer
-
-        The list returned can be filtered by using two optional parameters:
-            q:      string, generally a property name to filter by followed
-                    by an (often optional) hyphen and operator string.
-            qv:     string, the value to filter by
-
-        ..example:
-            To filter the list to only those created after 2015-01-29,
-            the query string would look like:
-                '?q=create_time-gt&qv=2015-01-29'
-
-            Multiple filters can be sent in using multiple q/qv pairs:
-                '?q=create_time-gt&qv=2015-01-29&q=tag-has&qv=experiment-1'
-
-        The list returned can be paginated using two optional parameters:
-            limit:  integer, defaults to no value and no limit (return all)
-                    how many items to return
-            offset: integer, defaults to 0 and starts at the beginning
-                    skip the first ( offset - 1 ) items and begin returning
-                    at the Nth item
-
-        ..example:
-            limit and offset can be combined. Skip the first two and return five:
-                '?limit=5&offset=3'
-
-        The list returned can be ordered using the optional parameter:
-            order:  string containing one of the valid ordering attributes followed
-                    (optionally) by '-asc' or '-dsc' for ascending and descending
-                    order respectively. Orders can be stacked as a comma-
-                    separated list of values.
-
-        ..example:
-            To sort by name descending then create time descending:
-                '?order=name-dsc,create_time'
-
-        The ordering attributes and their default orders are:
-            create_time defaults to 'create_time-dsc'
-            update_time defaults to 'update_time-dsc'
-            name    defaults to 'name-asc'
-
-        'order' defaults to 'create_time-dsc'
-        """
-        serialization_params = self._parse_serialization_params(kwd, 'summary')
-        limit, offset = self.parse_limit_offset(kwd)
-        filter_params = self.parse_filter_params(kwd)
+        :type   deleted_only: optional boolean
+        :param  deleted_only: if True, show only deleted histories, if False, non-deleted
 
+        .. note:: Anonymous users are allowed to get their current history
+        """
         # bail early with current history if user is anonymous
         current_user = self.user_manager.current_user(trans)
         if self.user_manager.is_anonymous(current_user):
             current_history = self.manager.get_current(trans)
             if not current_history:
                 return []
             # note: ignores filters, limit, offset
-            return [self.serializer.serialize_to_view(current_history,
-                     user=current_user, trans=trans, **serialization_params)]
+            return [self._serialize_history(trans, current_history, serialization_params)]
 
+        filter_params = self.filters.build_filter_params(filter_query_params)
         filters = []
-        # support the old default of not-returning/filtering-out deleted histories
-        filters += self._get_deleted_filter(deleted, filter_params)
-        # get optional parameter 'all'
-        all_histories = util.string_as_bool(kwd.get('all', False))
-        # if parameter 'all' is true, throw exception if not admin
+        # support the old default of not-returning/filtering-out deleted_only histories
+        filters += self._get_deleted_filter(deleted_only, filter_params)
+
+        # if parameter 'all_histories' is true, throw exception if not admin
         # else add current user filter to query (default behaviour)
         if all_histories:
             if not trans.user_is_admin:
                 message = "Only admins can query all histories"
-                raise exceptions.AdminRequiredException(message)
+                raise glx_exceptions.AdminRequiredException(message)
         else:
-            filters += [self.app.model.History.user == current_user]
+            filters += [model.History.user == current_user]
         # and any sent in from the query string
         filters += self.filters.parse_filters(filter_params)
+        order_by = self._build_order_by(filter_query_params.order)
 
-        order_by = self._parse_order_by(manager=self.manager, order_by_string=kwd.get('order', 'create_time-dsc'))
-        histories = self.manager.list(filters=filters, order_by=order_by, limit=limit, offset=offset)
-
-        rval = []
-        for history in histories:
-            history_dict = self.serializer.serialize_to_view(history, user=trans.user, trans=trans, **serialization_params)
-            rval.append(history_dict)
+        histories = self.manager.list(
+            filters=filters, order_by=order_by, limit=filter_query_params.limit, offset=filter_query_params.offset
+        )
+
+        rval = [
+            self._serialize_history(trans, history, serialization_params, default_view="summary")
+            for history in histories
+        ]
         return rval
 
-    def _get_deleted_filter(self, deleted, filter_params):
+    def _get_deleted_filter(self, deleted: Optional[bool], filter_params: List[Tuple[str, str, str]]):
         # TODO: this should all be removed (along with the default) in v2
         # support the old default of not-returning/filtering-out deleted histories
         try:
             # the consumer must explicitly ask for both deleted and non-deleted
             #   but pull it from the parsed params (as the filter system will error on None)
-            deleted_filter_index = filter_params.index(('deleted', 'eq', 'None'))
+            deleted_filter_index = filter_params.index(("deleted", "eq", "None"))
             filter_params.pop(deleted_filter_index)
             return []
         except ValueError:
             pass
 
         # the deleted string bool was also used as an 'include deleted' flag
-        if deleted in ('True', 'true'):
-            return [self.app.model.History.deleted == true()]
+        if deleted is True:
+            return [model.History.deleted == true()]
 
         # the third option not handled here is 'return only deleted'
         #   if this is passed in (in the form below), simply return and let the filter system handle it
-        if ('deleted', 'eq', 'True') in filter_params:
+        if ("deleted", "eq", "True") in filter_params:
             return []
 
         # otherwise, do the default filter of removing the deleted histories
-        return [self.app.model.History.deleted == false()]
-
-    @expose_api_anonymous
-    def show(self, trans, id, deleted='False', **kwd):
-        """
-        show( trans, id, deleted='False' )
-        * GET /api/histories/{id}:
-            return the history with ``id``
-        * GET /api/histories/deleted/{id}:
-            return the deleted history with ``id``
-        * GET /api/histories/most_recently_used:
-            return the most recently used history
-
-        :type   id:      an encoded id string
-        :param  id:      the encoded id of the history to query or the string 'most_recently_used'
-        :type   deleted: boolean
-        :param  deleted: if True, allow information on a deleted history to be shown.
-
-        :param  keys: same as the use of `keys` in the `index` function above
-        :param  view: same as the use of `view` in the `index` function above
-
-        :rtype:     dictionary
-        :returns:   detailed history information
-        """
-        history_id = id
-        deleted = string_as_bool(deleted)
-
-        if history_id == "most_recently_used":
-            history = self.manager.most_recent(trans.user,
-                filters=(self.app.model.History.deleted == false()), current_history=trans.history)
-        else:
-            history = self.manager.get_accessible(self.decode_id(history_id), trans.user, current_history=trans.history)
-
-        return self.serializer.serialize_to_view(history,
-            user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
-
-    @expose_api_anonymous
-    def citations(self, trans, history_id, **kwd):
-        """
-        GET /api/histories/{id}/citations
-        Return all the citations for the tools used to produce the datasets in
-        the history.
-        """
-        history = self.manager.get_accessible(self.decode_id(history_id), trans.user, current_history=trans.history)
-        tool_ids = set()
-        for dataset in history.datasets:
-            job = dataset.creating_job
-            if not job:
-                continue
-            tool_id = job.tool_id
-            if not tool_id:
-                continue
-            tool_ids.add(tool_id)
-        return [citation.to_dict("bibtex") for citation in self.citations_manager.citations_for_tool_ids(tool_ids)]
-
-    @expose_api_anonymous_and_sessionless
-    def published(self, trans, **kwd):
-        """
-        published( self, trans, **kwd ):
-        * GET /api/histories/published:
-            return all histories that are published
-
-        :rtype:     list
-        :returns:   list of dictionaries containing summary history information
-
-        Follows the same filtering logic as the index() method above.
-        """
-        limit, offset = self.parse_limit_offset(kwd)
-        filter_params = self.parse_filter_params(kwd)
-        filters = self.filters.parse_filters(filter_params)
-        order_by = self._parse_order_by(manager=self.manager, order_by_string=kwd.get('order', 'create_time-dsc'))
-        histories = self.manager.list_published(filters=filters, order_by=order_by, limit=limit, offset=offset)
-        rval = []
-        for history in histories:
-            history_dict = self.serializer.serialize_to_view(history, user=trans.user, trans=trans,
-                **self._parse_serialization_params(kwd, 'summary'))
-            rval.append(history_dict)
-        return rval
-
-    @expose_api
-    def shared_with_me(self, trans, **kwd):
-        """
-        shared_with_me( self, trans, **kwd )
-        * GET /api/histories/shared_with_me:
-            return all histories that are shared with the current user
-
-        :rtype:     list
-        :returns:   list of dictionaries containing summary history information
-
-        Follows the same filtering logic as the index() method above.
-        """
-        current_user = trans.user
-        limit, offset = self.parse_limit_offset(kwd)
-        filter_params = self.parse_filter_params(kwd)
-        filters = self.filters.parse_filters(filter_params)
-        order_by = self._parse_order_by(manager=self.manager, order_by_string=kwd.get('order', 'create_time-dsc'))
-        histories = self.manager.list_shared_with(current_user,
-            filters=filters, order_by=order_by, limit=limit, offset=offset)
-        rval = []
-        for history in histories:
-            history_dict = self.serializer.serialize_to_view(history, user=current_user, trans=trans,
-                **self._parse_serialization_params(kwd, 'summary'))
-            rval.append(history_dict)
-        return rval
-
-    @expose_api_anonymous
-    def create(self, trans, payload, **kwd):
-        """
-        create( trans, payload )
-        * POST /api/histories:
-            create a new history
-
-        :type   payload: dict
-        :param  payload: (optional) dictionary structure containing:
-            * name:             the new history's name
-            * history_id:       the id of the history to copy
-            * all_datasets:     copy deleted hdas/hdcas? 'True' or 'False', defaults to True
-            * archive_source:   the url that will generate the archive to import
-            * archive_type:     'url' (default)
+        return [model.History.deleted == false()]
 
-        :param  keys: same as the use of `keys` in the `index` function above
-        :param  view: same as the use of `view` in the `index` function above
-
-        :rtype:     dict
-        :returns:   element view of new history
-        """
+    def create(
+        self,
+        trans: ProvidesHistoryContext,
+        payload: CreateHistoryPayload,
+        serialization_params: SerializationParams,
+    ):
+        """Create a new history from scratch, by copying an existing one or by importing
+        from URL or File depending on the provided parameters in the payload.
+        """
+        copy_this_history_id = payload.history_id
+        if trans.anonymous and not copy_this_history_id:  # Copying/Importing histories is allowed for anonymous users
+            raise glx_exceptions.AuthenticationRequired("You need to be logged in to create histories.")
+        if trans.user and trans.user.bootstrap_admin_user:
+            raise glx_exceptions.RealUserRequiredException("Only real users can create histories.")
         hist_name = None
-        if payload.get('name', None):
-            hist_name = restore_text(payload['name'])
-        copy_this_history_id = payload.get('history_id', None)
-
-        all_datasets = util.string_as_bool(payload.get('all_datasets', True))
-
-        if "archive_source" in payload:
-            archive_source = payload["archive_source"]
-            archive_file = payload.get("archive_file")
+        if payload.name is not None:
+            hist_name = restore_text(payload.name)
+
+        if payload.archive_source is not None or hasattr(payload.archive_file, "file"):
+            archive_source = payload.archive_source
+            archive_file = payload.archive_file
             if archive_source:
-                archive_type = payload.get("archive_type", "url")
-            elif hasattr(archive_file, "file"):
-                archive_source = payload["archive_file"].file.name
-                archive_type = "file"
+                archive_type = payload.archive_type
+            elif archive_file is not None and hasattr(archive_file, "file"):
+                archive_source = archive_file.file.name
+                archive_type = HistoryImportArchiveSourceType.file
+                if isinstance(archive_file.file, SpooledTemporaryFile):
+                    archive_source = self._save_upload_file_tmp(archive_file)
             else:
-                raise exceptions.MessageException("Please provide a url or file.")
-            self.queue_history_import(trans, archive_type=archive_type, archive_source=archive_source)
-            return {"message": "Importing history from source '%s'. This history will be visible when the import is complete." % archive_source}
+                raise glx_exceptions.MessageException("Please provide a url or file.")
+            if archive_type == HistoryImportArchiveSourceType.url:
+                assert archive_source
+                validate_uri_access(archive_source, trans.user_is_admin, trans.app.config.fetch_url_allowlist_ips)
+            job = self.manager.queue_history_import(trans, archive_type=archive_type, archive_source=archive_source)
+            job_dict = job.to_dict()
+            job_dict[
+                "message"
+            ] = f"Importing history from source '{archive_source}'. This history will be visible when the import is complete."
+            job_dict = trans.security.encode_all_ids(job_dict)
+            return JobImportHistoryResponse.construct(**job_dict)
 
         new_history = None
         # if a history id was passed, copy that history
         if copy_this_history_id:
-            decoded_id = self.decode_id(copy_this_history_id)
-            original_history = self.manager.get_accessible(decoded_id, trans.user, current_history=trans.history)
-            hist_name = hist_name or ("Copy of '%s'" % original_history.name)
-            new_history = original_history.copy(name=hist_name, target_user=trans.user, all_datasets=all_datasets)
+            original_history = self.manager.get_accessible(
+                copy_this_history_id, trans.user, current_history=trans.history
+            )
+            hist_name = hist_name or (f"Copy of '{original_history.name}'")
+            new_history = original_history.copy(
+                name=hist_name, target_user=trans.user, all_datasets=payload.all_datasets
+            )
 
         # otherwise, create a new empty history
         else:
             new_history = self.manager.create(user=trans.user, name=hist_name)
 
+        trans.app.security_agent.history_set_default_permissions(new_history)
         trans.sa_session.add(new_history)
         trans.sa_session.flush()
 
         # an anonymous user can only have one history
         if self.user_manager.is_anonymous(trans.user):
             self.manager.set_current(trans, new_history)
 
-        return self.serializer.serialize_to_view(new_history,
-            user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
+        return self._serialize_history(trans, new_history, serialization_params)
+
+    def create_from_store(
+        self,
+        trans,
+        payload: CreateHistoryFromStore,
+        serialization_params: SerializationParams,
+    ) -> AnyHistoryView:
+        self._ensure_can_create_history(trans)
+        object_tracker = self.create_objects_from_store(
+            trans,
+            payload,
+        )
+        return self._serialize_history(trans, object_tracker.new_history, serialization_params)
+
+    def create_from_store_async(
+        self,
+        trans,
+        payload: CreateHistoryFromStore,
+    ) -> AsyncTaskResultSummary:
+        self._ensure_can_create_history(trans)
+        source_uri = payload_to_source_uri(payload)
+        request = ImportModelStoreTaskRequest(
+            user=trans.async_request_user,
+            source_uri=source_uri,
+            for_library=False,
+            model_store_format=payload.model_store_format,
+        )
+        result = import_model_store.delay(request=request)
+        return async_task_summary(result)
+
+    def _ensure_can_create_history(self, trans):
+        if trans.anonymous:
+            raise glx_exceptions.AuthenticationRequired("You need to be logged in to create histories.")
+        if trans.user and trans.user.bootstrap_admin_user:
+            raise glx_exceptions.RealUserRequiredException("Only real users can create histories.")
 
-    @expose_api
-    def delete(self, trans, id, **kwd):
+    def _save_upload_file_tmp(self, upload_file) -> str:
+        try:
+            suffix = Path(upload_file.filename).suffix
+            with NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
+                shutil.copyfileobj(upload_file.file, tmp)
+                tmp_path = Path(tmp.name)
+        finally:
+            upload_file.file.close()
+        return str(tmp_path)
+
+    def show(
+        self,
+        trans: ProvidesHistoryContext,
+        serialization_params: SerializationParams,
+        history_id: Optional[DecodedDatabaseIdField] = None,
+    ):
         """
-        delete( self, trans, id, **kwd )
-        * DELETE /api/histories/{id}
-            delete the history with the given ``id``
-        .. note:: Stops all active jobs in the history if purge is set.
+        Returns detailed information about the history with the given encoded `id`. If no `id` is
+        provided, then the most recently used history will be returned.
 
-        :type   id:     str
-        :param  id:     the encoded id of the history to delete
-        :type   kwd:    dict
-        :param  kwd:    (optional) dictionary structure containing extra parameters
+        :param  history_id:      the encoded id of the history to query or None to use the most recently used
 
-        You can purge a history, removing all it's datasets from disk (if unshared),
-        by passing in ``purge=True`` in the url.
+        :param  serialization_params:   contains the optional `view`, `keys` and `default_view` for serialization
+
+        :returns:   detailed history information
+        """
+        if history_id is None:  # By default display the most recent history
+            history = self.manager.most_recent(
+                trans.user, filters=(model.History.deleted == false()), current_history=trans.history
+            )
+        else:
+            history = self.manager.get_accessible(history_id, trans.user, current_history=trans.history)
+        return self._serialize_history(trans, history, serialization_params)
+
+    def prepare_download(
+        self, trans: ProvidesHistoryContext, history_id: DecodedDatabaseIdField, payload: StoreExportPayload
+    ) -> AsyncFile:
+        history = self.manager.get_accessible(history_id, trans.user, current_history=trans.history)
+        short_term_storage_target = model_store_storage_target(
+            self.short_term_storage_allocator,
+            history.name,
+            payload.model_store_format,
+        )
+        export_association = self.history_export_manager.create_export_association(history.id)
+        request = GenerateHistoryDownload(
+            history_id=history.id,
+            short_term_storage_request_id=short_term_storage_target.request_id,
+            duration=short_term_storage_target.duration,
+            user=trans.async_request_user,
+            export_association_id=export_association.id,
+            **payload.dict(),
+        )
+        result = prepare_history_download.delay(request=request)
+        task_summary = async_task_summary(result)
+        export_association.task_uuid = task_summary.id
+        trans.sa_session.flush()
+        return AsyncFile(storage_request_id=short_term_storage_target.request_id, task=task_summary)
+
+    def write_store(
+        self, trans: ProvidesHistoryContext, history_id: DecodedDatabaseIdField, payload: WriteStoreToPayload
+    ) -> AsyncTaskResultSummary:
+        history = self.manager.get_accessible(history_id, trans.user, current_history=trans.history)
+        export_association = self.history_export_manager.create_export_association(history.id)
+        request = WriteHistoryTo(
+            user=trans.async_request_user,
+            history_id=history.id,
+            export_association_id=export_association.id,
+            **payload.dict(),
+        )
+        result = write_history_to.delay(request=request)
+        task_summary = async_task_summary(result)
+        export_association.task_uuid = task_summary.id
+        trans.sa_session.flush()
+        return task_summary
+
+    def update(
+        self,
+        trans: ProvidesHistoryContext,
+        history_id: DecodedDatabaseIdField,
+        payload,
+        serialization_params: SerializationParams,
+    ):
+        """Updates the values for the history with the given ``id``
 
-        :param  keys: same as the use of `keys` in the `index` function above
-        :param  view: same as the use of `view` in the `index` function above
+        :param  history_id:      the encoded id of the history to update
+        :param  payload: a dictionary containing any or all the
+            fields in :func:`galaxy.model.History.to_dict` and/or the following:
 
-        :rtype:     dict
-        :returns:   the deleted or purged history
+            * annotation: an annotation for the history
+
+        :param  serialization_params:   contains the optional `view`, `keys` and `default_view` for serialization
+
+        :returns:   an error object if an error occurred or a dictionary containing
+            any values that were different from the original and, therefore, updated
         """
-        history_id = id
-        # a request body is optional here
-        purge = string_as_bool(kwd.get('purge', False))
-        # for backwards compat, keep the payload sub-dictionary
-        if kwd.get('payload', None):
-            purge = string_as_bool(kwd['payload'].get('purge', purge))
+        # TODO: PUT /api/histories/{encoded_history_id} payload = { rating: rating } (w/ no security checks)
+        history = self.manager.get_owned(history_id, trans.user, current_history=trans.history)
+        self.deserializer.deserialize(history, payload, user=trans.user, trans=trans)
+        return self._serialize_history(trans, history, serialization_params)
 
-        history = self.manager.get_owned(self.decode_id(history_id), trans.user, current_history=trans.history)
+    def delete(
+        self,
+        trans: ProvidesHistoryContext,
+        history_id: DecodedDatabaseIdField,
+        serialization_params: SerializationParams,
+        purge: bool = False,
+    ):
+        """Delete the history with the given ``id``
+
+        .. note:: Stops all active jobs in the history if purge is set.
+
+        You can purge a history, removing all it's datasets from disk (if unshared),
+        by passing in ``purge=True`` in the url.
+        """
+        history = self.manager.get_owned(history_id, trans.user, current_history=trans.history)
         if purge:
             self.manager.purge(history)
         else:
             self.manager.delete(history)
+        return self._serialize_history(trans, history, serialization_params)
 
-        return self.serializer.serialize_to_view(history,
-            user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
-
-    @expose_api
-    def undelete(self, trans, id, **kwd):
-        """
-        undelete( self, trans, id, **kwd )
-        * POST /api/histories/deleted/{id}/undelete:
-            undelete history (that hasn't been purged) with the given ``id``
+    def undelete(
+        self,
+        trans: ProvidesHistoryContext,
+        history_id: DecodedDatabaseIdField,
+        serialization_params: SerializationParams,
+    ):
+        """Undelete history (that hasn't been purged) with the given ``id``
 
-        :type   id:     str
-        :param  id:     the encoded id of the history to undelete
+        :param  history_id:     the encoded id of the history to undelete
 
-        :param  keys: same as the use of `keys` in the `index` function above
-        :param  view: same as the use of `view` in the `index` function above
+        :param  serialization_params:   contains the optional `view`, `keys` and `default_view` for serialization
 
-        :rtype:     str
-        :returns:   'OK' if the history was undeleted
+        :returns:   the undeleted history
         """
-        # TODO: remove at v2
-        history_id = id
-        history = self.manager.get_owned(self.decode_id(history_id), trans.user, current_history=trans.history)
+        history = self.manager.get_owned(history_id, trans.user, current_history=trans.history)
         self.manager.undelete(history)
+        return self._serialize_history(trans, history, serialization_params)
 
-        return self.serializer.serialize_to_view(history,
-            user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
-
-    @expose_api
-    def update(self, trans, id, payload, **kwd):
+    def shared_with_me(
+        self,
+        trans: ProvidesHistoryContext,
+        serialization_params: SerializationParams,
+        filter_query_params: FilterQueryParams,
+    ):
         """
-        update( self, trans, id, payload, **kwd )
-        * PUT /api/histories/{id}
-            updates the values for the history with the given ``id``
-
-        :type   id:      str
-        :param  id:      the encoded id of the history to update
-        :type   payload: dict
-        :param  payload: a dictionary containing any or all the
-            fields in :func:`galaxy.model.History.to_dict` and/or the following:
-
-            * annotation: an annotation for the history
+        Return all histories that are shared with the current user. The results can be filtered.
+        """
+        current_user = trans.user
+        filters = self.filters.parse_query_filters(filter_query_params)
+        order_by = self._build_order_by(filter_query_params.order)
+        histories = self.manager.list_shared_with(
+            current_user,
+            filters=filters,
+            order_by=order_by,
+            limit=filter_query_params.limit,
+            offset=filter_query_params.offset,
+        )
+        rval = [
+            self._serialize_history(trans, history, serialization_params, default_view="summary")
+            for history in histories
+        ]
+        return rval
 
-        :param  keys: same as the use of `keys` in the `index` function above
-        :param  view: same as the use of `view` in the `index` function above
+    def published(
+        self,
+        trans: ProvidesHistoryContext,
+        serialization_params: SerializationParams,
+        filter_query_params: FilterQueryParams,
+    ):
+        """
+        Return all histories that are published. The results can be filtered.
+        """
+        filters = self.filters.parse_query_filters(filter_query_params)
+        order_by = self._build_order_by(filter_query_params.order)
+        histories = self.manager.list_published(
+            filters=filters,
+            order_by=order_by,
+            limit=filter_query_params.limit,
+            offset=filter_query_params.offset,
+        )
+        rval = [
+            self._serialize_history(trans, history, serialization_params, default_view="summary")
+            for history in histories
+        ]
+        return rval
 
-        :rtype:     dict
-        :returns:   an error object if an error occurred or a dictionary containing
-            any values that were different from the original and, therefore, updated
+    def citations(self, trans: ProvidesHistoryContext, history_id: DecodedDatabaseIdField):
         """
-        # TODO: PUT /api/histories/{encoded_history_id} payload = { rating: rating } (w/ no security checks)
-        history = self.manager.get_owned(self.decode_id(id), trans.user, current_history=trans.history)
-
-        self.deserializer.deserialize(history, payload, user=trans.user, trans=trans)
-        return self.serializer.serialize_to_view(history,
-            user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
+        Return all the citations for the tools used to produce the datasets in
+        the history.
+        """
+        history = self.manager.get_accessible(history_id, trans.user, current_history=trans.history)
+        tool_ids = set()
+        for dataset in history.datasets:
+            job = dataset.creating_job
+            if not job:
+                continue
+            tool_id = job.tool_id
+            if not tool_id:
+                continue
+            tool_ids.add(tool_id)
+        return [citation.to_dict("bibtex") for citation in self.citations_manager.citations_for_tool_ids(tool_ids)]
 
-    @expose_api
-    def archive_export(self, trans, id, **kwds):
+    def index_exports(
+        self,
+        trans: ProvidesHistoryContext,
+        history_id: DecodedDatabaseIdField,
+        use_tasks: bool = False,
+        limit: Optional[int] = None,
+        offset: Optional[int] = None,
+    ):
+        if use_tasks:
+            return self.history_export_manager.get_task_exports(trans, history_id, limit, offset)
+        return self.history_export_manager.get_exports(trans, history_id)
+
+    def archive_export(
+        self,
+        trans,
+        history_id: DecodedDatabaseIdField,
+        payload: Optional[ExportHistoryArchivePayload] = None,
+    ) -> Tuple[HistoryArchiveExportResult, bool]:
         """
-        export_archive(self, trans, id, payload)
-        * PUT /api/histories/{id}/exports:
-            start job (if needed) to create history export for corresponding
-            history.
+        start job (if needed) to create history export for corresponding
+        history.
 
-        :type   id:     str
-        :param  id:     the encoded id of the history to export
+        :param  history_id:     the encoded id of the history to export
 
-        :rtype:     dict
         :returns:   object containing url to fetch export from.
         """
-        # PUT instead of POST because multiple requests should just result
-        # in one object being created.
-        history = self.manager.get_accessible(self.decode_id(id), trans.user, current_history=trans.history)
+        if payload is None:
+            payload = ExportHistoryArchivePayload()
+        history = self.manager.get_accessible(history_id, trans.user, current_history=trans.history)
         jeha = history.latest_export
-        up_to_date = jeha and jeha.up_to_date
-        if 'force' in kwds:
-            up_to_date = False  # Temp hack to force rebuild everytime during dev
+        exporting_to_uri = payload.directory_uri
+        # always just issue a new export when exporting to a URI.
+        up_to_date = not payload.force and not exporting_to_uri and (jeha and jeha.up_to_date)
+        job = None
         if not up_to_date:
             # Need to create new JEHA + job.
-            gzip = kwds.get("gzip", True)
-            include_hidden = kwds.get("include_hidden", False)
-            include_deleted = kwds.get("include_deleted", False)
-            self.queue_history_export(trans, history, gzip=gzip, include_hidden=include_hidden, include_deleted=include_deleted)
+            job = self.manager.queue_history_export(
+                trans,
+                history,
+                gzip=payload.gzip,
+                include_hidden=payload.include_hidden,
+                include_deleted=payload.include_deleted,
+                directory_uri=payload.directory_uri,
+                file_name=payload.file_name,
+            )
+        else:
+            job = jeha.job
+
+        ready = bool((up_to_date and jeha.ready) or exporting_to_uri)
+
+        if exporting_to_uri:
+            # we don't have a jeha, there will never be a download_url. Just let
+            # the client poll on the created job_id to determine when the file has been
+            # written.
+            return (JobIdResponse.construct(job_id=job.id), ready)
 
         if up_to_date and jeha.ready:
-            jeha_id = trans.security.encode_id(jeha.id)
-            return dict(download_url=url_for("history_archive_download", id=id, jeha_id=jeha_id))
+            serialized_jeha = self.history_export_manager.serialize(trans, history_id, jeha)
+            return (JobExportHistoryArchiveModel.construct(**serialized_jeha), ready)
         else:
             # Valid request, just resource is not ready yet.
-            trans.response.status = "202 Accepted"
-            return ''
+            if jeha:
+                serialized_jeha = self.history_export_manager.serialize(trans, history_id, jeha)
+                return (JobExportHistoryArchiveModel.construct(**serialized_jeha), ready)
+            else:
+                assert job is not None, "logic error, don't have a jeha or a job"
+                return (JobIdResponse.construct(job_id=job.id), ready)
 
-    @expose_api_raw
-    def archive_download(self, trans, id, jeha_id, **kwds):
+    def get_ready_history_export(
+        self,
+        trans: ProvidesHistoryContext,
+        history_id: DecodedDatabaseIdField,
+        jeha_id: Union[DecodedDatabaseIdField, LatestLiteral],
+    ) -> model.JobExportHistoryArchive:
+        """Returns the exported history archive information if it's ready
+        or raises an exception if not."""
+        return self.history_export_manager.get_ready_jeha(trans, history_id, jeha_id)
+
+    def get_archive_download_path(
+        self,
+        trans: ProvidesHistoryContext,
+        jeha: model.JobExportHistoryArchive,
+    ) -> str:
+        """
+        If ready and available, return raw contents of exported history
+        using a generator function.
+        """
+        return self.manager.get_ready_history_export_file_path(trans, jeha)
+
+    def get_archive_media_type(self, jeha: model.JobExportHistoryArchive):
+        media_type = "application/x-tar"
+        if jeha.compressed:
+            media_type = "application/x-gzip"
+        return media_type
+
+    # TODO: remove this function and HistoryManager.legacy_serve_ready_history_export when
+    # removing the legacy HistoriesController
+    def legacy_archive_download(
+        self,
+        trans: ProvidesHistoryContext,
+        history_id: DecodedDatabaseIdField,
+        jeha_id: DecodedDatabaseIdField,
+    ):
+        """
+        If ready and available, return raw contents of exported history.
+        """
+        jeha = self.history_export_manager.get_ready_jeha(trans, history_id, jeha_id)
+        return self.manager.legacy_serve_ready_history_export(trans, jeha)
+
+    def get_custom_builds_metadata(
+        self,
+        trans: ProvidesHistoryContext,
+        history_id: DecodedDatabaseIdField,
+    ) -> CustomBuildsMetadataResponse:
         """
-        export_download( self, trans, id, jeha_id )
-        * GET /api/histories/{id}/exports/{jeha_id}:
-            If ready and available, return raw contents of exported history.
-            Use/poll "PUT /api/histories/{id}/exports" to initiate the creation
-            of such an export - when ready that route will return 200 status
-            code (instead of 202) with a JSON dictionary containing a
-            `download_url`.
-        """
-        # Seems silly to put jeha_id in here, but want GET to be immuatable?
-        # and this is being accomplished this way.
-        history = self.manager.get_accessible(self.decode_id(id), trans.user, current_history=trans.history)
-        matching_exports = [e for e in history.exports if trans.security.encode_id(e.id) == jeha_id]
-        if not matching_exports:
-            raise exceptions.ObjectNotFound()
-
-        jeha = matching_exports[0]
-        if not jeha.ready:
-            # User should not have been given this URL, PUT export should have
-            # return a 202.
-            raise exceptions.MessageException("Export not available or not yet ready.")
-
-        return self.serve_ready_history_export(trans, jeha)
-
-    @expose_api
-    def get_custom_builds_metadata(self, trans, id, payload=None, **kwd):
+        Returns metadata for custom builds.
         """
-        GET /api/histories/{id}/custom_builds_metadata
-        Returns meta data for custom builds.
-
-        :param id: the encoded history id
-        :type  id: str
-        """
-        if payload is None:
-            payload = {}
-        history = self.manager.get_accessible(self.decode_id(id), trans.user, current_history=trans.history)
+        history = self.manager.get_accessible(history_id, trans.user, current_history=trans.history)
         installed_builds = []
         for build in glob.glob(os.path.join(trans.app.config.len_file_path, "*.len")):
             installed_builds.append(os.path.basename(build).split(".len")[0])
-        fasta_hdas = trans.sa_session.query(model.HistoryDatasetAssociation) \
-            .filter_by(history=history, extension="fasta", deleted=False) \
+        fasta_hdas = (
+            trans.sa_session.query(model.HistoryDatasetAssociation)
+            .filter_by(history=history, extension="fasta", deleted=False)
             .order_by(model.HistoryDatasetAssociation.hid.desc())
-        return {
-            'installed_builds'  : [{'label' : ins, 'value' : ins} for ins in installed_builds],
-            'fasta_hdas'        : [{'label' : '%s: %s' % (hda.hid, hda.name), 'value' : trans.security.encode_id(hda.id)} for hda in fasta_hdas],
-        }
+        )
+        return CustomBuildsMetadataResponse(
+            installed_builds=[LabelValuePair(label=ins, value=ins) for ins in installed_builds],
+            fasta_hdas=[
+                LabelValuePair(label=f"{hda.hid}: {hda.name}", value=trans.security.encode_id(hda.id))
+                for hda in fasta_hdas
+            ],
+        )
+
+    def _serialize_history(
+        self,
+        trans: ProvidesHistoryContext,
+        history: model.History,
+        serialization_params: SerializationParams,
+        default_view: str = "detailed",
+    ) -> AnyHistoryView:
+        """
+        Returns a dictionary with the corresponding values depending on the
+        serialization parameters provided.
+        """
+        serialization_params.default_view = default_view
+        serialized_history = self.serializer.serialize_to_view(
+            history, user=trans.user, trans=trans, **serialization_params.dict()
+        )
+        return serialized_history
+
+    def _build_order_by(self, order: Optional[str]):
+        return self.build_order_by(self.manager, order or DEFAULT_ORDER_BY)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/history_contents.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/dataset.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,1049 +1,1175 @@
-"""
-API operations on the contents of a history.
-"""
 import logging
 import os
-import re
+from urllib.parse import (
+    quote_plus,
+    unquote_plus,
+)
+
+import paste.httpexceptions
+from markupsafe import escape
 
 from galaxy import (
-    exceptions,
-    util
+    datatypes,
+    util,
+    web,
+)
+from galaxy.datatypes.data import DatatypeConverterNotFoundException
+from galaxy.datatypes.display_applications.util import (
+    decode_dataset_user,
+    encode_dataset_user,
 )
-from galaxy.managers import (
-    folders,
-    hdas,
-    hdcas,
-    histories,
-    history_contents
+from galaxy.datatypes.sniff import guess_ext
+from galaxy.exceptions import RequestParameterInvalidException
+from galaxy.managers.hdas import (
+    HDADeserializer,
+    HDAManager,
 )
-from galaxy.managers.collections_util import (
-    api_payload_to_create_params,
-    dictify_dataset_collection_instance,
-    get_hda_and_element_identifiers
+from galaxy.managers.histories import HistoryManager
+from galaxy.model import Dataset
+from galaxy.model.item_attrs import (
+    UsesAnnotations,
+    UsesItemRatings,
 )
-from galaxy.managers.jobs import fetch_job_states, summarize_jobs_to_dict
-from galaxy.util.json import safe_dumps
-from galaxy.util.streamball import StreamBall
-from galaxy.web import (
-    expose_api,
-    expose_api_anonymous,
-    expose_api_raw,
-    expose_api_raw_anonymous
+from galaxy.structured_app import StructuredApp
+from galaxy.util import (
+    inflector,
+    sanitize_text,
+    smart_str,
 )
+from galaxy.util.sanitize_html import sanitize_html
+from galaxy.util.zipstream import ZipstreamWrapper
+from galaxy.web import form_builder
+from galaxy.web.framework.helpers import iff
 from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    UsesLibraryMixin,
-    UsesLibraryMixinItems,
-    UsesTagsMixin
+    BaseUIController,
+    ERROR,
+    SUCCESS,
+    url_for,
+    UsesExtendedMetadataMixin,
 )
+from galaxy.webapps.galaxy.services.datasets import DatasetsService
+from ..api import depends
 
 log = logging.getLogger(__name__)
 
+comptypes = []
 
-class HistoryContentsController(BaseAPIController, UsesLibraryMixin, UsesLibraryMixinItems, UsesTagsMixin):
+try:
+    import zlib  # noqa: F401
 
-    def __init__(self, app):
-        super(HistoryContentsController, self).__init__(app)
-        self.hda_manager = hdas.HDAManager(app)
-        self.history_manager = histories.HistoryManager(app)
-        self.history_contents_manager = history_contents.HistoryContentsManager(app)
-        self.folder_manager = folders.FolderManager()
-        self.hda_serializer = hdas.HDASerializer(app)
-        self.hda_deserializer = hdas.HDADeserializer(app)
-        self.hdca_serializer = hdcas.HDCASerializer(app)
-        self.history_contents_filters = history_contents.HistoryContentsFilters(app)
+    comptypes.append("zip")
+except ImportError:
+    pass
 
-    @expose_api_anonymous
-    def index(self, trans, history_id, ids=None, v=None, **kwd):
-        """
-        index( self, trans, history_id, ids=None, **kwd )
-        * GET /api/histories/{history_id}/contents
-            return a list of HDA data for the history with the given ``id``
-        .. note:: Anonymous users are allowed to get their current history contents
-
-        If Ids is not given, index returns a list of *summary* objects for
-        every HDA associated with the given `history_id`.
-
-        If ids is given, index returns a *more complete* json object for each
-        HDA in the ids list.
-
-        :type   history_id: str
-        :param  history_id: encoded id string of the HDA's History
-        :type   ids:        str
-        :param  ids:        (optional) a comma separated list of encoded `HDA` ids
-        :param  types:      (optional) kinds of contents to index (currently just
-                            dataset, but dataset_collection will be added shortly).
-        :type   types:      str
 
-        :rtype:     list
-        :returns:   dictionaries containing summary or detailed HDA information
-        """
-        if v == 'dev':
-            return self.__index_v2(trans, history_id, **kwd)
-
-        rval = []
-
-        history = self.history_manager.get_accessible(self.decode_id(history_id), trans.user, current_history=trans.history)
-
-        # Allow passing in type or types - for continuity rest of methods
-        # take in type - but this one can be passed multiple types and
-        # type=dataset,dataset_collection is a bit silly.
-        types = kwd.get('type', kwd.get('types', None)) or []
-        if types:
-            types = util.listify(types)
-        else:
-            types = ['dataset', "dataset_collection"]
-
-        contents_kwds = {'types': types}
-        if ids:
-            ids = [self.decode_id(id) for id in ids.split(',')]
-            contents_kwds['ids'] = ids
-            # If explicit ids given, always used detailed result.
-            details = 'all'
-        else:
-            contents_kwds['deleted'] = kwd.get('deleted', None)
-            contents_kwds['visible'] = kwd.get('visible', None)
-            # details param allows a mixed set of summary and detailed hdas
-            # Ever more convoluted due to backwards compat..., details
-            # should be considered deprecated in favor of more specific
-            # dataset_details (and to be implemented dataset_collection_details).
-            details = kwd.get('details', None) or kwd.get('dataset_details', None) or []
-            if details and details != 'all':
-                details = util.listify(details)
-
-        for content in history.contents_iter(**contents_kwds):
-            encoded_content_id = trans.security.encode_id(content.id)
-            detailed = details == 'all' or (encoded_content_id in details)
-
-            if isinstance(content, trans.app.model.HistoryDatasetAssociation):
-                view = 'detailed' if detailed else 'summary'
-                hda_dict = self.hda_serializer.serialize_to_view(content, view=view, user=trans.user, trans=trans)
-                rval.append(hda_dict)
-
-            elif isinstance(content, trans.app.model.HistoryDatasetCollectionAssociation):
-                view = 'element' if detailed else 'collection'
-                collection_dict = self.__collection_dict(trans, content, view=view)
-                rval.append(collection_dict)
-
-        return rval
-
-    def __collection_dict(self, trans, dataset_collection_instance, **kwds):
-        return dictify_dataset_collection_instance(dataset_collection_instance,
-            security=trans.security, parent=dataset_collection_instance.history, **kwds)
-
-    @expose_api_anonymous
-    def show(self, trans, id, history_id, **kwd):
-        """
-        * GET /api/histories/{history_id}/contents/{id}
-        * GET /api/histories/{history_id}/contents/{type}/{id}
-            return detailed information about an HDA or HDCA within a history
-        .. note:: Anonymous users are allowed to get their current history contents
-
-        :type   id:         str
-        :param  id:         the encoded id of the HDA or HDCA to return
-        :type   type:       str
-        :param  id:         'dataset' or 'dataset_collection'
-        :type   history_id: str
-        :param  history_id: encoded id string of the HDA's or HDCA's History
-        :type   view:       str
-        :param  view:       if fetching a dataset collection - the view style of
-                            the dataset collection to produce.
-                            'collection' returns no element information, 'element'
-                            returns detailed element information for all datasets,
-                            'element-reference' returns a minimal set of information
-                            about datasets (for instance id, type, and state but not
-                            metadata, peek, info, or name). The default is 'element'.
-        :type  fuzzy_count: int
-        :param fuzzy_count: this value can be used to broadly restrict the magnitude
-                            of the number of elements returned via the API for large
-                            collections. The number of actual elements returned may
-                            be "a bit" more than this number or "a lot" less - varying
-                            on the depth of nesting, balance of nesting at each level,
-                            and size of target collection. The consumer of this API should
-                            not expect a stable number or pre-calculable number of
-                            elements to be produced given this parameter - the only
-                            promise is that this API will not respond with an order
-                            of magnitude more elements estimated with this value.
-                            The UI uses this parameter to fetch a "balanced" concept of
-                            the "start" of large collections at every depth of the
-                            collection.
-
-        :rtype:     dict
-        :returns:   dictionary containing detailed HDA or HDCA information
-        """
-        contents_type = self.__get_contents_type(trans, kwd)
-        if contents_type == 'dataset':
-            return self.__show_dataset(trans, id, **kwd)
-        elif contents_type == 'dataset_collection':
-            return self.__show_dataset_collection(trans, id, history_id, **kwd)
-
-    @expose_api_anonymous
-    def index_jobs_summary(self, trans, history_id, **kwd):
-        """
-        * GET /api/histories/{history_id}/jobs_summary
-            return job state summary info for jobs, implicit groups jobs for collections or workflow invocations
+class DatasetInterface(BaseUIController, UsesAnnotations, UsesItemRatings, UsesExtendedMetadataMixin):
+    history_manager: HistoryManager = depends(HistoryManager)
+    hda_manager: HDAManager = depends(HDAManager)
+    hda_deserializer: HDADeserializer = depends(HDADeserializer)
+    service: DatasetsService = depends(DatasetsService)
 
-        Warning: We allow anyone to fetch job state information about any object they
-        can guess an encoded ID for - it isn't considered protected data. This keeps
-        polling IDs as part of state calculation for large histories and collections as
-        efficient as possible.
-
-        :type   history_id: str
-        :param  history_id: encoded id string of the target history
-        :type   ids:        str[]
-        :param  ids:        the encoded ids of job summary objects to return - if ids
-                            is specified types must also be specified and have same length.
-        :type   types:      str[]
-        :param  types:      type of object represented by elements in the ids array - any of
-                            Job, ImplicitCollectionJob, or WorkflowInvocation.
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
 
-        :rtype:     dict[]
-        :returns:   an array of job summary object dictionaries.
+    def _get_job_for_dataset(self, trans, dataset_id):
         """
-        ids = kwd.get("ids", None)
-        types = kwd.get("types", None)
-        if ids is None:
-            assert types is None
-            # TODO: ...
-            pass
-        else:
-            ids = [self.app.security.decode_id(i) for i in util.listify(ids)]
-            types = util.listify(types)
-        return [self.encode_all_ids(trans, s) for s in fetch_job_states(trans.sa_session, ids, types)]
-
-    @expose_api_anonymous
-    def show_jobs_summary(self, trans, id, history_id, **kwd):
+        Return the job for the given dataset. This will throw an error if the
+        dataset is either nonexistent or inaccessible to the user.
         """
-        * GET /api/histories/{history_id}/contents/{type}/{id}/jobs_summary
-            return detailed information about an HDA or HDCAs jobs
-
-        Warning: We allow anyone to fetch job state information about any object they
-        can guess an encoded ID for - it isn't considered protected data. This keeps
-        polling IDs as part of state calculation for large histories and collections as
-        efficient as possible.
-
-        :type   id:         str
-        :param  id:         the encoded id of the HDA to return
-        :type   history_id: str
-        :param  history_id: encoded id string of the HDA's or the HDCA's History
+        hda = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(self.decode_id(dataset_id))
+        assert hda and self._can_access_dataset(trans, hda)
+        return hda.creating_job
 
-        :rtype:     dict
-        :returns:   dictionary containing jobs summary object
-        """
-        contents_type = self.__get_contents_type(trans, kwd)
-        # At most one of job or implicit_collection_jobs should be found.
-        job = None
-        implicit_collection_jobs = None
-        if contents_type == 'dataset':
-            hda = self.hda_manager.get_accessible(self.decode_id(id), trans.user)
-            job = hda.creating_job
-        elif contents_type == 'dataset_collection':
-            dataset_collection_instance = self.__get_accessible_collection(trans, id, history_id)
-            job_source_type = dataset_collection_instance.job_source_type
-            if job_source_type == "Job":
-                job = dataset_collection_instance.job
-            elif job_source_type == "ImplicitCollectionJobs":
-                implicit_collection_jobs = dataset_collection_instance.implicit_collection_jobs
-
-        assert job is None or implicit_collection_jobs is None
-        return self.encode_all_ids(trans, summarize_jobs_to_dict(trans.sa_session, job or implicit_collection_jobs))
-
-    def __get_contents_type(self, trans, kwd):
-        contents_type = kwd.get('type', 'dataset')
-        if contents_type not in ['dataset', 'dataset_collection']:
-            self.__handle_unknown_contents_type(trans, contents_type)
-
-        return contents_type
-
-    def __show_dataset(self, trans, id, **kwd):
-        hda = self.hda_manager.get_accessible(self.decode_id(id), trans.user)
-        return self.hda_serializer.serialize_to_view(hda,
-                                                     user=trans.user,
-                                                     trans=trans,
-                                                     **self._parse_serialization_params(kwd, 'detailed'))
-
-    def __show_dataset_collection(self, trans, id, history_id, **kwd):
-        dataset_collection_instance = self.__get_accessible_collection(trans, id, history_id)
-        view = kwd.get("view", "element")
-        fuzzy_count = kwd.get("fuzzy_count", None)
-        if fuzzy_count:
-            fuzzy_count = int(fuzzy_count)
-        return self.__collection_dict(trans, dataset_collection_instance, view=view, fuzzy_count=fuzzy_count)
-
-    def __get_accessible_collection(self, trans, id, history_id):
-        return trans.app.dataset_collections_service.get_dataset_collection_instance(
-            trans=trans,
-            instance_type="history",
-            id=id
+    def _can_access_dataset(self, trans, dataset_association, allow_admin=True, additional_roles=None):
+        roles = trans.get_current_user_roles()
+        if additional_roles:
+            roles = roles + additional_roles
+        return (allow_admin and trans.user_is_admin) or trans.app.security_agent.can_access_dataset(
+            roles, dataset_association.dataset
         )
 
-    @expose_api_raw_anonymous
-    def download_dataset_collection(self, trans, id, history_id=None, **kwd):
-        """
-        * GET /api/histories/{history_id}/contents/{id}/download
-        * GET /api/dataset_collection/{id}/download
-
-        Download the content of a HistoryDatasetCollection as a tgz archive
-        while maintaining approximate collection structure.
-
-        :param id: encoded HistoryDatasetCollectionAssociation (HDCA) id
-        :param history_id: encoded id string of the HDCA's History
-        """
+    @web.expose
+    def stdout(self, trans, dataset_id=None, **kwargs):
+        trans.response.set_content_type("text/plain")
+        stdout = ""
         try:
-            dataset_collection_instance = self.__get_accessible_collection(trans, id, history_id)
-            return self.__stream_dataset_collection(trans, dataset_collection_instance)
-        except Exception as e:
-            log.exception("Error in API while creating dataset collection archive")
-            trans.response.status = 500
-            return {'error': util.unicodify(e)}
-
-    def __stream_dataset_collection(self, trans, dataset_collection_instance):
-        archive_type_string = 'w|gz'
-        archive_ext = 'tgz'
-        if self.app.config.upstream_gzip:
-            archive_type_string = 'w|'
-            archive_ext = 'tar'
-        archive = StreamBall(mode=archive_type_string)
-        names, hdas = get_hda_and_element_identifiers(dataset_collection_instance)
-        for name, hda in zip(names, hdas):
-            if hda.state != hda.states.OK:
-                continue
-            for file_path, relpath in hda.datatype.to_archive(trans=trans, dataset=hda, name=name):
-                archive.add(file=file_path, relpath=relpath)
-        archive_name = "%s: %s.%s" % (dataset_collection_instance.hid, dataset_collection_instance.name, archive_ext)
-        trans.response.set_content_type("application/x-tar")
-        trans.response.headers["Content-Disposition"] = 'attachment; filename="{}"'.format(archive_name)
-        archive.wsgi_status = trans.response.wsgi_status()
-        archive.wsgi_headeritems = trans.response.wsgi_headeritems()
-        return archive.stream
-
-    @expose_api_anonymous
-    def create(self, trans, history_id, payload, **kwd):
-        """
-        create( self, trans, history_id, payload, **kwd )
-        * POST /api/histories/{history_id}/contents/{type}s
-        * POST /api/histories/{history_id}/contents
-            create a new HDA or HDCA
-
-        :type   history_id: str
-        :param  history_id: encoded id string of the new HDA's History
-        :type   type: str
-        :param  type: Type of history content - 'dataset' (default) or
-                      'dataset_collection'. This can be passed in via payload
-                      or parsed from the route.
-        :type   payload:    dict
-        :param  payload:    dictionary structure containing::
-            copy from library (for type 'dataset'):
-            'source'    = 'library'
-            'content'   = [the encoded id from the library dataset]
-
-            copy from library folder
-            'source'    = 'library_folder'
-            'content'   = [the encoded id from the library folder]
-
-            copy from history dataset (for type 'dataset'):
-            'source'    = 'hda'
-            'content'   = [the encoded id from the HDA]
-
-            copy from history dataset collection (for type 'dataset_collection')
-            'source'    = 'hdca'
-            'content'   = [the encoded id from the HDCA]
-            'copy_elements' = Copy child HDAs into the target history as well,
-                              defaults to False but this is less than ideal and may
-                              be changed in future releases.
-
-            create new history dataset collection (for type 'dataset_collection')
-            'source'              = 'new_collection' (default 'source' if type is
-                                    'dataset_collection' - no need to specify this)
-            'collection_type'     = For example, "list", "paired", "list:paired".
-            'copy_elements'       = Copy child HDAs when creating new collection,
-                                    defaults to False in the API but is set to True in the UI,
-                                    so that we can modify HDAs with tags when creating collections.
-            'name'                = Name of new dataset collection.
-            'element_identifiers' = Recursive list structure defining collection.
-                                    Each element must have 'src' which can be
-                                    'hda', 'ldda', 'hdca', or 'new_collection',
-                                    as well as a 'name' which is the name of
-                                    element (e.g. "forward" or "reverse" for
-                                    paired datasets, or arbitrary sample names
-                                    for instance for lists). For all src's except
-                                    'new_collection' - a encoded 'id' attribute
-                                    must be included wiht element as well.
-                                    'new_collection' sources must defined a
-                                    'collection_type' and their own list of
-                                    (potentially) nested 'element_identifiers'.
-
-        ..note:
-            Currently, a user can only copy an HDA from a history that the user owns.
-
-        :rtype:     dict
-        :returns:   dictionary containing detailed information for the new HDA
-        """
-        # TODO: Flush out create new collection documentation above, need some
-        # examples. See also bioblend and API tests for specific examples.
+            job = self._get_job_for_dataset(trans, dataset_id)
+            stdout = job.stdout
+        except Exception:
+            stdout = "Invalid dataset ID or you are not allowed to access this dataset"
+        return smart_str(stdout)
+
+    @web.expose
+    # TODO: Migrate stderr and stdout to use _get_job_for_dataset; it wasn't tested.
+    def stderr(self, trans, dataset_id=None, **kwargs):
+        trans.response.set_content_type("text/plain")
+        stderr = ""
+        try:
+            job = self._get_job_for_dataset(trans, dataset_id)
+            stderr = job.stderr
+        except Exception:
+            stderr = "Invalid dataset ID or you are not allowed to access this dataset"
+        return smart_str(stderr)
+
+    @web.expose
+    def exit_code(self, trans, dataset_id=None, **kwargs):
+        trans.response.set_content_type("text/plain")
+        exit_code = ""
+        try:
+            job = self._get_job_for_dataset(trans, dataset_id)
+            exit_code = job.exit_code
+        except Exception:
+            exit_code = "Invalid dataset ID or you are not allowed to access this dataset"
+        return exit_code
+
+    @web.expose
+    def default(self, trans, dataset_id=None, **kwd):
+        return "This link may not be followed from within Galaxy."
+
+    @web.expose_api_raw_anonymous_and_sessionless
+    def get_metadata_file(self, trans, hda_id, metadata_name):
+        """Allows the downloading of metadata files associated with datasets (eg. bai index for bam files)"""
+        # Backward compatibility with legacy links, should use `/api/datasets/{hda_id}/get_metadata_file` instead
+        fh, headers = self.service.get_metadata_file(
+            trans, history_content_id=hda_id, metadata_file=metadata_name, open_file=True
+        )
+        trans.response.headers.update(headers)
+        return fh
 
-        history = self.history_manager.get_owned(self.decode_id(history_id), trans.user,
-                                                 current_history=trans.history)
+    def _check_dataset(self, trans, hda_id):
+        # DEPRECATION: We still support unencoded ids for backward compatibility
+        try:
+            data = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(self.decode_id(hda_id))
+            if data is None:
+                raise ValueError(f"Invalid reference dataset id: {hda_id}.")
+        except Exception:
+            try:
+                data = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(int(hda_id))
+            except Exception:
+                data = None
+        if not data:
+            raise web.httpexceptions.HTTPNotFound(f"Invalid reference dataset id: {str(hda_id)}.")
+        if not self._can_access_dataset(trans, data):
+            return trans.show_error_message("You are not allowed to access this dataset")
+        if data.purged or data.dataset.purged:
+            return trans.show_error_message("The dataset you are attempting to view has been purged.")
+        elif data.deleted and not (trans.user_is_admin or (data.history and trans.get_user() == data.user)):
+            return trans.show_error_message("The dataset you are attempting to view has been deleted.")
+        elif data.state == Dataset.states.UPLOAD:
+            return trans.show_error_message(
+                "Please wait until this dataset finishes uploading before attempting to view it."
+            )
+        elif data.state == Dataset.states.DISCARDED:
+            return trans.show_error_message("The dataset you are attempting to view has been discarded.")
+        elif data.state == Dataset.states.DEFERRED:
+            return trans.show_error_message(
+                "The dataset you are attempting to view has deferred data. You can only use this dataset as input for jobs."
+            )
+        elif data.state == Dataset.states.PAUSED:
+            return trans.show_error_message(
+                "The dataset you are attempting to view is in paused state. One of the inputs for the job that creates this dataset has failed."
+            )
+        return data
 
-        type = payload.get('type', 'dataset')
-        if type == 'dataset':
-            source = payload.get('source', None)
-            if source == 'library_folder':
-                return self.__create_datasets_from_library_folder(trans, history, payload, **kwd)
-            else:
-                return self.__create_dataset(trans, history, payload, **kwd)
-        elif type == 'dataset_collection':
-            return self.__create_dataset_collection(trans, history, payload, **kwd)
+    @web.expose
+    @web.json
+    def transfer_status(self, trans, dataset_id, filename=None):
+        """Primarily used for the S3ObjectStore - get the status of data transfer
+        if the file is not in cache"""
+        data = self._check_dataset(trans, dataset_id)
+        if isinstance(data, str):
+            return data
+        log.debug(f"Checking transfer status for dataset {data.dataset.id}...")
+
+        # Pulling files in extra_files_path into cache is not handled via this
+        # method but that's primarily because those files are typically linked to
+        # through tool's output page anyhow so tying a JavaScript event that will
+        # call this method does not seem doable?
+        if data.dataset.external_filename:
+            return True
         else:
-            return self.__handle_unknown_contents_type(trans, type)
+            return trans.app.object_store.file_ready(data.dataset)
 
-    def __create_dataset(self, trans, history, payload, **kwd):
-        source = payload.get('source', None)
-        if source not in ('library', 'hda'):
-            raise exceptions.RequestParameterInvalidException(
-                "'source' must be either 'library' or 'hda': %s" % (source))
-        content = payload.get('content', None)
-        if content is None:
-            raise exceptions.RequestParameterMissingException("'content' id of lda or hda is missing")
-
-        # copy from library dataset
-        hda = None
-        if source == 'library':
-            hda = self.__create_hda_from_ldda(trans, content, history)
-
-        # copy an existing, accessible hda
-        elif source == 'hda':
-            unencoded_hda_id = self.decode_id(content)
-            original = self.hda_manager.get_accessible(unencoded_hda_id, trans.user)
-            # check for access on history that contains the original hda as well
-            self.history_manager.error_unless_accessible(original.history, trans.user, current_history=trans.history)
-            hda = self.hda_manager.copy(original, history=history)
-
-        trans.sa_session.flush()
-        if not hda:
-            return None
-        return self.hda_serializer.serialize_to_view(hda,
-            user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
-
-    def __create_hda_from_ldda(self, trans, content, history):
-        ld = self.get_library_dataset(trans, content)
-        if type(ld) is not trans.app.model.LibraryDataset:
-            raise exceptions.RequestParameterInvalidException(
-                "Library content id ( %s ) is not a dataset" % content)
-        hda = ld.library_dataset_dataset_association.to_history_dataset_association(history, add_to_history=True)
-        return hda
-
-    def __create_datasets_from_library_folder(self, trans, history, payload, **kwd):
-        rval = []
-
-        source = payload.get('source', None)
-        if source == 'library_folder':
-            content = payload.get('content', None)
-            if content is None:
-                raise exceptions.RequestParameterMissingException("'content' id of lda or hda is missing")
-
-            folder_id = self.folder_manager.cut_and_decode(trans, content)
-            folder = self.folder_manager.get(trans, folder_id)
-
-            current_user_roles = trans.get_current_user_roles()
-
-            def traverse(folder):
-                admin = trans.user_is_admin
-                rval = []
-                for subfolder in folder.active_folders:
-                    if not admin:
-                        can_access, folder_ids = trans.app.security_agent.check_folder_contents(trans.user, current_user_roles, subfolder)
-                    if (admin or can_access) and not subfolder.deleted:
-                        rval.extend(traverse(subfolder))
-                for ld in folder.datasets:
-                    if not admin:
-                        can_access = trans.app.security_agent.can_access_dataset(
-                            current_user_roles,
-                            ld.library_dataset_dataset_association.dataset
+    @web.expose
+    def display(
+        self, trans, dataset_id=None, preview=False, filename=None, to_ext=None, offset=None, ck_size=None, **kwd
+    ):
+        data = self._check_dataset(trans, dataset_id)
+        if not isinstance(data, trans.app.model.DatasetInstance):
+            return data
+        if "hdca" in kwd:
+            raise RequestParameterInvalidException("Invalid request parameter 'hdca' encountered.")
+        hdca_id = kwd.get("hdca_id", None)
+        if hdca_id:
+            hdca = self.app.dataset_collection_manager.get_dataset_collection_instance(trans, "history", hdca_id)
+            del kwd["hdca_id"]
+            kwd["hdca"] = hdca
+        # Ensure offset is an integer before passing through to datatypes.
+        if offset:
+            offset = int(offset)
+        # Ensure ck_size is an integer before passing through to datatypes.
+        if ck_size:
+            ck_size = int(ck_size)
+        kwd.pop("dataset", None)
+        # `dataset` in kwd would interfere with positional dataset argument of `display_data` method.
+        display_data, headers = data.datatype.display_data(
+            trans, data, preview, filename, to_ext, offset=offset, ck_size=ck_size, **kwd
+        )
+        if isinstance(display_data, ZipstreamWrapper):
+            trans.response.headers.update(headers)
+            return display_data.response()
+        trans.response.headers.update(headers)
+        return display_data
+
+    @web.legacy_expose_api_anonymous
+    def get_edit(self, trans, dataset_id=None, **kwd):
+        """Produces the input definitions available to modify dataset attributes"""
+        status = None
+        data, message = self._get_dataset_for_edit(trans, dataset_id)
+        if message:
+            return message
+
+        if self._can_access_dataset(trans, data):
+            if data.state == trans.model.Dataset.states.UPLOAD:
+                return self.message_exception(
+                    trans, "Please wait until this dataset finishes uploading before attempting to edit its metadata."
+                )
+            # let's not overwrite the imported datatypes module with the variable datatypes?
+            # the built-in 'id' is overwritten in lots of places as well
+            ldatatypes = [
+                (dtype_name, dtype_name)
+                for dtype_name, dtype_value in trans.app.datatypes_registry.datatypes_by_extension.items()
+                if dtype_value.is_datatype_change_allowed()
+            ]
+            ldatatypes.sort()
+            all_roles = [
+                (r.name, trans.security.encode_id(r.id))
+                for r in trans.app.security_agent.get_legitimate_roles(trans, data.dataset, "root")
+            ]
+            data_metadata = [(name, spec) for name, spec in data.metadata.spec.items()]
+            converters_collection = [(key, value.name) for key, value in data.get_converter_types().items()]
+            can_manage_dataset = trans.app.security_agent.can_manage_dataset(
+                trans.get_current_user_roles(), data.dataset
+            )
+            # attribute editing
+            attribute_inputs = [
+                {"name": "name", "type": "text", "label": "Name", "value": data.get_display_name()},
+                {"name": "info", "type": "text", "area": True, "label": "Info", "value": data.info},
+                {
+                    "name": "annotation",
+                    "type": "text",
+                    "area": True,
+                    "label": "Annotation",
+                    "optional": True,
+                    "value": self.get_item_annotation_str(trans.sa_session, trans.user, data),
+                    "help": "Add an annotation or notes to a dataset; annotations are available when a history is viewed.",
+                },
+            ]
+            for name, spec in data_metadata:
+                if spec.visible:
+                    attributes = data.metadata.get_metadata_parameter(name, trans=trans)
+                    if type(attributes) is form_builder.SelectField:
+                        attribute_inputs.append(
+                            {
+                                "type": "select",
+                                "multiple": attributes.multiple,
+                                "optional": spec.get("optional"),
+                                "name": name,
+                                "label": spec.desc,
+                                "options": attributes.options,
+                                "value": attributes.value if attributes.multiple else [attributes.value],
+                            }
+                        )
+                    elif type(attributes) is form_builder.TextField:
+                        attribute_inputs.append(
+                            {
+                                "type": "text",
+                                "name": name,
+                                "label": spec.desc,
+                                "value": attributes.value,
+                                "readonly": spec.get("readonly"),
+                            }
                         )
-                    if (admin or can_access) and not ld.deleted:
-                        rval.append(ld)
-                return rval
-
-            for ld in traverse(folder):
-                hda = ld.library_dataset_dataset_association.to_history_dataset_association(history, add_to_history=True)
-                hda_dict = self.hda_serializer.serialize_to_view(hda,
-                    user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
-                rval.append(hda_dict)
+            if data.missing_meta():
+                message = 'Required metadata values are missing. Some of these values may not be editable by the user. Selecting "Auto-detect" will attempt to fix these values.'
+                status = "warning"
+            metadata_disable = data.state not in [
+                trans.model.Dataset.states.OK,
+                trans.model.Dataset.states.FAILED_METADATA,
+            ]
+            # datatype conversion
+            conversion_options = [
+                (f"{convert_id} (using '{convert_name}')", convert_id)
+                for convert_id, convert_name in converters_collection
+            ]
+            conversion_disable = len(conversion_options) == 0
+            conversion_inputs = [
+                {
+                    "type": "select",
+                    "name": "target_type",
+                    "label": "Target datatype",
+                    "help": "This will create a new dataset with the contents of this dataset converted to a new format.",
+                    "options": conversion_options,
+                }
+            ]
+            # datatype changing
+            datatype_options = [(ext_name, ext_id) for ext_id, ext_name in ldatatypes]
+            datatype_disable = len(datatype_options) == 0
+            datatype_inputs = [
+                {
+                    "type": "select",
+                    "name": "datatype",
+                    "label": "New Type",
+                    "options": datatype_options,
+                    "value": [ext_id for ext_id, ext_name in ldatatypes if ext_id == data.ext],
+                    "help": "This will change the datatype of the existing dataset but not modify its contents. Use this if Galaxy has incorrectly guessed the type of your dataset.",
+                }
+            ]
+            # permissions
+            permission_disable = True
+            permission_inputs = list()
+            if trans.user:
+                if data.dataset.actions:
+                    in_roles = {}
+                    for action, roles in trans.app.security_agent.get_permissions(data.dataset).items():
+                        in_roles[action.action] = [trans.security.encode_id(role.id) for role in roles]
+                    for index, action in trans.app.model.Dataset.permitted_actions.items():
+                        if action == trans.app.security_agent.permitted_actions.DATASET_ACCESS:
+                            help_text = f"{action.description}<br/>NOTE: Users must have every role associated with this dataset in order to access it."
+                        else:
+                            help_text = action.description
+                        permission_inputs.append(
+                            {
+                                "type": "select",
+                                "multiple": True,
+                                "optional": True,
+                                "name": index,
+                                "label": action.action,
+                                "help": help_text,
+                                "options": all_roles,
+                                "value": in_roles.get(action.action),
+                                "readonly": not can_manage_dataset,
+                            }
+                        )
+                    permission_disable = not can_manage_dataset
+                else:
+                    permission_inputs.append(
+                        {
+                            "name": "access_public",
+                            "type": "hidden",
+                            "label": "This dataset is accessible by everyone (it is public).",
+                            "readonly": True,
+                        }
+                    )
+            else:
+                permission_inputs.append(
+                    {
+                        "name": "no_access",
+                        "type": "hidden",
+                        "label": "Permissions not available (not logged in).",
+                        "readonly": True,
+                    }
+                )
+            return {
+                "display_name": data.get_display_name(),
+                "message": message,
+                "status": status,
+                "dataset_id": dataset_id,
+                "metadata_disable": metadata_disable,
+                "attribute_inputs": attribute_inputs,
+                "conversion_inputs": conversion_inputs,
+                "conversion_disable": conversion_disable,
+                "datatype_inputs": datatype_inputs,
+                "datatype_disable": datatype_disable,
+                "permission_inputs": permission_inputs,
+                "permission_disable": permission_disable,
+            }
         else:
-            message = "Invalid 'source' parameter in request %s" % source
-            raise exceptions.RequestParameterInvalidException(message)
-
-        trans.sa_session.flush()
-        return rval
-
-    def __create_dataset_collection(self, trans, history, payload, **kwd):
-        """Create hdca in a history from the list of element identifiers
-
-        :param history: history the new hdca should be added to
-        :type  history: History
-        :param source: whether to create a new collection or copy existing one
-        :type  source: str
-        :param payload: dictionary structure containing:
-            :param collection_type: type (and depth) of the new collection
-            :type name: str
-            :param element_identifiers: list of elements that should be in the new collection
-                :param element: one member of the collection
-                    :param name: name of the element
-                    :type name: str
-                    :param src: source of the element (hda/ldda)
-                    :type src: str
-                    :param id: identifier
-                    :type id: str
-                    :param id: tags
-                    :type id: list
-                :type element: dict
-            :type name: list
-            :param name: name of the collection
-            :type name: str
-            :param hide_source_items: whether to mark the original hdas as hidden
-            :type name: bool
-            :param copy_elements: whether to copy HDAs when creating collection
-            :type name: bool
-        :type  payload: dict
-
-       .. note:: Elements may be nested depending on the collection_type
-
-        :returns:   dataset collection information
-        :rtype:     dict
+            return self.message_exception(
+                trans, "You do not have permission to edit this dataset's ( id: %s ) information." % str(dataset_id)
+            )
 
-        :raises: RequestParameterInvalidException, RequestParameterMissingException
-        """
-        source = kwd.get("source", payload.get("source", "new_collection"))
+    @web.expose_api_anonymous
+    def set_edit(self, trans, payload=None, **kwd):
+        """Allows user to modify parameters of an HDA."""
+        status = "success"
+        operation = payload.get("operation")
+        dataset_id = payload.get("dataset_id")
+        data, message = self._get_dataset_for_edit(trans, dataset_id)
+        if message:
+            return message
+
+        if operation == "attributes":
+            # The user clicked the Save button on the 'Edit Attributes' form
+            data.name = payload.get("name")
+            data.info = payload.get("info")
+            if data.ok_to_edit_metadata():
+                # The following for loop will save all metadata_spec items
+                for name, spec in data.datatype.metadata_spec.items():
+                    if not spec.get("readonly"):
+                        setattr(data.metadata, name, spec.unwrap(payload.get(name) or None))
+                data.datatype.after_setting_metadata(data)
+                # Sanitize annotation before adding it.
+                if payload.get("annotation"):
+                    annotation = sanitize_html(payload.get("annotation"))
+                    self.add_item_annotation(trans.sa_session, trans.get_user(), data, annotation)
+                # if setting metadata previously failed and all required elements have now been set, clear the failed state.
+                if data._state == trans.model.Dataset.states.FAILED_METADATA and not data.missing_meta():
+                    data._state = None
+                message = f"Attributes updated. {message}" if message else "Attributes updated."
+            else:
+                message = "Attributes updated, but metadata could not be changed because this dataset is currently being used as input or output. You must cancel or wait for these jobs to complete before changing metadata."
+                status = "warning"
+            trans.sa_session.flush()
+        elif operation == "datatype":
+            # The user clicked the Save button on the 'Change data type' form
+            datatype = payload.get("datatype")
+            self.hda_deserializer.deserialize(data, {"datatype": datatype}, trans=trans)
+            message = f"Changed the type to {datatype}."
+        elif operation == "datatype_detect":
+            # The user clicked the 'Detect datatype' button on the 'Change data type' form
+            if data.datatype.is_datatype_change_allowed():
+                # prevent modifying datatype when dataset is queued or running as input/output
+                if not data.ok_to_edit_metadata():
+                    return self.message_exception(
+                        trans,
+                        "This dataset is currently being used as input or output.  You cannot change datatype until the jobs have completed or you have canceled them.",
+                    )
+                else:
+                    path = data.dataset.file_name
+                    datatype = guess_ext(path, trans.app.datatypes_registry.sniff_order)
+                    trans.app.datatypes_registry.change_datatype(data, datatype)
+                    trans.sa_session.flush()
+                    job, *_ = trans.app.datatypes_registry.set_external_metadata_tool.tool_action.execute(
+                        trans.app.datatypes_registry.set_external_metadata_tool,
+                        trans,
+                        incoming={"input1": data},
+                        overwrite=False,
+                    )  # overwrite is False as per existing behavior
+                    trans.app.job_manager.enqueue(job, tool=trans.app.datatypes_registry.set_external_metadata_tool)
+                    message = f"Detection was finished and changed the datatype to {datatype}."
+            else:
+                return self.message_exception(trans, f'Changing datatype "{data.extension}" is not allowed.')
+        elif operation == "autodetect":
+            # The user clicked the Auto-detect button on the 'Edit Attributes' form
+            self.hda_manager.set_metadata(trans, data, overwrite=True)
+            message = "Auto-detect operation successfully submitted."
+        elif operation == "conversion":
+            target_type = payload.get("target_type")
+            if target_type:
+                try:
+                    message = data.datatype.convert_dataset(trans, data, target_type)
+                except DatatypeConverterNotFoundException as e:
+                    return self.message_exception(trans, str(e))
+        elif operation == "permission":
+            # Adapt form request to API - style.
+            payload_permissions = {}
+            for key, value in {"DATASET_MANAGE_PERMISSIONS": "manage_ids", "DATASET_ACCESS": "access_ids"}.items():
+                role_ids = util.listify(payload.get(key))
+                decoded_role_ids = list(map(self.decode_id, role_ids))
+                payload_permissions[f"{value}[]"] = decoded_role_ids
 
-        service = trans.app.dataset_collections_service
-        if source == "new_collection":
-            create_params = api_payload_to_create_params(payload)
-            dataset_collection_instance = service.create(
+            self.hda_manager.update_permissions(
                 trans,
-                parent=history,
-                history=history,
-                **create_params
-            )
-        elif source == "hdca":
-            content = payload.get('content', None)
-            if content is None:
-                raise exceptions.RequestParameterMissingException("'content' id of target to copy is missing")
-            copy_elements = payload.get('copy_elements', False)
-            dataset_collection_instance = service.copy(
-                trans=trans,
-                parent=history,
-                source="hdca",
-                encoded_source_id=content,
-                copy_elements=copy_elements,
+                data,
+                action="set_permissions",
+                **payload_permissions,
             )
+            message = "Your changes completed successfully."
         else:
-            message = "Invalid 'source' parameter in request %s" % source
-            raise exceptions.RequestParameterInvalidException(message)
-
-        # if the consumer specified keys or view, use the secondary serializer
-        if 'view' in kwd or 'keys' in kwd:
-            return self.hdca_serializer.serialize_to_view(dataset_collection_instance,
-                user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
-
-        return self.__collection_dict(trans, dataset_collection_instance, view="element")
-
-    @expose_api
-    def show_roles(self, trans, encoded_dataset_id, **kwd):
-        """
-        Display information about current or available roles for a given dataset permission.
+            return self.message_exception(trans, f"Invalid operation identifier ({operation}).")
+        return {"status": status, "message": sanitize_text(message)}
 
-        * GET /api/histories/{history_id}/contents/datasets/{encoded_dataset_id}/permissions
-
-        :param  encoded_dataset_id:      the encoded id of the dataset to query
-        :type   encoded_dataset_id:      an encoded id string
-
-        :returns:   either dict of current roles for all permission types
-                    or dict of available roles to choose from (is the same for any permission type)
-        :rtype:     dictionary
-
-        :raises: InsufficientPermissionsException
-        """
-        hda = self.hda_manager.get_owned(self.decode_id(encoded_dataset_id), trans.user, current_history=trans.history, trans=trans)
-        return self.hda_manager.serialize_dataset_association_roles(trans, hda)
-
-    @expose_api
-    def update_permissions(self, trans, history_id, history_content_id, payload=None, **kwd):
-        """
-        Set permissions of the given library dataset to the given role ids.
+    def _get_dataset_for_edit(self, trans, dataset_id):
+        if dataset_id is not None:
+            id = self.decode_id(dataset_id)
+            data = trans.sa_session.query(self.app.model.HistoryDatasetAssociation).get(id)
+        else:
+            trans.log_event("dataset_id is None, cannot load a dataset to edit.")
+            return None, self.message_exception(trans, "You must provide a dataset id to edit attributes.")
+        if data is None:
+            trans.log_event(f"Problem retrieving dataset id ({dataset_id}).")
+            return None, self.message_exception(trans, "The dataset id is invalid.")
+        if dataset_id is not None and data.user and data.user != trans.user:
+            trans.log_event(f"User attempted to edit a dataset they do not own (encoded: {dataset_id}, decoded: {id}).")
+            return None, self.message_exception(trans, "The dataset id is invalid.")
+        if data.history.user and not data.dataset.has_manage_permissions_roles(trans.app.security_agent):
+            # Permission setting related to DATASET_MANAGE_PERMISSIONS was broken for a period of time,
+            # so it is possible that some Datasets have no roles associated with the DATASET_MANAGE_PERMISSIONS
+            # permission.  In this case, we'll reset this permission to the hda user's private role.
+            manage_permissions_action = trans.app.security_agent.get_action(
+                trans.app.security_agent.permitted_actions.DATASET_MANAGE_PERMISSIONS.action
+            )
+            permissions = {manage_permissions_action: [trans.app.security_agent.get_private_user_role(data.user)]}
+            trans.app.security_agent.set_dataset_permission(data.dataset, permissions)
+        return data, None
+
+    @web.expose
+    def imp(self, trans, dataset_id=None, **kwd):
+        """Import another user's dataset via a shared URL; dataset is added to user's current history."""
+        # Set referer message.
+        referer = trans.request.referer
+        if referer and not referer.startswith(f"{trans.request.application_url}{url_for('/login')}"):
+            referer_message = f"<a href='{escape(referer)}'>return to the previous page</a>"
+        else:
+            referer_message = f"<a href='{url_for('/')}'>go to Galaxy's start page</a>"
+        # Error checking.
+        if not dataset_id:
+            return trans.show_error_message(
+                f"You must specify a dataset to import. You can {referer_message}.", use_panels=True
+            )
+        # Do import.
+        cur_history = trans.get_history(create=True)
+        status, message = self._copy_datasets(trans, [dataset_id], [cur_history], imported=True)
+        message = (
+            f"Dataset imported. <br>You can <a href='{url_for('/')}'>start using the dataset</a> or {referer_message}."
+        )
+        return trans.show_message(message, type=status, use_panels=True)
 
-        * PUT /api/histories/{history_id}/contents/datasets/{encoded_dataset_id}/permissions
+    @web.expose
+    @web.json
+    @web.require_login("use Galaxy datasets")
+    def get_name_and_link_async(self, trans, id=None):
+        """Returns dataset's name and link."""
+        decoded_id = self.decode_id(id)
+        dataset = self.hda_manager.get_accessible(decoded_id, trans.user)
+        dataset = self.hda_manager.error_if_uploading(dataset)
+        return_dict = {
+            "name": dataset.name,
+            "link": url_for(
+                controller="dataset",
+                action="display_by_username_and_slug",
+                username=dataset.history.user.username,
+                slug=trans.security.encode_id(dataset.id),
+            ),
+        }
+        return return_dict
+
+    @web.expose
+    def get_embed_html_async(self, trans, id):
+        """Returns HTML for embedding a dataset in a page."""
+        decoded_id = self.decode_id(id)
+        dataset = self.hda_manager.get_accessible(decoded_id, trans.user)
+        dataset = self.hda_manager.error_if_uploading(dataset)
+        if dataset:
+            return f"Embedded Dataset '{dataset.name}'"
+
+    @web.expose
+    @web.require_login("use Galaxy datasets")
+    def set_accessible_async(self, trans, id=None, accessible=False):
+        """Does nothing because datasets do not have an importable/accessible attribute. This method could potentially set another attribute."""
+        return
+
+    @web.expose
+    def display_by_username_and_slug(self, trans, username, slug, filename=None, preview=True, **kwargs):
+        """Display dataset by username and slug; because datasets do not yet have slugs, the slug is the dataset's id."""
+        dataset = self._check_dataset(trans, slug)
+        if not isinstance(dataset, trans.app.model.DatasetInstance):
+            return dataset
+        # Filename used for composite types.
+        if filename:
+            return self.display(trans, dataset_id=slug, filename=filename)
+
+        truncated, dataset_data = self.hda_manager.text_data(dataset, preview)
+        dataset.annotation = self.get_item_annotation_str(trans.sa_session, dataset.user, dataset)
+
+        # If dataset is chunkable, get first chunk.
+        first_chunk = None
+        if dataset.datatype.CHUNKABLE:
+            first_chunk = dataset.datatype.get_chunk(trans, dataset, 0)
+
+        # If data is binary or an image, stream without template; otherwise, use display template.
+        # TODO: figure out a way to display images in display template.
+        if (
+            isinstance(dataset.datatype, datatypes.binary.Binary)
+            or isinstance(dataset.datatype, datatypes.images.Image)
+            or isinstance(dataset.datatype, datatypes.text.Html)
+        ):
+            trans.response.set_content_type(dataset.get_mime())
+            return open(dataset.file_name, "rb")
+        else:
+            return trans.fill_template_mako(
+                "/dataset/display.mako",
+                item=dataset,
+                item_data=dataset_data,
+                truncated=truncated,
+                first_chunk=first_chunk,
+            )
 
-        :param  encoded_dataset_id:      the encoded id of the dataset to update permissions of
-        :type   encoded_dataset_id:      an encoded id string
-        :param   payload: dictionary structure containing:
-            :param  action:     (required) describes what action should be performed
-                                available actions: make_private, remove_restrictions, set_permissions
-            :type   action:     string
-            :param  access_ids[]:      list of Role.id defining roles that should have access permission on the dataset
-            :type   access_ids[]:      string or list
-            :param  manage_ids[]:      list of Role.id defining roles that should have manage permission on the dataset
-            :type   manage_ids[]:      string or list
-            :param  modify_ids[]:      list of Role.id defining roles that should have modify permission on the library dataset item
-            :type   modify_ids[]:      string or list
-        :type:      dictionary
+    @web.expose
+    def annotate_async(self, trans, id, new_annotation=None, **kwargs):
+        # TODO:?? why is this an access check only?
+        decoded_id = self.decode_id(id)
+        dataset = self.hda_manager.get_accessible(decoded_id, trans.user)
+        dataset = self.hda_manager.error_if_uploading(dataset)
+        if not dataset:
+            web.httpexceptions.HTTPNotFound()
+        if dataset and new_annotation:
+            # Sanitize annotation before adding it.
+            new_annotation = sanitize_html(new_annotation)
+            self.add_item_annotation(trans.sa_session, trans.get_user(), dataset, new_annotation)
+            trans.sa_session.flush()
+            return new_annotation
 
-        :returns:   dict of current roles for all available permission types
-        :rtype:     dictionary
+    @web.expose
+    def get_annotation_async(self, trans, id):
+        decoded_id = self.decode_id(id)
+        dataset = self.hda_manager.get_accessible(decoded_id, trans.user)
+        dataset = self.hda_manager.error_if_uploading(dataset)
+        if not dataset:
+            web.httpexceptions.HTTPNotFound()
+        annotation = self.get_item_annotation_str(trans.sa_session, trans.user, dataset)
+        if annotation and isinstance(annotation, str):
+            annotation = annotation.encode("ascii", "replace")  # paste needs ascii here
+        return annotation
+
+    @web.expose
+    def display_at(self, trans, dataset_id, filename=None, **kwd):
+        """Sets up a dataset permissions so it is viewable at an external site"""
+        if not trans.app.config.enable_old_display_applications:
+            return trans.show_error_message(
+                "This method of accessing external display applications has been disabled by a Galaxy administrator."
+            )
+        site = filename
+        data = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(dataset_id)
+        if not data:
+            raise paste.httpexceptions.HTTPRequestRangeNotSatisfiable(
+                f"Invalid reference dataset id: {str(dataset_id)}."
+            )
+        if "display_url" not in kwd or "redirect_url" not in kwd:
+            return trans.show_error_message(
+                'Invalid parameters specified for "display at" link, please contact a Galaxy administrator'
+            )
+        try:
+            redirect_url = kwd["redirect_url"] % quote_plus(kwd["display_url"])
+        except Exception:
+            redirect_url = kwd["redirect_url"]  # not all will need custom text
+        if trans.app.security_agent.dataset_is_public(data.dataset):
+            return trans.response.send_redirect(redirect_url)  # anon access already permitted by rbac
+        if self._can_access_dataset(trans, data):
+            trans.app.host_security_agent.set_dataset_permissions(data, trans.user, site)
+            return trans.response.send_redirect(redirect_url)
+        else:
+            return trans.show_error_message(
+                "You are not allowed to view this dataset at external sites.  Please contact your Galaxy administrator to acquire management permissions for this dataset."
+            )
 
-        :raises: RequestParameterInvalidException, ObjectNotFound, InsufficientPermissionsException, InternalServerError
-                    RequestParameterMissingException
-        """
-        if payload:
-            kwd.update(payload)
-        hda = self.hda_manager.get_owned(self.decode_id(history_content_id), trans.user, current_history=trans.history, trans=trans)
-        assert hda is not None
-        self.hda_manager.update_permissions(trans, hda, **kwd)
-        return self.hda_manager.serialize_dataset_association_roles(trans, hda)
+    @web.expose
+    @web.do_not_cache
+    def display_application(
+        self,
+        trans,
+        dataset_id=None,
+        user_id=None,
+        app_name=None,
+        link_name=None,
+        app_action=None,
+        action_param=None,
+        action_param_extra=None,
+        **kwds,
+    ):
+        """Access to external display applications"""
+        if None in [app_name, link_name]:
+            return trans.show_error_message("A display application name and link name must be provided.")
+        app_name = unquote_plus(app_name)
+        link_name = unquote_plus(link_name)
+        # Build list of parameters to pass in to display application logic (app_kwds)
+        app_kwds = {}
+        for name, value in dict(kwds).items():  # clone kwds because we remove stuff as we go.
+            if name.startswith("app_"):
+                app_kwds[name[len("app_") :]] = value
+                del kwds[name]
+        if kwds:
+            log.debug(f"Unexpected Keywords passed to display_application: {kwds}")  # route memory?
+        # decode ids
+        data, user = decode_dataset_user(trans, dataset_id, user_id)
+        if not data:
+            raise paste.httpexceptions.HTTPRequestRangeNotSatisfiable(
+                f"Invalid reference dataset id: {str(dataset_id)}."
+            )
+        if user is None:
+            user = trans.user
+        if user:
+            user_roles = user.all_roles()
+        else:
+            user_roles = []
+        # Decode application name and link name
+        if self._can_access_dataset(trans, data, additional_roles=user_roles):
+            msg = []
+            preparable_steps = []
+            refresh = False
+            display_app = trans.app.datatypes_registry.display_applications.get(app_name)
+            if not display_app:
+                log.debug("Unknown display application has been requested: %s", app_name)
+                return paste.httpexceptions.HTTPNotFound(
+                    f"The requested display application ({app_name}) is not available."
+                )
+            dataset_hash, user_hash = encode_dataset_user(trans, data, user)
+            try:
+                display_link = display_app.get_link(link_name, data, dataset_hash, user_hash, trans, app_kwds)
+            except Exception as e:
+                log.debug("Error generating display_link: %s", e)
+                # User can sometimes recover from, e.g. conversion errors by fixing input metadata, so use conflict
+                return paste.httpexceptions.HTTPConflict(f"Error generating display_link: {e}")
+            if not display_link:
+                log.debug("Unknown display link has been requested: %s", link_name)
+                return paste.httpexceptions.HTTPNotFound(f"Unknown display link has been requested: {link_name}")
+            if data.state == data.states.ERROR:
+                msg.append(
+                    (
+                        "This dataset is in an error state, you cannot view it at an external display application.",
+                        "error",
+                    )
+                )
+            elif data.deleted:
+                msg.append(
+                    ("This dataset has been deleted, you cannot view it at an external display application.", "error")
+                )
+            elif data.state != data.states.OK:
+                msg.append(
+                    (
+                        "You must wait for this dataset to be created before you can view it at an external display application.",
+                        "info",
+                    )
+                )
+                refresh = True
+            else:
+                # We have permissions, dataset is not deleted and is in OK state, allow access
+                if display_link.display_ready():
+                    if app_action in ["data", "param"]:
+                        assert action_param, "An action param must be provided for a data or param action"
+                        # data is used for things with filenames that could be passed off to a proxy
+                        # in case some display app wants all files to be in the same 'directory',
+                        # data can be forced to param, but not the other way (no filename for other direction)
+                        # get param name from url param name
+                        try:
+                            action_param = display_link.get_param_name_by_url(action_param)
+                        except ValueError as e:
+                            log.debug(e)
+                            return paste.httpexceptions.HTTPNotFound(util.unicodify(e))
+                        value = display_link.get_param_value(action_param)
+                        assert value, f"An invalid parameter name was provided: {action_param}"
+                        assert value.parameter.viewable, "This parameter is not viewable."
+                        if value.parameter.type == "data":
+                            try:
+                                if action_param_extra:
+                                    assert (
+                                        value.parameter.allow_extra_files_access
+                                    ), f"Extra file content requested ({action_param_extra}), but allow_extra_files_access is False."
+                                    file_name = os.path.join(value.extra_files_path, action_param_extra)
+                                else:
+                                    file_name = value.file_name
+                                content_length = os.path.getsize(file_name)
+                                rval = open(file_name, "rb")
+                            except OSError as e:
+                                log.debug("Unable to access requested file in display application: %s", e)
+                                return paste.httpexceptions.HTTPNotFound("This file is no longer available.")
+                        else:
+                            rval = str(value)
+                            content_length = len(rval)
+                        # Set Access-Control-Allow-Origin as specified in GEDA
+                        if value.parameter.allow_cors:
+                            trans.set_cors_origin()
+                            trans.set_cors_allow()
+                        trans.response.set_content_type(value.mime_type(action_param_extra=action_param_extra))
+                        trans.response.headers["Content-Length"] = str(content_length)
+                        return rval
+                    elif app_action is None:
+                        # redirect user to url generated by display link
+                        return trans.response.send_redirect(display_link.display_url())
+                    else:
+                        msg.append((f"Invalid action provided: {app_action}", "error"))
+                else:
+                    if app_action is None:
+                        if trans.history != data.history:
+                            msg.append(
+                                (
+                                    "You must import this dataset into your current history before you can view it at the desired display application.",
+                                    "error",
+                                )
+                            )
+                        else:
+                            refresh = True
+                            msg.append(
+                                (
+                                    "Launching this display application required additional datasets to be generated, you can view the status of these jobs below. ",
+                                    "info",
+                                )
+                            )
+                            if not display_link.preparing_display():
+                                display_link.prepare_display()
+                            preparable_steps = display_link.get_prepare_steps()
+                    else:
+                        raise Exception(f"Attempted a view action ({app_action}) on a non-ready display application")
+            return trans.fill_template_mako(
+                "dataset/display_application/display.mako",
+                msg=msg,
+                display_app=display_app,
+                display_link=display_link,
+                refresh=refresh,
+                preparable_steps=preparable_steps,
+            )
+        return trans.show_error_message(
+            "You do not have permission to view this dataset at an external display application."
+        )
 
-    @expose_api_anonymous
-    def update_batch(self, trans, history_id, payload, **kwd):
-        """
-        update( self, trans, history_id, id, payload, **kwd )
-        * PUT /api/histories/{history_id}/contents
+    def _delete(self, trans, dataset_id):
+        message = None
+        status = "done"
+        id = None
+        try:
+            id = self.decode_id(dataset_id)
+            hda = self.hda_manager.get_owned(id, trans.user, current_history=trans.history)
+            hda.mark_deleted()
+            hda.clear_associated_files()
+            trans.log_event(f"Dataset id {str(id)} marked as deleted")
+            self.hda_manager.stop_creating_job(hda, flush=True)
+        except Exception:
+            msg = f"HDA deletion failed (encoded: {dataset_id}, decoded: {id})"
+            log.exception(msg)
+            trans.log_event(msg)
+            message = "Dataset deletion failed"
+            status = "error"
+        return (message, status)
+
+    def _undelete(self, trans, dataset_id):
+        message = None
+        status = "done"
+        id = None
+        try:
+            id = self.decode_id(dataset_id)
+            item = self.hda_manager.get_owned(id, trans.user, current_history=trans.history)
+            self.hda_manager.undelete(item)
+            trans.log_event(f"Dataset id {str(id)} has been undeleted")
+        except Exception:
+            msg = f"HDA undeletion failed (encoded: {dataset_id}, decoded: {id})"
+            log.exception(msg)
+            trans.log_event(msg)
+            message = "Dataset undeletion failed"
+            status = "error"
+        return (message, status)
 
-        :type   history_id: str
-        :param  history_id: encoded id string of the history containing supplied items
-        :type   id:         str
-        :param  id:         the encoded id of the history to update
-        :type   payload:    dict
-        :param  payload:    a dictionary containing any or all the
-
-        :rtype:     dict
-        :returns:   an error object if an error occurred or a dictionary containing
-            any values that were different from the original and, therefore, updated
-        """
-        items = payload.get("items")
-        hda_ids = []
-        hdca_ids = []
-        for item in items:
-            contents_type = item["history_content_type"]
-            if contents_type == "dataset":
-                decoded_id = self.decode_id(item["id"])
-                hda_ids.append(decoded_id)
+    def _unhide(self, trans, dataset_id):
+        try:
+            id = self.decode_id(dataset_id)
+            item = self.hda_manager.get_owned(id, trans.user, current_history=trans.history)
+            item.mark_unhidden()
+            trans.sa_session.flush()
+            trans.log_event(f"Dataset id {str(id)} has been unhidden")
+            return True
+        except Exception:
+            return False
+
+    def _purge(self, trans, dataset_id):
+        message = None
+        status = "done"
+        try:
+            id = self.decode_id(dataset_id)
+            user = trans.get_user()
+            hda = trans.sa_session.query(self.app.model.HistoryDatasetAssociation).get(id)
+            # Invalid HDA
+            assert hda, "Invalid history dataset ID"
+
+            # If the user is anonymous, make sure the HDA is owned by the current session.
+            if not user:
+                current_history_id = trans.galaxy_session.current_history_id
+                assert hda.history.id == current_history_id, "Data does not belong to current user"
+            # If the user is known, make sure the HDA is owned by the current user.
             else:
-                hdca_ids.append(item["id"])
+                assert hda.history.user == user, "Data does not belong to current user"
 
-        history = self.history_manager.get_owned(self.decode_id(history_id), trans.user,
-                                                 current_history=trans.history)
-        hdas = self.__datasets_for_update(trans, history, hda_ids, payload)
-        rval = []
-        for hda in hdas:
-            self.__deserialize_dataset(hda, payload, trans)
-            rval.append(self.hda_serializer.serialize_to_view(hda,
-                                                              user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'summary')))
-        for hdca_id in hdca_ids:
-            self.__update_dataset_collection(trans, history_id, hdca_id, payload, **kwd)
-            dataset_collection_instance = self.__get_accessible_collection(trans, hdca_id, history_id)
-            rval.append(self.__collection_dict(trans, dataset_collection_instance, view="summary"))
-        return rval
+            # Ensure HDA is deleted
+            hda.deleted = True
+            # HDA is purgeable
+            # Decrease disk usage first
+            hda.purge_usage_from_quota(user)
+            # Mark purged
+            hda.purged = True
+            trans.sa_session.add(hda)
+            trans.log_event(f"HDA id {hda.id} has been purged")
+            trans.sa_session.flush()
+            # Don't delete anything if there are active HDAs or any LDDAs, even if
+            # the LDDAs are deleted.  Let the cleanup scripts get it in the latter
+            # case.
+            if hda.dataset.user_can_purge:
+                try:
+                    hda.dataset.full_delete()
+                    trans.log_event(
+                        f"Dataset id {hda.dataset.id} has been purged upon the the purge of HDA id {hda.id}"
+                    )
+                    trans.sa_session.add(hda.dataset)
+                except Exception:
+                    log.exception(f"Unable to purge dataset ({hda.dataset.id}) on purge of HDA ({hda.id}):")
+            trans.sa_session.flush()
+        except Exception:
+            msg = f"HDA purge failed (encoded: {dataset_id}, decoded: {id})"
+            log.exception(msg)
+            trans.log_event(msg)
+            message = "Dataset removal from disk failed"
+            status = "error"
+        return (message, status)
+
+    @web.expose
+    def delete(self, trans, dataset_id, filename, show_deleted_on_refresh=False):
+        message, status = self._delete(trans, dataset_id)
+        return trans.response.send_redirect(
+            web.url_for(
+                controller="root",
+                action="history",
+                show_deleted=show_deleted_on_refresh,
+                message=message,
+                status=status,
+            )
+        )
 
-    @expose_api_anonymous
-    def update(self, trans, history_id, id, payload, **kwd):
-        """
-        update( self, trans, history_id, id, payload, **kwd )
-        * PUT /api/histories/{history_id}/contents/{id}
-            updates the values for the history content item with the given ``id``
-
-        :type   history_id: str
-        :param  history_id: encoded id string of the items's History
-        :type   id:         str
-        :param  id:         the encoded id of the history item to update
-        :type   payload:    dict
-        :param  payload:    a dictionary containing any or all the
-            fields in :func:`galaxy.model.HistoryDatasetAssociation.to_dict`
-            and/or the following:
-
-            * annotation: an annotation for the HDA
-
-        :rtype:     dict
-        :returns:   an error object if an error occurred or a dictionary containing
-            any values that were different from the original and, therefore, updated
-        """
-        # TODO: PUT /api/histories/{encoded_history_id} payload = { rating: rating } (w/ no security checks)
-        contents_type = kwd.get('type', 'dataset')
-        if contents_type == "dataset":
-            return self.__update_dataset(trans, history_id, id, payload, **kwd)
-        elif contents_type == "dataset_collection":
-            return self.__update_dataset_collection(trans, history_id, id, payload, **kwd)
+    @web.expose
+    def delete_async(self, trans, dataset_id, filename):
+        message, status = self._delete(trans, dataset_id)
+        if status == "done":
+            return "OK"
         else:
-            return self.__handle_unknown_contents_type(trans, contents_type)
-
-    @expose_api_anonymous
-    def validate(self, trans, history_id, history_content_id, payload=None, **kwd):
-        """
-        update( self, trans, history_id, id, payload, **kwd )
-        * PUT /api/histories/{history_id}/contents/{id}/validate
-            updates the values for the history content item with the given ``id``
-
-        :type   history_id: str
-        :param  history_id: encoded id string of the items's History
-        :type   id:         str
-        :param  id:         the encoded id of the history item to validate
+            raise Exception(message)
 
-        :rtype:     dict
-        :returns:   TODO
-        """
-        decoded_id = self.decode_id(history_content_id)
-        history = self.history_manager.get_owned(self.decode_id(history_id), trans.user,
-                                                 current_history=trans.history)
-        hda = self.hda_manager.get_owned_ids([decoded_id], history=history)[0]
-        if hda:
-            self.hda_manager.set_metadata(trans, hda, overwrite=True, validate=True)
-        return {}
-
-    def __update_dataset(self, trans, history_id, id, payload, **kwd):
-        # anon user: ensure that history ids match up and the history is the current,
-        #   check for uploading, and use only the subset of attribute keys manipulatable by anon users
-        decoded_id = self.decode_id(id)
-        history = self.history_manager.get_owned(self.decode_id(history_id), trans.user,
-                                                 current_history=trans.history)
-        hda = self.__datasets_for_update(trans, history, [decoded_id], payload)[0]
-        if hda:
-            self.__deserialize_dataset(hda, payload, trans)
-            return self.hda_serializer.serialize_to_view(hda,
-                                                         user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
-
-        return {}
-
-    def __datasets_for_update(self, trans, history, hda_ids, payload):
-        anonymous_user = not trans.user_is_admin and trans.user is None
-        if anonymous_user:
-            anon_allowed_payload = {}
-            if 'deleted' in payload:
-                anon_allowed_payload['deleted'] = payload['deleted']
-            if 'visible' in payload:
-                anon_allowed_payload['visible'] = payload['visible']
-            payload = anon_allowed_payload
-
-        hdas = self.hda_manager.get_owned_ids(hda_ids, history=history)
-
-        # only check_state if not deleting, otherwise cannot delete uploading files
-        check_state = not payload.get('deleted', False)
-        if check_state:
-            for hda in hdas:
-                hda = self.hda_manager.error_if_uploading(hda)
-
-        return hdas
-
-    def __deserialize_dataset(self, hda, payload, trans):
-        self.hda_deserializer.deserialize(hda, payload, user=trans.user, trans=trans)
-        # TODO: this should be an effect of deleting the hda
-        if payload.get('deleted', False):
-            self.hda_manager.stop_creating_job(hda)
-
-    def __update_dataset_collection(self, trans, history_id, id, payload, **kwd):
-        return trans.app.dataset_collections_service.update(trans, "history", id, payload)
-
-    # TODO: allow anonymous del/purge and test security on this
-    @expose_api
-    def delete(self, trans, history_id, id, purge=False, recursive=False, **kwd):
-        """
-        delete( self, trans, history_id, id, **kwd )
-        * DELETE /api/histories/{history_id}/contents/{id}
-        * DELETE /api/histories/{history_id}/contents/{type}s/{id}
-            delete the history content with the given ``id`` and specified type (defaults to dataset)
-        .. note:: Currently does not stop any active jobs for which this dataset is an output.
-
-        :type   id:     str
-        :param  id:     the encoded id of the history to delete
-        :type   recursive:  bool
-        :param  recursive:  if True, and deleted an HDCA also delete containing HDAs
-        :type   purge:  bool
-        :param  purge:  if True, purge the target HDA or child HDAs of the target HDCA
-        :type   kwd:    dict
-        :param  kwd:    (optional) dictionary structure containing:
-
-            * payload:     a dictionary itself containing:
-                * purge:   if True, purge the HDA
-                * recursive: if True, see above.
-
-        .. note:: that payload optionally can be placed in the query string of the request.
-            This allows clients that strip the request body to still purge the dataset.
-
-        :rtype:     dict
-        :returns:   an error object if an error occurred or a dictionary containing:
-            * id:         the encoded id of the history,
-            * deleted:    if the history content was marked as deleted,
-            * purged:     if the history content was purged
-        """
-        contents_type = kwd.get('type', 'dataset')
-        if contents_type == "dataset":
-            return self.__delete_dataset(trans, history_id, id, purge=purge, **kwd)
-        elif contents_type == "dataset_collection":
-            purge = util.string_as_bool(purge)
-            recursive = util.string_as_bool(recursive)
-            if kwd.get('payload', None):
-                # payload takes priority
-                purge = util.string_as_bool(kwd['payload'].get('purge', purge))
-                recursive = util.string_as_bool(kwd['payload'].get('recursive', recursive))
+    @web.expose
+    def undelete(self, trans, dataset_id, filename):
+        message, status = self._undelete(trans, dataset_id)
+        return trans.response.send_redirect(
+            web.url_for(controller="root", action="history", show_deleted=True, message=message, status=status)
+        )
 
-            trans.app.dataset_collections_service.delete(trans, "history", id, recursive=recursive, purge=purge)
-            return {'id' : id, "deleted": True}
+    @web.expose
+    def undelete_async(self, trans, dataset_id, filename):
+        message, status = self._undelete(trans, dataset_id)
+        if status == "done":
+            return "OK"
         else:
-            return self.__handle_unknown_contents_type(trans, contents_type)
+            raise Exception(message)
 
-    def __delete_dataset(self, trans, history_id, id, purge, **kwd):
-        # get purge from the query or from the request body payload (a request body is optional here)
-        purge = util.string_as_bool(purge)
-        if kwd.get('payload', None):
-            # payload takes priority
-            purge = util.string_as_bool(kwd['payload'].get('purge', purge))
-
-        hda = self.hda_manager.get_owned(self.decode_id(id), trans.user, current_history=trans.history)
-        self.hda_manager.error_if_uploading(hda)
-
-        if purge:
-            self.hda_manager.purge(hda)
+    @web.expose
+    def unhide(self, trans, dataset_id, filename):
+        if self._unhide(trans, dataset_id):
+            return trans.response.send_redirect(web.url_for(controller="root", action="history", show_hidden=True))
+        raise Exception("Error unhiding")
+
+    @web.expose
+    def purge(self, trans, dataset_id, filename, show_deleted_on_refresh=False):
+        if trans.app.config.allow_user_dataset_purge:
+            message, status = self._purge(trans, dataset_id)
         else:
-            self.hda_manager.delete(hda)
-        return self.hda_serializer.serialize_to_view(hda,
-                                                     user=trans.user, trans=trans, **self._parse_serialization_params(kwd, 'detailed'))
-
-    def __handle_unknown_contents_type(self, trans, contents_type):
-        raise exceptions.UnknownContentsType('Unknown contents type: %s' % type)
-
-    def __index_v2(self, trans, history_id, **kwd):
-        """
-        index( self, trans, history_id, **kwd )
-        * GET /api/histories/{history_id}/contents
-            return a list of HDA data for the history with the given ``id``
-        .. note:: Anonymous users are allowed to get their current history contents
-
-        If ids is given, index returns a *more complete* json object for each
-        HDA in the ids list.
-
-        :type   history_id: str
-        :param  history_id: encoded id string of the HDA's History
-
-        :rtype:     list
-        :returns:   dictionaries containing summary or detailed HDA information
-
-        The following are optional parameters:
-            view:   string, one of ('summary','detailed'), defaults to 'summary'
-                    controls which set of properties to return
-            keys:   comma separated strings, unused by default
-                    keys/names of individual properties to return
-
-        If neither keys or views are sent, the default view (set of keys) is returned.
-        If both a view and keys are sent, the key list and the view's keys are
-        combined.
-        If keys are sent and no view, only those properties in keys are returned.
-
-        For which properties are available see:
-            galaxy/managers/hdas/HDASerializer
-        and:
-            galaxy/managers/collection_util
-
-        The list returned can be filtered by using two optional parameters:
-            q:      string, generally a property name to filter by followed
-                    by an (often optional) hyphen and operator string.
-            qv:     string, the value to filter by
-
-        ..example:
-            To filter the list to only those created after 2015-01-29,
-            the query string would look like:
-                '?q=create_time-gt&qv=2015-01-29'
-
-            Multiple filters can be sent in using multiple q/qv pairs:
-                '?q=create_time-gt&qv=2015-01-29&q=name-contains&qv=experiment-1'
-
-        The list returned can be paginated using two optional parameters:
-            limit:  integer, defaults to no value and no limit (return all)
-                    how many items to return
-            offset: integer, defaults to 0 and starts at the beginning
-                    skip the first ( offset - 1 ) items and begin returning
-                    at the Nth item
-
-        ..example:
-            limit and offset can be combined. Skip the first two and return five:
-                '?limit=5&offset=3'
-
-        The list returned can be ordered using the optional parameter:
-            order:  string containing one of the valid ordering attributes followed
-                    (optionally) by '-asc' or '-dsc' for ascending and descending
-                    order respectively. Orders can be stacked as a comma-
-                    separated list of values.
-
-        ..example:
-            To sort by name descending then create time descending:
-                '?order=name-dsc,create_time'
-
-        The ordering attributes and their default orders are:
-            hid defaults to 'hid-asc'
-            create_time defaults to 'create_time-dsc'
-            update_time defaults to 'update_time-dsc'
-            name    defaults to 'name-asc'
-
-        'order' defaults to 'hid-asc'
-        """
-        rval = []
+            message = "Removal of datasets by users is not allowed in this Galaxy instance.  Please contact your Galaxy administrator."
+            status = "error"
+        return trans.response.send_redirect(
+            web.url_for(
+                controller="root",
+                action="history",
+                show_deleted=show_deleted_on_refresh,
+                message=message,
+                status=status,
+            )
+        )
 
-        history = self.history_manager.get_accessible(self.decode_id(history_id), trans.user,
-            current_history=trans.history)
+    @web.expose
+    def purge_async(self, trans, dataset_id, filename):
+        if trans.app.config.allow_user_dataset_purge:
+            message, status = self._purge(trans, dataset_id)
+        else:
+            message = "Removal of datasets by users is not allowed in this Galaxy instance.  Please contact your Galaxy administrator."
+            status = "error"
+        if status == "done":
+            return "OK"
+        else:
+            raise Exception(message)
 
-        filter_params = self.parse_filter_params(kwd)
-        filters = self.history_contents_filters.parse_filters(filter_params)
-        limit, offset = self.parse_limit_offset(kwd)
-        order_by = self._parse_order_by(manager=self.history_contents_manager, order_by_string=kwd.get('order', 'hid-asc'))
-        serialization_params = self._parse_serialization_params(kwd, 'summary')
-        # TODO: > 16.04: remove these
-        # TODO: remove 'dataset_details' and the following section when the UI doesn't need it
-        # details param allows a mixed set of summary and detailed hdas
-        # Ever more convoluted due to backwards compat..., details
-        # should be considered deprecated in favor of more specific
-        # dataset_details (and to be implemented dataset_collection_details).
-        details = kwd.get('details', [])
-        if details and details != 'all':
-            details = util.listify(details)
-        view = serialization_params.pop('view')
-
-        contents = self.history_contents_manager.contents(history,
-            filters=filters, limit=limit, offset=offset, order_by=order_by)
-        for content in contents:
-
-            # TODO: remove split
-            if isinstance(content, trans.app.model.HistoryDatasetAssociation):
-                # TODO: remove split
-                if details == 'all' or trans.security.encode_id(content.id) in details:
-                    rval.append(self.hda_serializer.serialize_to_view(content,
-                        user=trans.user, trans=trans, view='detailed', **serialization_params))
+    @web.expose
+    def copy_datasets(
+        self,
+        trans,
+        source_history=None,
+        source_content_ids="",
+        target_history_id=None,
+        target_history_ids="",
+        new_history_name="",
+        do_copy=False,
+        **kwd,
+    ):
+        user = trans.get_user()
+        if source_history is not None:
+            decoded_source_history_id = self.decode_id(source_history)
+            history = self.history_manager.get_owned(
+                decoded_source_history_id, trans.user, current_history=trans.history
+            )
+            current_history = trans.get_history()
+        else:
+            history = current_history = trans.get_history()
+        refresh_frames = []
+        if source_content_ids:
+            if not isinstance(source_content_ids, list):
+                source_content_ids = source_content_ids.split(",")
+            encoded_dataset_collection_ids = [
+                s[len("dataset_collection|") :] for s in source_content_ids if s.startswith("dataset_collection|")
+            ]
+            encoded_dataset_ids = [s[len("dataset|") :] for s in source_content_ids if s.startswith("dataset|")]
+            decoded_dataset_collection_ids = set(map(self.decode_id, encoded_dataset_collection_ids))
+            decoded_dataset_ids = set(map(self.decode_id, encoded_dataset_ids))
+        else:
+            decoded_dataset_collection_ids = []
+            decoded_dataset_ids = []
+        if new_history_name:
+            target_history_ids = []
+        else:
+            if target_history_id:
+                target_history_ids = [self.decode_id(target_history_id)]
+            elif target_history_ids:
+                if not isinstance(target_history_ids, list):
+                    target_history_ids = target_history_ids.split(",")
+                target_history_ids = list({self.decode_id(h) for h in target_history_ids if h})
+            else:
+                target_history_ids = []
+        done_msg = error_msg = ""
+        new_history = None
+        if do_copy:
+            invalid_contents = 0
+            if not (decoded_dataset_ids or decoded_dataset_collection_ids) or not (
+                target_history_ids or new_history_name
+            ):
+                error_msg = "You must provide both source datasets and target histories. "
+            else:
+                if new_history_name:
+                    new_history = trans.app.model.History()
+                    new_history.name = new_history_name
+                    new_history.user = user
+                    trans.sa_session.add(new_history)
+                    trans.sa_session.flush()
+                    target_history_ids.append(new_history.id)
+                if user:
+                    target_histories = [
+                        hist
+                        for hist in map(trans.sa_session.query(trans.app.model.History).get, target_history_ids)
+                        if hist is not None and hist.user == user
+                    ]
                 else:
-                    rval.append(self.hda_serializer.serialize_to_view(content,
-                        user=trans.user, trans=trans, view=view, **serialization_params))
-
-            elif isinstance(content, trans.app.model.HistoryDatasetCollectionAssociation):
-                collection = self.hdca_serializer.serialize_to_view(content,
-                    user=trans.user, trans=trans, view=view, **serialization_params)
-                rval.append(collection)
-
-        return rval
-
-    def encode_type_id(self, type_id):
-        TYPE_ID_SEP = '-'
-        split = type_id.split(TYPE_ID_SEP, 1)
-        return TYPE_ID_SEP.join((split[0], self.app.security.encode_id(split[1])))
-
-    @expose_api_raw
-    def archive(self, trans, history_id, filename='', format='tgz', dry_run=True, **kwd):
-        """
-        archive( self, trans, history_id, filename='', format='tgz', dry_run=True, **kwd )
-        * GET /api/histories/{history_id}/contents/archive/{id}
-        * GET /api/histories/{history_id}/contents/archive/{filename}.{format}
-            build and return a compressed archive of the selected history contents
-
-        :type   filename:  string
-        :param  filename:  (optional) archive name (defaults to history name)
-        :type   dry_run:   boolean
-        :param  dry_run:   (optional) if True, return the archive and file paths only
-                           as json and not an archive file
-
-        :returns:   archive file for download
+                    target_histories = [history]
+                if len(target_histories) != len(target_history_ids):
+                    error_msg = (
+                        error_msg
+                        + "You do not have permission to add datasets to %i requested histories.  "
+                        % (len(target_history_ids) - len(target_histories))
+                    )
+                source_contents = list(
+                    map(trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get, decoded_dataset_ids)
+                )
+                source_contents.extend(
+                    map(
+                        trans.sa_session.query(trans.app.model.HistoryDatasetCollectionAssociation).get,
+                        decoded_dataset_collection_ids,
+                    )
+                )
+                source_contents.sort(key=lambda content: content.hid)
+                for content in source_contents:
+                    if content is None:
+                        error_msg = f"{error_msg}You tried to copy a dataset that does not exist. "
+                        invalid_contents += 1
+                    elif content.history != history:
+                        error_msg = f"{error_msg}You tried to copy a dataset which is not in your current history. "
+                        invalid_contents += 1
+                    else:
+                        for hist in target_histories:
+                            if content.history_content_type == "dataset":
+                                copy = content.copy(flush=False)
+                                hist.stage_addition(copy)
+                            else:
+                                copy = content.copy(element_destination=hist)
+                            if user:
+                                copy.copy_tags_from(user, content)
+                        for hist in target_histories:
+                            hist.add_pending_items()
+                trans.sa_session.flush()
+                if current_history in target_histories:
+                    refresh_frames = ["history"]
+                hist_names_str = ", ".join(
+                    '<a href="%s" target="_top">%s</a>'
+                    % (
+                        url_for(
+                            controller="history", action="switch_to_history", hist_id=trans.security.encode_id(hist.id)
+                        ),
+                        escape(hist.name),
+                    )
+                    for hist in target_histories
+                )
+                num_source = len(source_content_ids) - invalid_contents
+                num_target = len(target_histories)
+                done_msg = "%i %s copied to %i %s: %s." % (
+                    num_source,
+                    inflector.cond_plural(num_source, "dataset"),
+                    num_target,
+                    inflector.cond_plural(num_target, "history"),
+                    hist_names_str,
+                )
+                trans.sa_session.refresh(history)
+        source_contents = history.active_contents
+        target_histories = [history]
+        if user:
+            target_histories = user.active_histories
+        return trans.fill_template(
+            "/dataset/copy_view.mako",
+            source_history=history,
+            current_history=current_history,
+            source_content_ids=source_content_ids,
+            target_history_id=target_history_id,
+            target_history_ids=target_history_ids,
+            source_contents=source_contents,
+            target_histories=target_histories,
+            new_history_name=new_history_name,
+            done_msg=done_msg,
+            error_msg=error_msg,
+            refresh_frames=refresh_frames,
+        )
 
-        .. note:: this is a volatile endpoint and settings and behavior may change.
-        """
-        # roughly from: http://stackoverflow.com/a/31976060 (windows, linux)
-        invalid_filename_char_regex = re.compile(r'[:<>|\\\/\?\* "]')
-        # path format string - dot separator between id and name
-        id_name_format = u'{}.{}'
-
-        def name_to_filename(name, max_length=150, replace_with=u'_'):
-            # TODO: seems like shortening unicode with [:] would cause unpredictable display strings
-            return invalid_filename_char_regex.sub(replace_with, name)[0:max_length]
-
-        # given a set of parents for a dataset (HDCAs, DC, DCEs, etc.) - build a directory structure that
-        # (roughly) recreates the nesting in the contents using the parent names and ids
-        def build_path_from_parents(parents):
-            parent_names = []
-            for parent in parents:
-                # an HDCA
-                if hasattr(parent, 'hid'):
-                    name = name_to_filename(parent.name)
-                    parent_names.append(id_name_format.format(parent.hid, name))
-                # a DCE
-                elif hasattr(parent, 'element_index'):
-                    name = name_to_filename(parent.element_identifier)
-                    parent_names.append(id_name_format.format(parent.element_index, name))
-            # NOTE: DCs are skipped and use the wrapping DCE info instead
-            return parent_names
-
-        # get the history used for the contents query and check for accessibility
-        history = self.history_manager.get_accessible(trans.security.decode_id(history_id), trans.user)
-        archive_base_name = filename or name_to_filename(history.name)
-
-        # this is the fn applied to each dataset contained in the query
-        paths_and_files = []
-
-        def build_archive_files_and_paths(content, *parents):
-            archive_path = archive_base_name
-            if not self.hda_manager.is_accessible(content, trans.user):
-                # if the underlying dataset is not accessible, skip it silently
-                return
-
-            content_container_id = content.hid
-            content_name = name_to_filename(content.name)
-            if parents:
-                if hasattr(parents[0], 'element_index'):
-                    # if content is directly wrapped in a DCE, strip it from parents (and the resulting path)
-                    # and instead replace the content id and name with the DCE index and identifier
-                    parent_dce, parents = parents[0], parents[1:]
-                    content_container_id = parent_dce.element_index
-                    content_name = name_to_filename(parent_dce.element_identifier)
-                # reverse for path from parents: oldest parent first
-                archive_path = os.path.join(archive_path, *build_path_from_parents(parents)[::-1])
-                # TODO: this is brute force - building the path each time instead of re-using it
-                # possibly cache
-
-            # add the name as the last element in the archive path
-            content_id_and_name = id_name_format.format(content_container_id, content_name)
-            archive_path = os.path.join(archive_path, content_id_and_name)
-
-            # ---- for composite files, we use id and name for a directory and, inside that, ...
-            if self.hda_manager.is_composite(content):
-                # ...save the 'main' composite file (gen. html)
-                paths_and_files.append((content.file_name, os.path.join(archive_path, content.name + '.html')))
-                for extra_file in self.hda_manager.extra_files(content):
-                    extra_file_basename = os.path.basename(extra_file)
-                    archive_extra_file_path = os.path.join(archive_path, extra_file_basename)
-                    # ...and one for each file in the composite
-                    paths_and_files.append((extra_file, archive_extra_file_path))
+    def _copy_datasets(self, trans, dataset_ids, target_histories, imported=False):
+        """Helper method for copying datasets."""
+        user = trans.get_user()
+        done_msg = error_msg = ""
+
+        invalid_datasets = 0
+        if not dataset_ids or not target_histories:
+            error_msg = "You must provide both source datasets and target histories."
+        else:
+            # User must own target histories to copy datasets to them.
+            for history in target_histories:
+                if user != history.user:
+                    error_msg = (
+                        error_msg
+                        + "You do not have permission to add datasets to %i requested histories.  "
+                        % (len(target_histories))
+                    )
+            for dataset_id in dataset_ids:
+                decoded_id = self.decode_id(dataset_id)
+                data = self.hda_manager.get_accessible(decoded_id, trans.user)
+                data = self.hda_manager.error_if_uploading(data)
+
+                if data is None:
+                    error_msg = f"{error_msg}You tried to copy a dataset that does not exist or that you do not have access to.  "
+                    invalid_datasets += 1
+                else:
+                    for hist in target_histories:
+                        dataset_copy = data.copy()
+                        if imported:
+                            dataset_copy.name = f"imported: {dataset_copy.name}"
+                        hist.add_dataset(dataset_copy)
+            trans.sa_session.flush()
+            num_datasets_copied = len(dataset_ids) - invalid_datasets
+            done_msg = "%i dataset%s copied to %i histor%s." % (
+                num_datasets_copied,
+                iff(num_datasets_copied == 1, "", "s"),
+                len(target_histories),
+                iff(len(target_histories) == 1, "y", "ies"),
+            )
+            trans.sa_session.refresh(history)
 
-            # ---- for single files, we add the true extension to id and name and store that single filename
-            else:
-                # some dataset names can contain their original file extensions, don't repeat
-                if not archive_path.endswith('.' + content.extension):
-                    archive_path += '.' + content.extension
-                paths_and_files.append((content.file_name, archive_path))
-
-        # filter the contents that contain datasets using any filters possible from index above and map the datasets
-        filter_params = self.parse_filter_params(kwd)
-        filters = self.history_contents_filters.parse_filters(filter_params)
-        self.history_contents_manager.map_datasets(history, build_archive_files_and_paths, filters=filters)
-
-        # if dry_run, return the structure as json for debugging
-        if dry_run == 'True':
-            trans.response.headers['Content-Type'] = 'application/json'
-            return safe_dumps(paths_and_files)
-
-        # create the archive, add the dataset files, then stream the archive as a download
-        archive_type_string = 'w|gz'
-        archive_ext = 'tgz'
-        if self.app.config.upstream_gzip:
-            archive_type_string = 'w|'
-            archive_ext = 'tar'
-        archive = StreamBall(archive_type_string)
-
-        for file_path, archive_path in paths_and_files:
-            archive.add(file_path, archive_path)
-
-        archive_name = '.'.join((archive_base_name, archive_ext))
-        trans.response.set_content_type("application/x-tar")
-        trans.response.headers["Content-Disposition"] = 'attachment; filename="{}"'.format(archive_name)
-        archive.wsgi_status = trans.response.wsgi_status()
-        archive.wsgi_headeritems = trans.response.wsgi_headeritems()
-        return archive.stream
+        if error_msg != "":
+            status = ERROR
+            message = error_msg
+        else:
+            status = SUCCESS
+            message = done_msg
+        return status, message
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/item_tags.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/item_tags.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,65 +1,59 @@
 """
 API operations related to tagging items.
 """
 import logging
 
 from galaxy import exceptions
 from galaxy.web import expose_api
-from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    UsesTagsMixin
-)
+from galaxy.webapps.base.controller import UsesTagsMixin
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class BaseItemTagsController(BaseAPIController, UsesTagsMixin):
-    """
-    """
+class BaseItemTagsController(BaseGalaxyAPIController, UsesTagsMixin):
+    """ """
+
     @expose_api
     def index(self, trans, **kwd):
-        """
-        """
+        """ """
         tags = self._get_user_tags(trans, self.tagged_item_class, kwd[self.tagged_item_id])
-        return [self._api_value(tag, trans, view='collection') for tag in tags]
+        return [self._api_value(tag, trans, view="collection") for tag in tags]
 
     @expose_api
     def show(self, trans, tag_name, **kwd):
-        """
-        """
+        """ """
         tag = self._get_item_tag_assoc(trans, self.tagged_item_class, kwd[self.tagged_item_id], tag_name)
         if not tag:
             raise exceptions.ObjectNotFound("Failed to retrieve specified tag.")
         return self._api_value(tag, trans)
 
     @expose_api
     def create(self, trans, tag_name, payload=None, **kwd):
-        """
-        """
+        """ """
         payload = payload or {}
         value = payload.get("value", None)
         tag = self._apply_item_tag(trans, self.tagged_item_class, kwd[self.tagged_item_id], tag_name, value)
         return self._api_value(tag, trans)
 
     # Not handling these differently at this time
     update = create
 
     @expose_api
     def delete(self, trans, tag_name, **kwd):
-        """
-        """
+        """ """
         deleted = self._remove_items_tag(trans, self.tagged_item_class, kwd[self.tagged_item_id], tag_name)
         if not deleted:
             raise exceptions.RequestParameterInvalidException("Failed to delete specified tag.")
         # TODO: ugh - 204 would be better
-        return 'OK'
+        return "OK"
 
-    def _api_value(self, tag, trans, view='element'):
-        return tag.to_dict(view=view, value_mapper={'id': trans.security.encode_id})
+    def _api_value(self, tag, trans, view="element"):
+        return tag.to_dict(view=view, value_mapper={"id": trans.security.encode_id})
 
 
 class HistoryContentTagsController(BaseItemTagsController):
     controller_name = "history_content_tags"
     tagged_item_class = "HistoryDatasetAssociation"
     tagged_item_id = "history_content_id"
 
@@ -71,8 +65,9 @@
 
 
 class WorkflowTagsController(BaseItemTagsController):
     controller_name = "workflow_tags"
     tagged_item_class = "StoredWorkflow"
     tagged_item_id = "workflow_id"
 
+
 # TODO: Visualization and Pages once APIs for those are available
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/job_files.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/job_files.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,62 +4,63 @@
 import logging
 import os
 import shutil
 
 from galaxy import (
     exceptions,
     model,
-    util
+    util,
 )
 from galaxy.web import (
     expose_api_anonymous_and_sessionless,
     expose_api_raw_anonymous_and_sessionless,
 )
-from galaxy.webapps.base.controller import BaseAPIController
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class JobFilesAPIController(BaseAPIController):
-    """ This job files controller allows remote job running mechanisms to
+class JobFilesAPIController(BaseGalaxyAPIController):
+    """This job files controller allows remote job running mechanisms to
     read and modify the current state of files for queued and running jobs.
     It is certainly not meant to represent part of Galaxy's stable, user
     facing API.
 
     Furthermore, even if a user key corresponds to the user running the job,
     it should not be accepted for authorization - this API allows access to
     low-level unfiltered files and such authorization would break Galaxy's
     security model for tool execution.
     """
 
     @expose_api_raw_anonymous_and_sessionless
     def index(self, trans, job_id, **kwargs):
         """
-        index( self, trans, job_id, **kwargs )
-        * GET /api/jobs/{job_id}/files
-            Get a file required to staging a job (proper datasets, extra inputs,
-            task-split inputs, working directory files).
+        GET /api/jobs/{job_id}/files
+
+        Get a file required to staging a job (proper datasets, extra inputs,
+        task-split inputs, working directory files).
 
         :type   job_id: str
         :param  job_id: encoded id string of the job
         :type   path: str
         :param  path: Path to file.
         :type   job_key: str
         :param  job_key: A key used to authenticate this request as acting on
                          behalf or a job runner for the specified job.
+
         ..note:
             This API method is intended only for consumption by job runners,
             not end users.
 
         :rtype:     binary
         :returns:   contents of file
         """
         self.__authorize_job_access(trans, job_id, **kwargs)
         path = kwargs.get("path", None)
-        return open(path, 'rb')
+        return open(path, "rb")
 
     @expose_api_anonymous_and_sessionless
     def create(self, trans, job_id, payload, **kwargs):
         """
         create( self, trans, job_id, payload, **kwargs )
         * POST /api/jobs/{job_id}/files
             Populate an output file (formal dataset, task split part, working
@@ -82,27 +83,28 @@
         :returns:   an okay message
         """
         job = self.__authorize_job_access(trans, job_id, **payload)
         path = payload.get("path")
         self.__check_job_can_write_to_path(trans, job, path)
 
         # Is this writing an unneeded file? Should this just copy in Python?
-        if '__file_path' in payload:
-            file_path = payload.get('__file_path')
+        if "__file_path" in payload:
+            file_path = payload.get("__file_path")
             upload_store = trans.app.config.nginx_upload_job_files_store
-            assert upload_store, ("Request appears to have been processed by"
-                                  " nginx_upload_module but Galaxy is not"
-                                  " configured to recognize it")
-            assert file_path.startswith(upload_store), \
-                ("Filename provided by nginx (%s) is not in correct"
-                 " directory (%s)" % (file_path, upload_store))
+            assert upload_store, (
+                "Request appears to have been processed by"
+                " nginx_upload_module but Galaxy is not"
+                " configured to recognize it"
+            )
+            assert file_path.startswith(
+                upload_store
+            ), "Filename provided by nginx (%s) is not in correct" " directory (%s)" % (file_path, upload_store)
             input_file = open(file_path)
         else:
-            input_file = payload.get("file",
-                                     payload.get("__file", None)).file
+            input_file = payload.get("file", payload.get("__file", None)).file
         target_dir = os.path.dirname(path)
         util.safe_makedirs(target_dir)
         try:
             shutil.move(input_file.name, path)
         finally:
             try:
                 input_file.close()
@@ -111,51 +113,44 @@
                 # tempfile has moved and Python wants to delete it.
                 pass
         return {"message": "ok"}
 
     def __authorize_job_access(self, trans, encoded_job_id, **kwargs):
         for key in ["path", "job_key"]:
             if key not in kwargs:
-                error_message = "Job files action requires a valid '%s'." % key
+                error_message = f"Job files action requires a valid '{key}'."
                 raise exceptions.ObjectAttributeMissingException(error_message)
 
         job_id = trans.security.decode_id(encoded_job_id)
         job_key = trans.security.encode_id(job_id, kind="jobs_files")
-        if not util.safe_str_cmp(kwargs["job_key"], job_key):
+        if not util.safe_str_cmp(str(kwargs["job_key"]), job_key):
             raise exceptions.ItemAccessibilityException("Invalid job_key supplied.")
 
         # Verify job is active. Don't update the contents of complete jobs.
         job = trans.sa_session.query(model.Job).get(job_id)
         if job.finished:
             error_message = "Attempting to read or modify the files of a job that has already completed."
             raise exceptions.ItemAccessibilityException(error_message)
         return job
 
     def __check_job_can_write_to_path(self, trans, job, path):
-        """ Verify an idealized job runner should actually be able to write to
+        """Verify an idealized job runner should actually be able to write to
         the specified path - it must be a dataset output, a dataset "extra
         file", or a some place in the working directory of this job.
 
         Would like similar checks for reading the unstructured nature of loc
         files make this very difficult. (See abandoned work here
         https://gist.github.com/jmchilton/9103619.)
         """
         in_work_dir = self.__in_working_directory(job, path, trans.app)
-        allow_temp_dir_file = self.__is_allowed_temp_dir_file(trans.app, job, path)
-        if not in_work_dir and not allow_temp_dir_file and not self.__is_output_dataset_path(job, path):
+        if not in_work_dir and not self.__is_output_dataset_path(job, path):
             raise exceptions.ItemAccessibilityException("Job is not authorized to write to supplied path.")
 
-    def __is_allowed_temp_dir_file(self, app, job, path):
-        # grrr.. need to get away from new_file_path - these should be written
-        # to job working directory like metadata files.
-        in_temp_dir = util.in_directory(path, app.config.new_file_path)
-        return in_temp_dir and os.path.split(path)[-1].startswith("GALAXY_VERSION_")
-
     def __is_output_dataset_path(self, job, path):
-        """ Check if is an output path for this job or a file in the an
+        """Check if is an output path for this job or a file in the an
         output's extra files path.
         """
         da_lists = [job.output_datasets, job.output_library_datasets]
         for da_list in da_lists:
             for job_dataset_association in da_list:
                 dataset = job_dataset_association.dataset
                 if not dataset:
@@ -163,9 +158,11 @@
                 if os.path.abspath(dataset.file_name) == os.path.abspath(path):
                     return True
                 elif util.in_directory(path, dataset.extra_files_path):
                     return True
         return False
 
     def __in_working_directory(self, job, path, app):
-        working_directory = app.object_store.get_filename(job, base_dir='job_work', dir_only=True, extra_dir=str(job.id))
+        working_directory = app.object_store.get_filename(
+            job, base_dir="job_work", dir_only=True, extra_dir=str(job.id)
+        )
         return util.in_directory(path, working_directory)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/job_ports.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/job_ports.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 """ API for asynchronous job running mechanisms can use to fetch or put files
 related to running and queued jobs.
 """
 from galaxy.job_execution.ports import JobPortsView
+from galaxy.structured_app import StructuredApp
 from galaxy.web import expose_api_anonymous_and_sessionless
-from galaxy.webapps.base.controller import BaseAPIController
+from . import BaseGalaxyAPIController
 
 
-class JobPortsAPIController(BaseAPIController):
-    """ This job files controller allows remote job running mechanisms to
+class JobPortsAPIController(BaseGalaxyAPIController):
+    """This job files controller allows remote job running mechanisms to
     modify the current state of ports for queued and running jobs.
     It is certainly not meant to represent part of Galaxy's stable, user
     facing API.
 
     See the JobFiles API for information about per-job API keys.
     """
 
-    def __init__(self, app):
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
         self._job_ports_view = JobPortsView(app)
 
     @expose_api_anonymous_and_sessionless
     def create(self, trans, job_id, payload, **kwargs):
         """
         create( self, trans, job_id, payload, **kwargs )
         * POST /api/jobs/{job_id}/ports
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/library_datasets.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/library_datasets.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,63 +1,57 @@
 """API operations on the library datasets."""
 import glob
 import logging
 import os
 import os.path
 import string
-import tempfile
-import zipfile
 from json import dumps
 
-from paste.httpexceptions import HTTPBadRequest, HTTPInternalServerError
+from paste.httpexceptions import (
+    HTTPBadRequest,
+    HTTPInternalServerError,
+)
 
 from galaxy import (
     exceptions,
     util,
-    web
+    web,
 )
 from galaxy.actions.library import LibraryActions
 from galaxy.exceptions import ObjectNotFound
 from galaxy.managers import (
     base as managers_base,
     folders,
     lddas,
     library_datasets,
-    roles
+    roles,
 )
+from galaxy.structured_app import StructuredApp
 from galaxy.tools.actions import upload_common
 from galaxy.tools.parameters import populate_state
 from galaxy.util.path import (
     full_path_permission_for_user,
     safe_contains,
     safe_relpath,
     unsafe_walk,
 )
-from galaxy.util.streamball import StreamBall
+from galaxy.util.zipstream import ZipstreamWrapper
 from galaxy.web import (
     expose_api,
     expose_api_anonymous,
 )
-from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    UsesVisualizationMixin,
-)
-
-try:
-    maketrans = str.maketrans
-except AttributeError:
-    from string import maketrans
+from galaxy.webapps.base.controller import UsesVisualizationMixin
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class LibraryDatasetsController(BaseAPIController, UsesVisualizationMixin, LibraryActions):
-
-    def __init__(self, app):
-        super(LibraryDatasetsController, self).__init__(app)
+class LibraryDatasetsController(BaseGalaxyAPIController, UsesVisualizationMixin, LibraryActions):
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
         self.app = app
         self.folder_manager = folders.FolderManager()
         self.role_manager = roles.RoleManager(app)
         self.ld_manager = library_datasets.LibraryDatasetsManager(app)
         self.ldda_manager = lddas.LDDAManager(app)
 
     @expose_api_anonymous
@@ -94,20 +88,22 @@
         :rtype:     dictionary
 
         :raises: ObjectNotFound
         """
         library_dataset = self.ld_manager.get(trans, managers_base.decode_id(self.app, encoded_dataset_id))
 
         try:
-            ldda = self.get_library_dataset_dataset_association(trans, id=encoded_ldda_id, check_ownership=False, check_accessible=False)
+            ldda = self.get_library_dataset_dataset_association(
+                trans, id=encoded_ldda_id, check_ownership=False, check_accessible=False
+            )
         except Exception as e:
-            raise exceptions.ObjectNotFound('Requested version of library dataset was not found.' + util.unicodify(e))
+            raise exceptions.ObjectNotFound(f"Requested version of library dataset was not found.{util.unicodify(e)}")
 
         if ldda not in library_dataset.expired_datasets:
-            raise exceptions.ObjectNotFound('Given library dataset does not have the requested version.')
+            raise exceptions.ObjectNotFound("Given library dataset does not have the requested version.")
 
         rval = trans.security.encode_all_ids(ldda.to_dict())
         return rval
 
     @expose_api
     def show_roles(self, trans, encoded_dataset_id, **kwd):
         """
@@ -129,38 +125,42 @@
         """
         current_user_roles = trans.get_current_user_roles()
         library_dataset = self.ld_manager.get(trans, managers_base.decode_id(self.app, encoded_dataset_id))
         dataset = library_dataset.library_dataset_dataset_association.dataset
         # User has to have manage permissions permission in order to see the roles.
         can_manage = trans.app.security_agent.can_manage_dataset(current_user_roles, dataset) or trans.user_is_admin
         if not can_manage:
-            raise exceptions.InsufficientPermissionsException('You do not have proper permission to access permissions.')
-        scope = kwd.get('scope', None)
-        if scope in ['current', None]:
+            raise exceptions.InsufficientPermissionsException(
+                "You do not have proper permission to access permissions."
+            )
+        scope = kwd.get("scope", None)
+        if scope in ["current", None]:
             return self._get_current_roles(trans, library_dataset)
-        elif scope in ['available']:
-            page = kwd.get('page', None)
+        elif scope in ["available"]:
+            page = kwd.get("page", None)
             if page is not None:
                 page = int(page)
             else:
                 page = 1
-            page_limit = kwd.get('page_limit', None)
+            page_limit = kwd.get("page_limit", None)
             if page_limit is not None:
                 page_limit = int(page_limit)
             else:
                 page_limit = 10
-            query = kwd.get('q', None)
+            query = kwd.get("q", None)
             roles, total_roles = trans.app.security_agent.get_valid_roles(trans, dataset, query, page, page_limit)
             return_roles = []
             for role in roles:
                 role_id = trans.security.encode_id(role.id)
                 return_roles.append(dict(id=role_id, name=role.name, type=role.type))
             return dict(roles=return_roles, page=page, page_limit=page_limit, total=total_roles)
         else:
-            raise exceptions.RequestParameterInvalidException("The value of 'scope' parameter is invalid. Alllowed values: current, available")
+            raise exceptions.RequestParameterInvalidException(
+                "The value of 'scope' parameter is invalid. Alllowed values: current, available"
+            )
 
     def _get_current_roles(self, trans, library_dataset):
         """
         Find all roles currently connected to relevant permissions
         on the library dataset and the underlying dataset.
 
         :param  library_dataset:      the model object
@@ -207,76 +207,86 @@
         POST /api/libraries/datasets/{encoded_dataset_id}/permissions
 
         Set permissions of the given library dataset to the given role ids.
 
         :param  encoded_dataset_id:      the encoded id of the dataset to update permissions of
         :type   encoded_dataset_id:      an encoded id string
         :param   payload: dictionary structure containing:
+
             :param  action:     (required) describes what action should be performed
                                 available actions: make_private, remove_restrictions, set_permissions
             :type   action:     string
             :param  access_ids[]:      list of Role.id defining roles that should have access permission on the dataset
             :type   access_ids[]:      string or list
             :param  manage_ids[]:      list of Role.id defining roles that should have manage permission on the dataset
             :type   manage_ids[]:      string or list
             :param  modify_ids[]:      list of Role.id defining roles that should have modify permission on the library dataset item
             :type   modify_ids[]:      string or list
+
         :type:      dictionary
 
         :returns:   dict of current roles for all available permission types
         :rtype:     dictionary
 
         :raises: RequestParameterInvalidException, ObjectNotFound, InsufficientPermissionsException, InternalServerError
                     RequestParameterMissingException
         """
         if payload:
             kwd.update(payload)
-        action = kwd.get('action', None)
-        if action not in ['remove_restrictions', 'make_private', 'set_permissions']:
-            raise exceptions.RequestParameterInvalidException('The mandatory parameter "action" has an invalid value. '
-                                                              'Allowed values are: "remove_restrictions", "make_private", "set_permissions"')
+        action = kwd.get("action", None)
+        if action not in ["remove_restrictions", "make_private", "set_permissions"]:
+            raise exceptions.RequestParameterInvalidException(
+                'The mandatory parameter "action" has an invalid value. '
+                'Allowed values are: "remove_restrictions", "make_private", "set_permissions"'
+            )
         library_dataset = self.ld_manager.get(trans, managers_base.decode_id(self.app, encoded_dataset_id))
         # Some permissions are attached directly to the underlying dataset.
         dataset = library_dataset.library_dataset_dataset_association.dataset
         current_user_roles = trans.get_current_user_roles()
         can_manage = trans.app.security_agent.can_manage_dataset(current_user_roles, dataset) or trans.user_is_admin
         if not can_manage:
-            raise exceptions.InsufficientPermissionsException('You do not have proper permissions to manage permissions on this dataset.')
-        new_access_roles_ids = util.listify(kwd.get('access_ids[]', None))
-        new_manage_roles_ids = util.listify(kwd.get('manage_ids[]', None))
-        new_modify_roles_ids = util.listify(kwd.get('modify_ids[]', None))
-        if action == 'remove_restrictions':
+            raise exceptions.InsufficientPermissionsException(
+                "You do not have proper permissions to manage permissions on this dataset."
+            )
+        new_access_roles_ids = util.listify(kwd.get("access_ids[]", None))
+        new_manage_roles_ids = util.listify(kwd.get("manage_ids[]", None))
+        new_modify_roles_ids = util.listify(kwd.get("modify_ids[]", None))
+        if action == "remove_restrictions":
             trans.app.security_agent.make_dataset_public(dataset)
             if not trans.app.security_agent.dataset_is_public(dataset):
-                raise exceptions.InternalServerError('An error occurred while making dataset public.')
-        elif action == 'make_private':
+                raise exceptions.InternalServerError("An error occurred while making dataset public.")
+        elif action == "make_private":
             if not trans.app.security_agent.dataset_is_private_to_user(trans, dataset):
                 private_role = trans.app.security_agent.get_private_user_role(trans.user)
-                dp = trans.app.model.DatasetPermissions(trans.app.security_agent.permitted_actions.DATASET_ACCESS.action, dataset, private_role)
+                dp = trans.app.model.DatasetPermissions(
+                    trans.app.security_agent.permitted_actions.DATASET_ACCESS.action, dataset, private_role
+                )
                 trans.sa_session.add(dp)
                 trans.sa_session.flush()
             if not trans.app.security_agent.dataset_is_private_to_user(trans, dataset):
                 # Check again and inform the user if dataset is not private.
-                raise exceptions.InternalServerError('An error occurred and the dataset is NOT private.')
-        elif action == 'set_permissions':
+                raise exceptions.InternalServerError("An error occurred and the dataset is NOT private.")
+        elif action == "set_permissions":
             # ACCESS DATASET ROLES
             valid_access_roles = []
             invalid_access_roles_ids = []
             valid_roles_for_dataset, total_roles = trans.app.security_agent.get_valid_roles(trans, dataset)
             if new_access_roles_ids is None:
                 trans.app.security_agent.make_dataset_public(dataset)
             else:
                 for role_id in new_access_roles_ids:
                     role = self.role_manager.get(trans, managers_base.decode_id(self.app, role_id))
                     if role in valid_roles_for_dataset:
                         valid_access_roles.append(role)
                     else:
                         invalid_access_roles_ids.append(role_id)
                 if len(invalid_access_roles_ids) > 0:
-                    log.warning("The following roles could not be added to the dataset access permission: " + str(invalid_access_roles_ids))
+                    log.warning(
+                        f"The following roles could not be added to the dataset access permission: {str(invalid_access_roles_ids)}"
+                    )
 
                 access_permission = dict(access=valid_access_roles)
                 trans.app.security_agent.set_dataset_permission(dataset, access_permission)
 
             # MANAGE DATASET ROLES
             valid_manage_roles = []
             invalid_manage_roles_ids = []
@@ -284,30 +294,36 @@
             for role_id in new_manage_roles_ids:
                 role = self.role_manager.get(trans, managers_base.decode_id(self.app, role_id))
                 if role in valid_roles_for_dataset:
                     valid_manage_roles.append(role)
                 else:
                     invalid_manage_roles_ids.append(role_id)
             if len(invalid_manage_roles_ids) > 0:
-                log.warning("The following roles could not be added to the dataset manage permission: " + str(invalid_manage_roles_ids))
-            manage_permission = {trans.app.security_agent.permitted_actions.DATASET_MANAGE_PERMISSIONS: valid_manage_roles}
+                log.warning(
+                    f"The following roles could not be added to the dataset manage permission: {str(invalid_manage_roles_ids)}"
+                )
+            manage_permission = {
+                trans.app.security_agent.permitted_actions.DATASET_MANAGE_PERMISSIONS: valid_manage_roles
+            }
             trans.app.security_agent.set_dataset_permission(dataset, manage_permission)
 
             # MODIFY LIBRARY ITEM ROLES
             valid_modify_roles = []
             invalid_modify_roles_ids = []
             new_modify_roles_ids = util.listify(new_modify_roles_ids)
             for role_id in new_modify_roles_ids:
                 role = self.role_manager.get(trans, managers_base.decode_id(self.app, role_id))
                 if role in valid_roles_for_dataset:
                     valid_modify_roles.append(role)
                 else:
                     invalid_modify_roles_ids.append(role_id)
             if len(invalid_modify_roles_ids) > 0:
-                log.warning("The following roles could not be added to the dataset modify permission: " + str(invalid_modify_roles_ids))
+                log.warning(
+                    f"The following roles could not be added to the dataset modify permission: {str(invalid_modify_roles_ids)}"
+                )
             modify_permission = {trans.app.security_agent.permitted_actions.LIBRARY_MODIFY: valid_modify_roles}
             trans.app.security_agent.set_library_item_permission(library_dataset, modify_permission)
         return self._get_current_roles(trans, library_dataset)
 
     @expose_api
     def delete(self, trans, encoded_dataset_id, **kwd):
         """
@@ -319,413 +335,438 @@
         :type   encoded_dataset_id:      an encoded id string
         :param  undelete:                flag whether to undeleted instead of deleting
         :type   undelete:                bool
 
         :returns:   dict containing information about the dataset
         :rtype:     dictionary
         """
-        undelete = util.string_as_bool(kwd.get('undelete', False))
+        undelete = util.string_as_bool(kwd.get("undelete", False))
         library_dataset = self.ld_manager.get(trans, managers_base.decode_id(self.app, encoded_dataset_id))
         current_user_roles = trans.get_current_user_roles()
         allowed = trans.app.security_agent.can_modify_library_item(current_user_roles, library_dataset)
         if (not allowed) and (not trans.user_is_admin):
-            raise exceptions.InsufficientPermissionsException('You do not have proper permissions to delete this dataset.')
+            raise exceptions.InsufficientPermissionsException(
+                "You do not have proper permissions to delete this dataset."
+            )
 
         if undelete:
             library_dataset.deleted = False
         else:
             library_dataset.deleted = True
 
         trans.sa_session.add(library_dataset)
         trans.sa_session.flush()
 
         rval = trans.security.encode_all_ids(library_dataset.to_dict())
-        nice_size = util.nice_size(int(library_dataset.library_dataset_dataset_association.get_size()))
-        rval['file_size'] = nice_size
-        rval['update_time'] = library_dataset.update_time.strftime("%Y-%m-%d %I:%M %p")
-        rval['deleted'] = library_dataset.deleted
-        rval['folder_id'] = 'F' + rval['folder_id']
+        nice_size = util.nice_size(
+            int(library_dataset.library_dataset_dataset_association.get_size(calculate_size=False))
+        )
+        rval["file_size"] = nice_size
+        rval["update_time"] = library_dataset.update_time.strftime("%Y-%m-%d %I:%M %p")
+        rval["deleted"] = library_dataset.deleted
+        rval["folder_id"] = f"F{rval['folder_id']}"
         return rval
 
     @expose_api
     def load(self, trans, payload=None, **kwd):
         """
         POST /api/libraries/datasets
 
         Load dataset(s) from the given source into the library.
 
         :param   payload: dictionary structure containing:
             :param  encoded_folder_id:      the encoded id of the folder to import dataset(s) to
             :type   encoded_folder_id:      an encoded id string
-            :param  source:                 source the datasets should be loaded from
-                    Source can be:
-                        user directory - root folder specified in galaxy.ini as "$user_library_import_dir"
-                            example path: path/to/galaxy/$user_library_import_dir/user@example.com/{user can browse everything here}
-                            the folder with the user login has to be created beforehand
-                        (admin)import directory - root folder specified in galaxy ini as "$library_import_dir"
-                            example path: path/to/galaxy/$library_import_dir/{admin can browse everything here}
-                        (admin)any absolute or relative path - option allowed with "allow_library_path_paste" in galaxy.ini
+            :param  source:
+
+                source the datasets should be loaded from. Source can be:
+
+                    - user directory
+
+                        root folder specified in galaxy.ini as "$user_library_import_dir"
+                        example path: path/to/galaxy/$user_library_import_dir/user@example.com/{user can browse everything here}
+                        the folder with the user login has to be created beforehand
+
+                    - (admin)import directory
+
+                        root folder specified in galaxy ini as "$library_import_dir"
+                        example path: path/to/galaxy/$library_import_dir/{admin can browse everything here}
+
+                    - (admin)any absolute or relative path
+
+                        option allowed with "allow_library_path_paste" in galaxy.ini
+
             :type   source:                 str
-            :param  link_data:              flag whether to link the dataset to data or copy it to Galaxy, defaults to copy
-                                            while linking is set to True all symlinks will be resolved _once_
+            :param  link_data:
+
+                flag whether to link the dataset to data or copy it to Galaxy, defaults to copy
+                while linking is set to True all symlinks will be resolved _once_
+
             :type   link_data:              bool
-            :param  preserve_dirs:          flag whether to preserve the directory structure when importing dir
-                                            if False only datasets will be imported
+            :param  preserve_dirs:
+
+                flag whether to preserve the directory structure when importing dir
+                if False only datasets will be imported
+
             :type   preserve_dirs:          bool
             :param  file_type:              file type of the loaded datasets, defaults to 'auto' (autodetect)
             :type   file_type:              str
             :param  dbkey:                  dbkey of the loaded genome, defaults to '?' (unknown)
             :type   dbkey:                  str
             :param  tag_using_filenames:    flag whether to generate dataset tags from filenames
             :type   tag_using_filenames:    bool
+
         :type   dictionary
 
         :returns:   dict containing information about the created upload job
         :rtype:     dictionary
 
         :raises: RequestParameterMissingException, AdminRequiredException, ConfigDoesNotAllowException, RequestParameterInvalidException
                     InsufficientPermissionsException, ObjectNotFound
         """
         if payload:
             kwd.update(payload)
-        kwd['space_to_tab'] = False
-        kwd['to_posix_lines'] = True
-        kwd['dbkey'] = kwd.get('dbkey', '?')
-        kwd['file_type'] = kwd.get('file_type', 'auto')
-        kwd['link_data_only'] = 'link_to_files' if util.string_as_bool(kwd.get('link_data', False)) else 'copy_files'
-        kwd['tag_using_filenames'] = util.string_as_bool(kwd.get('tag_using_filenames', None))
-        encoded_folder_id = kwd.get('encoded_folder_id', None)
+        kwd["space_to_tab"] = False
+        kwd["to_posix_lines"] = True
+        kwd["dbkey"] = kwd.get("dbkey", "?")
+        kwd["file_type"] = kwd.get("file_type", "auto")
+        kwd["link_data_only"] = "link_to_files" if util.string_as_bool(kwd.get("link_data", False)) else "copy_files"
+        kwd["tag_using_filenames"] = util.string_as_bool(kwd.get("tag_using_filenames", None))
+        encoded_folder_id = kwd.get("encoded_folder_id", None)
         if encoded_folder_id is not None:
             folder_id = self.folder_manager.cut_and_decode(trans, encoded_folder_id)
         else:
-            raise exceptions.RequestParameterMissingException('The required attribute encoded_folder_id is missing.')
-        path = kwd.get('path', None)
+            raise exceptions.RequestParameterMissingException("The required attribute encoded_folder_id is missing.")
+        path = kwd.get("path", None)
         if path is None:
-            raise exceptions.RequestParameterMissingException('The required attribute path is missing.')
+            raise exceptions.RequestParameterMissingException("The required attribute path is missing.")
+        if not isinstance(path, str):
+            raise exceptions.RequestParameterInvalidException("The required attribute path is not String.")
+
         folder = self.folder_manager.get(trans, folder_id)
 
-        source = kwd.get('source', None)
-        if source not in ['userdir_file', 'userdir_folder', 'importdir_file', 'importdir_folder', 'admin_path']:
-            raise exceptions.RequestParameterMissingException('You have to specify "source" parameter. Possible values are "userdir_file", "userdir_folder", "admin_path", "importdir_file" and "importdir_folder". ')
-        elif source in ['importdir_file', 'importdir_folder']:
+        source = kwd.get("source", None)
+        if source not in ["userdir_file", "userdir_folder", "importdir_file", "importdir_folder", "admin_path"]:
+            raise exceptions.RequestParameterMissingException(
+                'You have to specify "source" parameter. Possible values are "userdir_file", "userdir_folder", "admin_path", "importdir_file" and "importdir_folder". '
+            )
+        elif source in ["importdir_file", "importdir_folder"]:
             if not trans.user_is_admin:
-                raise exceptions.AdminRequiredException('Only admins can import from importdir.')
+                raise exceptions.AdminRequiredException("Only admins can import from importdir.")
             if not trans.app.config.library_import_dir:
-                raise exceptions.ConfigDoesNotAllowException('The configuration of this Galaxy instance does not allow admins to import into library from importdir.')
+                raise exceptions.ConfigDoesNotAllowException(
+                    "The configuration of this Galaxy instance does not allow admins to import into library from importdir."
+                )
             import_base_dir = trans.app.config.library_import_dir
             if not safe_relpath(path):
                 # admins shouldn't be able to explicitly specify a path outside server_dir, but symlinks are allowed.
                 # the reasoning here is that galaxy admins may not have direct filesystem access or can only access
                 # library_import_dir via FTP (which cannot create symlinks), and may rely on sysadmins to set up the
                 # import directory. if they have filesystem access, all bets are off.
-                raise exceptions.RequestParameterInvalidException('The given path is invalid.')
+                raise exceptions.RequestParameterInvalidException("The given path is invalid.")
             path = os.path.join(import_base_dir, path)
-        elif source in ['userdir_file', 'userdir_folder']:
-            unsafe = None
+        elif source in ["userdir_file", "userdir_folder"]:
             username = trans.user.username if trans.app.config.user_library_import_check_permissions else None
             user_login = trans.user.email
             user_base_dir = trans.app.config.user_library_import_dir
             if user_base_dir is None:
-                raise exceptions.ConfigDoesNotAllowException('The configuration of this Galaxy instance does not allow upload from user directories.')
+                raise exceptions.ConfigDoesNotAllowException(
+                    "The configuration of this Galaxy instance does not allow upload from user directories."
+                )
             full_dir = os.path.join(user_base_dir, user_login)
 
-            if not safe_contains(full_dir, path, whitelist=trans.app.config.user_library_import_symlink_whitelist):
+            if not safe_contains(full_dir, path, allowlist=trans.app.config.user_library_import_symlink_allowlist):
                 # the path is a symlink outside the user dir
                 path = os.path.join(full_dir, path)
-                log.error('User attempted to import a path that resolves to a path outside of their import dir: %s -> %s', path, os.path.realpath(path))
-                raise exceptions.RequestParameterInvalidException('The given path is invalid.')
-            if trans.app.config.user_library_import_check_permissions and not full_path_permission_for_user(full_dir, path, username):
-                log.error('User attempted to import a path that resolves to a path outside of their import dir: '
-                        '%s -> %s and cannot be read by them.', path, os.path.realpath(path))
-                raise exceptions.RequestParameterInvalidException('The given path is invalid.')
+                log.error(
+                    "User attempted to import a path that resolves to a path outside of their import dir: %s -> %s",
+                    path,
+                    os.path.realpath(path),
+                )
+                raise exceptions.RequestParameterInvalidException("The given path is invalid.")
+            if trans.app.config.user_library_import_check_permissions and not full_path_permission_for_user(
+                full_dir, path, username
+            ):
+                log.error(
+                    "User attempted to import a path that resolves to a path outside of their import dir: "
+                    "%s -> %s and cannot be read by them.",
+                    path,
+                    os.path.realpath(path),
+                )
+                raise exceptions.RequestParameterInvalidException("The given path is invalid.")
             path = os.path.join(full_dir, path)
-            for unsafe in unsafe_walk(path, whitelist=[full_dir] + trans.app.config.user_library_import_symlink_whitelist, username=username):
+            if unsafe_walk(
+                path, allowlist=[full_dir] + trans.app.config.user_library_import_symlink_allowlist, username=username
+            ):
                 # the path is a dir and contains files that symlink outside the user dir
-                error = 'User attempted to import a path that resolves to a path outside of their import dir: %s -> %s', \
-                        path, os.path.realpath(path)
+                error = "User attempted to import a path that resolves to a path outside of their import dir: {} -> {}".format(
+                    path, os.path.realpath(path)
+                )
                 if trans.app.config.user_library_import_check_permissions:
-                    error += ' or is not readable for them.'
+                    error += " or is not readable for them."
                 log.error(error)
-            if unsafe:
-                raise exceptions.RequestParameterInvalidException('The given path is invalid.')
+                raise exceptions.RequestParameterInvalidException("The given path is invalid.")
             if not os.path.exists(path):
-                raise exceptions.RequestParameterInvalidException('Given path does not exist on the host.')
+                raise exceptions.RequestParameterInvalidException("Given path does not exist on the host.")
             if not self.folder_manager.can_add_item(trans, folder):
-                raise exceptions.InsufficientPermissionsException('You do not have proper permission to add items to the given folder.')
-        elif source == 'admin_path':
+                raise exceptions.InsufficientPermissionsException(
+                    "You do not have proper permission to add items to the given folder."
+                )
+        elif source == "admin_path":
             if not trans.app.config.allow_library_path_paste:
-                raise exceptions.ConfigDoesNotAllowException('The configuration of this Galaxy instance does not allow admins to import into library from path.')
+                raise exceptions.ConfigDoesNotAllowException(
+                    "The configuration of this Galaxy instance does not allow admins to import into library from path."
+                )
             if not trans.user_is_admin:
-                raise exceptions.AdminRequiredException('Only admins can import from path.')
+                raise exceptions.AdminRequiredException("Only admins can import from path.")
 
         # Set up the traditional tool state/params
-        tool_id = 'upload1'
+        tool_id = "upload1"
         tool = trans.app.toolbox.get_tool(tool_id)
         state = tool.new_state(trans)
         populate_state(trans, tool.inputs, kwd, state.inputs)
         tool_params = state.inputs
         dataset_upload_inputs = []
         for input in tool.inputs.values():
             if input.type == "upload_dataset":
                 dataset_upload_inputs.append(input)
-        library_bunch = upload_common.handle_library_params(trans, {}, trans.security.encode_id(folder.id))
+        library_bunch = upload_common.handle_library_params(trans, {}, folder.id)
         abspath_datasets = []
-        kwd['filesystem_paths'] = path
-        if source in ['importdir_folder']:
-            kwd['filesystem_paths'] = os.path.join(import_base_dir, path)
+        kwd["filesystem_paths"] = path
+        if source in ["importdir_folder"]:
+            kwd["filesystem_paths"] = os.path.join(import_base_dir, path)
         # user wants to import one file only
         elif source in ["userdir_file", "importdir_file"]:
             file = os.path.abspath(path)
-            abspath_datasets.append(self._make_library_uploaded_dataset(
-                trans, kwd, os.path.basename(file), file, 'server_dir', library_bunch))
+            abspath_datasets.append(
+                self._make_library_uploaded_dataset(
+                    trans, kwd, os.path.basename(file), file, "server_dir", library_bunch
+                )
+            )
         # user wants to import whole folder
         elif source == "userdir_folder":
-            uploaded_datasets_bunch = self._get_path_paste_uploaded_datasets(
-                trans, kwd, library_bunch, 200, '')
+            uploaded_datasets_bunch = self._get_path_paste_uploaded_datasets(trans, kwd, library_bunch, 200, "")
             uploaded_datasets = uploaded_datasets_bunch[0]
             if uploaded_datasets is None:
-                raise exceptions.ObjectNotFound('Given folder does not contain any datasets.')
+                raise exceptions.ObjectNotFound("Given folder does not contain any datasets.")
             for ud in uploaded_datasets:
                 ud.path = os.path.abspath(ud.path)
                 abspath_datasets.append(ud)
         #  user wants to import from path
         if source in ["admin_path", "importdir_folder"]:
             # validate the path is within root
-            uploaded_datasets_bunch = self._get_path_paste_uploaded_datasets(
-                trans, kwd, library_bunch, 200, '')
+            uploaded_datasets_bunch = self._get_path_paste_uploaded_datasets(trans, kwd, library_bunch, 200, "")
             uploaded_datasets = uploaded_datasets_bunch[0]
             if uploaded_datasets is None:
-                raise exceptions.ObjectNotFound('Given folder does not contain any datasets.')
+                raise exceptions.ObjectNotFound("Given folder does not contain any datasets.")
             for ud in uploaded_datasets:
                 ud.path = os.path.abspath(ud.path)
                 abspath_datasets.append(ud)
         json_file_path = upload_common.create_paramfile(trans, abspath_datasets)
         data_list = [ud.data for ud in abspath_datasets]
         job_params = {}
-        job_params['link_data_only'] = dumps(kwd.get('link_data_only', 'copy_files'))
-        job_params['uuid'] = dumps(kwd.get('uuid', None))
-        job, output = upload_common.create_job(trans, tool_params, tool, json_file_path, data_list, folder=folder, job_params=job_params)
-        trans.sa_session.add(job)
-        trans.sa_session.flush()
+        job_params["link_data_only"] = dumps(kwd.get("link_data_only", "copy_files"))
+        job_params["uuid"] = dumps(kwd.get("uuid", None))
+        job, output = upload_common.create_job(
+            trans, tool_params, tool, json_file_path, data_list, folder=folder, job_params=job_params
+        )
+        trans.app.job_manager.enqueue(job, tool=tool)
         job_dict = job.to_dict()
-        job_dict['id'] = trans.security.encode_id(job_dict['id'])
+        job_dict["id"] = trans.security.encode_id(job_dict["id"])
         return job_dict
 
     @web.expose
     #  TODO convert to expose_api
-    def download(self, trans, format, **kwd):
+    def download(self, trans, archive_format, **kwd):
         """
-        GET /api/libraries/datasets/download/{format}
-        POST /api/libraries/datasets/download/{format}
+        GET /api/libraries/datasets/download/{archive_format}
+        POST /api/libraries/datasets/download/{archive_format}
 
-        Download requested datasets (identified by encoded IDs) in requested format.
+        Download requested datasets (identified by encoded IDs) in requested archive_format.
 
         example: ``GET localhost:8080/api/libraries/datasets/download/tbz?ld_ids%255B%255D=a0d84b45643a2678&ld_ids%255B%255D=fe38c84dcd46c828``
 
-        .. note:: supported format values are: 'zip', 'tgz', 'tbz', 'uncompressed'
+        .. note:: supported archive_format values are: 'zip', 'tgz', 'tbz', 'uncompressed'
 
-        :param  format:      string representing requested archive format
-        :type   format:      string
+        :param  archive_format:      string representing requested archive archive_format
+        :type   archive_format:      string
         :param  ld_ids[]:      an array of encoded dataset ids
         :type   ld_ids[]:      an array
         :param  folder_ids[]:      an array of encoded folder ids
         :type   folder_ids[]:      an array
 
         :returns: either archive with the requested datasets packed inside or a single uncompressed dataset
         :rtype:   file
 
         :raises: MessageException, ItemDeletionException, ItemAccessibilityException, HTTPBadRequest, OSError, IOError, ObjectNotFound
         """
         library_datasets = []
-        datasets_to_download = kwd.get('ld_ids%5B%5D', None)
+        datasets_to_download = kwd.get("ld_ids%5B%5D", None)
         if datasets_to_download is None:
-            datasets_to_download = kwd.get('ld_ids', None)
+            datasets_to_download = kwd.get("ld_ids", None)
         if datasets_to_download is not None:
             datasets_to_download = util.listify(datasets_to_download)
             for dataset_id in datasets_to_download:
                 try:
-                    library_dataset = self.get_library_dataset(trans, id=dataset_id, check_ownership=False, check_accessible=True)
+                    library_dataset = self.get_library_dataset(
+                        trans, id=dataset_id, check_ownership=False, check_accessible=True
+                    )
                     library_datasets.append(library_dataset)
                 except HTTPBadRequest:
-                    raise exceptions.RequestParameterInvalidException('Bad Request.')
+                    raise exceptions.RequestParameterInvalidException("Bad Request.")
                 except HTTPInternalServerError:
-                    raise exceptions.InternalServerError('Internal error.')
+                    raise exceptions.InternalServerError("Internal error.")
                 except Exception as e:
-                    raise exceptions.InternalServerError('Unknown error.' + util.unicodify(e))
+                    raise exceptions.InternalServerError(f"Unknown error.{util.unicodify(e)}")
 
-        folders_to_download = kwd.get('folder_ids%5B%5D', None)
+        folders_to_download = kwd.get("folder_ids%5B%5D", None)
         if folders_to_download is None:
-            folders_to_download = kwd.get('folder_ids', None)
+            folders_to_download = kwd.get("folder_ids", None)
         if folders_to_download is not None:
             folders_to_download = util.listify(folders_to_download)
 
             current_user_roles = trans.get_current_user_roles()
 
             def traverse(folder):
                 admin = trans.user_is_admin
                 rval = []
                 for subfolder in folder.active_folders:
                     if not admin:
-                        can_access, folder_ids = trans.app.security_agent.check_folder_contents(trans.user, current_user_roles, subfolder)
+                        can_access, folder_ids = trans.app.security_agent.check_folder_contents(
+                            trans.user, current_user_roles, subfolder
+                        )
                     if (admin or can_access) and not subfolder.deleted:
                         rval.extend(traverse(subfolder))
                 for ld in folder.datasets:
                     if not admin:
                         can_access = trans.app.security_agent.can_access_dataset(
-                            current_user_roles,
-                            ld.library_dataset_dataset_association.dataset
+                            current_user_roles, ld.library_dataset_dataset_association.dataset
                         )
                     if (admin or can_access) and not ld.deleted:
                         rval.append(ld)
                 return rval
 
             for encoded_folder_id in folders_to_download:
                 folder_id = self.folder_manager.cut_and_decode(trans, encoded_folder_id)
                 folder = self.folder_manager.get(trans, folder_id)
                 library_datasets.extend(traverse(folder))
 
         if not library_datasets:
-            raise exceptions.RequestParameterMissingException('Request has to contain a list of dataset ids or folder ids to download.')
-
-        if format in ['zip', 'tgz', 'tbz']:
-            # error = False
+            raise exceptions.RequestParameterMissingException(
+                "Request has to contain a list of dataset ids or folder ids to download."
+            )
+
+        if archive_format == "zip":
+            archive = ZipstreamWrapper(
+                archive_name="selected_library_files",
+                upstream_mod_zip=self.app.config.upstream_mod_zip,
+                upstream_gzip=self.app.config.upstream_gzip,
+            )
             killme = string.punctuation + string.whitespace
-            trantab = maketrans(killme, '_' * len(killme))
-            try:
-                outext = 'zip'
-                if format == 'zip':
-                    # Can't use mkstemp - the file must not exist first
-                    tmpd = tempfile.mkdtemp()
-                    util.umask_fix_perms(tmpd, trans.app.config.umask, 0o777, self.app.config.gid)
-                    tmpf = os.path.join(tmpd, 'library_download.' + format)
-                    if trans.app.config.upstream_gzip:
-                        archive = zipfile.ZipFile(tmpf, 'w', zipfile.ZIP_STORED, True)
-                    else:
-                        archive = zipfile.ZipFile(tmpf, 'w', zipfile.ZIP_DEFLATED, True)
-                    archive.add = lambda x, y: archive.write(x, y.encode('CP437'))
-                elif format == 'tgz':
-                    if trans.app.config.upstream_gzip:
-                        archive = StreamBall('w|')
-                        outext = 'tar'
-                    else:
-                        archive = StreamBall('w|gz')
-                        outext = 'tgz'
-                elif format == 'tbz':
-                    archive = StreamBall('w|bz2')
-                    outext = 'tbz2'
-            except (OSError, zipfile.BadZipfile):
-                log.exception("Unable to create archive for download")
-                raise exceptions.InternalServerError("Unable to create archive for download.")
-            except Exception:
-                log.exception("Unexpected error in create archive for download")
-                raise exceptions.InternalServerError("Unable to create archive for download.")
-            composite_extensions = trans.app.datatypes_registry.get_composite_extensions()
+            trantab = str.maketrans(killme, "_" * len(killme))
             seen = []
             for ld in library_datasets:
                 ldda = ld.library_dataset_dataset_association
-                ext = ldda.extension
-                is_composite = ext in composite_extensions
+                is_composite = ldda.datatype.composite_type
                 path = ""
                 parent_folder = ldda.library_dataset.folder
                 while parent_folder is not None:
                     # Exclude the now-hidden "root folder"
                     if parent_folder.parent is None:
                         path = os.path.join(parent_folder.library_root[0].name, path)
                         break
                     path = os.path.join(parent_folder.name, path)
                     parent_folder = parent_folder.parent
                 path += ldda.name
                 while path in seen:
-                    path += '_'
-                path = "{path}.{extension}".format(path=path, extension=ldda.extension)
+                    path += "_"
+                path = f"{path}.{ldda.extension}"
                 seen.append(path)
                 zpath = os.path.split(path)[-1]  # comes as base_name/fname
                 outfname, zpathext = os.path.splitext(zpath)
 
                 if is_composite:
                     # need to add all the components from the extra_files_path to the zip
-                    if zpathext == '':
-                        zpath = '%s.html' % zpath  # fake the real nature of the html file
+                    if zpathext == "":
+                        zpath = f"{zpath}.html"  # fake the real nature of the html file
                     try:
-                        if format == 'zip':
-                            archive.add(ldda.dataset.file_name, zpath)  # add the primary of a composite set
+                        if archive_format == "zip":
+                            archive.write(ldda.dataset.file_name, zpath)  # add the primary of a composite set
                         else:
-                            archive.add(ldda.dataset.file_name, zpath, check_file=True)  # add the primary of a composite set
-                    except IOError:
-                        log.exception("Unable to add composite parent %s to temporary library download archive", ldda.dataset.file_name)
+                            archive.write(ldda.dataset.file_name, zpath)  # add the primary of a composite set
+                    except OSError:
+                        log.exception(
+                            "Unable to add composite parent %s to temporary library download archive",
+                            ldda.dataset.file_name,
+                        )
                         raise exceptions.InternalServerError("Unable to create archive for download.")
                     except ObjectNotFound:
                         log.exception("Requested dataset %s does not exist on the host.", ldda.dataset.file_name)
                         raise exceptions.ObjectNotFound("Requested dataset not found. ")
                     except Exception as e:
-                        log.exception("Unable to add composite parent %s to temporary library download archive", ldda.dataset.file_name)
-                        raise exceptions.InternalServerError("Unable to add composite parent to temporary library download archive. " + util.unicodify(e))
+                        log.exception(
+                            "Unable to add composite parent %s to temporary library download archive",
+                            ldda.dataset.file_name,
+                        )
+                        raise exceptions.InternalServerError(
+                            f"Unable to add composite parent to temporary library download archive. {util.unicodify(e)}"
+                        )
 
-                    flist = glob.glob(os.path.join(ldda.dataset.extra_files_path, '*.*'))  # glob returns full paths
+                    flist = glob.glob(os.path.join(ldda.dataset.extra_files_path, "*.*"))  # glob returns full paths
                     for fpath in flist:
                         efp, fname = os.path.split(fpath)
-                        if fname > '':
+                        if fname > "":
                             fname = fname.translate(trantab)
                         try:
-                            if format == 'zip':
-                                archive.add(fpath, fname)
-                            else:
-                                archive.add(fpath, fname, check_file=True)
-                        except IOError:
+                            archive.write(fpath, fname)
+                        except OSError:
                             log.exception("Unable to add %s to temporary library download archive %s", fname, outfname)
                             raise exceptions.InternalServerError("Unable to create archive for download.")
                         except ObjectNotFound:
                             log.exception("Requested dataset %s does not exist on the host.", fpath)
                             raise exceptions.ObjectNotFound("Requested dataset not found.")
                         except Exception as e:
                             log.exception("Unable to add %s to temporary library download archive %s", fname, outfname)
-                            raise exceptions.InternalServerError("Unable to add dataset to temporary library download archive . " + util.unicodify(e))
+                            raise exceptions.InternalServerError(
+                                f"Unable to add dataset to temporary library download archive . {util.unicodify(e)}"
+                            )
                 else:
                     try:
-                        if format == 'zip':
-                            archive.add(ldda.dataset.file_name, path)
-                        else:
-                            archive.add(ldda.dataset.file_name, path, check_file=True)
-                    except IOError:
-                        log.exception("Unable to write %s to temporary library download archive", ldda.dataset.file_name)
+                        archive.write(ldda.dataset.file_name, path)
+                    except OSError:
+                        log.exception(
+                            "Unable to write %s to temporary library download archive", ldda.dataset.file_name
+                        )
                         raise exceptions.InternalServerError("Unable to create archive for download")
                     except ObjectNotFound:
                         log.exception("Requested dataset %s does not exist on the host.", ldda.dataset.file_name)
                         raise exceptions.ObjectNotFound("Requested dataset not found.")
                     except Exception as e:
-                        log.exception("Unable to add %s to temporary library download archive %s", ldda.dataset.file_name, outfname)
-                        raise exceptions.InternalServerError("Unknown error. " + util.unicodify(e))
-            lname = 'selected_dataset'
-            fname = lname.replace(' ', '_') + '_files'
-            if format == 'zip':
-                archive.close()
-                trans.response.set_content_type("application/octet-stream")
-                trans.response.headers["Content-Disposition"] = 'attachment; filename="%s.%s"' % (fname, outext)
-                archive = util.streamball.ZipBall(tmpf, tmpd)
-                archive.wsgi_status = trans.response.wsgi_status()
-                archive.wsgi_headeritems = trans.response.wsgi_headeritems()
-                return archive.stream
-            else:
-                trans.response.set_content_type("application/x-tar")
-                trans.response.headers["Content-Disposition"] = 'attachment; filename="%s.%s"' % (fname, outext)
-                archive.wsgi_status = trans.response.wsgi_status()
-                archive.wsgi_headeritems = trans.response.wsgi_headeritems()
-                return archive.stream
-        elif format == 'uncompressed':
+                        log.exception(
+                            "Unable to add %s to temporary library download archive %s",
+                            ldda.dataset.file_name,
+                            outfname,
+                        )
+                        raise exceptions.InternalServerError(f"Unknown error. {util.unicodify(e)}")
+            trans.response.headers.update(archive.get_headers())
+            return archive.response()
+        elif archive_format == "uncompressed":
             if len(library_datasets) != 1:
-                raise exceptions.RequestParameterInvalidException("You can download only one uncompressed file at once.")
+                raise exceptions.RequestParameterInvalidException(
+                    "You can download only one uncompressed file at once."
+                )
             else:
                 single_ld = library_datasets[0]
                 ldda = single_ld.library_dataset_dataset_association
                 dataset = ldda.dataset
                 fStat = os.stat(dataset.file_name)
                 trans.response.set_content_type(ldda.get_mime())
-                trans.response.headers['Content-Length'] = int(fStat.st_size)
-                fname = "{path}.{extension}".format(path=ldda.name, extension=ldda.extension)
-                fname = ''.join(c in util.FILENAME_VALID_CHARS and c or '_' for c in fname)[0:150]
-                trans.response.headers["Content-Disposition"] = 'attachment; filename="%s"' % fname
+                trans.response.headers["Content-Length"] = str(fStat.st_size)
+                fname = f"{ldda.name}.{ldda.extension}"
+                fname = "".join(c in util.FILENAME_VALID_CHARS and c or "_" for c in fname)[0:150]
+                trans.response.headers["Content-Disposition"] = f'attachment; filename="{fname}"'
                 try:
-                    return open(dataset.file_name, 'rb')
+                    return open(dataset.file_name, "rb")
                 except Exception:
                     raise exceptions.InternalServerError("This dataset contains no content.")
         else:
-            raise exceptions.RequestParameterInvalidException("Wrong format parameter specified")
+            raise exceptions.RequestParameterInvalidException("Wrong archive_format parameter specified")
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/page_revisions.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/page_revisions.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,44 +1,39 @@
 """
 API for updating Galaxy Pages
 """
 import logging
 
 from galaxy.managers.base import get_object
 from galaxy.managers.pages import PageManager
-from galaxy.model.item_attrs import UsesAnnotations
 from galaxy.web import expose_api
-from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    SharableItemSecurityMixin,
-    SharableMixin
+from . import (
+    BaseGalaxyAPIController,
+    depends,
 )
 
 log = logging.getLogger(__name__)
 
 
-class PageRevisionsController(BaseAPIController, SharableItemSecurityMixin, UsesAnnotations, SharableMixin):
-
-    def __init__(self, app):
-        super(PageRevisionsController, self).__init__(app)
-        self.manager = PageManager(app)
+class PageRevisionsController(BaseGalaxyAPIController):
+    manager: PageManager = depends(PageManager)
 
     @expose_api
     def index(self, trans, page_id, **kwd):
         """
         index( self, trans, page_id, **kwd )
         * GET /api/pages/{page_id}/revisions
             return a list of Page revisions
 
         :param page_id: Display the revisions of Page with ID=page_id
 
         :rtype:     list
         :returns:   dictionaries containing different revisions of the page
         """
-        page = get_object(trans, page_id, 'Page', check_ownership=False, check_accessible=True)
+        page = get_object(trans, page_id, "Page", check_ownership=False, check_accessible=True)
         r = trans.sa_session.query(trans.app.model.PageRevision).filter_by(page_id=page.id)
         out = []
         for page in r:
             as_dict = self.encode_all_ids(trans, page.to_dict(), True)
             self.manager.rewrite_content_for_export(trans, as_dict)
             out.append(as_dict)
         return out
@@ -53,12 +48,12 @@
         :param page_id: Add revision to Page with ID=page_id
         :param payload: A dictionary containing::
             'content'   = New content of new page revision
 
         :rtype:     dictionary
         :returns:   Dictionary with 'success' or 'error' element to indicate the result of the request
         """
-        page = get_object(trans, page_id, 'Page', check_ownership=True)
+        page = get_object(trans, page_id, "Page", check_ownership=True)
         page_revision = self.manager.save_new_revision(trans, page, payload)
         rval = self.encode_all_ids(trans, page_revision.to_dict(view="element"), True)
         self.manager.rewrite_content_for_export(trans, rval)
         return rval
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/pages.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/services/pages.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,147 +1,142 @@
-"""
-API for updating Galaxy Pages
-"""
 import logging
 
-from galaxy.exceptions import RequestParameterInvalidException
-from galaxy.managers.base import get_object
-from galaxy.managers.markdown_util import internal_galaxy_markdown_to_pdf
+from galaxy import exceptions
+from galaxy.celery.tasks import prepare_pdf_download
+from galaxy.managers import base
+from galaxy.managers.markdown_util import (
+    internal_galaxy_markdown_to_pdf,
+    to_basic_markdown,
+)
 from galaxy.managers.pages import (
     PageManager,
-    PageSerializer
+    PageSerializer,
+)
+from galaxy.schema import PdfDocumentType
+from galaxy.schema.fields import DecodedDatabaseIdField
+from galaxy.schema.schema import (
+    AsyncFile,
+    CreatePagePayload,
+    PageContentFormat,
+    PageDetails,
+    PageIndexQueryPayload,
+    PageSummary,
+    PageSummaryList,
 )
-from galaxy.model.item_attrs import UsesAnnotations
-from galaxy.web import expose_api, expose_api_raw
-from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    SharableItemSecurityMixin,
-    SharableMixin
+from galaxy.schema.tasks import GeneratePdfDownload
+from galaxy.security.idencoding import IdEncodingHelper
+from galaxy.web.short_term_storage import ShortTermStorageAllocator
+from galaxy.webapps.galaxy.services.base import (
+    async_task_summary,
+    ensure_celery_tasks_enabled,
+    ServiceBase,
 )
+from galaxy.webapps.galaxy.services.sharable import ShareableService
 
 log = logging.getLogger(__name__)
 
 
-class PagesController(BaseAPIController, SharableItemSecurityMixin, UsesAnnotations, SharableMixin):
-    """
-    RESTful controller for interactions with pages.
+class PagesService(ServiceBase):
+    """Common interface/service logic for interactions with pages in the context of the API.
+
+    Provides the logic of the actions invoked by API controllers and uses type definitions
+    and pydantic models to declare its parameters and return types.
     """
 
-    def __init__(self, app):
-        super(PagesController, self).__init__(app)
-        self.manager = PageManager(app)
-        self.serializer = PageSerializer(app)
-
-    @expose_api
-    def index(self, trans, deleted=False, **kwd):
-        """
-        index( self, trans, deleted=False, **kwd )
-        * GET /api/pages
-            return a list of Pages viewable by the user
+    def __init__(
+        self,
+        security: IdEncodingHelper,
+        manager: PageManager,
+        serializer: PageSerializer,
+        short_term_storage_allocator: ShortTermStorageAllocator,
+    ):
+        super().__init__(security)
+        self.manager = manager
+        self.serializer = serializer
+        self.shareable_service = ShareableService(self.manager, self.serializer)
+        self.short_term_storage_allocator = short_term_storage_allocator
+
+    def index(self, trans, payload: PageIndexQueryPayload) -> PageSummaryList:
+        """Return a list of Pages viewable by the user
 
         :param deleted: Display deleted pages
 
         :rtype:     list
         :returns:   dictionaries containing summary or detailed Page information
         """
-        out = []
-
-        if trans.user_is_admin:
-            r = trans.sa_session.query(trans.app.model.Page)
-            if not deleted:
-                r = r.filter_by(deleted=False)
-            for row in r:
-                out.append(self.encode_all_ids(trans, row.to_dict(), True))
-        else:
-            user = trans.get_user()
-            r = trans.sa_session.query(trans.app.model.Page).filter_by(user=user)
-            if not deleted:
-                r = r.filter_by(deleted=False)
-            for row in r:
-                out.append(self.encode_all_ids(trans, row.to_dict(), True))
-            r = trans.sa_session.query(trans.app.model.Page).filter(trans.app.model.Page.user != user).filter_by(published=True)
-            if not deleted:
-                r = r.filter_by(deleted=False)
-            for row in r:
-                out.append(self.encode_all_ids(trans, row.to_dict(), True))
-
-        return out
-
-    @expose_api
-    def create(self, trans, payload, **kwd):
-        """
-        create( self, trans, payload, **kwd )
-        * POST /api/pages
-            Create a page and return dictionary containing Page summary
-
-        :param  payload:    dictionary structure containing::
-            'slug'           = The title slug for the page URL, must be unique
-            'title'          = Title of the page
-            'content'        = contents of the first page revision (type dependent on content_format)
-            'content_format' = 'html' or 'markdown'
-            'annotation'     = Annotation that will be attached to the page
-
-        :rtype:     dict
-        :returns:   Dictionary return of the Page.to_dict call
-        """
-        page = self.manager.create(trans, payload)
-        rval = self.encode_all_ids(trans, page.to_dict(), True)
-        rval['content'] = page.latest_revision.content
+        if not trans.user_is_admin:
+            user_id = trans.user.id
+            if payload.user_id and payload.user_id != user_id:
+                raise exceptions.AdminRequiredException("Only admins can index the pages of others")
+
+        pages, _ = self.manager.index_query(trans, payload)
+        return PageSummaryList.construct(
+            __root__=[trans.security.encode_all_ids(p.to_dict(), recursive=True) for p in pages]
+        )
+
+    def create(self, trans, payload: CreatePagePayload) -> PageSummary:
+        """
+        Create a page and return Page summary
+        """
+        page = self.manager.create_page(trans, payload)
+        rval = trans.security.encode_all_ids(page.to_dict(), recursive=True)
+        rval["content"] = page.latest_revision.content
         self.manager.rewrite_content_for_export(trans, rval)
-        return rval
+        return PageSummary.construct(**rval)
 
-    @expose_api
-    def delete(self, trans, id, **kwd):
+    def delete(self, trans, id: DecodedDatabaseIdField):
         """
-        delete( self, trans, id, **kwd )
-        * DELETE /api/pages/{id}
-            Create a page and return dictionary containing Page summary
-
-        :param  id:    ID of page to be deleted
-
-        :rtype:     dict
-        :returns:   Dictionary with 'success' or 'error' element to indicate the result of the request
+        Deletes a page (or marks it as deleted)
         """
-        page = get_object(trans, id, 'Page', check_ownership=True)
+        page = base.get_object(trans, id, "Page", check_ownership=True)
 
         # Mark a page as deleted
         page.deleted = True
         trans.sa_session.flush()
-        return ''  # TODO: Figure out what to return on DELETE, document in guidelines!
 
-    @expose_api
-    def show(self, trans, id, **kwd):
-        """
-        show( self, trans, id, **kwd )
-        * GET /api/pages/{id}
-            View a page summary and the content of the latest revision
+    def show(self, trans, id: DecodedDatabaseIdField) -> PageDetails:
+        """View a page summary and the content of the latest revision
 
         :param  id:    ID of page to be displayed
 
         :rtype:     dict
         :returns:   Dictionary return of the Page.to_dict call with the 'content' field populated by the most recent revision
         """
-        page = get_object(trans, id, 'Page', check_ownership=False, check_accessible=True)
-        rval = self.encode_all_ids(trans, page.to_dict(), True)
-        rval['content'] = page.latest_revision.content
-        rval['content_format'] = page.latest_revision.content_format
+        page = base.get_object(trans, id, "Page", check_ownership=False, check_accessible=True)
+        rval = trans.security.encode_all_ids(page.to_dict(), recursive=True)
+        rval["content"] = page.latest_revision.content
+        rval["content_format"] = page.latest_revision.content_format
         self.manager.rewrite_content_for_export(trans, rval)
-        return rval
+        return PageDetails.construct(**rval)
 
-    @expose_api_raw
-    def show_pdf(self, trans, id, **kwd):
+    def show_pdf(self, trans, id: DecodedDatabaseIdField):
         """
-        show( self, trans, id, **kwd )
-        * GET /api/pages/{id}.pdf
-            View a page summary and the content of the latest revision as PDF.
+        View a page summary and the content of the latest revision as PDF.
 
-        :param  id:    ID of page to be displayed
+        :param  id: ID of page to be displayed
 
-        :rtype:     dict
-        :returns:   Dictionary return of the Page.to_dict call with the 'content' field populated by the most recent revision
+        :rtype: dict
+        :returns: Dictionary return of the Page.to_dict call with the 'content' field populated by the most recent revision
         """
-        page = get_object(trans, id, 'Page', check_ownership=False, check_accessible=True)
-        if page.latest_revision.content_format != "markdown":
-            raise RequestParameterInvalidException("PDF export only allowed for Markdown based pages")
+        page = base.get_object(trans, id, "Page", check_ownership=False, check_accessible=True)
+        if page.latest_revision.content_format != PageContentFormat.markdown.value:
+            raise exceptions.RequestParameterInvalidException("PDF export only allowed for Markdown based pages")
+        internal_galaxy_markdown = page.latest_revision.content
+        return internal_galaxy_markdown_to_pdf(trans, internal_galaxy_markdown, PdfDocumentType.page)
+
+    def prepare_pdf(self, trans, id: DecodedDatabaseIdField) -> AsyncFile:
+        ensure_celery_tasks_enabled(trans.app.config)
+        page = base.get_object(trans, id, "Page", check_ownership=False, check_accessible=True)
+        short_term_storage_target = self.short_term_storage_allocator.new_target(
+            f"{page.title}.pdf",
+            "application/pdf",
+        )
+        request_id = short_term_storage_target.request_id
         internal_galaxy_markdown = page.latest_revision.content
-        trans.response.set_content_type("application/pdf")
-        return internal_galaxy_markdown_to_pdf(trans, internal_galaxy_markdown, 'page')
+        basic_markdown = to_basic_markdown(trans, internal_galaxy_markdown)
+        pdf_download_request = GeneratePdfDownload(
+            basic_markdown=basic_markdown,
+            document_type=PdfDocumentType.page,
+            short_term_storage_request_id=request_id,
+        )
+        result = prepare_pdf_download.delay(request=pdf_download_request)
+        return AsyncFile(storage_request_id=request_id, task=async_task_summary(result))
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/plugins.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/plugins.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,59 +1,65 @@
 """
 Plugins resource control over the API.
 """
 import logging
 
 from galaxy import exceptions
-from galaxy.managers import hdas, histories
+from galaxy.managers import (
+    hdas,
+    histories,
+)
+from galaxy.util import asbool
 from galaxy.web import expose_api
-from galaxy.webapps.base.controller import BaseAPIController
+from . import (
+    BaseGalaxyAPIController,
+    depends,
+)
 
 log = logging.getLogger(__name__)
 
 
-class PluginsController(BaseAPIController):
+class PluginsController(BaseGalaxyAPIController):
     """
     RESTful controller for interactions with plugins.
     """
 
-    def __init__(self, app):
-        super(PluginsController, self).__init__(app)
-        self.hda_manager = hdas.HDAManager(app)
-        self.history_manager = histories.HistoryManager(app)
+    hda_manager: hdas.HDAManager = depends(hdas.HDAManager)
+    history_manager: histories.HistoryManager = depends(histories.HistoryManager)
 
     @expose_api
     def index(self, trans, **kwargs):
         """
         GET /api/plugins:
         """
         registry = self._get_registry(trans)
         dataset_id = kwargs.get("dataset_id")
         if dataset_id is not None:
             hda = self.hda_manager.get_accessible(self.decode_id(dataset_id), trans.user)
             return registry.get_visualizations(trans, hda)
         else:
-            return registry.get_plugins()
+            embeddable = asbool(kwargs.get("embeddable"))
+            return registry.get_plugins(embeddable=embeddable)
 
     @expose_api
     def show(self, trans, id, **kwargs):
         """
         GET /api/plugins/{id}:
         """
         registry = self._get_registry(trans)
-        result = {}
         history_id = kwargs.get("history_id")
         if history_id is not None:
-            history = self.history_manager.get_owned(trans.security.decode_id(history_id), trans.user, current_history=trans.history)
-            result["hdas"] = []
-            for hda in history.datasets:
+            history = self.history_manager.get_owned(
+                trans.security.decode_id(history_id), trans.user, current_history=trans.history
+            )
+            result = {"hdas": []}
+            for hda in history.contents_iter(types=["dataset"], deleted=False, visible=True):
                 if registry.get_visualization(trans, id, hda):
-                    result["hdas"].append({
-                        "id": trans.security.encode_id(hda.id),
-                        "name": hda.name
-                    })
+                    result["hdas"].append({"id": trans.security.encode_id(hda.id), "name": hda.name})
+        else:
+            result = registry.get_plugin(id).to_dict()
         return result
 
     def _get_registry(self, trans):
         if not trans.app.visualizations_registry:
             raise exceptions.MessageException("The visualization registry has not been configured.")
         return trans.app.visualizations_registry
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/provenance.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/provenance.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,57 +1,53 @@
 """
 API operations provenance
 """
 import logging
 
 from paste.httpexceptions import (
     HTTPBadRequest,
-    HTTPNotImplemented
+    HTTPNotImplemented,
 )
 
-from galaxy import (
-    managers,
-    web
+from galaxy import web
+from galaxy.managers.hdas import HDAManager
+from galaxy.util import string_as_bool
+from . import (
+    BaseGalaxyAPIController,
+    depends,
 )
-from galaxy.webapps.base.controller import BaseAPIController
 
 log = logging.getLogger(__name__)
 
 
-class BaseProvenanceController(BaseAPIController):
-    """
-    """
-
-    def __init__(self, app):
-        super(BaseProvenanceController, self).__init__(app)
-        self.hda_manager = managers.hdas.HDAManager(app)
+class BaseProvenanceController(BaseGalaxyAPIController):
+    """ """
 
     @web.legacy_expose_api
     def index(self, trans, **kwd):
-        follow = kwd.get('follow', False)
+        follow = string_as_bool(kwd.get("follow", False))
         value = self._get_provenance(trans, self.provenance_item_class, kwd[self.provenance_item_id], follow)
         return value
 
     @web.legacy_expose_api
     def show(self, trans, elem_name, **kwd):
-        follow = kwd.get('follow', False)
-        value = self._get_provenance(trans, self.provenance_item_class, kwd[self.provenance_item_id], follow)
-        return value
+        raise HTTPNotImplemented()
 
     @web.legacy_expose_api
     def create(self, trans, tag_name, payload=None, **kwd):
-        payload = payload or {}
         raise HTTPNotImplemented()
 
     @web.legacy_expose_api
     def delete(self, trans, tag_name, **kwd):
         raise HTTPBadRequest("Cannot Delete Provenance")
 
     def _get_provenance(self, trans, item_class_name, item_id, follow=True):
-        provenance_item = self.get_object(trans, item_id, item_class_name, check_ownership=False, check_accessible=False)
+        provenance_item = self.get_object(
+            trans, item_id, item_class_name, check_ownership=False, check_accessible=False
+        )
         if item_class_name == "HistoryDatasetAssociation":
             self.hda_manager.error_unless_accessible(provenance_item, trans.user)
         else:
             self.security_check(trans, provenance_item, check_accessible=True)
         out = self._get_record(trans, provenance_item, follow)
         return out
 
@@ -69,15 +65,15 @@
                     "parameters": self._get_job_record(trans, job, follow),
                     "stderr": job.stderr,
                     "stdout": job.stdout,
                 }
             else:
                 return {
                     "id": trans.security.encode_id(item.id),
-                    "uuid": (lambda uuid: str(uuid) if uuid else None)(item.dataset.uuid)
+                    "uuid": (lambda uuid: str(uuid) if uuid else None)(item.dataset.uuid),
                 }
         return None
 
     def _get_job_record(self, trans, job, follow):
         out = {}
         for p in job.parameters:
             out[p.name] = p.value
@@ -94,13 +90,15 @@
         return out
 
 
 class HDAProvenanceController(BaseProvenanceController):
     controller_name = "history_content_provenance"
     provenance_item_class = "HistoryDatasetAssociation"
     provenance_item_id = "history_content_id"
+    hda_manager: HDAManager = depends(HDAManager)
 
 
 class LDDAProvenanceController(BaseProvenanceController):
     controller_name = "ldda_provenance"
     provenance_item_class = "LibraryDatasetDatasetAssociation"
     provenance_item_id = "library_content_id"
+    hda_manager: HDAManager = depends(HDAManager)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/search.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/search.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,71 +1,78 @@
 """
 API for searching Galaxy Datasets
 """
 import logging
 
-from galaxy import web
+from galaxy import (
+    model,
+    web,
+)
 from galaxy.exceptions import ItemAccessibilityException
+from galaxy.managers.context import ProvidesUserContext
 from galaxy.model.search import GalaxySearchEngine
 from galaxy.util import unicodify
-from galaxy.webapps.base.controller import (
-    BaseAPIController,
-    SharableItemSecurityMixin
-)
+from galaxy.webapps.base.controller import SharableItemSecurityMixin
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class SearchController(BaseAPIController, SharableItemSecurityMixin):
-
+class SearchController(BaseGalaxyAPIController, SharableItemSecurityMixin):
     @web.legacy_expose_api
-    def create(self, trans, payload, **kwd):
+    def create(self, trans: ProvidesUserContext, payload: dict, **kwd):
         """
         POST /api/search
         Do a search of the various elements of Galaxy.
         """
         query_txt = payload.get("query", None)
         out = []
         if query_txt is not None:
             se = GalaxySearchEngine()
             try:
                 query = se.query(query_txt)
             except Exception as e:
-                return {'error': unicodify(e)}
+                return {"error": unicodify(e)}
             if query is not None:
                 query.decode_query_ids(trans)
                 current_user_roles = trans.get_current_user_roles()
                 try:
                     results = query.process(trans)
                 except Exception as e:
-                    return {'error': unicodify(e)}
+                    return {"error": unicodify(e)}
                 for item in results:
                     append = False
                     if trans.user_is_admin:
                         append = True
                     if not append:
-                        if type(item) in [trans.app.model.LibraryFolder, trans.app.model.LibraryDatasetDatasetAssociation, trans.app.model.LibraryDataset]:
-                            if (trans.app.security_agent.can_access_library_item(trans.get_current_user_roles(), item, trans.user)):
+                        if type(item) in [
+                            model.LibraryFolder,
+                            model.LibraryDatasetDatasetAssociation,
+                            model.LibraryDataset,
+                        ]:
+                            if trans.app.security_agent.can_access_library_item(
+                                trans.get_current_user_roles(), item, trans.user
+                            ):
                                 append = True
-                        elif type(item) in [trans.app.model.Job]:
+                        elif type(item) in [model.Job]:
                             if item.used_id == trans.user or trans.user_is_admin:
                                 append = True
-                        elif type(item) in [trans.app.model.Page, trans.app.model.StoredWorkflow]:
+                        elif type(item) in [model.Page, model.StoredWorkflow]:
                             try:
                                 if self.security_check(trans, item, False, True):
                                     append = True
                             except ItemAccessibilityException:
                                 append = False
-                        elif type(item) in [trans.app.model.PageRevision]:
+                        elif type(item) in [model.PageRevision]:
                             try:
                                 if self.security_check(trans, item.page, False, True):
                                     append = True
                             except ItemAccessibilityException:
                                 append = False
-                        elif hasattr(item, 'dataset'):
+                        elif hasattr(item, "dataset"):
                             if trans.app.security_agent.can_access_dataset(current_user_roles, item.dataset):
                                 append = True
 
                     if append:
                         row = query.item_to_api_value(item)
                         out.append(self.encode_all_ids(trans, row, True))
-        return {'results': out}
+        return {"results": out}
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tool_dependencies.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tool_dependencies.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,39 +1,41 @@
 """
 API operations allowing clients to manage tool dependencies.
 """
 import logging
+from typing import Optional
 
+from galaxy.managers.context import ProvidesAppContext
+from galaxy.structured_app import StructuredApp
 from galaxy.tool_util.deps import views
 from galaxy.web import (
     expose_api,
-    require_admin
+    require_admin,
 )
-from galaxy.webapps.base.controller import BaseAPIController
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class ToolDependenciesAPIController(BaseAPIController):
-
-    def __init__(self, app):
-        super(ToolDependenciesAPIController, self).__init__(app)
+class ToolDependenciesAPIController(BaseGalaxyAPIController):
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
         self._view = views.DependencyResolversView(app)
 
     @require_admin
     @expose_api
-    def index(self, trans, **kwd):
+    def index(self, trans: ProvidesAppContext, **kwd):
         """
         GET /api/dependency_resolvers
         """
         return self._view.index()
 
     @require_admin
     @expose_api
-    def show(self, trans, id):
+    def show(self, trans: ProvidesAppContext, id: str):
         """
         GET /api/dependency_resolvers/<id>
         """
         return self._view.show(id)
 
     @require_admin
     @expose_api
@@ -43,15 +45,15 @@
 
         Reload tool dependency resolution configuration.
         """
         return self._view.reload()
 
     @require_admin
     @expose_api
-    def resolver_dependency(self, trans, id, **kwds):
+    def resolver_dependency(self, trans: ProvidesAppContext, id: str, **kwds):
         """
         GET /api/dependency_resolvers/{index}/dependency
 
         Resolve described requirement against specified dependency resolver.
 
         :type   index:    int
         :param  index:    index of the dependency resolver
@@ -69,15 +71,15 @@
         :returns:   a dictified description of the dependency, with attribute
                     ``dependency_type: None`` if no match was found.
         """
         return self._view.resolver_dependency(id, **kwds)
 
     @require_admin
     @expose_api
-    def install_dependency(self, trans, id=None, **kwds):
+    def install_dependency(self, trans: ProvidesAppContext, id: Optional[str] = None, **kwds):
         """
         POST /api/dependency_resolvers/{index}/dependency
         POST /api/dependency_resolvers/dependency
 
         Install described requirement against specified dependency resolver.
 
         :type   index:    int
@@ -99,15 +101,15 @@
                     ``dependency_type: None`` if no match was found.
         """
         self._view.install_dependency(id, **kwds)
         return self._view.manager_dependency(**kwds)
 
     @require_admin
     @expose_api
-    def manager_dependency(self, trans, **kwds):
+    def manager_dependency(self, trans: ProvidesAppContext, **kwds):
         """
         GET /api/dependency_resolvers/dependency
 
         Resolve described requirement against all dependency resolvers, returning
         the match with highest priority.
 
         :type   index:    int
@@ -126,15 +128,15 @@
         :returns:   a dictified description of the dependency, with type: None
                     if no match was found.
         """
         return self._view.manager_dependency(**kwds)
 
     @require_admin
     @expose_api
-    def resolver_requirements(self, trans, id, **kwds):
+    def resolver_requirements(self, trans: ProvidesAppContext, id, **kwds):
         """
         GET /api/dependency_resolvers/{index}/requirements
 
         Find all "simple" requirements that could be resolved "exactly"
         by this dependency resolver. The dependency resolver must implement
         ListDependencyResolver.
 
@@ -145,15 +147,15 @@
         :returns:   a dictified description of the requirement that could
                     be resolved.
         """
         return self._view.resolver_requirements(id)
 
     @require_admin
     @expose_api
-    def manager_requirements(self, trans, **kwds):
+    def manager_requirements(self, trans: ProvidesAppContext, **kwds):
         """
         GET /api/dependency_resolvers/requirements
 
         Find all "simple" requirements that could be resolved "exactly"
         by all dependency resolvers that support this operation.
 
         :type   index:    int
@@ -164,15 +166,15 @@
                     be resolved (keyed on 'requirement') and the index of
                     the corresponding resolver (keyed on 'index').
         """
         return self._view.manager_requirements()
 
     @require_admin
     @expose_api
-    def clean(self, trans, id=None, **kwds):
+    def clean(self, trans: ProvidesAppContext, id=None, **kwds):
         """
         POST /api/dependency_resolvers/{index}/clean
 
         Cleans up intermediate files created by resolvers during the dependency
         installation.
 
         :type   index:    int
@@ -183,15 +185,15 @@
                     be resolved (keyed on 'requirement') and the index of
                     the corresponding resolver (keyed on 'index').
         """
         return self._view.clean(id, **kwds)
 
     @expose_api
     @require_admin
-    def summarize_toolbox(self, trans, **kwds):
+    def summarize_toolbox(self, trans: ProvidesAppContext, **kwds):
         """
         GET /api/dependency_resolvers/toolbox
 
         Summarize requirements across toolbox (for Tool Management grid). This is an experiemental
         API particularly tied to the GUI - expect breaking changes until this notice is removed.
 
         Container resolution via this API is especially experimental and the container resolution
@@ -221,15 +223,15 @@
         if index_by == "requirements":
             return self._view.summarize_requirements(**kwds)
         else:
             return self._view.summarize_tools(**kwds)
 
     @expose_api
     @require_admin
-    def toolbox_install(self, trans, payload, index=None, **kwds):
+    def toolbox_install(self, trans: ProvidesAppContext, payload, index=None, **kwds):
         """
         POST /api/dependency_resolvers/{index}/toolbox/install
         POST /api/dependency_resolvers/toolbox/install
 
         Install described requirement against specified dependency resolver(s). This is an experiemental
         API particularly tied to the GUI - expect breaking changes until this notice is removed.
 
@@ -245,25 +247,25 @@
         :param  container_type: restrict to uninstall to specified container type
         """
         tools_by_id = trans.app.toolbox.tools_by_id.copy()
         tool_ids = payload.get("tool_ids")
         requirements = {tools_by_id[tid].tool_requirements for tid in tool_ids}
         install_kwds = {}
         for source in [payload, kwds]:
-            if 'include_containers' in source:
-                install_kwds['include_containers'] = source['container_type']
-            if 'container_type' in kwds:
-                install_kwds['container_type'] = source['container_type']
-            if 'resolver_type' in source:
-                install_kwds['resolver_type'] = source['resolver_type']
+            if "include_containers" in source:
+                install_kwds["include_containers"] = source["container_type"]
+            if "container_type" in kwds:
+                install_kwds["container_type"] = source["container_type"]
+            if "resolver_type" in source:
+                install_kwds["resolver_type"] = source["resolver_type"]
         [self._view.install_dependencies(requirements=r, index=index, **install_kwds) for r in requirements]
 
     @expose_api
     @require_admin
-    def toolbox_uninstall(self, trans, payload, index=None, **kwds):
+    def toolbox_uninstall(self, trans: ProvidesAppContext, payload, index=None, **kwds):
         """
         POST /api/dependency_resolvers/{index}/toolbox/uninstall
         POST /api/dependency_resolvers/toolbox/uninstall
 
         Uninstall described requirement against specified dependency resolver(s). This is an experiemental
         API particularly tied to the GUI - expect breaking changes until this notice is removed.
 
@@ -274,37 +276,37 @@
         :type   include_containers: bool
         :param  include_containers: include container resolvers in resolution
         :type   container_type: str
         :param  container_type: restrict to uninstall to specified container type
         :type   resolver_type:  str
         :param  resolver_type:  restrict to uninstall to specified resolver type
         """
-        tools_by_id = trans.app.toolbox.tools_by_id.copy()
+        tools_by_id = self.app.toolbox.tools_by_id.copy()
         tool_ids = payload.get("tool_ids")
         requirements = {tools_by_id[tid].tool_requirements for tid in tool_ids}
         install_kwds = {}
         for source in [payload, kwds]:
-            if 'include_containers' in source:
-                install_kwds['include_containers'] = source['container_type']
-            if 'container_type' in kwds:
-                install_kwds['container_type'] = source['container_type']
-            if 'resolver_type' in source:
-                install_kwds['resolver_type'] = source['resolver_type']
+            if "include_containers" in source:
+                install_kwds["include_containers"] = source["container_type"]
+            if "container_type" in kwds:
+                install_kwds["container_type"] = source["container_type"]
+            if "resolver_type" in source:
+                install_kwds["resolver_type"] = source["resolver_type"]
 
         [self._view.uninstall_dependencies(index=index, requirements=r, **install_kwds) for r in requirements]
 
     @expose_api
     @require_admin
-    def unused_dependency_paths(self, trans, **kwds):
+    def unused_dependency_paths(self, trans: ProvidesAppContext, **kwds):
         """
         GET /api/dependency_resolvers/unused_paths
         """
         return list(self._view.unused_dependency_paths)
 
     @expose_api
     @require_admin
-    def delete_unused_dependency_paths(self, trans, payload, **kwds):
+    def delete_unused_dependency_paths(self, trans: ProvidesAppContext, payload, **kwds):
         """
         PUT /api/dependency_resolvers/unused_paths
         """
         paths = payload.get("paths")
         self._view.remove_unused_dependency_paths(paths)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tool_shed_repositories.py` & `galaxy-web-apps-23.0.2/tool_shed/util/repository_util.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,398 +1,574 @@
-import json
+import configparser
 import logging
-from time import strftime
+import os
+import re
 
-from paste.httpexceptions import (
-    HTTPBadRequest,
-    HTTPForbidden
-)
-from sqlalchemy import and_
+from markupsafe import escape
+from sqlalchemy import false
+from sqlalchemy.orm import joinedload
+from sqlalchemy.sql import select
 
+import tool_shed.dependencies.repository
 from galaxy import (
-    exceptions,
-    util
+    util,
+    web,
 )
-from galaxy.tool_shed.galaxy_install.install_manager import InstallRepositoryManager
-from galaxy.tool_shed.galaxy_install.installed_repository_manager import InstalledRepositoryManager
-from galaxy.tool_shed.galaxy_install.metadata.installed_repository_metadata_manager import InstalledRepositoryMetadataManager
 from galaxy.tool_shed.util.repository_util import (
-    check_for_updates,
+    create_or_update_tool_shed_repository,
+    extract_components_from_tuple,
+    generate_tool_shed_repository_install_dir,
+    get_absolute_path_to_file_in_repository,
+    get_ids_of_tool_shed_repositories_being_installed,
     get_installed_repository,
     get_installed_tool_shed_repository,
+    get_prior_import_or_install_required_dict,
+    get_repo_info_tuple_contents,
+    get_repository_admin_role_name,
+    get_repository_and_repository_dependencies_from_repo_info_dict,
+    get_repository_by_id,
+    get_repository_by_name,
+    get_repository_by_name_and_owner,
+    get_repository_dependency_types,
+    get_repository_for_dependency_relationship,
+    get_repository_ids_requiring_prior_import_or_install,
+    get_repository_owner,
+    get_repository_owner_from_clone_url,
+    get_repository_query,
+    get_role_by_id,
+    get_tool_shed_from_clone_url,
     get_tool_shed_repository_by_id,
+    get_tool_shed_status_for_installed_repository,
+    is_tool_shed_client,
+    repository_was_previously_installed,
+    set_repository_attributes,
 )
-from galaxy.tool_shed.util.shed_util_common import have_shed_tool_conf_for_install
-from galaxy.tool_shed.util.tool_util import generate_message_for_invalid_tools
-from galaxy.web import (
-    expose_api,
-    require_admin,
-    url_for
+from galaxy.util.tool_shed import common_util
+from tool_shed.util.hg_util import (
+    changeset2rev,
+    create_hgrc_file,
+    get_hgrc_path,
+    init_repository,
+)
+from tool_shed.util.metadata_util import (
+    get_next_downloadable_changeset_revision,
+    get_repository_metadata_by_changeset_revision,
 )
-from galaxy.webapps.base.controller import BaseAPIController
-
 
 log = logging.getLogger(__name__)
 
+VALID_REPOSITORYNAME_RE = re.compile(r"^[a-z0-9\_]+$")
 
-def get_message_for_no_shed_tool_config():
-    # This Galaxy instance is not configured with a shed-related tool panel configuration file.
-    message = 'The tool_config_file setting in galaxy.ini must include at least one shed tool configuration file name with a <toolbox> '
-    message += 'tag that includes a tool_path attribute value which is a directory relative to the Galaxy installation directory in order to '
-    message += 'automatically install tools from a tool shed into Galaxy (e.g., the file name shed_tool_conf.xml whose <toolbox> tag is '
-    message += '<toolbox tool_path="database/shed_tools">).  For details, see the "Installation of Galaxy tool shed repository tools into a '
-    message += 'local Galaxy instance" section of the Galaxy tool shed wiki at https://galaxyproject.org/installing-repositories-to-galaxy/'
-    return message
-
-
-class ToolShedRepositoriesController(BaseAPIController):
-    """RESTful controller for interactions with tool shed repositories."""
-
-    def __ensure_can_install_repos(self, trans):
-        # Make sure this Galaxy instance is configured with a shed-related tool panel configuration file.
-        if not have_shed_tool_conf_for_install(self.app):
-            message = get_message_for_no_shed_tool_config()
-            log.debug(message)
-            return dict(status='error', error=message)
-        # Make sure the current user's API key proves he is an admin user in this Galaxy instance.
-        if not trans.user_is_admin:
-            raise exceptions.AdminRequiredException('You are not authorized to request the latest installable revision for a repository in this Galaxy instance.')
-
-    def __get_value_mapper(self, trans, tool_shed_repository):
-        value_mapper = {'id': trans.security.encode_id(tool_shed_repository.id),
-                        'error_message': tool_shed_repository.error_message or ''}
-        return value_mapper
-
-    @expose_api
-    def index(self, trans, **kwd):
-        """
-        GET /api/tool_shed_repositories
-        Display a list of dictionaries containing information about installed tool shed repositories.
-        """
-        # Example URL: http://localhost:8763/api/tool_shed_repositories
-        clause_list = []
-        if 'name' in kwd:
-            clause_list.append(self.app.install_model.ToolShedRepository.table.c.name == kwd.get('name'))
-        if 'owner' in kwd:
-            clause_list.append(self.app.install_model.ToolShedRepository.table.c.owner == kwd.get('owner'))
-        if 'changeset' in kwd:
-            clause_list.append(self.app.install_model.ToolShedRepository.table.c.changeset_revision == kwd.get('changeset'))
-        if 'deleted' in kwd:
-            clause_list.append(self.app.install_model.ToolShedRepository.table.c.deleted == util.asbool(kwd.get('deleted')))
-        if 'uninstalled' in kwd:
-            clause_list.append(self.app.install_model.ToolShedRepository.table.c.uninstalled == util.asbool(kwd.get('uninstalled')))
-        tool_shed_repository_dicts = []
-        query = trans.install_model.context.query(self.app.install_model.ToolShedRepository) \
-                                           .order_by(self.app.install_model.ToolShedRepository.table.c.name)
-        if len(clause_list) > 0:
-            query = query.filter(and_(*clause_list))
-        for tool_shed_repository in query.all():
-            tool_shed_repository_dict = \
-                tool_shed_repository.to_dict(value_mapper=self.__get_value_mapper(trans, tool_shed_repository))
-            tool_shed_repository_dict['url'] = url_for(controller='tool_shed_repositories',
-                                                       action='show',
-                                                       id=trans.security.encode_id(tool_shed_repository.id))
-            tool_shed_repository_dicts.append(tool_shed_repository_dict)
-        return tool_shed_repository_dicts
-
-    @require_admin
-    @expose_api
-    def install_repository_revision(self, trans, payload, **kwd):
-        """
-        POST /api/tool_shed_repositories/install_repository_revision
-        Install a specified repository revision from a specified tool shed into Galaxy.
-
-        :param key: the current Galaxy admin user's API key
-
-        The following parameters are included in the payload.
-        :param tool_shed_url (required): the base URL of the Tool Shed from which to install the Repository
-        :param name (required): the name of the Repository
-        :param owner (required): the owner of the Repository
-        :param changeset_revision (required): the changeset_revision of the RepositoryMetadata object associated with the Repository
-        :param new_tool_panel_section_label (optional): label of a new section to be added to the Galaxy tool panel in which to load
-                                                        tools contained in the Repository.  Either this parameter must be an empty string or
-                                                        the tool_panel_section_id parameter must be an empty string or both must be an empty
-                                                        string (both cannot be used simultaneously).
-        :param tool_panel_section_id (optional): id of the Galaxy tool panel section in which to load tools contained in the Repository.
-                                                 If this parameter is an empty string and the above new_tool_panel_section_label parameter is an
-                                                 empty string, tools will be loaded outside of any sections in the tool panel.  Either this
-                                                 parameter must be an empty string or the tool_panel_section_id parameter must be an empty string
-                                                 of both must be an empty string (both cannot be used simultaneously).
-        :param install_repository_dependencies (optional): Set to True if you want to install repository dependencies defined for the specified
-                                                           repository being installed.  The default setting is False.
-        :param install_tool_dependencies (optional): Set to True if you want to install tool dependencies defined for the specified repository being
-                                                     installed.  The default setting is False.
-        :param shed_tool_conf (optional): The shed-related tool panel configuration file configured in the "tool_config_file" setting in the Galaxy config file
-                                          (e.g., galaxy.ini).  At least one shed-related tool panel config file is required to be configured. Setting
-                                          this parameter to a specific file enables you to choose where the specified repository will be installed because
-                                          the tool_path attribute of the <toolbox> from the specified file is used as the installation location
-                                          (e.g., <toolbox tool_path="database/shed_tools">).  If this parameter is not set, a shed-related tool panel
-                                          configuration file will be selected automatically.
-        """
-        # Get the information about the repository to be installed from the payload.
-        tool_shed_url, name, owner, changeset_revision = self.__parse_repository_from_payload(payload, include_changeset=True)
-        self.__ensure_can_install_repos(trans)
-        irm = InstallRepositoryManager(self.app)
-        installed_tool_shed_repositories = irm.install(tool_shed_url,
-                                                       name,
-                                                       owner,
-                                                       changeset_revision,
-                                                       payload)
-
-        def to_dict(tool_shed_repository):
-            tool_shed_repository_dict = tool_shed_repository.as_dict(value_mapper=self.__get_value_mapper(trans, tool_shed_repository))
-            tool_shed_repository_dict['url'] = url_for(controller='tool_shed_repositories',
-                                                       action='show',
-                                                       id=trans.security.encode_id(tool_shed_repository.id))
-            return tool_shed_repository_dict
-        if installed_tool_shed_repositories:
-            return list(map(to_dict, installed_tool_shed_repositories))
-        message = "No repositories were installed, possibly because the selected repository has already been installed."
-        return dict(status="ok", message=message)
-
-    @require_admin
-    @expose_api
-    def install_repository_revisions(self, trans, payload, **kwd):
-        """
-        POST /api/tool_shed_repositories/install_repository_revisions
-        Install one or more specified repository revisions from one or more specified tool sheds into Galaxy.  The received parameters
-        must be ordered lists so that positional values in tool_shed_urls, names, owners and changeset_revisions are associated.
-
-        It's questionable whether this method is needed as the above method for installing a single repository can probably cover all
-        desired scenarios.  We'll keep this one around just in case...
-
-        :param key: the current Galaxy admin user's API key
-
-        The following parameters are included in the payload.
-        :param tool_shed_urls: the base URLs of the Tool Sheds from which to install a specified Repository
-        :param names: the names of the Repositories to be installed
-        :param owners: the owners of the Repositories to be installed
-        :param changeset_revisions: the changeset_revisions of each RepositoryMetadata object associated with each Repository to be installed
-        :param new_tool_panel_section_label: optional label of a new section to be added to the Galaxy tool panel in which to load
-                                             tools contained in the Repository.  Either this parameter must be an empty string or
-                                             the tool_panel_section_id parameter must be an empty string, as both cannot be used.
-        :param tool_panel_section_id: optional id of the Galaxy tool panel section in which to load tools contained in the Repository.
-                                      If not set, tools will be loaded outside of any sections in the tool panel.  Either this
-                                      parameter must be an empty string or the tool_panel_section_id parameter must be an empty string,
-                                      as both cannot be used.
-        :param install_repository_dependencies (optional): Set to True if you want to install repository dependencies defined for the specified
-                                                           repository being installed.  The default setting is False.
-        :param install_tool_dependencies (optional): Set to True if you want to install tool dependencies defined for the specified repository being
-                                                     installed.  The default setting is False.
-        :param shed_tool_conf (optional): The shed-related tool panel configuration file configured in the "tool_config_file" setting in the Galaxy config file
-                                          (e.g., galaxy.ini).  At least one shed-related tool panel config file is required to be configured. Setting
-                                          this parameter to a specific file enables you to choose where the specified repository will be installed because
-                                          the tool_path attribute of the <toolbox> from the specified file is used as the installation location
-                                          (e.g., <toolbox tool_path="database/shed_tools">).  If this parameter is not set, a shed-related tool panel
-                                          configuration file will be selected automatically.
-        """
-        self.__ensure_can_install_repos(trans)
-        # Get the information about all of the repositories to be installed.
-        tool_shed_urls = util.listify(payload.get('tool_shed_urls', ''))
-        names = util.listify(payload.get('names', ''))
-        owners = util.listify(payload.get('owners', ''))
-        changeset_revisions = util.listify(payload.get('changeset_revisions', ''))
-        num_specified_repositories = len(tool_shed_urls)
-        if len(names) != num_specified_repositories or \
-                len(owners) != num_specified_repositories or \
-                len(changeset_revisions) != num_specified_repositories:
-            message = 'Error in tool_shed_repositories API in install_repository_revisions: the received parameters must be ordered '
-            message += 'lists so that positional values in tool_shed_urls, names, owners and changeset_revisions are associated.'
-            log.debug(message)
-            return dict(status='error', error=message)
-        # Get the information about the Galaxy components (e.g., tool pane section, tool config file, etc) that will contain information
-        # about each of the repositories being installed.
-        # TODO: we may want to enhance this method to allow for each of the following to be associated with each repository instead of
-        # forcing all repositories to use the same settings.
-        install_repository_dependencies = payload.get('install_repository_dependencies', False)
-        install_resolver_dependencies = payload.get('install_resolver_dependencies', False)
-        install_tool_dependencies = payload.get('install_tool_dependencies', False)
-        new_tool_panel_section_label = payload.get('new_tool_panel_section_label', '')
-        shed_tool_conf = payload.get('shed_tool_conf', None)
-        tool_panel_section_id = payload.get('tool_panel_section_id', '')
-        all_installed_tool_shed_repositories = []
-        for tool_shed_url, name, owner, changeset_revision in zip(tool_shed_urls, names, owners, changeset_revisions):
-            current_payload = dict(tool_shed_url=tool_shed_url,
-                                   name=name,
-                                   owner=owner,
-                                   changeset_revision=changeset_revision,
-                                   new_tool_panel_section_label=new_tool_panel_section_label,
-                                   tool_panel_section_id=tool_panel_section_id,
-                                   install_repository_dependencies=install_repository_dependencies,
-                                   install_resolver_dependencies=install_resolver_dependencies,
-                                   install_tool_dependencies=install_tool_dependencies,
-                                   shed_tool_conf=shed_tool_conf)
-            installed_tool_shed_repositories = self.install_repository_revision(trans, **current_payload)
-            if isinstance(installed_tool_shed_repositories, dict):
-                # We encountered an error.
-                return installed_tool_shed_repositories
-            elif isinstance(installed_tool_shed_repositories, list):
-                all_installed_tool_shed_repositories.extend(installed_tool_shed_repositories)
-        return all_installed_tool_shed_repositories
-
-    @require_admin
-    @expose_api
-    def uninstall_repository(self, trans, id=None, **kwd):
-        """
-        DELETE /api/tool_shed_repositories/id
-        DELETE /api/tool_shed_repositories/
-
-        :param id:  encoded repository id. Either id or name, owner, changeset_revision and tool_shed_url need to be supplied
-        :param kwd: 'remove_from_disk'  : Remove repository from disk or deactivate repository.
-                                          Defaults to `True` (= remove repository from disk).
-                    'name'   : Repository name
-                    'owner'  : Repository owner
-                    'changeset_revision': Changeset revision to uninstall
-                    'tool_shed_url'     : Tool Shed URL
-        """
-        remove_from_disk = util.asbool(kwd.get('remove_from_disk', True))
-        if id:
-            try:
-                repository = get_tool_shed_repository_by_id(self.app, id)
-            except ValueError:
-                raise HTTPBadRequest(detail="No repository with id '%s' found" % id)
+
+def create_repo_info_dict(
+    app,
+    repository_clone_url,
+    changeset_revision,
+    ctx_rev,
+    repository_owner,
+    repository_name=None,
+    repository=None,
+    repository_metadata=None,
+    tool_dependencies=None,
+    repository_dependencies=None,
+):
+    """
+    Return a dictionary that includes all of the information needed to install a repository into a local
+    Galaxy instance.  The dictionary will also contain the recursive list of repository dependencies defined
+    for the repository, as well as the defined tool dependencies.
+
+    This method is called from Galaxy under four scenarios:
+    1. During the tool shed repository installation process via the tool shed's get_repository_information()
+    method.  In this case both the received repository and repository_metadata will be objects, but
+    tool_dependencies and repository_dependencies will be None.
+    2. When getting updates for an installed repository where the updates include newly defined repository
+    dependency definitions.  This scenario is similar to 1. above. The tool shed's get_repository_information()
+    method is the caller, and both the received repository and repository_metadata will be objects, but
+    tool_dependencies and repository_dependencies will be None.
+    3. When a tool shed repository that was uninstalled from a Galaxy instance is being reinstalled with no
+    updates available.  In this case, both repository and repository_metadata will be None, but tool_dependencies
+    and repository_dependencies will be objects previously retrieved from the tool shed if the repository includes
+    definitions for them.
+    4. When a tool shed repository that was uninstalled from a Galaxy instance is being reinstalled with updates
+    available.  In this case, this method is reached via the tool shed's get_updated_repository_information()
+    method, and both repository and repository_metadata will be objects but tool_dependencies and
+    repository_dependencies will be None.
+    """
+    repo_info_dict = {}
+    repository = get_repository_by_name_and_owner(app, repository_name, repository_owner)
+    if app.name == "tool_shed":
+        # We're in the tool shed.
+        repository_metadata = get_repository_metadata_by_changeset_revision(
+            app, app.security.encode_id(repository.id), changeset_revision
+        )
+        if repository_metadata:
+            metadata = repository_metadata.metadata
+            if metadata:
+                tool_shed_url = web.url_for("/", qualified=True).rstrip("/")
+                rb = tool_shed.dependencies.repository.relation_builder.RelationBuilder(
+                    app, repository, repository_metadata, tool_shed_url
+                )
+                # Get a dictionary of all repositories upon which the contents of the received repository depends.
+                repository_dependencies = rb.get_repository_dependencies_for_changeset_revision()
+                tool_dependencies = metadata.get("tool_dependencies", {})
+    if tool_dependencies:
+        new_tool_dependencies = {}
+        for dependency_key, requirements_dict in tool_dependencies.items():
+            if dependency_key in ["set_environment"]:
+                new_set_environment_dict_list = []
+                for set_environment_dict in requirements_dict:
+                    set_environment_dict["repository_name"] = repository_name
+                    set_environment_dict["repository_owner"] = repository_owner
+                    set_environment_dict["changeset_revision"] = changeset_revision
+                    new_set_environment_dict_list.append(set_environment_dict)
+                new_tool_dependencies[dependency_key] = new_set_environment_dict_list
+            else:
+                requirements_dict["repository_name"] = repository_name
+                requirements_dict["repository_owner"] = repository_owner
+                requirements_dict["changeset_revision"] = changeset_revision
+                new_tool_dependencies[dependency_key] = requirements_dict
+        tool_dependencies = new_tool_dependencies
+    repo_info_dict[repository.name] = (
+        repository.description,
+        repository_clone_url,
+        changeset_revision,
+        ctx_rev,
+        repository_owner,
+        repository_dependencies,
+        tool_dependencies,
+    )
+    return repo_info_dict
+
+
+def create_repository_admin_role(app, repository):
+    """
+    Create a new role with name-spaced name based on the repository name and its owner's public user
+    name.  This will ensure that the tole name is unique.
+    """
+    sa_session = app.model.session
+    name = get_repository_admin_role_name(str(repository.name), str(repository.user.username))
+    description = "A user or group member with this role can administer this repository."
+    role = app.model.Role(name=name, description=description, type=app.model.Role.types.SYSTEM)
+    sa_session.add(role)
+    sa_session.flush()
+    # Associate the role with the repository owner.
+    app.model.UserRoleAssociation(repository.user, role)
+    # Associate the role with the repository.
+    rra = app.model.RepositoryRoleAssociation(repository, role)
+    sa_session.add(rra)
+    sa_session.flush()
+    return role
+
+
+def create_repository(
+    app,
+    name,
+    type,
+    description,
+    long_description,
+    user_id,
+    category_ids=None,
+    remote_repository_url=None,
+    homepage_url=None,
+):
+    """Create a new ToolShed repository"""
+    category_ids = category_ids or []
+    sa_session = app.model.session
+    # Add the repository record to the database.
+    repository = app.model.Repository(
+        name=name,
+        type=type,
+        remote_repository_url=remote_repository_url,
+        homepage_url=homepage_url,
+        description=description,
+        long_description=long_description,
+        user_id=user_id,
+    )
+    # Flush to get the id.
+    sa_session.add(repository)
+    sa_session.flush()
+    # Create an admin role for the repository.
+    create_repository_admin_role(app, repository)
+    # Determine the repository's repo_path on disk.
+    dir = os.path.join(app.config.file_path, *util.directory_hash_id(repository.id))
+    # Create directory if it does not exist.
+    if not os.path.exists(dir):
+        os.makedirs(dir)
+    # Define repo name inside hashed directory.
+    repository_path = os.path.join(dir, "repo_%d" % repository.id)
+    # Create local repository directory.
+    if not os.path.exists(repository_path):
+        os.makedirs(repository_path)
+    # Create the local repository.
+    init_repository(repo_path=repository_path)
+    # Add an entry in the hgweb.config file for the local repository.
+    lhs = f"repos/{repository.user.username}/{repository.name}"
+    app.hgweb_config_manager.add_entry(lhs, repository_path)
+    # Create a .hg/hgrc file for the local repository.
+    create_hgrc_file(app, repository)
+    flush_needed = False
+    if category_ids:
+        # Create category associations
+        for category_id in category_ids:
+            category = sa_session.query(app.model.Category).get(app.security.decode_id(category_id))
+            rca = app.model.RepositoryCategoryAssociation(repository, category)
+            sa_session.add(rca)
+            flush_needed = True
+    if flush_needed:
+        sa_session.flush()
+    # Update the repository registry.
+    app.repository_registry.add_entry(repository)
+    message = f"Repository <b>{escape(str(repository.name))}</b> has been created."
+    return repository, message
+
+
+def generate_sharable_link_for_repository_in_tool_shed(repository, changeset_revision=None):
+    """Generate the URL for sharing a repository that is in the tool shed."""
+    base_url = web.url_for("/", qualified=True).rstrip("/")
+    sharable_url = f"{base_url}/view/{repository.user.username}/{repository.name}"
+    if changeset_revision:
+        sharable_url += f"/{changeset_revision}"
+    return sharable_url
+
+
+def get_repository_in_tool_shed(app, id, eagerload_columns=None):
+    """Get a repository on the tool shed side from the database via id."""
+    q = get_repository_query(app)
+    if eagerload_columns:
+        q = q.options(joinedload(*eagerload_columns))
+    return q.get(app.security.decode_id(id))
+
+
+def get_repo_info_dict(app, user, repository_id, changeset_revision):
+    repository = get_repository_in_tool_shed(app, repository_id)
+    repository_clone_url = common_util.generate_clone_url_for_repository_in_tool_shed(user, repository)
+    repository_metadata = get_repository_metadata_by_changeset_revision(app, repository_id, changeset_revision)
+    if not repository_metadata:
+        # The received changeset_revision is no longer installable, so get the next changeset_revision
+        # in the repository's changelog.  This generally occurs only with repositories of type
+        # repository_suite_definition or tool_dependency_definition.
+        next_downloadable_changeset_revision = get_next_downloadable_changeset_revision(
+            app, repository, changeset_revision
+        )
+        if next_downloadable_changeset_revision and next_downloadable_changeset_revision != changeset_revision:
+            repository_metadata = get_repository_metadata_by_changeset_revision(
+                app, repository_id, next_downloadable_changeset_revision
+            )
+    if repository_metadata:
+        # For now, we'll always assume that we'll get repository_metadata, but if we discover our assumption
+        # is not valid we'll have to enhance the callers to handle repository_metadata values of None in the
+        # returned repo_info_dict.
+        metadata = repository_metadata.metadata
+        if "tools" in metadata:
+            includes_tools = True
         else:
-            tsr_arguments = ['name', 'owner', 'changeset_revision', 'tool_shed_url']
-            try:
-                tsr_arguments = {key: kwd[key] for key in tsr_arguments}
-            except KeyError as e:
-                raise HTTPBadRequest(detail="Missing required parameter '%s'" % e.args[0])
-            repository = get_installed_repository(app=self.app,
-                                                  tool_shed=tsr_arguments['tool_shed_url'],
-                                                  name=tsr_arguments['name'],
-                                                  owner=tsr_arguments['owner'],
-                                                  changeset_revision=tsr_arguments['changeset_revision'])
-            if not repository:
-                raise HTTPBadRequest(detail="Repository not found")
-        irm = InstalledRepositoryManager(app=self.app)
-        errors = irm.uninstall_repository(repository=repository, remove_from_disk=remove_from_disk)
-        if not errors:
-            action = 'removed' if remove_from_disk else 'deactivated'
-            return {'message': 'The repository named %s has been %s.' % (repository.name, action)}
+            includes_tools = False
+        includes_tools_for_display_in_tool_panel = repository_metadata.includes_tools_for_display_in_tool_panel
+        repository_dependencies_dict = metadata.get("repository_dependencies", {})
+        repository_dependencies = repository_dependencies_dict.get("repository_dependencies", [])
+        (
+            has_repository_dependencies,
+            has_repository_dependencies_only_if_compiling_contained_td,
+        ) = get_repository_dependency_types(repository_dependencies)
+        if "tool_dependencies" in metadata:
+            includes_tool_dependencies = True
         else:
-            raise Exception('Attempting to uninstall tool dependencies for repository named %s resulted in errors: %s' % (repository.name, errors))
-
-    def __parse_repository_from_payload(self, payload, include_changeset=False):
-        # Get the information about the repository to be installed from the payload.
-        tool_shed_url = payload.get('tool_shed_url', '')
-        if not tool_shed_url:
-            raise exceptions.RequestParameterMissingException("Missing required parameter 'tool_shed_url'.")
-        name = payload.get('name', '')
-        if not name:
-            raise exceptions.RequestParameterMissingException("Missing required parameter 'name'.")
-        owner = payload.get('owner', '')
-        if not owner:
-            raise exceptions.RequestParameterMissingException("Missing required parameter 'owner'.")
-        if not include_changeset:
-            return tool_shed_url, name, owner
-
-        changeset_revision = payload.get('changeset_revision', '')
-        if not changeset_revision:
-            raise HTTPBadRequest(detail="Missing required parameter 'changeset_revision'.")
-
-        return tool_shed_url, name, owner, changeset_revision
-
-    @require_admin
-    @expose_api
-    def check_for_updates(self, trans, **kwd):
-        '''
-        GET /api/tool_shed_repositories/check_for_updates
-        Check for updates to the specified repository, or all installed repositories.
-
-        :param id: the encoded repository id
-        '''
-        repository_id = kwd.get('id', None)
-        message, status = check_for_updates(self.app, trans.install_model, repository_id)
-        return {'status': status, 'message': message}
-
-    @require_admin
-    @expose_api
-    def reset_metadata_on_selected_installed_repositories(self, trans, **kwd):
-        repository_ids = util.listify(kwd.get("repository_ids"))
-        if repository_ids:
-            irmm = InstalledRepositoryMetadataManager(self.app)
-            failed = []
-            successful = []
-            for repository_id in repository_ids:
-                try:
-                    repository = get_installed_tool_shed_repository(self.app, repository_id)
-                    irmm.set_repository(repository)
-                    irmm.reset_all_metadata_on_installed_repository()
-                    if irmm.invalid_file_tups:
-                        failed.append(repository_id)
-                    else:
-                        successful.append(repository_id)
-                except Exception:
-                    failed.append(repository_id)
-            if successful:
-                message = "Successful reset of metadata for %s." % len(successful)
-                if failed:
-                    message += " Failed for %s." % len(failed)
-            elif failed:
-                message = "Failed to reset metadata for %s." % len(failed)
-            return dict(message=message, successful=successful, failed=failed)
+            includes_tool_dependencies = False
+    else:
+        # Here's where we may have to handle enhancements to the callers. See above comment.
+        includes_tools = False
+        has_repository_dependencies = False
+        has_repository_dependencies_only_if_compiling_contained_td = False
+        includes_tool_dependencies = False
+        includes_tools_for_display_in_tool_panel = False
+    repo_path = repository.repo_path(app)
+    ctx_rev = str(changeset2rev(repo_path, changeset_revision))
+    repo_info_dict = create_repo_info_dict(
+        app=app,
+        repository_clone_url=repository_clone_url,
+        changeset_revision=changeset_revision,
+        ctx_rev=ctx_rev,
+        repository_owner=repository.user.username,
+        repository_name=repository.name,
+        repository=repository,
+        repository_metadata=repository_metadata,
+        tool_dependencies=None,
+        repository_dependencies=None,
+    )
+    return (
+        repo_info_dict,
+        includes_tools,
+        includes_tool_dependencies,
+        includes_tools_for_display_in_tool_panel,
+        has_repository_dependencies,
+        has_repository_dependencies_only_if_compiling_contained_td,
+    )
+
+
+def get_repositories_by_category(
+    app, category_id, installable=False, sort_order="asc", sort_key="name", page=None, per_page=25
+):
+    sa_session = app.model.session
+    query = (
+        sa_session.query(app.model.Repository)
+        .join(
+            app.model.RepositoryCategoryAssociation,
+            app.model.Repository.id == app.model.RepositoryCategoryAssociation.repository_id,
+        )
+        .join(app.model.User, app.model.User.id == app.model.Repository.user_id)
+        .filter(app.model.RepositoryCategoryAssociation.category_id == category_id)
+    )
+    if installable:
+        subquery = select([app.model.RepositoryMetadata.table.c.repository_id])
+        query = query.filter(app.model.Repository.id.in_(subquery))
+    if sort_key == "owner":
+        query = (
+            query.order_by(app.model.User.username)
+            if sort_order == "asc"
+            else query.order_by(app.model.User.username.desc())
+        )
+    else:
+        query = (
+            query.order_by(app.model.Repository.name)
+            if sort_order == "asc"
+            else query.order_by(app.model.Repository.name.desc())
+        )
+    if page is not None:
+        page = int(page)
+        query = query.limit(per_page)
+        if page > 1:
+            query = query.offset((page - 1) * per_page)
+    resultset = query.all()
+    repositories = []
+    for repository in resultset:
+        default_value_mapper = {
+            "id": app.security.encode_id,
+            "user_id": app.security.encode_id,
+            "repository_id": app.security.encode_id,
+        }
+        repository_dict = repository.to_dict(value_mapper=default_value_mapper)
+        repository_dict["metadata"] = {}
+        for changeset, changehash in repository.installable_revisions(app):
+            encoded_id = app.security.encode_id(repository.id)
+            metadata = get_repository_metadata_by_changeset_revision(app, encoded_id, changehash)
+            repository_dict["metadata"][f"{changeset}:{changehash}"] = metadata.to_dict(
+                value_mapper=default_value_mapper
+            )
+        if installable:
+            if len(repository.installable_revisions(app)):
+                repositories.append(repository_dict)
         else:
-            raise exceptions.MessageException("Please specify repository ids [repository_ids].")
+            repositories.append(repository_dict)
+    return repositories
+
 
-    @expose_api
-    def reset_metadata_on_installed_repositories(self, trans, payload, **kwd):
-        """
-        PUT /api/tool_shed_repositories/reset_metadata_on_installed_repositories
-
-        Resets all metadata on all repositories installed into Galaxy in an "orderly fashion".
-
-        :param key: the API key of the Galaxy admin user.
-        """
-        start_time = strftime("%Y-%m-%d %H:%M:%S")
-        results = dict(start_time=start_time,
-                       successful_count=0,
-                       unsuccessful_count=0,
-                       repository_status=[])
-        # Make sure the current user's API key proves he is an admin user in this Galaxy instance.
-        if not trans.user_is_admin:
-            raise HTTPForbidden(detail='You are not authorized to reset metadata on repositories installed into this Galaxy instance.')
-        irmm = InstalledRepositoryMetadataManager(self.app)
-        query = irmm.get_query_for_setting_metadata_on_repositories(order=False)
-        # Now reset metadata on all remaining repositories.
-        for repository in query:
-            try:
-                irmm.set_repository(repository)
-                irmm.reset_all_metadata_on_installed_repository()
-                irmm_invalid_file_tups = irmm.get_invalid_file_tups()
-                if irmm_invalid_file_tups:
-                    message = generate_message_for_invalid_tools(self.app,
-                                                                 irmm_invalid_file_tups,
-                                                                 repository,
-                                                                 None,
-                                                                 as_html=False)
-                    results['unsuccessful_count'] += 1
-                else:
-                    message = "Successfully reset metadata on repository %s owned by %s" % \
-                        (str(repository.name), str(repository.owner))
-                    results['successful_count'] += 1
-            except Exception as e:
-                message = "Error resetting metadata on repository %s owned by %s: %s" % \
-                    (str(repository.name), str(repository.owner), util.unicodify(e))
-                results['unsuccessful_count'] += 1
-            results['repository_status'].append(message)
-        stop_time = strftime("%Y-%m-%d %H:%M:%S")
-        results['stop_time'] = stop_time
-        return json.dumps(results, sort_keys=True, indent=4)
-
-    @expose_api
-    def show(self, trans, id, **kwd):
-        """
-        GET /api/tool_shed_repositories/{encoded_tool_shed_repsository_id}
-        Display a dictionary containing information about a specified tool_shed_repository.
-
-        :param id: the encoded id of the ToolShedRepository object
-        """
-        # Example URL: http://localhost:8763/api/tool_shed_repositories/df7a1f0c02a5b08e
-        tool_shed_repository = get_tool_shed_repository_by_id(self.app, id)
-        if tool_shed_repository is None:
-            log.debug("Unable to locate tool_shed_repository record for id %s." % (str(id)))
-            return {}
-        tool_shed_repository_dict = tool_shed_repository.as_dict(value_mapper=self.__get_value_mapper(trans, tool_shed_repository))
-        tool_shed_repository_dict['url'] = url_for(controller='tool_shed_repositories',
-                                                   action='show',
-                                                   id=trans.security.encode_id(tool_shed_repository.id))
-        return tool_shed_repository_dict
+def handle_role_associations(app, role, repository, **kwd):
+    sa_session = app.model.session
+    message = escape(kwd.get("message", ""))
+    status = kwd.get("status", "done")
+    repository_owner = repository.user
+    if kwd.get("manage_role_associations_button", False):
+        in_users_list = util.listify(kwd.get("in_users", []))
+        in_users = [sa_session.query(app.model.User).get(x) for x in in_users_list]
+        # Make sure the repository owner is always associated with the repostory's admin role.
+        owner_associated = False
+        for user in in_users:
+            if user.id == repository_owner.id:
+                owner_associated = True
+                break
+        if not owner_associated:
+            in_users.append(repository_owner)
+            message += "The repository owner must always be associated with the repository's administrator role.  "
+            status = "error"
+        in_groups_list = util.listify(kwd.get("in_groups", []))
+        in_groups = [sa_session.query(app.model.Group).get(x) for x in in_groups_list]
+        in_repositories = [repository]
+        app.security_agent.set_entity_role_associations(
+            roles=[role], users=in_users, groups=in_groups, repositories=in_repositories
+        )
+        sa_session.refresh(role)
+        message += "Role <b>%s</b> has been associated with %d users, %d groups and %d repositories.  " % (
+            escape(str(role.name)),
+            len(in_users),
+            len(in_groups),
+            len(in_repositories),
+        )
+    in_users = []
+    out_users = []
+    in_groups = []
+    out_groups = []
+    for user in (
+        sa_session.query(app.model.User)
+        .filter(app.model.User.table.c.deleted == false())
+        .order_by(app.model.User.table.c.email)
+    ):
+        if user in [x.user for x in role.users]:
+            in_users.append((user.id, user.email))
+        else:
+            out_users.append((user.id, user.email))
+    for group in (
+        sa_session.query(app.model.Group)
+        .filter(app.model.Group.table.c.deleted == false())
+        .order_by(app.model.Group.table.c.name)
+    ):
+        if group in [x.group for x in role.groups]:
+            in_groups.append((group.id, group.name))
+        else:
+            out_groups.append((group.id, group.name))
+    associations_dict = dict(
+        in_users=in_users,
+        out_users=out_users,
+        in_groups=in_groups,
+        out_groups=out_groups,
+        message=message,
+        status=status,
+    )
+    return associations_dict
+
+
+def change_repository_name_in_hgrc_file(hgrc_file, new_name):
+    config = configparser.ConfigParser()
+    config.read(hgrc_file)
+    config.set("web", "name", new_name)
+    with open(hgrc_file, "w") as fh:
+        config.write(fh)
+
+
+def update_repository(app, trans, id, **kwds):
+    """Update an existing ToolShed repository"""
+    message = None
+    flush_needed = False
+    sa_session = app.model.session
+    repository = sa_session.query(app.model.Repository).get(app.security.decode_id(id))
+    if repository is None:
+        return None, "Unknown repository ID"
+
+    if not (trans.user_is_admin or trans.app.security_agent.user_can_administer_repository(trans.user, repository)):
+        message = "You are not the owner of this repository, so you cannot administer it."
+        return None, message
+
+    # Allowlist properties that can be changed via this method
+    for key in ("type", "description", "long_description", "remote_repository_url", "homepage_url"):
+        # If that key is available, not None and different than what's in the model
+        if key in kwds and kwds[key] is not None and kwds[key] != getattr(repository, key):
+            setattr(repository, key, kwds[key])
+            flush_needed = True
+
+    if "category_ids" in kwds and isinstance(kwds["category_ids"], list):
+        # Get existing category associations
+        category_associations = sa_session.query(app.model.RepositoryCategoryAssociation).filter(
+            app.model.RepositoryCategoryAssociation.table.c.repository_id == app.security.decode_id(id)
+        )
+        # Remove all of them
+        for rca in category_associations:
+            sa_session.delete(rca)
+
+        # Then (re)create category associations
+        for category_id in kwds["category_ids"]:
+            category = sa_session.query(app.model.Category).get(app.security.decode_id(category_id))
+            if category:
+                rca = app.model.RepositoryCategoryAssociation(repository, category)
+                sa_session.add(rca)
+            else:
+                pass
+        flush_needed = True
+
+    # However some properties are special, like 'name'
+    if "name" in kwds and kwds["name"] is not None and repository.name != kwds["name"]:
+        if repository.times_downloaded != 0:
+            message = "Repository names cannot be changed if the repository has been cloned."
+        else:
+            message = validate_repository_name(trans.app, kwds["name"], trans.user)
+        if message:
+            return None, message
+
+        repo_dir = repository.repo_path(app)
+        # Change the entry in the hgweb.config file for the repository.
+        old_lhs = f"repos/{repository.user.username}/{repository.name}"
+        new_lhs = f"repos/{repository.user.username}/{kwds['name']}"
+        trans.app.hgweb_config_manager.change_entry(old_lhs, new_lhs, repo_dir)
+
+        # Change the entry in the repository's hgrc file.
+        hgrc_file = get_hgrc_path(repo_dir)
+        change_repository_name_in_hgrc_file(hgrc_file, kwds["name"])
+
+        # Rename the repository's admin role to match the new repository name.
+        repository_admin_role = repository.admin_role
+        repository_admin_role.name = get_repository_admin_role_name(str(kwds["name"]), str(repository.user.username))
+        trans.sa_session.add(repository_admin_role)
+        repository.name = kwds["name"]
+        flush_needed = True
+
+    if flush_needed:
+        trans.sa_session.add(repository)
+        trans.sa_session.flush()
+        message = "The repository information has been updated."
+    else:
+        message = None
+    return repository, message
+
+
+def validate_repository_name(app, name, user):
+    """
+    Validate whether the given name qualifies as a new TS repo name.
+    Repository names must be unique for each user, must be at least two characters
+    in length and must contain only lower-case letters, numbers, and the '_' character.
+    """
+    if name in ["None", None, ""]:
+        return "Enter the required repository name."
+    if name in ["repos"]:
+        return f"The term '{name}' is a reserved word in the Tool Shed, so it cannot be used as a repository name."
+    check_existing = get_repository_by_name_and_owner(app, name, user.username)
+    if check_existing is not None:
+        if check_existing.deleted:
+            return f"You own a deleted repository named <b>{escape(name)}</b>, please choose a different name."
+        else:
+            return f"You already own a repository named <b>{escape(name)}</b>, please choose a different name."
+    if len(name) < 2:
+        return "Repository names must be at least 2 characters in length."
+    if len(name) > 80:
+        return "Repository names cannot be more than 80 characters in length."
+    if not (VALID_REPOSITORYNAME_RE.match(name)):
+        return "Repository names must contain only lower-case letters, numbers and underscore."
+    return ""
+
+
+__all__ = (
+    "change_repository_name_in_hgrc_file",
+    "create_or_update_tool_shed_repository",
+    "create_repo_info_dict",
+    "create_repository_admin_role",
+    "create_repository",
+    "extract_components_from_tuple",
+    "generate_sharable_link_for_repository_in_tool_shed",
+    "generate_tool_shed_repository_install_dir",
+    "get_absolute_path_to_file_in_repository",
+    "get_ids_of_tool_shed_repositories_being_installed",
+    "get_installed_repository",
+    "get_installed_tool_shed_repository",
+    "get_prior_import_or_install_required_dict",
+    "get_repo_info_dict",
+    "get_repo_info_tuple_contents",
+    "get_repositories_by_category",
+    "get_repository_admin_role_name",
+    "get_repository_and_repository_dependencies_from_repo_info_dict",
+    "get_repository_by_id",
+    "get_repository_by_name",
+    "get_repository_by_name_and_owner",
+    "get_repository_dependency_types",
+    "get_repository_for_dependency_relationship",
+    "get_repository_ids_requiring_prior_import_or_install",
+    "get_repository_in_tool_shed",
+    "get_repository_owner",
+    "get_repository_owner_from_clone_url",
+    "get_repository_query",
+    "get_role_by_id",
+    "get_tool_shed_from_clone_url",
+    "get_tool_shed_repository_by_id",
+    "get_tool_shed_status_for_installed_repository",
+    "handle_role_associations",
+    "is_tool_shed_client",
+    "repository_was_previously_installed",
+    "set_repository_attributes",
+    "update_repository",
+    "validate_repository_name",
+)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/tools.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/tools.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,324 +1,439 @@
 import logging
 import os
-from json import dumps, loads
+from json import loads
+from typing import (
+    Any,
+    cast,
+    Dict,
+    List,
+    Optional,
+)
+
+from fastapi import (
+    Body,
+    Depends,
+    Request,
+    UploadFile,
+)
+from starlette.datastructures import UploadFile as StarletteUploadFile
 
-from galaxy import exceptions, managers, util, web
-from galaxy.managers.collections_util import dictify_dataset_collection_instance
-from galaxy.tools import global_tool_errors
+from galaxy import (
+    exceptions,
+    util,
+    web,
+)
+from galaxy.datatypes.data import get_params_and_input_name
+from galaxy.managers.collections import DatasetCollectionManager
+from galaxy.managers.context import ProvidesHistoryContext
+from galaxy.managers.hdas import HDAManager
+from galaxy.managers.histories import HistoryManager
+from galaxy.schema.fetch_data import (
+    FetchDataFormPayload,
+    FetchDataPayload,
+)
+from galaxy.tools.evaluation import global_tool_errors
+from galaxy.util.zipstream import ZipstreamWrapper
 from galaxy.web import (
     expose_api,
     expose_api_anonymous,
     expose_api_anonymous_and_sessionless,
     expose_api_raw_anonymous_and_sessionless,
 )
-from galaxy.webapps.base.controller import BaseAPIController
 from galaxy.webapps.base.controller import UsesVisualizationMixin
-from ._fetch_util import validate_and_normalize_targets
+from galaxy.webapps.base.webapp import GalaxyWebTransaction
+from galaxy.webapps.galaxy.services.tools import ToolsService
+from . import (
+    APIContentTypeRoute,
+    as_form,
+    BaseGalaxyAPIController,
+    depends,
+    DependsOnTrans,
+    Router,
+)
 
 log = logging.getLogger(__name__)
 
 # Do not allow these tools to be called directly - they (it) enforces extra security and
 # provides access via a different API endpoint.
 PROTECTED_TOOLS = ["__DATA_FETCH__"]
 # Tool search bypasses the fulltext for the following list of terms
-SEARCH_RESERVED_TERMS_FAVORITES = ['#favs', '#favorites', '#favourites']
+SEARCH_RESERVED_TERMS_FAVORITES = ["#favs", "#favorites", "#favourites"]
+
+
+class FormDataApiRoute(APIContentTypeRoute):
+    match_content_type = "multipart/form-data"
+
+
+class JsonApiRoute(APIContentTypeRoute):
+    match_content_type = "application/json"
+
+
+router = Router(tags=["tools"])
+
+FetchDataForm = as_form(FetchDataFormPayload)
+
+
+@router.cbv
+class FetchTools:
+    service: ToolsService = depends(ToolsService)
+
+    @router.post("/api/tools/fetch", summary="Upload files to Galaxy", route_class_override=JsonApiRoute)
+    async def fetch_json(self, payload: FetchDataPayload = Body(...), trans: ProvidesHistoryContext = DependsOnTrans):
+        return self.service.create_fetch(trans, payload)
+
+    @router.post(
+        "/api/tools/fetch",
+        summary="Upload files to Galaxy",
+        route_class_override=FormDataApiRoute,
+    )
+    async def fetch_form(
+        self,
+        request: Request,
+        payload: FetchDataFormPayload = Depends(FetchDataForm.as_form),
+        files: Optional[List[UploadFile]] = None,
+        trans: ProvidesHistoryContext = DependsOnTrans,
+    ):
+        files2: List[StarletteUploadFile] = cast(List[StarletteUploadFile], files or [])
 
+        # FastAPI's UploadFile is a very light wrapper around starlette's UploadFile
+        if not files2:
+            data = await request.form()
+            for value in data.values():
+                if isinstance(value, StarletteUploadFile):
+                    files2.append(value)
+        return self.service.create_fetch(trans, payload, files2)
 
-class ToolsController(BaseAPIController, UsesVisualizationMixin):
+
+class ToolsController(BaseGalaxyAPIController, UsesVisualizationMixin):
     """
     RESTful controller for interactions with tools.
     """
 
-    def __init__(self, app):
-        super(ToolsController, self).__init__(app)
-        self.history_manager = managers.histories.HistoryManager(app)
-        self.hda_manager = managers.hdas.HDAManager(app)
+    history_manager: HistoryManager = depends(HistoryManager)
+    hda_manager: HDAManager = depends(HDAManager)
+    hdca_manager: DatasetCollectionManager = depends(DatasetCollectionManager)
+    service: ToolsService = depends(ToolsService)
 
     @expose_api_anonymous_and_sessionless
-    def index(self, trans, **kwds):
+    def index(self, trans: GalaxyWebTransaction, **kwds):
         """
-        GET /api/tools: returns a list of tools defined by parameters::
+        GET /api/tools
 
-            parameters:
+        returns a list of tools defined by parameters
 
-                in_panel  - if true, tools are returned in panel structure,
-                            including sections and labels
-                trackster - if true, only tools that are compatible with
-                            Trackster are returned
-                q         - if present search on the given query will be performed
-                tool_id   - if present the given tool_id will be searched for
-                            all installed versions
+        :param in_panel: if true, tools are returned in panel structure,
+                         including sections and labels
+        :param view: ToolBox view to apply (default is 'default')
+        :param trackster: if true, only tools that are compatible with
+                          Trackster are returned
+        :param q: if present search on the given query will be performed
+        :param tool_id: if present the given tool_id will be searched for
+                        all installed versions
         """
 
         # Read params.
-        in_panel = util.string_as_bool(kwds.get('in_panel', 'True'))
-        trackster = util.string_as_bool(kwds.get('trackster', 'False'))
-        q = kwds.get('q', '')
-        tool_id = kwds.get('tool_id', '')
-        tool_help = util.string_as_bool(kwds.get('tool_help', 'False'))
+        in_panel = util.string_as_bool(kwds.get("in_panel", "True"))
+        trackster = util.string_as_bool(kwds.get("trackster", "False"))
+        q = kwds.get("q", "")
+        tool_id = kwds.get("tool_id", "")
+        tool_help = util.string_as_bool(kwds.get("tool_help", "False"))
+        view = kwds.get("view", None)
 
         # Find whether to search.
         if q:
             if trans.user and q in SEARCH_RESERVED_TERMS_FAVORITES:
-                if 'favorites' in trans.user.preferences:
-                    favorites = loads(trans.user.preferences['favorites'])
-                    hits = favorites['tools']
+                if "favorites" in trans.user.preferences:
+                    favorites = loads(trans.user.preferences["favorites"])
+                    hits = favorites["tools"]
                 else:
                     hits = None
             else:
-                hits = self._search(q)
+                hits = self.service._search(q, view)
             results = []
             if hits:
                 for hit in hits:
                     try:
-                        tool = self._get_tool(hit, user=trans.user)
+                        tool = self.service._get_tool(trans, hit, user=trans.user)
                         if tool:
                             results.append(tool.id)
                     except exceptions.AuthenticationFailed:
                         pass
                     except exceptions.ObjectNotFound:
                         pass
             return results
 
         # Find whether to detect.
         if tool_id:
-            detected_versions = self._detect(trans, tool_id)
+            detected_versions = self.service._detect(trans, tool_id)
             return detected_versions
 
         # Return everything.
         try:
-            return self.app.toolbox.to_dict(trans, in_panel=in_panel, trackster=trackster, tool_help=tool_help)
+            return self.app.toolbox.to_dict(
+                trans, in_panel=in_panel, trackster=trackster, tool_help=tool_help, view=view
+            )
+        except exceptions.MessageException:
+            raise
         except Exception:
             raise exceptions.InternalServerError("Error: Could not convert toolbox to dictionary")
 
     @expose_api_anonymous_and_sessionless
-    def show(self, trans, id, **kwd):
+    def show(self, trans: GalaxyWebTransaction, id, **kwd):
         """
         GET /api/tools/{tool_id}
 
         Returns tool information
 
             parameters:
 
                 io_details   - if true, parameters and inputs are returned
                 link_details - if true, hyperlink to the tool is returned
                 tool_version - if provided return this tool version
         """
-        io_details = util.string_as_bool(kwd.get('io_details', False))
-        link_details = util.string_as_bool(kwd.get('link_details', False))
-        tool_version = kwd.get('tool_version')
-        tool = self._get_tool(id, user=trans.user, tool_version=tool_version)
+        io_details = util.string_as_bool(kwd.get("io_details", False))
+        link_details = util.string_as_bool(kwd.get("link_details", False))
+        tool_version = kwd.get("tool_version")
+        tool = self.service._get_tool(trans, id, user=trans.user, tool_version=tool_version)
         return tool.to_dict(trans, io_details=io_details, link_details=link_details)
 
     @expose_api_anonymous
-    def build(self, trans, id, **kwd):
+    def build(self, trans: GalaxyWebTransaction, id, **kwd):
         """
         GET /api/tools/{tool_id}/build
         Returns a tool model including dynamic parameters and updated values, repeats block etc.
         """
-        if 'payload' in kwd:
-            kwd = kwd.get('payload')
-        tool_version = kwd.get('tool_version', None)
-        tool = self._get_tool(id, tool_version=tool_version, user=trans.user)
-        return tool.to_json(trans, kwd.get('inputs', kwd))
+        kwd = _kwd_or_payload(kwd)
+        tool_version = kwd.get("tool_version")
+        history_id = kwd.pop("history_id", None)
+        history = None
+        if history_id:
+            history = self.history_manager.get_owned(
+                self.decode_id(history_id), trans.user, current_history=trans.history
+            )
+        tool = self.service._get_tool(trans, id, tool_version=tool_version, user=trans.user)
+        return tool.to_json(trans, kwd.get("inputs", kwd), history=history)
 
     @web.require_admin
     @expose_api
-    def test_data_path(self, trans, id, **kwd):
+    def test_data_path(self, trans: GalaxyWebTransaction, id, **kwd):
         """
         GET /api/tools/{tool_id}/test_data_path?tool_version={tool_version}
         """
-        # TODO: eliminate copy and paste with above code.
-        if 'payload' in kwd:
-            kwd = kwd.get('payload')
-        tool_version = kwd.get('tool_version', None)
-        tool = self._get_tool(id, tool_version=tool_version, user=trans.user)
+        kwd = _kwd_or_payload(kwd)
+        tool_version = kwd.get("tool_version", None)
+        tool = self.service._get_tool(trans, id, tool_version=tool_version, user=trans.user)
         path = tool.test_data_path(kwd.get("filename"))
         if path:
             return path
         else:
             raise exceptions.ObjectNotFound("Specified test data path not found.")
 
     @expose_api_raw_anonymous_and_sessionless
-    def test_data_download(self, trans, id, **kwd):
+    def test_data_download(self, trans: GalaxyWebTransaction, id, **kwd):
         """
         GET /api/tools/{tool_id}/test_data_download?tool_version={tool_version}&filename={filename}
         """
-        tool_version = kwd.get('tool_version', None)
-        tool = self._get_tool(id, tool_version=tool_version, user=trans.user)
+        tool_version = kwd.get("tool_version", None)
+        tool = self.service._get_tool(trans, id, tool_version=tool_version, user=trans.user)
         filename = kwd.get("filename")
         if filename is None:
             raise exceptions.ObjectNotFound("Test data filename not specified.")
         path = tool.test_data_path(filename)
         if path:
             if os.path.isfile(path):
-                trans.response.headers["Content-Disposition"] = 'attachment; filename="%s"' % filename
-                return open(path, mode='rb')
+                trans.response.headers["Content-Disposition"] = f'attachment; filename="{filename}"'
+                return open(path, mode="rb")
             elif os.path.isdir(path):
-                return util.streamball.stream_archive(trans=trans, path=path, upstream_gzip=self.app.config.upstream_gzip)
+                # Set upstream_mod_zip to false, otherwise tool data must be among allowed internal routes
+                archive = ZipstreamWrapper(
+                    upstream_mod_zip=False,
+                    upstream_gzip=self.app.config.upstream_gzip,
+                    archive_name=filename,
+                )
+                archive.write(path)
+                trans.response.headers.update(archive.get_headers())
+                return archive.response()
         raise exceptions.ObjectNotFound("Specified test data path not found.")
 
     @expose_api_anonymous_and_sessionless
-    def tests_summary(self, trans, **kwd):
+    def tests_summary(self, trans: GalaxyWebTransaction, **kwd):
         """
         GET /api/tools/tests_summary
 
         Fetch summary information for each tool and version combination with tool tests
         defined. This summary information currently includes tool name and a count of
         the tests.
 
         Fetch complete test data for each tool with /api/tools/{tool_id}/test_data?tool_version=<tool_version>
         """
-        test_counts_by_tool = {}
-        for id, tool in self.app.toolbox.tools():
+        test_counts_by_tool: Dict[str, Dict] = {}
+        for _id, tool in self.app.toolbox.tools():
             if not tool.is_datatype_converter:
                 tests = tool.tests
                 if tests:
                     if tool.id not in test_counts_by_tool:
                         test_counts_by_tool[tool.id] = {}
                     available_versions = test_counts_by_tool[tool.id]
                     available_versions[tool.version] = {
                         "tool_name": tool.name,
                         "count": len(tests),
                     }
         return test_counts_by_tool
 
     @expose_api_anonymous_and_sessionless
-    def test_data(self, trans, id, **kwd):
+    def test_data(self, trans: GalaxyWebTransaction, id, **kwd):
         """
         GET /api/tools/{tool_id}/test_data?tool_version={tool_version}
 
         This API endpoint is unstable and experimental. In particular the format of the
         response has not been entirely nailed down (it exposes too many Galaxy
         internals/Pythonisms in a rough way). If this endpoint is being used from outside
         of scripts shipped with Galaxy let us know and please be prepared for the response
         from this API to change its format in some ways.
+
+        If tool version is not passed, it is assumed to be latest. Tool version can be
+        set as '*' to get tests for all configured versions.
         """
-        # TODO: eliminate copy and paste with above code.
-        if 'payload' in kwd:
-            kwd = kwd.get('payload')
-        tool_version = kwd.get('tool_version', None)
-        tool = self._get_tool(id, tool_version=tool_version, user=trans.user)
-        return [t.to_dict() for t in tool.tests]
+        kwd = _kwd_or_payload(kwd)
+        tool_version = kwd.get("tool_version", None)
+        if tool_version == "*":
+            tools = self.app.toolbox.get_tool(id, get_all_versions=True)
+            for tool in tools:
+                if not tool.allow_user_access(trans.user):
+                    raise exceptions.AuthenticationFailed(f"Access denied, please login for tool with id '{id}'.")
+        else:
+            tools = [self.service._get_tool(trans, id, tool_version=tool_version, user=trans.user)]
+
+        test_defs = []
+        for tool in tools:
+            test_defs.extend([t.to_dict() for t in tool.tests])
+        return test_defs
 
     @web.require_admin
     @expose_api
-    def reload(self, trans, id, **kwd):
+    def reload(self, trans: GalaxyWebTransaction, id, **kwd):
         """
         GET /api/tools/{tool_id}/reload
         Reload specified tool.
         """
-        trans.app.queue_worker.send_control_task('reload_tool', noop_self=True, kwargs={'tool_id': id})
+        trans.app.queue_worker.send_control_task("reload_tool", noop_self=True, kwargs={"tool_id": id})
         message, status = trans.app.toolbox.reload_tool_by_id(id)
-        if status == 'error':
+        if status == "error":
             raise exceptions.MessageException(message)
-        return {'message': message}
+        return {"message": message}
 
     @web.require_admin
     @expose_api
-    def all_requirements(self, trans, **kwds):
+    def all_requirements(self, trans: GalaxyWebTransaction, **kwds):
         """
         GET /api/tools/all_requirements
         Return list of unique requirements for all tools.
         """
 
         return trans.app.toolbox.all_requirements
 
     @web.require_admin
     @expose_api
-    def requirements(self, trans, id, **kwds):
+    def requirements(self, trans: GalaxyWebTransaction, id, **kwds):
         """
         GET /api/tools/{tool_id}/requirements
         Return the resolver status for a specific tool id.
         [{"status": "installed", "name": "hisat2", "versionless": false, "resolver_type": "conda", "version": "2.0.3", "type": "package"}]
         """
-        tool = self._get_tool(id, user=trans.user)
+        tool = self.service._get_tool(trans, id, user=trans.user)
         return tool.tool_requirements_status
 
     @web.require_admin
     @expose_api
-    def install_dependencies(self, trans, id, **kwds):
+    def install_dependencies(self, trans: GalaxyWebTransaction, id, **kwds):
         """
         POST /api/tools/{tool_id}/dependencies
 
         This endpoint is also available through POST /api/tools/{tool_id}/install_dependencies,
         but will be deprecated in the future.
 
         Attempts to install requirements via the dependency resolver
 
         parameters:
-            index:                   index of dependency resolver to use when installing dependency.
-                                     Defaults to using the highest ranking resolver
+            index:
+                index of dependency resolver to use when installing dependency.
+                Defaults to using the highest ranking resolver
+
             resolver_type:           Use the dependency resolver of this resolver_type to install dependency.
             build_dependency_cache:  If true, attempts to cache dependencies for this tool
             force_rebuild:           If true and cache dir exists, attempts to delete cache dir
         """
-        tool = self._get_tool(id, user=trans.user)
+        tool = self.service._get_tool(trans, id, user=trans.user)
         tool._view.install_dependencies(tool.requirements, **kwds)
-        if kwds.get('build_dependency_cache'):
+        if kwds.get("build_dependency_cache"):
             tool.build_dependency_cache(**kwds)
         # TODO: rework resolver install system to log and report what has been done.
         # _view.install_dependencies should return a dict with stdout, stderr and success status
         return tool.tool_requirements_status
 
     @web.require_admin
     @expose_api
-    def uninstall_dependencies(self, trans, id, **kwds):
+    def uninstall_dependencies(self, trans: GalaxyWebTransaction, id, **kwds):
         """
         DELETE /api/tools/{tool_id}/dependencies
+
         Attempts to uninstall requirements via the dependency resolver
 
         parameters:
-            index:                   index of dependency resolver to use when installing dependency.
-                                     Defaults to using the highest ranking resolver
-            resolver_type:           Use the dependency resolver of this resolver_type to install dependency
+
+            index:
+
+                index of dependency resolver to use when installing dependency.
+                Defaults to using the highest ranking resolver
+
+            resolver_type: Use the dependency resolver of this resolver_type to install dependency
         """
-        tool = self._get_tool(id, user=trans.user)
+        tool = self.service._get_tool(trans, id, user=trans.user)
         tool._view.uninstall_dependencies(requirements=tool.requirements, **kwds)
         # TODO: rework resolver install system to log and report what has been done.
         return tool.tool_requirements_status
 
     @web.require_admin
     @expose_api
-    def build_dependency_cache(self, trans, id, **kwds):
+    def build_dependency_cache(self, trans: GalaxyWebTransaction, id, **kwds):
         """
         POST /api/tools/{tool_id}/build_dependency_cache
         Attempts to cache installed dependencies.
 
         parameters:
             force_rebuild:           If true and chache dir exists, attempts to delete cache dir
         """
-        tool = self._get_tool(id)
+        tool = self.service._get_tool(trans, id)
         tool.build_dependency_cache(**kwds)
         # TODO: Should also have a more meaningful return.
         return tool.tool_requirements_status
 
     @web.require_admin
     @expose_api
-    def diagnostics(self, trans, id, **kwd):
+    def diagnostics(self, trans: GalaxyWebTransaction, id, **kwd):
         """
         GET /api/tools/{tool_id}/diagnostics
         Return diagnostic information to help debug panel
         and dependency related problems.
         """
+
         # TODO: Move this into tool.
         def to_dict(x):
             return x.to_dict()
 
-        tool = self._get_tool(id, user=trans.user)
-        if hasattr(tool, 'lineage'):
+        tool = self.service._get_tool(trans, id, user=trans.user)
+        if hasattr(tool, "lineage"):
             lineage_dict = tool.lineage.to_dict()
         else:
             lineage_dict = None
         tool_shed_dependencies = tool.installed_tool_dependencies
+        tool_shed_dependencies_dict: Optional[list] = None
         if tool_shed_dependencies:
             tool_shed_dependencies_dict = list(map(to_dict, tool_shed_dependencies))
-        else:
-            tool_shed_dependencies_dict = None
         return {
             "tool_id": tool.id,
             "tool_version": tool.version,
             "dependency_shell_commands": tool.build_dependency_shell_commands(),
             "lineage": lineage_dict,
             "requirements": list(map(to_dict, tool.requirements)),
             "installed_tool_shed_dependencies": tool_shed_dependencies_dict,
@@ -326,273 +441,124 @@
             "tool_shed": tool.tool_shed,
             "repository_name": tool.repository_name,
             "repository_owner": tool.repository_owner,
             "installed_changeset_revision": None,
             "guid": tool.guid,
         }
 
-    def _detect(self, trans, tool_id):
-        """
-        Detect whether the tool with the given id is installed.
-
-        :param tool_id: exact id of the tool
-        :type tool_id:  str
-
-        :return:      list with available versions
-        "return type: list
-        """
-        tools = self.app.toolbox.get_tool(tool_id, get_all_versions=True)
-        detected_versions = []
-        if tools:
-            for tool in tools:
-                if tool and tool.allow_user_access(trans.user):
-                    detected_versions.append(tool.version)
-        return detected_versions
-
-    def _search(self, q):
-        """
-        Perform the search on the given query.
-        Boosts and numer of results are configurable in galaxy.ini file.
-
-        :param q: the query to search with
-        :type  q: str
-
-        :return:      Dictionary containing the tools' ids of the best hits.
-        :return type: dict
-        """
-        tool_name_boost = self.app.config.get('tool_name_boost', 9)
-        tool_section_boost = self.app.config.get('tool_section_boost', 3)
-        tool_description_boost = self.app.config.get('tool_description_boost', 2)
-        tool_label_boost = self.app.config.get('tool_label_boost', 1)
-        tool_stub_boost = self.app.config.get('tool_stub_boost', 5)
-        tool_help_boost = self.app.config.get('tool_help_boost', 0.5)
-        tool_search_limit = self.app.config.get('tool_search_limit', 20)
-        tool_enable_ngram_search = self.app.config.get('tool_enable_ngram_search', False)
-        tool_ngram_minsize = self.app.config.get('tool_ngram_minsize', 3)
-        tool_ngram_maxsize = self.app.config.get('tool_ngram_maxsize', 4)
-
-        results = self.app.toolbox_search.search(q=q,
-                                                 tool_name_boost=tool_name_boost,
-                                                 tool_section_boost=tool_section_boost,
-                                                 tool_description_boost=tool_description_boost,
-                                                 tool_label_boost=tool_label_boost,
-                                                 tool_stub_boost=tool_stub_boost,
-                                                 tool_help_boost=tool_help_boost,
-                                                 tool_search_limit=tool_search_limit,
-                                                 tool_enable_ngram_search=tool_enable_ngram_search,
-                                                 tool_ngram_minsize=tool_ngram_minsize,
-                                                 tool_ngram_maxsize=tool_ngram_maxsize)
-        return results
-
     @expose_api_anonymous_and_sessionless
-    def citations(self, trans, id, **kwds):
-        tool = self._get_tool(id, user=trans.user)
+    def citations(self, trans: GalaxyWebTransaction, id, **kwds):
+        tool = self.service._get_tool(trans, id, user=trans.user)
         rval = []
         for citation in tool.citations:
-            rval.append(citation.to_dict('bibtex'))
+            rval.append(citation.to_dict("bibtex"))
         return rval
 
+    @expose_api
+    def conversion(self, trans: GalaxyWebTransaction, tool_id, payload, **kwd):
+        converter = self.service._get_tool(trans, tool_id, user=trans.user)
+        target_type = payload.get("target_type")
+        source_type = payload.get("source_type")
+        input_src = payload.get("src")
+        input_id = payload.get("id")
+        # List of string of dependencies
+        try:
+            deps = trans.app.datatypes_registry.converter_deps[source_type][target_type]
+        except KeyError:
+            deps = {}
+        # Generate parameter dictionary
+        params, input_name = get_params_and_input_name(converter, deps)
+        params = {}
+        # determine input parameter name and add to params
+
+        params[input_name] = {
+            "values": [
+                {
+                    "id": input_id,
+                    "src": input_src,
+                }
+            ],
+            "batch": input_src == "hdca",
+        }
+        history_id = payload.get("history_id")
+        if history_id:
+            decoded_id = self.decode_id(history_id)
+            target_history = self.history_manager.get_owned(decoded_id, trans.user, current_history=trans.history)
+        else:
+            if input_src == "hdca":
+                target_history = self.hdca_manager.get_dataset_collection_instance(
+                    trans, instance_type="history", id=input_id
+                ).history
+            elif input_src == "hda":
+                decoded_id = trans.app.security.decode_id(input_id)
+                target_history = self.hda_manager.get_accessible(decoded_id, trans.user).history
+                self.history_manager.error_unless_owner(target_history, trans.user, current_history=trans.history)
+            else:
+                raise exceptions.RequestParameterInvalidException("Must run conversion on either hdca or hda.")
+
+        # Make the target datatype available to the converter
+        params["__target_datatype__"] = target_type
+        vars = converter.handle_input(trans, params, history=target_history)
+        return self.service._handle_inputs_output_to_api_response(trans, converter, target_history, vars)
+
     @expose_api_anonymous_and_sessionless
-    def xrefs(self, trans, id, **kwds):
-        tool = self._get_tool(id, user=trans.user)
+    def xrefs(self, trans: GalaxyWebTransaction, id, **kwds):
+        tool = self.service._get_tool(trans, id, user=trans.user)
         return tool.xrefs
 
     @web.require_admin
     @web.legacy_expose_api_raw
-    def download(self, trans, id, **kwds):
+    def download(self, trans: GalaxyWebTransaction, id, **kwds):
         tool_tarball = trans.app.toolbox.package_tool(trans, id)
-        trans.response.set_content_type('application/x-gzip')
+        trans.response.set_content_type("application/x-gzip")
         download_file = open(tool_tarball, "rb")
-        trans.response.headers["Content-Disposition"] = 'attachment; filename="%s.tgz"' % (id)
+        trans.response.headers["Content-Disposition"] = f'attachment; filename="{id}.tgz"'
         return download_file
 
-    @expose_api_anonymous
-    def fetch(self, trans, payload, **kwd):
-        """Adapt clean API to tool-constrained API.
-        """
-        request_version = '1'
-        history_id = payload.pop("history_id")
-        clean_payload = {}
-        files_payload = {}
-        for key, value in payload.items():
-            if key == "key":
-                continue
-            if key.startswith('files_') or key.startswith('__files_'):
-                files_payload[key] = value
-                continue
-            clean_payload[key] = value
-        validate_and_normalize_targets(trans, clean_payload)
-        clean_payload["check_content"] = trans.app.config.check_upload_content
-        request = dumps(clean_payload)
-        create_payload = {
-            'tool_id': "__DATA_FETCH__",
-            'history_id': history_id,
-            'inputs': {
-                'request_version': request_version,
-                'request_json': request,
-                'file_count': str(len(files_payload))
-            },
-        }
-        create_payload.update(files_payload)
-        return self._create(trans, create_payload, **kwd)
+    @expose_api_raw_anonymous_and_sessionless
+    def raw_tool_source(self, trans: GalaxyWebTransaction, id, **kwds):
+        """Returns tool source. ``language`` is included in the response header."""
+        if not trans.app.config.enable_tool_source_display and not trans.user_is_admin:
+            raise exceptions.InsufficientPermissionsException(
+                "Only administrators may display tool sources on this Galaxy server."
+            )
+        tool = self.service._get_tool(trans, id, user=trans.user, tool_version=kwds.get("tool_version"))
+        trans.response.headers["language"] = tool.tool_source.language
+        return tool.tool_source.to_string()
 
     @web.require_admin
     @expose_api
-    def error_stack(self, trans, **kwd):
+    def error_stack(self, trans: GalaxyWebTransaction, **kwd):
         """
         GET /api/tools/error_stack
         Returns global tool error stack
         """
         return global_tool_errors.error_stack
 
     @expose_api_anonymous
-    def create(self, trans, payload, **kwd):
+    def create(self, trans: GalaxyWebTransaction, payload, **kwd):
         """
         POST /api/tools
         Execute tool with a given parameter payload
+
+        :param input_format: input format for the payload. Possible values are
+          the default 'legacy' (where inputs nested inside conditionals or
+          repeats are identified with e.g. '<conditional_name>|<input_name>') or
+          '21.01' (where inputs inside conditionals or repeats are nested
+          elements).
+        :type input_format: str
         """
         tool_id = payload.get("tool_id")
         tool_uuid = payload.get("tool_uuid")
         if tool_id in PROTECTED_TOOLS:
-            raise exceptions.RequestParameterInvalidException("Cannot execute tool [%s] directly, must use alternative endpoint." % tool_id)
+            raise exceptions.RequestParameterInvalidException(
+                f"Cannot execute tool [{tool_id}] directly, must use alternative endpoint."
+            )
         if tool_id is None and tool_uuid is None:
             raise exceptions.RequestParameterInvalidException("Must specify a valid tool_id to use this endpoint.")
-        return self._create(trans, payload, **kwd)
-
-    def _create(self, trans, payload, **kwd):
-        action = payload.get('action', None)
-        if action == 'rerun':
-            raise Exception("'rerun' action has been deprecated")
-
-        # Get tool.
-        tool_version = payload.get('tool_version', None)
-        tool_id = payload.get('tool_id', None)
-        tool_uuid = payload.get('tool_uuid', None)
-        get_kwds = dict(
-            tool_id=tool_id,
-            tool_uuid=tool_uuid,
-            tool_version=tool_version,
-        )
-        if tool_id is None and tool_uuid is None:
-            raise exceptions.RequestParameterMissingException("Must specify either a tool_id or a tool_uuid.")
-
-        tool = trans.app.toolbox.get_tool(**get_kwds)
-        if not tool or not tool.allow_user_access(trans.user):
-            raise exceptions.MessageException('Tool not found or not accessible.')
-        if trans.app.config.user_activation_on:
-            if not trans.user:
-                log.warning("Anonymous user attempts to execute tool, but account activation is turned on.")
-            elif not trans.user.active:
-                log.warning("User \"%s\" attempts to execute tool, but account activation is turned on and user account is not active." % trans.user.email)
-
-        # Set running history from payload parameters.
-        # History not set correctly as part of this API call for
-        # dataset upload.
-        history_id = payload.get('history_id', None)
-        if history_id:
-            decoded_id = self.decode_id(history_id)
-            target_history = self.history_manager.get_owned(decoded_id, trans.user, current_history=trans.history)
-        else:
-            target_history = None
-
-        # Set up inputs.
-        inputs = payload.get('inputs', {})
-
-        # Find files coming in as multipart file data and add to inputs.
-        for k, v in payload.items():
-            if k.startswith('files_') or k.startswith('__files_'):
-                inputs[k] = v
-
-        # for inputs that are coming from the Library, copy them into the history
-        self._patch_library_inputs(trans, inputs, target_history)
-
-        # TODO: encode data ids and decode ids.
-        # TODO: handle dbkeys
-        params = util.Params(inputs, sanitize=False)
-        incoming = params.__dict__
-
-        # use_cached_job can be passed in via the top-level payload or among the tool inputs.
-        # I think it should be a top-level parameter, but because the selector is implemented
-        # as a regular tool parameter we accept both.
-        use_cached_job = payload.get('use_cached_job', False) or util.string_as_bool(inputs.get('use_cached_job', 'false'))
-        vars = tool.handle_input(trans, incoming, history=target_history, use_cached_job=use_cached_job)
-
-        # TODO: check for errors and ensure that output dataset(s) are available.
-        output_datasets = vars.get('out_data', [])
-        rval = {'outputs': [], 'output_collections': [], 'jobs': [], 'implicit_collections': []}
-        rval['produces_entry_points'] = tool.produces_entry_points
-        job_errors = vars.get('job_errors', [])
-        if job_errors:
-            # If we are here - some jobs were successfully executed but some failed.
-            rval['errors'] = job_errors
-
-        outputs = rval['outputs']
-        # TODO:?? poss. only return ids?
-        for output_name, output in output_datasets:
-            output_dict = output.to_dict()
-            # add the output name back into the output data structure
-            # so it's possible to figure out which newly created elements
-            # correspond with which tool file outputs
-            output_dict['output_name'] = output_name
-            outputs.append(trans.security.encode_dict_ids(output_dict, skip_startswith="metadata_"))
-
-        new_pja_flush = False
-        for job in vars.get('jobs', []):
-            rval['jobs'].append(self.encode_all_ids(trans, job.to_dict(view='collection'), recursive=True))
-            if inputs.get('send_email_notification', False):
-                # Unless an anonymous user is invoking this via the API it
-                # should never be an option, but check and enforce that here
-                if trans.user is None:
-                    raise exceptions.ToolExecutionError("Anonymously run jobs cannot send an email notification.")
-                else:
-                    job_email_action = trans.model.PostJobAction('EmailAction')
-                    job.add_post_job_action(job_email_action)
-                    new_pja_flush = True
-
-        if new_pja_flush:
-            trans.sa_session.flush()
-
-        for output_name, collection_instance in vars.get('output_collections', []):
-            history = target_history or trans.history
-            output_dict = dictify_dataset_collection_instance(collection_instance, security=trans.security, parent=history)
-            output_dict['output_name'] = output_name
-            rval['output_collections'].append(output_dict)
-
-        for output_name, collection_instance in vars.get('implicit_collections', {}).items():
-            history = target_history or trans.history
-            output_dict = dictify_dataset_collection_instance(collection_instance, security=trans.security, parent=history)
-            output_dict['output_name'] = output_name
-            rval['implicit_collections'].append(output_dict)
+        return self.service._create(trans, payload, **kwd)
 
-        return rval
 
-    def _patch_library_inputs(self, trans, inputs, target_history):
-        """
-        Transform inputs from the data libaray to history items.
-        """
-        for k, v in inputs.items():
-            new_value = self._patch_library_dataset(trans, v, target_history)
-            if new_value:
-                v = new_value
-            elif isinstance(v, dict) and 'values' in v:
-                for index, value in enumerate(v['values']):
-                    patched = self._patch_library_dataset(trans, value, target_history)
-                    if patched:
-                        v['values'][index] = patched
-            inputs[k] = v
-
-    def _patch_library_dataset(self, trans, v, target_history):
-        if isinstance(v, dict) and 'id' in v and v.get('src') == 'ldda':
-            ldda = trans.sa_session.query(trans.app.model.LibraryDatasetDatasetAssociation).get(self.decode_id(v['id']))
-            if trans.user_is_admin or trans.app.security_agent.can_access_dataset(trans.get_current_user_roles(), ldda.dataset):
-                return ldda.to_history_dataset_association(target_history, add_to_history=True)
-
-    #
-    # -- Helper methods --
-    #
-    def _get_tool(self, id, tool_version=None, user=None):
-        tool = self.app.toolbox.get_tool(id, tool_version)
-        if not tool:
-            raise exceptions.ObjectNotFound("Could not find tool with id '%s'." % id)
-        if not tool.allow_user_access(user):
-            raise exceptions.AuthenticationFailed("Access denied, please login for tool with id '%s'." % id)
-        return tool
+def _kwd_or_payload(kwd: Dict[str, Any]) -> Dict[str, Any]:
+    if "payload" in kwd:
+        kwd = cast(Dict[str, Any], kwd.get("payload"))
+    return kwd
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/toolshed.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/toolshed.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,32 +1,34 @@
 import json
 import logging
-
-from six.moves.urllib.parse import quote
+from urllib.parse import quote
 
 from galaxy.exceptions import MessageException
 from galaxy.util import url_get
-from galaxy.web import expose_api, require_admin
-from galaxy.webapps.base.controller import BaseAPIController
+from galaxy.web import (
+    expose_api,
+    require_admin,
+)
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class ToolShedController(BaseAPIController):
+class ToolShedController(BaseGalaxyAPIController):
     """RESTful controller for interactions with Toolsheds."""
 
     @expose_api
     def index(self, trans, **kwd):
         """
         GET /api/tool_shed
         Interact with the Toolshed registry of this instance.
         """
         tool_sheds = []
         for name, url in trans.app.tool_shed_registry.tool_sheds.items():
-            tool_sheds.append(dict(name=name, url=quote(url, '')))
+            tool_sheds.append(dict(name=name, url=quote(url, "")))
         return tool_sheds
 
     @require_admin
     @expose_api
     def request(self, trans, **params):
         """
         GET /api/tool_shed/request
@@ -43,10 +45,10 @@
             if "id" in params:
                 pathspec.append(params.pop("id"))
             if "action" in params:
                 pathspec.append(params.pop("action"))
             try:
                 return json.loads(url_get(tool_shed_url, params=dict(params), pathspec=pathspec))
             except Exception as e:
-                raise MessageException("Invalid server response. %s." % str(e))
+                raise MessageException(f"Invalid server response. {str(e)}.")
         else:
             raise MessageException("Invalid toolshed url.")
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/uploads.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/uploads.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,37 +2,47 @@
 API operations for uploaded files in storage.
 """
 import logging
 import os
 import re
 
 from galaxy import exceptions
-from galaxy.web import legacy_expose_api_anonymous
-from galaxy.webapps.base.controller import BaseAPIController
+from galaxy.web.framework.decorators import (
+    expose_api_raw_anonymous,
+    legacy_expose_api_anonymous,
+)
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class UploadsAPIController(BaseAPIController):
+class UploadsAPIController(BaseGalaxyAPIController):
+    READ_CHUNK_SIZE = 2**16
 
-    READ_CHUNK_SIZE = 2 ** 16
+    @expose_api_raw_anonymous
+    def hooks(self, trans, **kwds):
+        """
+        Exposed as POST /api/upload/hooks and /api/upload/resumable_upload
+        """
+        # Internal endpoint, only purpose is to authenticate user, but may grow additional functionality in the future
+        return None
 
     @legacy_expose_api_anonymous
     def index(self, trans, **kwd):
         raise exceptions.NotImplemented("Listing uploads is not implemented.")
 
     @legacy_expose_api_anonymous
     def create(self, trans, payload, **kwd):
         """
         POST /api/uploads/
         """
         session_id = payload.get("session_id")
         session_start = payload.get("session_start")
         session_chunk = payload.get("session_chunk")
-        if re.match(r'^[\w-]+$', session_id) is None:
+        if re.match(r"^[\w-]+$", session_id) is None:
             raise exceptions.MessageException("Requires a session id.")
         if session_start is None:
             raise exceptions.MessageException("Requires a session start.")
         if not hasattr(session_chunk, "file"):
             raise exceptions.MessageException("Requires a session chunk.")
         target_file = os.path.join(trans.app.config.new_file_path, session_id)
         target_size = 0
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/users.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/users.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,68 +1,195 @@
 """
 API operations on User objects.
 """
 import copy
 import json
 import logging
 import re
-from collections import OrderedDict
 
-import six
+from fastapi import (
+    Body,
+    Path,
+    Response,
+    status,
+)
 from markupsafe import escape
 from sqlalchemy import (
     false,
     or_,
-    true
+    true,
 )
 
 from galaxy import (
     exceptions,
     util,
-    web
+    web,
 )
 from galaxy.exceptions import ObjectInvalid
-from galaxy.managers import users
+from galaxy.managers import (
+    base as managers_base,
+    users,
+)
+from galaxy.managers.context import ProvidesUserContext
+from galaxy.model import (
+    User,
+    UserAddress,
+)
+from galaxy.schema import APIKeyModel
+from galaxy.schema.fields import DecodedDatabaseIdField
+from galaxy.schema.schema import UserBeaconSetting
 from galaxy.security.validate_user_input import (
     validate_email,
     validate_password,
-    validate_publicname
+    validate_publicname,
 )
-from galaxy.tools.toolbox.filters import FilterFactory
+from galaxy.security.vault import UserVaultWrapper
+from galaxy.tool_util.toolbox.filters import FilterFactory
 from galaxy.util import (
     docstring_trim,
-    listify
+    listify,
 )
 from galaxy.web import (
     expose_api,
-    expose_api_anonymous
+    expose_api_anonymous,
 )
 from galaxy.web.form_builder import AddressField
 from galaxy.webapps.base.controller import (
-    BaseAPIController,
     BaseUIController,
-    CreatesApiKeysMixin,
     UsesFormDefinitionsMixin,
-    UsesTagsMixin
+    UsesTagsMixin,
 )
-
+from galaxy.webapps.base.webapp import GalaxyWebTransaction
+from galaxy.webapps.galaxy.api import (
+    BaseGalaxyAPIController,
+    depends,
+    DependsOnTrans,
+    Router,
+)
+from galaxy.webapps.galaxy.services.users import UsersService
 
 log = logging.getLogger(__name__)
 
+router = Router(tags=["users"])
+
+UserIdPathParam: DecodedDatabaseIdField = Path(..., title="User ID", description="The ID of the user to get.")
+APIKeyPathParam: str = Path(..., title="API Key", description="The API key of the user.")
+
+
+@router.cbv
+class FastAPIHistories:
+    service: UsersService = depends(UsersService)
+
+    @router.put(
+        "/api/users/recalculate_disk_usage",
+        summary="Triggers a recalculation of the current user disk usage.",
+        status_code=status.HTTP_204_NO_CONTENT,
+    )
+    def recalculate_disk_usage(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+    ):
+        self.service.recalculate_disk_usage(trans)
+        return Response(status_code=status.HTTP_204_NO_CONTENT)
+
+    @router.get(
+        "/api/users/{user_id}/api_key",
+        name="get_or_create_api_key",
+        summary="Return the user's API key",
+    )
+    def get_or_create_api_key(
+        self, trans: ProvidesUserContext = DependsOnTrans, user_id: DecodedDatabaseIdField = UserIdPathParam
+    ) -> str:
+        return self.service.get_or_create_api_key(trans, user_id)
+
+    @router.get(
+        "/api/users/{user_id}/api_key/detailed",
+        name="get_api_key_detailed",
+        summary="Return the user's API key with extra information.",
+        responses={
+            200: {
+                "model": APIKeyModel,
+                "description": "The API key of the user.",
+            },
+            204: {
+                "description": "The user doesn't have an API key.",
+            },
+        },
+    )
+    def get_api_key(
+        self, trans: ProvidesUserContext = DependsOnTrans, user_id: DecodedDatabaseIdField = UserIdPathParam
+    ):
+        api_key = self.service.get_api_key(trans, user_id)
+        return api_key if api_key else Response(status_code=status.HTTP_204_NO_CONTENT)
+
+    @router.post("/api/users/{user_id}/api_key", summary="Creates a new API key for the user")
+    def create_api_key(
+        self, trans: ProvidesUserContext = DependsOnTrans, user_id: DecodedDatabaseIdField = UserIdPathParam
+    ) -> str:
+        return self.service.create_api_key(trans, user_id).key
+
+    @router.delete(
+        "/api/users/{user_id}/api_key",
+        summary="Delete the current API key of the user",
+        status_code=status.HTTP_204_NO_CONTENT,
+    )
+    def delete_api_key(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        user_id: DecodedDatabaseIdField = UserIdPathParam,
+    ):
+        self.service.delete_api_key(trans, user_id)
+        return Response(status_code=status.HTTP_204_NO_CONTENT)
+
+    @router.get(
+        "/api/users/{user_id}/beacon",
+        summary="Returns information about beacon share settings",
+    )
+    def get_beacon(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        user_id: DecodedDatabaseIdField = UserIdPathParam,
+    ) -> UserBeaconSetting:
+        """
+        **Warning**: This endpoint is experimental and might change or disappear in future versions.
+        """
+        user = self.service._get_user(trans, user_id)
+
+        enabled = user.preferences["beacon_enabled"] if "beacon_enabled" in user.preferences else False
+
+        return UserBeaconSetting(enabled=enabled)
+
+    @router.post(
+        "/api/users/{user_id}/beacon",
+        summary="Changes beacon setting",
+    )
+    def set_beacon(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        user_id: DecodedDatabaseIdField = UserIdPathParam,
+        payload: UserBeaconSetting = Body(...),
+    ) -> UserBeaconSetting:
+        """
+        **Warning**: This endpoint is experimental and might change or disappear in future versions.
+        """
+        user = self.service._get_user(trans, user_id)
+
+        user.preferences["beacon_enabled"] = payload.enabled
+        trans.sa_session.flush()
+
+        return payload
 
-class UserAPIController(BaseAPIController, UsesTagsMixin, CreatesApiKeysMixin, BaseUIController, UsesFormDefinitionsMixin):
 
-    def __init__(self, app):
-        super(UserAPIController, self).__init__(app)
-        self.user_manager = users.UserManager(app)
-        self.user_serializer = users.UserSerializer(app)
-        self.user_deserializer = users.UserDeserializer(app)
+class UserAPIController(BaseGalaxyAPIController, UsesTagsMixin, BaseUIController, UsesFormDefinitionsMixin):
+    user_manager: users.UserManager = depends(users.UserManager)
+    user_serializer: users.UserSerializer = depends(users.UserSerializer)
+    user_deserializer: users.UserDeserializer = depends(users.UserDeserializer)
 
     @expose_api
-    def index(self, trans, deleted='False', f_email=None, f_name=None, f_any=None, **kwd):
+    def index(self, trans: ProvidesUserContext, deleted="False", f_email=None, f_name=None, f_any=None, **kwd):
         """
         GET /api/users
         GET /api/users/deleted
         Displays a collection (list) of users.
 
         :param deleted: (optional) If true, show deleted users
         :type  deleted: bool
@@ -80,55 +207,53 @@
         :param f_any: (optional) Filter on username OR email. (Non-admin users
                        can use this, the email filter and username filter will
                        only be active if their corresponding ``expose_user_*`` is
                        ``True`` in galaxy.ini)
         :type  f_any: str
         """
         rval = []
-        query = trans.sa_session.query(trans.app.model.User)
+        query = trans.sa_session.query(User)
         deleted = util.string_as_bool(deleted)
 
         if f_email and (trans.user_is_admin or trans.app.config.expose_user_email):
-            query = query.filter(trans.app.model.User.email.like("%%%s%%" % f_email))
+            query = query.filter(User.email.like(f"%{f_email}%"))
 
         if f_name and (trans.user_is_admin or trans.app.config.expose_user_name):
-            query = query.filter(trans.app.model.User.username.like("%%%s%%" % f_name))
+            query = query.filter(User.username.like(f"%{f_name}%"))
 
         if f_any:
             if trans.user_is_admin:
-                query = query.filter(or_(
-                    trans.app.model.User.email.like("%%%s%%" % f_any),
-                    trans.app.model.User.username.like("%%%s%%" % f_any)
-                ))
+                query = query.filter(or_(User.email.like(f"%{f_any}%"), User.username.like(f"%{f_any}%")))
             else:
                 if trans.app.config.expose_user_email and trans.app.config.expose_user_name:
-                    query = query.filter(or_(
-                        trans.app.model.User.email.like("%%%s%%" % f_any),
-                        trans.app.model.User.username.like("%%%s%%" % f_any)
-                    ))
+                    query = query.filter(or_(User.email.like(f"%{f_any}%"), User.username.like(f"%{f_any}%")))
                 elif trans.app.config.expose_user_email:
-                    query = query.filter(trans.app.model.User.email.like("%%%s%%" % f_any))
+                    query = query.filter(User.email.like(f"%{f_any}%"))
                 elif trans.app.config.expose_user_name:
-                    query = query.filter(trans.app.model.User.username.like("%%%s%%" % f_any))
+                    query = query.filter(User.username.like(f"%{f_any}%"))
 
         if deleted:
             # only admins can see deleted users
             if not trans.user_is_admin:
                 return []
-            query = query.filter(trans.app.model.User.table.c.deleted == true())
+            query = query.filter(User.table.c.deleted == true())
         else:
             # special case: user can see only their own user
             # special case2: if the galaxy admin has specified that other user email/names are
             #   exposed, we don't want special case #1
-            if not trans.user_is_admin and not trans.app.config.expose_user_name and not trans.app.config.expose_user_email:
-                item = trans.user.to_dict(value_mapper={'id': trans.security.encode_id})
+            if (
+                not trans.user_is_admin
+                and not trans.app.config.expose_user_name
+                and not trans.app.config.expose_user_email
+            ):
+                item = trans.user.to_dict(value_mapper={"id": trans.security.encode_id})
                 return [item]
-            query = query.filter(trans.app.model.User.table.c.deleted == false())
+            query = query.filter(User.table.c.deleted == false())
         for user in query:
-            item = user.to_dict(value_mapper={'id': trans.security.encode_id})
+            item = user.to_dict(value_mapper={"id": trans.security.encode_id})
             # If NOT configured to expose_email, do not expose email UNLESS the user is self, or
             # the user is an admin
             if user is not trans.user and not trans.user_is_admin:
                 expose_keys = ["id"]
                 if trans.app.config.expose_user_name:
                     expose_keys.append("username")
                 if trans.app.config.expose_user_email:
@@ -140,74 +265,90 @@
                 item = new_item
 
             # TODO: move into api_values
             rval.append(item)
         return rval
 
     @expose_api_anonymous
-    def show(self, trans, id, deleted='False', **kwd):
+    def show(self, trans: ProvidesUserContext, id, **kwd):
         """
         GET /api/users/{encoded_id}
         GET /api/users/deleted/{encoded_id}
         GET /api/users/current
         Displays information about a user.
         """
+        user = self._get_user_full(trans, id, **kwd)
+        if user is not None:
+            return self.user_serializer.serialize_to_view(user, view="detailed")
+        else:
+            return self.anon_user_api_value(trans)
+
+    def _get_user_full(self, trans, user_id, **kwd):
+        """Return referenced user or None if anonymous user is referenced."""
+        deleted = kwd.get("deleted", "False")
         deleted = util.string_as_bool(deleted)
         try:
             # user is requesting data about themselves
-            if id == "current":
+            if user_id == "current":
                 # ...and is anonymous - return usage and quota (if any)
                 if not trans.user:
-                    item = self.anon_user_api_value(trans)
-                    return item
+                    return None
 
                 # ...and is logged in - return full
                 else:
                     user = trans.user
             else:
-                user = self.get_user(trans, id, deleted=deleted)
+                user = managers_base.get_object(
+                    trans,
+                    user_id,
+                    "User",
+                    deleted=deleted,
+                )
             # check that the user is requesting themselves (and they aren't del'd) unless admin
             if not trans.user_is_admin:
-                assert trans.user == user
-                assert not user.deleted
-        except exceptions.ItemDeletionException:
+                if trans.user != user or user.deleted:
+                    raise exceptions.RequestParameterInvalidException("Invalid user id specified")
+            return user
+        except exceptions.MessageException:
             raise
         except Exception:
-            raise exceptions.RequestParameterInvalidException('Invalid user id specified', id=id)
-        return self.user_serializer.serialize_to_view(user, view='detailed')
+            raise exceptions.RequestParameterInvalidException("Invalid user id specified")
 
     @expose_api
-    def create(self, trans, payload, **kwd):
+    def create(self, trans: GalaxyWebTransaction, payload: dict, **kwd):
         """
         POST /api/users
         Creates a new Galaxy user.
         """
         if not trans.app.config.allow_user_creation and not trans.user_is_admin:
-            raise exceptions.ConfigDoesNotAllowException('User creation is not allowed in this Galaxy instance')
+            raise exceptions.ConfigDoesNotAllowException("User creation is not allowed in this Galaxy instance")
         if trans.app.config.use_remote_user and trans.user_is_admin:
-            user = trans.get_or_create_remote_user(remote_user_email=payload['remote_user_email'])
+            user = trans.get_or_create_remote_user(remote_user_email=payload["remote_user_email"])
         elif trans.user_is_admin:
-            username = payload['username']
-            email = payload['email']
-            password = payload['password']
-            message = "\n".join((validate_email(trans, email),
-                                 validate_password(trans, password, password),
-                                 validate_publicname(trans, username))).rstrip()
+            username = payload["username"]
+            email = payload["email"]
+            password = payload["password"]
+            message = "\n".join(
+                (
+                    validate_email(trans, email),
+                    validate_password(trans, password, password),
+                    validate_publicname(trans, username),
+                )
+            ).rstrip()
             if message:
                 raise exceptions.RequestParameterInvalidException(message)
             else:
                 user = self.user_manager.create(email=email, username=username, password=password)
         else:
             raise exceptions.NotImplemented()
-        item = user.to_dict(view='element', value_mapper={'id': trans.security.encode_id,
-                                                          'total_disk_usage': float})
+        item = user.to_dict(view="element", value_mapper={"id": trans.security.encode_id, "total_disk_usage": float})
         return item
 
     @expose_api
-    def update(self, trans, id, payload, **kwd):
+    def update(self, trans: ProvidesUserContext, id: str, payload: dict, **kwd):
         """
         update( self, trans, id, payload, **kwd )
         * PUT /api/users/{id}
             updates the values for the item with the given ``id``
 
         :type id: str
         :param id: the encoded id of the item to update
@@ -215,101 +356,123 @@
         :param payload: a dictionary of new attribute values
 
         :rtype: dict
         :returns: an error object if an error occurred or a dictionary containing
             the serialized item after any changes
         """
         current_user = trans.user
-        user_to_update = self.user_manager.by_id(self.decode_id(id))
-
-        # only allow updating other users if they're admin
-        editing_someone_else = current_user != user_to_update
-        is_admin = trans.api_inherit_admin or self.user_manager.is_admin(current_user)
-        if editing_someone_else and not is_admin:
-            raise exceptions.InsufficientPermissionsException('you are not allowed to update that user', id=id)
-
+        user_to_update = self._get_user_full(trans, id, **kwd)
         self.user_deserializer.deserialize(user_to_update, payload, user=current_user, trans=trans)
-        return self.user_serializer.serialize_to_view(user_to_update, view='detailed')
+        return self.user_serializer.serialize_to_view(user_to_update, view="detailed")
 
-    @web.require_admin
     @expose_api
     def delete(self, trans, id, **kwd):
         """
         DELETE /api/users/{id}
         delete the user with the given ``id``
+        Functionality restricted based on admin status
 
         :param id: the encoded id of the user to delete
         :type  id: str
 
         :param purge: (optional) if True, purge the user
         :type  purge: bool
         """
-        user = self.get_user(trans, id)
-        purge = util.string_as_bool(kwd.get('purge', False))
-        if purge:
-            log.debug("Purging user %s" % user)
-            self.user_manager.purge(user)
+        user_to_update = self.user_manager.by_id(self.decode_id(id))
+        if trans.user_is_admin:
+            purge = util.string_as_bool(kwd.get("purge", False))
+            if purge:
+                log.debug("Purging user %s", user_to_update)
+                self.user_manager.purge(user_to_update)
+            else:
+                self.user_manager.delete(user_to_update)
         else:
-            self.user_manager.delete(user)
-        return self.user_serializer.serialize_to_view(user, view='detailed')
+            if trans.user == user_to_update:
+                self.user_manager.delete(user_to_update)
+            else:
+                raise exceptions.InsufficientPermissionsException("You may only delete your own account.")
+        return self.user_serializer.serialize_to_view(user_to_update, view="detailed")
 
     @web.require_admin
     @expose_api
     def undelete(self, trans, id, **kwd):
         """
         POST /api/users/deleted/{id}/undelete
         Undelete the user with the given ``id``
 
         :param id: the encoded id of the user to be undeleted
         :type  id: str
         """
         user = self.get_user(trans, id)
         self.user_manager.undelete(user)
-        return self.user_serializer.serialize_to_view(user, view='detailed')
+        return self.user_serializer.serialize_to_view(user, view="detailed")
 
     # TODO: move to more basal, common resource than this
     def anon_user_api_value(self, trans):
         """Return data for an anonymous user, truncated to only usage and quota_percent"""
+        if not trans.user and not trans.history:
+            # Can't return info about this user, may not have a history yet.
+            return {}
         usage = trans.app.quota_agent.get_usage(trans)
         percent = trans.app.quota_agent.get_percent(trans=trans, usage=usage)
-        return {'total_disk_usage': int(usage),
-                'nice_total_disk_usage': util.nice_size(usage),
-                'quota_percent': percent}
+        return {
+            "total_disk_usage": int(usage),
+            "nice_total_disk_usage": util.nice_size(usage),
+            "quota_percent": percent,
+        }
 
     def _get_extra_user_preferences(self, trans):
         """
         Reads the file user_preferences_extra_conf.yml to display
         admin defined user informations
         """
-        return trans.app.config.user_preferences_extra['preferences']
+        return trans.app.config.user_preferences_extra["preferences"]
 
-    def _build_extra_user_pref_inputs(self, preferences, user):
+    def _build_extra_user_pref_inputs(self, trans, preferences, user):
         """
         Build extra user preferences inputs list.
         Add values to the fields if present
         """
         if not preferences:
             return []
         extra_pref_inputs = list()
         # Build sections for different categories of inputs
+        user_vault = UserVaultWrapper(trans.app.vault, user)
         for item, value in preferences.items():
             if value is not None:
                 input_fields = copy.deepcopy(value["inputs"])
                 for input in input_fields:
-                    help = input.get('help', '')
-                    required = 'Required' if util.string_as_bool(input.get('required')) else ''
+                    help = input.get("help", "")
+                    required = "Required" if util.string_as_bool(input.get("required")) else ""
                     if help:
-                        input['help'] = "%s %s" % (help, required)
+                        input["help"] = f"{help} {required}"
                     else:
-                        input['help'] = required
-                    field = item + '|' + input['name']
-                    for data_item in user.extra_preferences:
-                        if field in data_item:
-                            input['value'] = user.extra_preferences[data_item]
-                extra_pref_inputs.append({'type': 'section', 'title': value['description'], 'name': item, 'expanded': True, 'inputs': input_fields})
+                        input["help"] = required
+                    if input.get("store") == "vault":
+                        field = f"{item}/{input['name']}"
+                        input["value"] = user_vault.read_secret(f"preferences/{field}")
+                    else:
+                        field = f"{item}|{input['name']}"
+                        for data_item in user.extra_preferences:
+                            if field in data_item:
+                                input["value"] = user.extra_preferences[data_item]
+                    # regardless of the store, do not send secret type values to client
+                    if input.get("type") == "secret":
+                        input["value"] = "__SECRET_PLACEHOLDER__"
+                        # let the client treat it as a password field
+                        input["type"] = "password"
+                extra_pref_inputs.append(
+                    {
+                        "type": "section",
+                        "title": value["description"],
+                        "name": item,
+                        "expanded": True,
+                        "inputs": input_fields,
+                    }
+                )
         return extra_pref_inputs
 
     @expose_api
     def get_information(self, trans, id, **kwd):
         """
         GET /api/users/{id}/information/inputs
         Return user details such as username, email, addresses etc.
@@ -317,602 +480,642 @@
         :param id: the encoded id of the user
         :type  id: str
         """
         user = self._get_user(trans, id)
         email = user.email
         username = user.username
         inputs = list()
-        inputs.append({
-            'id': 'email_input',
-            'name': 'email',
-            'type': 'text',
-            'label': 'Email address',
-            'value': email,
-            'help': 'If you change your email address you will receive an activation link in the new mailbox and you have to activate your account by visiting it.'})
-        if trans.webapp.name == 'galaxy':
-            inputs.append({
-                'id': 'name_input',
-                'name': 'username',
-                'type': 'text',
-                'label': 'Public name',
-                'value': username,
-                'help': 'Your public name is an identifier that will be used to generate addresses for information you share publicly. Public names must be at least three characters in length and contain only lower-case letters, numbers, and the "-" character.'})
-            info_form_models = self.get_all_forms(trans, filter=dict(deleted=False), form_type=trans.app.model.FormDefinition.types.USER_INFO)
+        user_info = {
+            "email": email,
+            "username": username,
+        }
+        is_galaxy_app = trans.webapp.name == "galaxy"
+        if trans.app.config.enable_account_interface or not is_galaxy_app:
+            inputs.append(
+                {
+                    "id": "email_input",
+                    "name": "email",
+                    "type": "text",
+                    "label": "Email address",
+                    "value": email,
+                    "help": "If you change your email address you will receive an activation link in the new mailbox and you have to activate your account by visiting it.",
+                }
+            )
+        if is_galaxy_app:
+            if trans.app.config.enable_account_interface:
+                inputs.append(
+                    {
+                        "id": "name_input",
+                        "name": "username",
+                        "type": "text",
+                        "label": "Public name",
+                        "value": username,
+                        "help": 'Your public name is an identifier that will be used to generate addresses for information you share publicly. Public names must be at least three characters in length and contain only lower-case letters, numbers, dots, underscores, and dashes (".", "_", "-").',
+                    }
+                )
+            info_form_models = self.get_all_forms(
+                trans, filter=dict(deleted=False), form_type=trans.app.model.FormDefinition.types.USER_INFO
+            )
             if info_form_models:
                 info_form_id = trans.security.encode_id(user.values.form_definition.id) if user.values else None
                 info_field = {
-                    'type': 'conditional',
-                    'name': 'info',
-                    'cases': [],
-                    'test_param': {
-                        'name': 'form_id',
-                        'label': 'User type',
-                        'type': 'select',
-                        'value': info_form_id,
-                        'help': '',
-                        'data': []
-                    }
+                    "type": "conditional",
+                    "name": "info",
+                    "cases": [],
+                    "test_param": {
+                        "name": "form_id",
+                        "label": "User type",
+                        "type": "select",
+                        "value": info_form_id,
+                        "help": "",
+                        "data": [],
+                    },
                 }
                 for f in info_form_models:
                     values = None
                     if info_form_id == trans.security.encode_id(f.id) and user.values:
                         values = user.values.content
                     info_form = f.to_dict(user=user, values=values, security=trans.security)
-                    info_field['test_param']['data'].append({'label': info_form['name'], 'value': info_form['id']})
-                    info_field['cases'].append({'value': info_form['id'], 'inputs': info_form['inputs']})
+                    info_field["test_param"]["data"].append({"label": info_form["name"], "value": info_form["id"]})
+                    info_field["cases"].append({"value": info_form["id"], "inputs": info_form["inputs"]})
                 inputs.append(info_field)
 
-            address_inputs = [{'type': 'hidden', 'name': 'id', 'hidden': True}]
-            for field in AddressField.fields():
-                address_inputs.append({'type': 'text', 'name': field[0], 'label': field[1], 'help': field[2]})
-            address_repeat = {'title': 'Address', 'name': 'address', 'type': 'repeat', 'inputs': address_inputs, 'cache': []}
-            address_values = [address.to_dict(trans) for address in user.addresses]
-            for address in address_values:
-                address_cache = []
-                for input in address_inputs:
-                    input_copy = input.copy()
-                    input_copy['value'] = address.get(input['name'])
-                    address_cache.append(input_copy)
-                address_repeat['cache'].append(address_cache)
-            inputs.append(address_repeat)
+            if trans.app.config.enable_account_interface:
+                address_inputs = [{"type": "hidden", "name": "id", "hidden": True}]
+                for field in AddressField.fields():
+                    address_inputs.append({"type": "text", "name": field[0], "label": field[1], "help": field[2]})
+                address_repeat = {
+                    "title": "Address",
+                    "name": "address",
+                    "type": "repeat",
+                    "inputs": address_inputs,
+                    "cache": [],
+                }
+                address_values = [address.to_dict(trans) for address in user.addresses]
+                for address in address_values:
+                    address_cache = []
+                    for input in address_inputs:
+                        input_copy = input.copy()
+                        input_copy["value"] = address.get(input["name"])
+                        address_cache.append(input_copy)
+                    address_repeat["cache"].append(address_cache)
+                inputs.append(address_repeat)
+                user_info["addresses"] = [address.to_dict(trans) for address in user.addresses]
 
             # Build input sections for extra user preferences
-            extra_user_pref = self._build_extra_user_pref_inputs(self._get_extra_user_preferences(trans), user)
+            extra_user_pref = self._build_extra_user_pref_inputs(trans, self._get_extra_user_preferences(trans), user)
             for item in extra_user_pref:
                 inputs.append(item)
         else:
             if user.active_repositories:
-                inputs.append(dict(id='name_input', name='username', label='Public name:', type='hidden', value=username, help='You cannot change your public name after you have created a repository in this tool shed.'))
+                inputs.append(
+                    dict(
+                        id="name_input",
+                        name="username",
+                        label="Public name:",
+                        type="hidden",
+                        value=username,
+                        help="You cannot change your public name after you have created a repository in this tool shed.",
+                    )
+                )
             else:
-                inputs.append(dict(id='name_input', name='username', label='Public name:', type='text', value=username, help='Your public name provides a means of identifying you publicly within this tool shed. Public names must be at least three characters in length and contain only lower-case letters, numbers, and the "-" character. You cannot change your public name after you have created a repository in this tool shed.'))
-        return {
-            'email': email,
-            'username': username,
-            'addresses': [address.to_dict(trans) for address in user.addresses],
-            'inputs': inputs,
-        }
+                inputs.append(
+                    dict(
+                        id="name_input",
+                        name="username",
+                        label="Public name:",
+                        type="text",
+                        value=username,
+                        help='Your public name provides a means of identifying you publicly within this tool shed. Public names must be at least three characters in length and contain only lower-case letters, numbers, dots, underscores, and dashes (".", "_", "-"). You cannot change your public name after you have created a repository in this tool shed.',
+                    )
+                )
+        user_info["inputs"] = inputs
+        return user_info
 
     @expose_api
-    def set_information(self, trans, id, payload={}, **kwd):
+    def set_information(self, trans, id, payload=None, **kwd):
         """
         PUT /api/users/{id}/information/inputs
         Save a user's email, username, addresses etc.
 
         :param id: the encoded id of the user
         :type  id: str
 
         :param payload: data with new settings
         :type  payload: dict
         """
+        payload = payload or {}
         user = self._get_user(trans, id)
         # Update email
-        if 'email' in payload:
-            email = payload.get('email')
-            message = self._validate_email(email) or validate_email(trans, email, user)
+        if "email" in payload:
+            email = payload.get("email")
+            message = validate_email(trans, email, user)
             if message:
                 raise exceptions.RequestParameterInvalidException(message)
             if user.email != email:
                 # Update user email and user's private role name which must match
                 private_role = trans.app.security_agent.get_private_user_role(user)
                 private_role.name = email
-                private_role.description = 'Private role for ' + email
+                private_role.description = f"Private role for {email}"
                 user.email = email
                 trans.sa_session.add(user)
                 trans.sa_session.add(private_role)
                 trans.sa_session.flush()
                 if trans.app.config.user_activation_on:
                     # Deactivate the user if email was changed and activation is on.
                     user.active = False
                     if self.user_manager.send_activation_email(trans, user.email, user.username):
-                        message = 'The login information has been updated with the changes.<br>Verification email has been sent to your new email address. Please verify it by clicking the activation link in the email.<br>Please check your spam/trash folder in case you cannot find the message.'
+                        message = "The login information has been updated with the changes.<br>Verification email has been sent to your new email address. Please verify it by clicking the activation link in the email.<br>Please check your spam/trash folder in case you cannot find the message."
                     else:
-                        message = 'Unable to send activation email, please contact your local Galaxy administrator.'
+                        message = "Unable to send activation email, please contact your local Galaxy administrator."
                         if trans.app.config.error_email_to is not None:
-                            message += ' Contact: %s' % trans.app.config.error_email_to
+                            message += f" Contact: {trans.app.config.error_email_to}"
                         raise exceptions.InternalServerError(message)
         # Update public name
-        if 'username' in payload:
-            username = payload.get('username')
-            message = self._validate_publicname(username) or validate_publicname(trans, username, user)
+        if "username" in payload:
+            username = payload.get("username")
+            message = validate_publicname(trans, username, user)
             if message:
                 raise exceptions.RequestParameterInvalidException(message)
             if user.username != username:
                 user.username = username
         # Update user custom form
-        user_info_form_id = payload.get('info|form_id')
+        user_info_form_id = payload.get("info|form_id")
         if user_info_form_id:
-            prefix = 'info|'
-            user_info_form = trans.sa_session.query(trans.app.model.FormDefinition).get(trans.security.decode_id(user_info_form_id))
+            prefix = "info|"
+            user_info_form = trans.sa_session.query(trans.app.model.FormDefinition).get(
+                trans.security.decode_id(user_info_form_id)
+            )
             user_info_values = {}
             for item in payload:
                 if item.startswith(prefix):
-                    user_info_values[item[len(prefix):]] = payload[item]
+                    user_info_values[item[len(prefix) :]] = payload[item]
             form_values = trans.model.FormValues(user_info_form, user_info_values)
             trans.sa_session.add(form_values)
             user.values = form_values
 
         # Update values for extra user preference items
         extra_user_pref_data = dict()
         extra_pref_keys = self._get_extra_user_preferences(trans)
+        user_vault = UserVaultWrapper(trans.app.vault, user)
         if extra_pref_keys is not None:
             for key in extra_pref_keys:
-                key_prefix = key + '|'
+                key_prefix = f"{key}|"
                 for item in payload:
                     if item.startswith(key_prefix):
-                        # Show error message if the required field is empty
-                        if payload[item] == "":
-                            # Raise an exception when a required field is empty while saving the form
-                            keys = item.split("|")
-                            section = extra_pref_keys[keys[0]]
-                            for input in section['inputs']:
-                                if input['name'] == keys[1] and input['required']:
-                                    raise exceptions.ObjectAttributeMissingException("Please fill the required field")
-                        extra_user_pref_data[item] = payload[item]
+                        keys = item.split("|")
+                        section = extra_pref_keys[keys[0]]
+                        matching_input = [input for input in section["inputs"] if input["name"] == keys[1]]
+                        if matching_input:
+                            input = matching_input[0]
+                            if input.get("required") and payload[item] == "":
+                                raise exceptions.ObjectAttributeMissingException("Please fill the required field")
+                            if not (input.get("type") == "secret" and payload[item] == "__SECRET_PLACEHOLDER__"):
+                                if input.get("store") == "vault":
+                                    user_vault.write_secret(f"preferences/{keys[0]}/{keys[1]}", str(payload[item]))
+                                else:
+                                    extra_user_pref_data[item] = payload[item]
+                        else:
+                            extra_user_pref_data[item] = payload[item]
             user.preferences["extra_user_preferences"] = json.dumps(extra_user_pref_data)
 
         # Update user addresses
         address_dicts = {}
         address_count = 0
         for item in payload:
-            match = re.match(r'^address_(?P<index>\d+)\|(?P<attribute>\S+)', item)
+            match = re.match(r"^address_(?P<index>\d+)\|(?P<attribute>\S+)", item)
             if match:
                 groups = match.groupdict()
-                index = int(groups['index'])
-                attribute = groups['attribute']
+                index = int(groups["index"])
+                attribute = groups["attribute"]
                 address_dicts[index] = address_dicts.get(index) or {}
                 address_dicts[index][attribute] = payload[item]
                 address_count = max(address_count, index + 1)
         user.addresses = []
         for index in range(0, address_count):
             d = address_dicts[index]
-            if d.get('id'):
+            if d.get("id"):
                 try:
-                    user_address = trans.sa_session.query(trans.app.model.UserAddress).get(trans.security.decode_id(d['id']))
+                    user_address = trans.sa_session.query(UserAddress).get(trans.security.decode_id(d["id"]))
                 except Exception as e:
-                    raise exceptions.ObjectNotFound('Failed to access user address (%s). %s' % (d['id'], e))
+                    raise exceptions.ObjectNotFound(f"Failed to access user address ({d['id']}). {e}")
             else:
-                user_address = trans.model.UserAddress()
-                trans.log_event('User address added')
+                user_address = UserAddress()
+                trans.log_event("User address added")
             for field in AddressField.fields():
-                if str(field[2]).lower() == 'required' and not d.get(field[0]):
-                    raise exceptions.ObjectAttributeMissingException('Address %s: %s (%s) required.' % (index + 1, field[1], field[0]))
-                setattr(user_address, field[0], str(d.get(field[0], '')))
+                if str(field[2]).lower() == "required" and not d.get(field[0]):
+                    raise exceptions.ObjectAttributeMissingException(
+                        f"Address {index + 1}: {field[1]} ({field[0]}) required."
+                    )
+                setattr(user_address, field[0], str(d.get(field[0], "")))
             user_address.user = user
             user.addresses.append(user_address)
             trans.sa_session.add(user_address)
         trans.sa_session.add(user)
         trans.sa_session.flush()
-        trans.log_event('User information added')
-        return {'message': 'User information has been saved.'}
+        trans.log_event("User information added")
+        return {"message": "User information has been saved."}
 
     @expose_api
-    def set_favorite(self, trans, id, object_type, payload={}, **kwd):
+    def set_favorite(self, trans, id, object_type, payload=None, **kwd):
         """Add the object to user's favorites
         PUT /api/users/{id}/favorites/{object_type}
 
         :param id: the encoded id of the user
         :type  id: str
         :param object_type: the object type that users wants to favorite
         :type  object_type: str
         :param object_id: the id of an object that users wants to favorite
         :type  object_id: str
         """
+        payload = payload or {}
         self._validate_favorite_object_type(object_type)
         user = self._get_user(trans, id)
-        favorites = json.loads(user.preferences['favorites']) if 'favorites' in user.preferences else {}
-        if object_type == 'tools':
-            tool_id = payload.get('object_id')
+        favorites = json.loads(user.preferences["favorites"]) if "favorites" in user.preferences else {}
+        if object_type == "tools":
+            tool_id = payload.get("object_id")
             tool = self.app.toolbox.get_tool(tool_id)
             if not tool:
-                raise exceptions.ObjectNotFound("Could not find tool with id '%s'." % tool_id)
+                raise exceptions.ObjectNotFound(f"Could not find tool with id '{tool_id}'.")
             if not tool.allow_user_access(user):
-                raise exceptions.AuthenticationFailed("Access denied for tool with id '%s'." % tool_id)
-            if 'tools' in favorites:
-                favorite_tools = favorites['tools']
+                raise exceptions.AuthenticationFailed(f"Access denied for tool with id '{tool_id}'.")
+            if "tools" in favorites:
+                favorite_tools = favorites["tools"]
             else:
                 favorite_tools = []
             if tool_id not in favorite_tools:
                 favorite_tools.append(tool_id)
-                favorites['tools'] = favorite_tools
-                user.preferences['favorites'] = json.dumps(favorites)
+                favorites["tools"] = favorite_tools
+                user.preferences["favorites"] = json.dumps(favorites)
                 trans.sa_session.flush()
         return favorites
 
     @expose_api
-    def remove_favorite(self, trans, id, object_type, object_id, payload={}, **kwd):
+    def remove_favorite(self, trans, id, object_type, object_id, payload=None, **kwd):
         """Remove the object from user's favorites
         DELETE /api/users/{id}/favorites/{object_type}/{object_id:.*?}
 
         :param id: the encoded id of the user
         :type  id: str
         :param object_type: the object type that users wants to favorite
         :type  object_type: str
         :param object_id: the id of an object that users wants to remove from favorites
         :type  object_id: str
         """
+        payload = payload or {}
         self._validate_favorite_object_type(object_type)
         user = self._get_user(trans, id)
-        favorites = json.loads(user.preferences['favorites']) if 'favorites' in user.preferences else {}
-        if object_type == 'tools':
-            if 'tools' in favorites:
-                favorite_tools = favorites['tools']
+        favorites = json.loads(user.preferences["favorites"]) if "favorites" in user.preferences else {}
+        if object_type == "tools":
+            if "tools" in favorites:
+                favorite_tools = favorites["tools"]
                 if object_id in favorite_tools:
                     del favorite_tools[favorite_tools.index(object_id)]
-                    favorites['tools'] = favorite_tools
-                    user.preferences['favorites'] = json.dumps(favorites)
+                    favorites["tools"] = favorite_tools
+                    user.preferences["favorites"] = json.dumps(favorites)
                     trans.sa_session.flush()
                 else:
-                    raise exceptions.ObjectNotFound('Given object is not in the list of favorites')
+                    raise exceptions.ObjectNotFound("Given object is not in the list of favorites")
         return favorites
 
     def _validate_favorite_object_type(self, object_type):
-        if object_type in ['tools']:
+        if object_type in ["tools"]:
             pass
         else:
-            raise exceptions.ObjectAttributeInvalidException("This type is not supported. Given object_type: %s" % object_type)
+            raise exceptions.ObjectAttributeInvalidException(
+                f"This type is not supported. Given object_type: {object_type}"
+            )
 
-    def _validate_email(self, email):
-        ''' Validate email and username using regex '''
-        if email == '' or not isinstance(email, six.string_types):
-            return 'Please provide your email address.'
-        if not re.match(r'^(([^<>()[\]\.,;:\s@"]+(\.[^<>()[\]\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$', email):
-            return 'Please provide your valid email address.'
-        if len(email) > 255:
-            return 'Email cannot be more than 255 characters in length.'
-
-    def _validate_publicname(self, username):
-        if not re.match(r'^[a-z0-9\-]{3,255}$', username):
-            return 'Public name must contain only lowercase letters, numbers and "-". It also has to be shorter than 255 characters but longer than 2.'
+    @expose_api
+    def set_theme(self, trans, id: str, theme: str, payload=None, **kwd) -> str:
+        """Sets the user's theme choice.
+        PUT /api/users/{id}/theme/{theme}
+
+        :param id: the encoded id of the user
+        :type  id: str
+        :param theme: the theme identifier/name that the user has selected as preference
+        :type  theme: str
+        """
+        payload = payload or {}
+        user = self._get_user(trans, id)
+        user.preferences["theme"] = theme
+        trans.sa_session.flush()
+        return theme
 
     @expose_api
-    def get_password(self, trans, id, payload={}, **kwd):
+    def get_password(self, trans, id, payload=None, **kwd):
         """
         Return available password inputs.
         """
-        return {'inputs': [{'name': 'current', 'type': 'password', 'label': 'Current password'},
-                           {'name': 'password', 'type': 'password', 'label': 'New password'},
-                           {'name': 'confirm', 'type': 'password', 'label': 'Confirm password'}]}
+        payload = payload or {}
+        return {
+            "inputs": [
+                {"name": "current", "type": "password", "label": "Current password"},
+                {"name": "password", "type": "password", "label": "New password"},
+                {"name": "confirm", "type": "password", "label": "Confirm password"},
+            ]
+        }
 
     @expose_api
-    def set_password(self, trans, id, payload={}, **kwd):
+    def set_password(self, trans, id, payload=None, **kwd):
         """
         Allows to the logged-in user to change own password.
         """
+        payload = payload or {}
         user, message = self.user_manager.change_password(trans, id=id, **payload)
         if user is None:
             raise exceptions.AuthenticationRequired(message)
         return {"message": "Password has been changed."}
 
     @expose_api
-    def get_permissions(self, trans, id, payload={}, **kwd):
+    def get_permissions(self, trans, id, payload=None, **kwd):
         """
         Get the user's default permissions for the new histories
         """
+        payload = payload or {}
         user = self._get_user(trans, id)
         roles = user.all_roles()
         inputs = []
         for index, action in trans.app.model.Dataset.permitted_actions.items():
-            inputs.append({'type': 'select',
-                           'multiple': True,
-                           'optional': True,
-                           'name': index,
-                           'label': action.action,
-                           'help': action.description,
-                           'options': list(set((r.name, r.id) for r in roles)),
-                           'value': [a.role.id for a in user.default_permissions if a.action == action.action]})
-        return {'inputs': inputs}
+            inputs.append(
+                {
+                    "type": "select",
+                    "multiple": True,
+                    "optional": True,
+                    "name": index,
+                    "label": action.action,
+                    "help": action.description,
+                    "options": list({(r.name, r.id) for r in roles}),
+                    "value": [a.role.id for a in user.default_permissions if a.action == action.action],
+                }
+            )
+        return {"inputs": inputs}
 
     @expose_api
-    def set_permissions(self, trans, id, payload={}, **kwd):
+    def set_permissions(self, trans, id, payload=None, **kwd):
         """
         Set the user's default permissions for the new histories
         """
+        payload = payload or {}
         user = self._get_user(trans, id)
         permissions = {}
         for index, action in trans.app.model.Dataset.permitted_actions.items():
             action_id = trans.app.security_agent.get_action(action.action).action
-            permissions[action_id] = [trans.sa_session.query(trans.app.model.Role).get(x) for x in (payload.get(index) or [])]
+            permissions[action_id] = [
+                trans.sa_session.query(trans.app.model.Role).get(x) for x in (payload.get(index) or [])
+            ]
         trans.app.security_agent.user_set_default_permissions(user, permissions)
-        return {'message': 'Permissions have been saved.'}
+        return {"message": "Permissions have been saved."}
 
     @expose_api
-    def get_toolbox_filters(self, trans, id, payload={}, **kwd):
+    def get_toolbox_filters(self, trans, id, payload=None, **kwd):
         """
         API call for fetching toolbox filters data. Toolbox filters are specified in galaxy.ini.
         The user can activate them and the choice is stored in user_preferences.
         """
+        payload = payload or {}
         user = self._get_user(trans, id)
         filter_types = self._get_filter_types(trans)
         saved_values = {}
         for name, value in user.preferences.items():
             if name in filter_types:
                 saved_values[name] = listify(value, do_strip=True)
-        inputs = [{
-            'type': 'hidden',
-            'name': 'helptext',
-            'label': 'In this section you may enable or disable Toolbox filters. Please contact your admin to configure filters as necessary.'
-        }]
+        inputs = [
+            {
+                "type": "hidden",
+                "name": "helptext",
+                "label": "In this section you may enable or disable Toolbox filters. Please contact your admin to configure filters as necessary.",
+            }
+        ]
         errors = {}
         factory = FilterFactory(trans.app.toolbox)
         for filter_type in filter_types:
             self._add_filter_inputs(factory, filter_types, inputs, errors, filter_type, saved_values)
-        return {'inputs': inputs, 'errors': errors}
+        return {"inputs": inputs, "errors": errors}
 
     @expose_api
-    def set_toolbox_filters(self, trans, id, payload={}, **kwd):
+    def set_toolbox_filters(self, trans, id, payload=None, **kwd):
         """
         API call to update toolbox filters data.
         """
+        payload = payload or {}
         user = self._get_user(trans, id)
         filter_types = self._get_filter_types(trans)
         for filter_type in filter_types:
             new_filters = []
             for prefixed_name in payload:
-                if payload.get(prefixed_name) == 'true' and prefixed_name.startswith(filter_type):
-                    prefix = filter_type + '|'
-                    new_filters.append(prefixed_name[len(prefix):])
-            user.preferences[filter_type] = ','.join(new_filters)
+                if prefixed_name.startswith(filter_type):
+                    filter_selection = payload.get(prefixed_name)
+                    if type(filter_selection) != bool:
+                        raise exceptions.RequestParameterInvalidException(
+                            "Please specify the filter selection as boolean value."
+                        )
+                    if filter_selection:
+                        prefix = f"{filter_type}|"
+                        new_filters.append(prefixed_name[len(prefix) :])
+            user.preferences[filter_type] = ",".join(new_filters)
         trans.sa_session.add(user)
         trans.sa_session.flush()
-        return {'message': 'Toolbox filters have been saved.'}
+        return {"message": "Toolbox filters have been saved."}
 
     def _add_filter_inputs(self, factory, filter_types, inputs, errors, filter_type, saved_values):
         filter_inputs = list()
         filter_values = saved_values.get(filter_type, [])
-        filter_config = filter_types[filter_type]['config']
-        filter_title = filter_types[filter_type]['title']
+        filter_config = filter_types[filter_type]["config"]
+        filter_title = filter_types[filter_type]["title"]
         for filter_name in filter_config:
             function = factory.build_filter_function(filter_name)
             if function is None:
-                errors['%s|%s' % (filter_type, filter_name)] = 'Filter function not found.'
+                errors[f"{filter_type}|{filter_name}"] = "Filter function not found."
 
             short_description, description = None, None
             doc_string = docstring_trim(function.__doc__)
-            split = doc_string.split('\n\n')
+            split = doc_string.split("\n\n")
             if split:
                 short_description = split[0]
                 if len(split) > 1:
                     description = split[1]
             else:
-                log.warning('No description specified in the __doc__ string for %s.' % filter_name)
+                log.warning(f"No description specified in the __doc__ string for {filter_name}.")
 
-            filter_inputs.append({
-                'type': 'boolean',
-                'name': filter_name,
-                'label': short_description or filter_name,
-                'help': description or 'No description available.',
-                'value': 'true' if filter_name in filter_values else 'false'
-            })
+            filter_inputs.append(
+                {
+                    "type": "boolean",
+                    "name": filter_name,
+                    "label": short_description or filter_name,
+                    "help": description or "No description available.",
+                    "value": True if filter_name in filter_values else False,
+                }
+            )
         if filter_inputs:
-            inputs.append({'type': 'section', 'title': filter_title, 'name': filter_type, 'expanded': True, 'inputs': filter_inputs})
+            inputs.append(
+                {
+                    "type": "section",
+                    "title": filter_title,
+                    "name": filter_type,
+                    "expanded": True,
+                    "inputs": filter_inputs,
+                }
+            )
 
     def _get_filter_types(self, trans):
-        return OrderedDict([('toolbox_tool_filters', {'title': 'Tools', 'config': trans.app.config.user_tool_filters}),
-                            ('toolbox_section_filters', {'title': 'Sections', 'config': trans.app.config.user_tool_section_filters}),
-                            ('toolbox_label_filters', {'title': 'Labels', 'config': trans.app.config.user_tool_label_filters})])
-
-    @expose_api
-    def api_key(self, trans, id, payload={}, **kwd):
-        """
-        Create API key.
-        """
-        user = self._get_user(trans, id)
-        return self.create_api_key(trans, user)
-
-    @expose_api
-    def get_api_key(self, trans, id, payload={}, **kwd):
-        """
-        Get API key inputs.
-        """
-        user = self._get_user(trans, id)
-        return self._build_inputs_api_key(user)
-
-    @expose_api
-    def set_api_key(self, trans, id, payload={}, **kwd):
-        """
-        Get API key inputs with new API key.
-        """
-        user = self._get_user(trans, id)
-        self.create_api_key(trans, user)
-        return self._build_inputs_api_key(user, message='Generated a new web API key.')
-
-    def _build_inputs_api_key(self, user, message=''):
-        """
-        Build API key inputs.
-        """
-        inputs = [{'name': 'api-key',
-                   'type': 'text',
-                   'label': 'Current API key:',
-                   'value': user.api_keys[0].key if user.api_keys else 'Not available.',
-                   'readonly': True,
-                   'help': ' An API key will allow you to access via web API. Please note that this key acts as an alternate means to access your account and should be treated with the same care as your login password.'}]
-        return {'message': message, 'inputs': inputs}
-
-    @expose_api
-    def get_communication(self, trans, id, payload={}, **kwd):
-        """
-        Build communication server inputs.
-        """
-        user = self._get_user(trans, id)
-        return {'inputs': [{'name': 'enable',
-                            'type': 'boolean',
-                            'label': 'Enable communication',
-                            'value': user.preferences.get('communication_server', 'false')}]}
-
-    @expose_api
-    def set_communication(self, trans, id, payload={}, **kwd):
-        """
-        Allows the user to activate/deactivate the communication server.
-        """
-        user = self._get_user(trans, id)
-        enable = payload.get('enable', 'false')
-        if enable == 'true':
-            message = 'Your communication server has been activated.'
-        else:
-            message = 'Your communication server has been disabled.'
-        user.preferences['communication_server'] = enable
-        trans.sa_session.add(user)
-        trans.sa_session.flush()
-        return {'message': message}
+        return {
+            "toolbox_tool_filters": {"title": "Tools", "config": trans.app.config.user_tool_filters},
+            "toolbox_section_filters": {"title": "Sections", "config": trans.app.config.user_tool_section_filters},
+            "toolbox_label_filters": {"title": "Labels", "config": trans.app.config.user_tool_label_filters},
+        }
 
     @expose_api
-    def get_custom_builds(self, trans, id, payload={}, **kwd):
+    def get_custom_builds(self, trans, id, payload=None, **kwd):
         """
         GET /api/users/{id}/custom_builds
         Returns collection of custom builds.
 
         :param id: the encoded id of the user
         :type  id: str
         """
+        payload = payload or {}
         user = self._get_user(trans, id)
-        dbkeys = json.loads(user.preferences['dbkeys']) if 'dbkeys' in user.preferences else {}
+        dbkeys = json.loads(user.preferences["dbkeys"]) if "dbkeys" in user.preferences else {}
         valid_dbkeys = {}
         update = False
         for key, dbkey in dbkeys.items():
-            if 'count' not in dbkey and 'linecount' in dbkey:
-                chrom_count_dataset = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(dbkey['linecount'])
-                if chrom_count_dataset and not chrom_count_dataset.deleted and chrom_count_dataset.state == trans.app.model.HistoryDatasetAssociation.states.OK:
+            if "count" not in dbkey and "linecount" in dbkey:
+                chrom_count_dataset = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(
+                    dbkey["linecount"]
+                )
+                if (
+                    chrom_count_dataset
+                    and not chrom_count_dataset.deleted
+                    and chrom_count_dataset.state == trans.app.model.HistoryDatasetAssociation.states.OK
+                ):
                     chrom_count = int(open(chrom_count_dataset.file_name).readline())
-                    dbkey['count'] = chrom_count
+                    dbkey["count"] = chrom_count
                     valid_dbkeys[key] = dbkey
                     update = True
             else:
                 valid_dbkeys[key] = dbkey
         if update:
-            user.preferences['dbkeys'] = json.dumps(valid_dbkeys)
+            user.preferences["dbkeys"] = json.dumps(valid_dbkeys)
         dbkey_collection = []
         for key, attributes in valid_dbkeys.items():
-            attributes['id'] = key
+            attributes["id"] = key
             dbkey_collection.append(attributes)
         return dbkey_collection
 
     @expose_api
-    def add_custom_builds(self, trans, id, key, payload={}, **kwd):
+    def add_custom_builds(self, trans, id, key, payload=None, **kwd):
         """
         PUT /api/users/{id}/custom_builds/{key}
         Add new custom build.
 
         :param id: the encoded id of the user
         :type  id: str
 
         :param id: custom build key
         :type  id: str
 
         :param payload: data with new build details
         :type  payload: dict
         """
+        payload = payload or {}
         user = self._get_user(trans, id)
-        dbkeys = json.loads(user.preferences['dbkeys']) if 'dbkeys' in user.preferences else {}
-        name = payload.get('name')
-        len_type = payload.get('len|type')
-        len_value = payload.get('len|value')
-        if len_type not in ['file', 'fasta', 'text'] or not len_value:
-            raise exceptions.RequestParameterInvalidException('Please specify a valid data source type.')
+        dbkeys = json.loads(user.preferences["dbkeys"]) if "dbkeys" in user.preferences else {}
+        name = payload.get("name")
+        len_type = payload.get("len|type")
+        len_value = payload.get("len|value")
+        if len_type not in ["file", "fasta", "text"] or not len_value:
+            raise exceptions.RequestParameterInvalidException("Please specify a valid data source type.")
         if not name or not key:
-            raise exceptions.RequestParameterMissingException('You must specify values for all the fields.')
+            raise exceptions.RequestParameterMissingException("You must specify values for all the fields.")
         elif key in dbkeys:
-            raise exceptions.DuplicatedIdentifierException('There is already a custom build with that key. Delete it first if you want to replace it.')
+            raise exceptions.DuplicatedIdentifierException(
+                "There is already a custom build with that key. Delete it first if you want to replace it."
+            )
         else:
             # Have everything needed; create new build.
-            build_dict = {'name': name}
-            if len_type in ['text', 'file']:
+            build_dict = {"name": name}
+            if len_type in ["text", "file"]:
                 # Create new len file
-                new_len = trans.app.model.HistoryDatasetAssociation(extension='len', create_dataset=True, sa_session=trans.sa_session)
+                new_len = trans.app.model.HistoryDatasetAssociation(
+                    extension="len", create_dataset=True, sa_session=trans.sa_session
+                )
                 trans.sa_session.add(new_len)
                 new_len.name = name
                 new_len.visible = False
                 new_len.state = trans.app.model.Job.states.OK
-                new_len.info = 'custom build .len file'
+                new_len.info = "custom build .len file"
                 try:
                     trans.app.object_store.create(new_len.dataset)
                 except ObjectInvalid:
-                    raise exceptions.InternalServerError('Unable to create output dataset: object store is full.')
+                    raise exceptions.InternalServerError("Unable to create output dataset: object store is full.")
                 trans.sa_session.flush()
                 counter = 0
                 lines_skipped = 0
-                with open(new_len.file_name, 'w') as f:
+                with open(new_len.file_name, "w") as f:
                     # LEN files have format:
                     #   <chrom_name><tab><chrom_length>
-                    for line in len_value.split('\n'):
+                    for line in len_value.split("\n"):
                         # Splits at the last whitespace in the line
                         lst = line.strip().rsplit(None, 1)
                         if not lst or len(lst) < 2:
                             lines_skipped += 1
                             continue
                         chrom, length = lst[0], lst[1]
                         try:
                             length = int(length)
                         except ValueError:
                             lines_skipped += 1
                             continue
                         if chrom != escape(chrom):
-                            build_dict['message'] = 'Invalid chromosome(s) with HTML detected and skipped.'
+                            build_dict["message"] = "Invalid chromosome(s) with HTML detected and skipped."
                             lines_skipped += 1
                             continue
                         counter += 1
-                        f.write('%s\t%s\n' % (chrom, length))
-                build_dict['len'] = new_len.id
-                build_dict['count'] = counter
+                        f.write(f"{chrom}\t{length}\n")
+                build_dict["len"] = new_len.id
+                build_dict["count"] = counter
             else:
-                build_dict['fasta'] = trans.security.decode_id(len_value)
-                dataset = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(build_dict['fasta'])
+                build_dict["fasta"] = trans.security.decode_id(len_value)
+                dataset = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(build_dict["fasta"])
                 try:
-                    new_len = dataset.get_converted_dataset(trans, 'len')
-                    new_linecount = new_len.get_converted_dataset(trans, 'linecount')
-                    build_dict['len'] = new_len.id
-                    build_dict['linecount'] = new_linecount.id
+                    new_len = dataset.get_converted_dataset(trans, "len")
+                    new_linecount = new_len.get_converted_dataset(trans, "linecount")
+                    build_dict["len"] = new_len.id
+                    build_dict["linecount"] = new_linecount.id
                 except Exception:
-                    raise exceptions.ToolExecutionError('Failed to convert dataset.')
+                    raise exceptions.ToolExecutionError("Failed to convert dataset.")
             dbkeys[key] = build_dict
-            user.preferences['dbkeys'] = json.dumps(dbkeys)
+            user.preferences["dbkeys"] = json.dumps(dbkeys)
             trans.sa_session.flush()
             return build_dict
 
     @expose_api
-    def delete_custom_builds(self, trans, id, key, payload={}, **kwd):
+    def delete_custom_builds(self, trans, id, key, payload=None, **kwd):
         """
         DELETE /api/users/{id}/custom_builds/{key}
         Delete a custom build.
 
         :param id: the encoded id of the user
         :type  id: str
 
         :param id: custom build key to be deleted
         :type  id: str
         """
+        payload = payload or {}
         user = self._get_user(trans, id)
-        dbkeys = json.loads(user.preferences['dbkeys']) if 'dbkeys' in user.preferences else {}
+        dbkeys = json.loads(user.preferences["dbkeys"]) if "dbkeys" in user.preferences else {}
         if key and key in dbkeys:
             del dbkeys[key]
-            user.preferences['dbkeys'] = json.dumps(dbkeys)
+            user.preferences["dbkeys"] = json.dumps(dbkeys)
             trans.sa_session.flush()
-            return {'message': 'Deleted %s.' % key}
+            return {"message": f"Deleted {key}."}
         else:
-            raise exceptions.ObjectNotFound('Could not find and delete build (%s).' % key)
+            raise exceptions.ObjectNotFound(f"Could not find and delete build ({key}).")
 
     def _get_user(self, trans, id):
         user = self.get_user(trans, id)
         if not user:
-            raise exceptions.RequestParameterInvalidException('Invalid user (%s).' % id)
+            raise exceptions.RequestParameterInvalidException("Invalid user id specified.")
         if user != trans.user and not trans.user_is_admin:
-            raise exceptions.InsufficientPermissionsException('Access denied.')
+            raise exceptions.InsufficientPermissionsException("Access denied.")
         return user
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/webhooks.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/webhooks.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,47 +1,51 @@
 """
 API Controller providing Galaxy Webhooks
 """
-import imp
+import importlib.util
 import logging
+from typing import Any
 
 from galaxy.web import expose_api_anonymous_and_sessionless
-from galaxy.webapps.base.controller import BaseAPIController
+from galaxy.webapps.base.webapp import GalaxyWebTransaction
+from . import BaseGalaxyAPIController
 
 log = logging.getLogger(__name__)
 
 
-class WebhooksController(BaseAPIController):
-    def __init__(self, app):
-        super(WebhooksController, self).__init__(app)
-
+class WebhooksController(BaseGalaxyAPIController):
     @expose_api_anonymous_and_sessionless
-    def all_webhooks(self, trans, **kwd):
+    def all_webhooks(self, trans: GalaxyWebTransaction, **kwd):
         """
-        *GET /api/webhooks/
-        Returns all webhooks
+        GET /api/webhooks/
+
+        Return all webhooks.
         """
-        return [
-            webhook.to_dict()
-            for webhook in self.app.webhooks_registry.webhooks
-        ]
+        return [webhook.to_dict() for webhook in self.app.webhooks_registry.webhooks]
 
     @expose_api_anonymous_and_sessionless
-    def webhook_data(self, trans, webhook_id, **kwd):
+    def webhook_data(self, trans: Any, webhook_id, **kwd):
         """
-        *GET /api/webhooks/{webhook_id}/data/{params}
-        Returns the result of executing helper function
+        GET /api/webhooks/{webhook_id}/data/{params}
+
+        Return the result of executing helper function.
         """
         params = {}
 
         for key, value in kwd.items():
             params[key] = value
 
-        webhook = next(
-            webhook
-            for webhook in self.app.webhooks_registry.webhooks
-            if webhook.id == webhook_id
-        )
-
-        return imp.load_source(webhook.path, webhook.helper).main(
-            trans, webhook, params,
-        ) if webhook and webhook.helper != '' else {}
+        webhook = next(webhook for webhook in self.app.webhooks_registry.webhooks if webhook.id == webhook_id)
+
+        if webhook and webhook.helper != "":
+            spec = importlib.util.spec_from_file_location(webhook.path, webhook.helper)
+            assert spec
+            module = importlib.util.module_from_spec(spec)
+            assert spec.loader
+            spec.loader.exec_module(module)
+            return module.main(
+                trans,
+                webhook,
+                params,
+            )
+        else:
+            return {}
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/api/workflows.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/api/workflows.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,105 +1,156 @@
 """
 API operations for Workflows
 """
-from __future__ import absolute_import
 
-import io
 import json
 import logging
 import os
+from io import BytesIO
+from typing import (
+    Any,
+    Dict,
+    List,
+    Optional,
+)
 
-import requests
+from fastapi import (
+    Body,
+    Path,
+    Query,
+    Response,
+    status,
+)
 from gxformat2._yaml import ordered_dump
 from markupsafe import escape
-from sqlalchemy import desc, false, or_, true
-from sqlalchemy.orm import joinedload
+from pydantic import Extra
+from starlette.responses import StreamingResponse
 
 from galaxy import (
     exceptions,
     model,
-    util
+    util,
+)
+from galaxy.files.uris import (
+    stream_url_to_str,
+    validate_uri_access,
+)
+from galaxy.managers.context import ProvidesUserContext
+from galaxy.managers.jobs import (
+    fetch_job_states,
+    invocation_job_source_iter,
 )
-from galaxy.managers import (
-    histories,
-    workflows
+from galaxy.managers.workflows import (
+    MissingToolsException,
+    RefactorRequest,
+    WorkflowCreateOptions,
+    WorkflowUpdateOptions,
 )
-from galaxy.managers.jobs import fetch_job_states, invocation_job_source_iter
 from galaxy.model.item_attrs import UsesAnnotations
+from galaxy.model.store import BcoExportOptions
+from galaxy.schema.fields import DecodedDatabaseIdField
+from galaxy.schema.invocation import InvocationMessageResponseModel
+from galaxy.schema.schema import (
+    AsyncFile,
+    AsyncTaskResultSummary,
+    SetSlugPayload,
+    ShareWithPayload,
+    ShareWithStatus,
+    SharingStatus,
+    StoreContentSource,
+    WorkflowSortByEnum,
+)
+from galaxy.structured_app import StructuredApp
 from galaxy.tool_shed.galaxy_install.install_manager import InstallRepositoryManager
 from galaxy.tools import recommendations
 from galaxy.tools.parameters import populate_state
 from galaxy.tools.parameters.basic import workflow_building_modes
 from galaxy.util.sanitize_html import sanitize_html
+from galaxy.version import VERSION
 from galaxy.web import (
     expose_api,
+    expose_api_anonymous,
     expose_api_anonymous_and_sessionless,
     expose_api_raw,
     expose_api_raw_anonymous_and_sessionless,
     format_return_as_json,
 )
 from galaxy.webapps.base.controller import (
-    BaseAPIController,
     SharableMixin,
     url_for,
-    UsesStoredWorkflowMixin
+    UsesStoredWorkflowMixin,
+)
+from galaxy.webapps.base.webapp import GalaxyWebTransaction
+from galaxy.webapps.galaxy.api import (
+    BaseGalaxyAPIController,
+    depends,
+    DependsOnTrans,
+    IndexQueryTag,
+    Router,
+    search_query_param,
+)
+from galaxy.webapps.galaxy.services.base import (
+    ConsumesModelStores,
+    ServesExportStores,
+)
+from galaxy.webapps.galaxy.services.invocations import (
+    InvocationIndexPayload,
+    InvocationSerializationParams,
+    InvocationsService,
+    PrepareStoreDownloadPayload,
+    WriteInvocationStoreToPayload,
+)
+from galaxy.webapps.galaxy.services.workflows import (
+    WorkflowIndexPayload,
+    WorkflowsService,
 )
 from galaxy.workflow.extract import extract_workflow
 from galaxy.workflow.modules import module_factory
-from galaxy.workflow.reports import generate_report
-from galaxy.workflow.run import invoke, queue_invoke
+from galaxy.workflow.run import queue_invoke
 from galaxy.workflow.run_request import build_workflow_run_configs
 
 log = logging.getLogger(__name__)
 
+router = Router(tags=["workflows"])
 
-class WorkflowsAPIController(BaseAPIController, UsesStoredWorkflowMixin, UsesAnnotations, SharableMixin):
 
-    def __init__(self, app):
-        super(WorkflowsAPIController, self).__init__(app)
-        self.history_manager = histories.HistoryManager(app)
-        self.workflow_manager = workflows.WorkflowsManager(app)
-        self.workflow_contents_manager = workflows.WorkflowContentsManager(app)
-        self.tool_recommendations = recommendations.ToolRecommendations()
+class CreateInvocationFromStore(StoreContentSource):
+    history_id: Optional[str]
 
-    def __get_full_shed_url(self, url):
-        for name, shed_url in self.app.tool_shed_registry.tool_sheds.items():
-            if url in shed_url:
-                return shed_url
-        return None
+    class Config:
+        extra = Extra.allow
 
-    @expose_api_anonymous_and_sessionless
-    def index(self, trans, **kwd):
-        """
-        GET /api/workflows
-        """
-        return self.get_workflows_list(trans, kwd)
 
-    @expose_api
-    def get_workflow_menu(self, trans, **kwd):
-        """
-        Get workflows present in the tools panel
-        GET /api/workflows/menu
-        """
-        user = trans.get_user()
-        ids_in_menu = [x.stored_workflow_id for x in user.stored_workflow_menu_entries]
-        return {
-            'ids_in_menu': ids_in_menu,
-            'workflows': self.get_workflows_list(trans, kwd)
-        }
+class WorkflowsAPIController(
+    BaseGalaxyAPIController,
+    UsesStoredWorkflowMixin,
+    UsesAnnotations,
+    SharableMixin,
+    ServesExportStores,
+    ConsumesModelStores,
+):
+    service: WorkflowsService = depends(WorkflowsService)
+    invocations_service: InvocationsService = depends(InvocationsService)
+
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
+        self.history_manager = app.history_manager
+        self.workflow_manager = app.workflow_manager
+        self.workflow_contents_manager = app.workflow_contents_manager
+        self.tool_recommendations = recommendations.ToolRecommendations()
 
     @expose_api
-    def set_workflow_menu(self, trans, **kwd):
+    def set_workflow_menu(self, trans: GalaxyWebTransaction, payload=None, **kwd):
         """
         Save workflow menu to be shown in the tool panel
         PUT /api/workflows/menu
         """
-        payload = kwd.get('payload')
-        user = trans.get_user()
-        workflow_ids = payload.get('workflow_ids')
+        payload = payload or {}
+        user = trans.user
+        workflow_ids = payload.get("workflow_ids")
         if workflow_ids is None:
             workflow_ids = []
         elif type(workflow_ids) != list:
             workflow_ids = [workflow_ids]
         workflow_ids_decoded = []
         # Decode the encoded workflow ids
         for ids in workflow_ids:
@@ -120,336 +171,190 @@
                 seen_workflow_ids.add(wf_id)
             m = model.StoredWorkflowMenuEntry()
             m.stored_workflow = q.get(wf_id)
             user.stored_workflow_menu_entries.append(m)
         sess.flush()
         message = "Menu updated."
         trans.set_message(message)
-        return {'message': message, 'status': 'done'}
-
-    def get_workflows_list(self, trans, kwd):
-        """
-        Displays a collection of workflows.
-
-        :param  show_published:      if True, show also published workflows
-        :type   show_published:      boolean
-        :param  missing_tools:       if True, include a list of missing tools per workflow
-        :type   missing_tools:       boolean
-        """
-        missing_tools = util.string_as_bool(kwd.get('missing_tools', 'False'))
-        rval = []
-        filter1 = (trans.app.model.StoredWorkflow.user == trans.user)
-        user = trans.get_user()
-        if user is None:
-            show_published = util.string_as_bool(kwd.get('show_published', 'True'))
-        else :
-            show_published = util.string_as_bool(kwd.get('show_published', 'False'))
-        if show_published:
-            filter1 = or_(filter1, (trans.app.model.StoredWorkflow.published == true()))
-        for wf in trans.sa_session.query(trans.app.model.StoredWorkflow).options(
-                joinedload("annotations")).options(
-                joinedload("latest_workflow").undefer("step_count").lazyload("steps")).options(
-                joinedload("tags")).filter(
-                    filter1, trans.app.model.StoredWorkflow.table.c.deleted == false()).order_by(
-                    desc(trans.app.model.StoredWorkflow.table.c.update_time)).all():
-            item = wf.to_dict(value_mapper={'id': trans.security.encode_id})
-            encoded_id = trans.security.encode_id(wf.id)
-            item['annotations'] = [x.annotation for x in wf.annotations]
-            item['url'] = url_for('workflow', id=encoded_id)
-            item['owner'] = wf.user.username
-            item['number_of_steps'] = wf.latest_workflow.step_count
-            item['show_in_tool_panel'] = False
-            if user is not None:
-                item['show_in_tool_panel'] = wf.show_in_tool_panel(user_id=user.id)
-            rval.append(item)
-        for wf_sa in trans.sa_session.query(model.StoredWorkflowUserShareAssociation).join(
-                model.StoredWorkflowUserShareAssociation.stored_workflow).options(
-                joinedload("stored_workflow").joinedload("annotations")).options(
-                joinedload("stored_workflow").joinedload("latest_workflow").undefer("step_count").lazyload("steps")).options(
-                joinedload("stored_workflow").joinedload("user")).options(
-                joinedload("stored_workflow").joinedload("tags")).filter(model.StoredWorkflowUserShareAssociation.user == trans.user).filter(
-                model.StoredWorkflow.deleted == false()).order_by(
-                desc(model.StoredWorkflow.update_time)).all():
-            item = wf_sa.stored_workflow.to_dict(value_mapper={'id': trans.security.encode_id})
-            encoded_id = trans.security.encode_id(wf_sa.stored_workflow.id)
-            item['annotations'] = [x.annotation for x in wf_sa.stored_workflow.annotations]
-            item['url'] = url_for('workflow', id=encoded_id)
-            item['slug'] = wf_sa.stored_workflow.slug
-            item['owner'] = wf_sa.stored_workflow.user.username
-            item['number_of_steps'] = wf_sa.stored_workflow.latest_workflow.step_count
-            item['show_in_tool_panel'] = False
-            if user is not None:
-                item['show_in_tool_panel'] = wf_sa.stored_workflow.show_in_tool_panel(user_id=user.id)
-            rval.append(item)
-        if missing_tools:
-            workflows_missing_tools = []
-            workflows = []
-            workflows_by_toolshed = dict()
-            for key, value in enumerate(rval):
-                tool_ids = []
-                workflow_details = self.workflow_contents_manager.workflow_to_dict(trans, self.__get_stored_workflow(trans, value['id']), style='instance')
-                if 'steps' in workflow_details:
-                    for step in workflow_details['steps']:
-                        tool_id = workflow_details['steps'][step].get('tool_id')
-                        if tool_id and tool_id not in tool_ids and self.app.toolbox.is_missing_shed_tool(tool_id):
-                            tool_ids.append(tool_id)
-                if len(tool_ids) > 0:
-                    value['missing_tools'] = tool_ids
-                    workflows_missing_tools.append(value)
-            for workflow in workflows_missing_tools:
-                for tool_id in workflow['missing_tools']:
-                    toolshed, _, owner, name, tool, version = tool_id.split('/')
-                    shed_url = self.__get_full_shed_url(toolshed)
-                    repo_identifier = '/'.join((toolshed, owner, name))
-                    if repo_identifier not in workflows_by_toolshed:
-                        workflows_by_toolshed[repo_identifier] = dict(shed=shed_url.rstrip('/'), repository=name, owner=owner, tools=[tool_id], workflows=[workflow['name']])
-                    else:
-                        if tool_id not in workflows_by_toolshed[repo_identifier]['tools']:
-                            workflows_by_toolshed[repo_identifier]['tools'].append(tool_id)
-                        if workflow['name'] not in workflows_by_toolshed[repo_identifier]['workflows']:
-                            workflows_by_toolshed[repo_identifier]['workflows'].append(workflow['name'])
-            for repo_tag in workflows_by_toolshed:
-                workflows.append(workflows_by_toolshed[repo_tag])
-            return workflows
-        return rval
+        return {"message": message, "status": "done"}
 
     @expose_api_anonymous_and_sessionless
-    def show(self, trans, id, **kwd):
+    def show(self, trans: GalaxyWebTransaction, id, **kwd):
         """
         GET /api/workflows/{encoded_workflow_id}
 
         :param  instance:                 true if fetch by Workflow ID instead of StoredWorkflow id, false
                                           by default.
         :type   instance:                 boolean
 
         Displays information needed to run a workflow.
         """
         stored_workflow = self.__get_stored_workflow(trans, id, **kwd)
         if stored_workflow.importable is False and stored_workflow.user != trans.user and not trans.user_is_admin:
-            if trans.sa_session.query(trans.app.model.StoredWorkflowUserShareAssociation).filter_by(user=trans.user, stored_workflow=stored_workflow).count() == 0:
+            if (
+                trans.sa_session.query(model.StoredWorkflowUserShareAssociation)
+                .filter_by(user=trans.user, stored_workflow=stored_workflow)
+                .count()
+                == 0
+            ):
                 message = "Workflow is neither importable, nor owned by or shared with current user"
                 raise exceptions.ItemAccessibilityException(message)
         if kwd.get("legacy", False):
             style = "legacy"
         else:
             style = "instance"
-        version = kwd.get('version')
+        version = kwd.get("version")
+        if version is None and util.string_as_bool(kwd.get("instance", "false")):
+            # A Workflow instance may not be the latest workflow version attached to StoredWorkflow.
+            # This figures out the correct version so that we return the correct Workflow and version.
+            workflow_id = self.decode_id(id)
+            for i, workflow in enumerate(reversed(stored_workflow.workflows)):
+                if workflow.id == workflow_id:
+                    version = i
+                    break
         return self.workflow_contents_manager.workflow_to_dict(trans, stored_workflow, style=style, version=version)
 
     @expose_api
-    def show_versions(self, trans, workflow_id, **kwds):
-        """
-        GET /api/workflows/{encoded_workflow_id}/versions
-
-        :param  instance:                 true if fetch by Workflow ID instead of StoredWorkflow id, false
-                                          by default.
-        :type   instance:                 boolean
-
-        Lists all versions of this workflow.
-        """
-        stored_workflow = self.workflow_manager.get_stored_accessible_workflow(trans, workflow_id, **kwds)
-        return [{'version': i, 'update_time': str(w.update_time), 'steps': len(w.steps)} for i, w in enumerate(reversed(stored_workflow.workflows))]
-
-    @expose_api
-    def create(self, trans, payload, **kwd):
+    def create(self, trans: GalaxyWebTransaction, payload=None, **kwd):
         """
         POST /api/workflows
 
-        Run or create workflows from the api.
-
-        .. tip:: When executing a workflow externally (e.g. from a script) it is
-            recommended to use the :func:`galaxy.webapps.galaxy.api.workflows.WorkflowsAPIController.invoke` method below instead.
-
-        If installed_repository_file or from_history_id is specified a new
-        workflow will be created for this user. Otherwise, workflow_id must be
-        specified and this API method will cause a workflow to execute.
-
-        :param  installed_repository_file    The path of a workflow to import. Either workflow_id, installed_repository_file or from_history_id must be specified
-        :type   installed_repository_file    str
-
-        :param  workflow_id:                 An existing workflow id. Either workflow_id, installed_repository_file or from_history_id must be specified
-        :type   workflow_id:                 str
-
-        :param  parameters:                  If workflow_id is set - see _step_parameters() in lib/galaxy/workflow/run_request.py
-        :type   parameters:                  dict
-
-        :param  ds_map:                      If workflow_id is set - a dictionary mapping each input step id to a dictionary with 2 keys: 'src' (which can be 'ldda', 'ld' or 'hda') and 'id' (which should be the id of a LibraryDatasetDatasetAssociation, LibraryDataset or HistoryDatasetAssociation respectively)
-        :type   ds_map:                      dict
-
-        :param  no_add_to_history:           If workflow_id is set - if present in the payload with any value, the input datasets will not be added to the selected history
-        :type   no_add_to_history:           str
-
-        :param  history:                     If workflow_id is set - optional history where to run the workflow, either the name of a new history or "hist_id=HIST_ID" where HIST_ID is the id of an existing history. If not specified, the workflow will be run a new unnamed history
-        :type   history:                     str
-
-        :param  replacement_params:          If workflow_id is set - an optional dictionary used when renaming datasets
-        :type   replacement_params:          dict
+        Create workflows in various ways.
 
-        :param  from_history_id:             Id of history to extract a workflow from. Either workflow_id, installed_repository_file or from_history_id must be specified
+        :param  from_history_id:             Id of history to extract a workflow from.
         :type   from_history_id:             str
 
         :param  job_ids:                     If from_history_id is set - optional list of jobs to include when extracting a workflow from history
         :type   job_ids:                     str
 
-        :param  dataset_ids:                 If from_history_id is set - optional list of HDA `hid`s corresponding to workflow inputs when extracting a workflow from history
+        :param  dataset_ids:                 If from_history_id is set - optional list of HDA "hid"s corresponding to workflow inputs when extracting a workflow from history
         :type   dataset_ids:                 str
 
-        :param  dataset_collection_ids:      If from_history_id is set - optional list of HDCA `hid`s corresponding to workflow inputs when extracting a workflow from history
+        :param  dataset_collection_ids:      If from_history_id is set - optional list of HDCA "hid"s corresponding to workflow inputs when extracting a workflow from history
         :type   dataset_collection_ids:      str
 
         :param  workflow_name:               If from_history_id is set - name of the workflow to create when extracting a workflow from history
         :type   workflow_name:               str
 
-        :param  allow_tool_state_corrections:  If set to True, any Tool parameter changes will not prevent running workflow, defaults to False
-        :type   allow_tool_state_corrections:  bool
-
-        :param use_cached_job:               If set to True galaxy will attempt to find previously executed steps for all workflow steps with the exact same parameter combinations
-                                             and will copy the outputs of the previously executed step.
         """
         ways_to_create = {
-            'archive_source',
-            'workflow_id',
-            'installed_repository_file',
-            'from_history_id',
-            'from_path',
-            'shared_workflow_id',
-            'workflow',
+            "archive_file",
+            "archive_source",
+            "from_history_id",
+            "from_path",
+            "shared_workflow_id",
+            "workflow",
         }
 
-        if len(ways_to_create.intersection(payload)) == 0:
-            message = "One parameter among - %s - must be specified" % ", ".join(ways_to_create)
+        if trans.user_is_bootstrap_admin:
+            raise exceptions.RealUserRequiredException("Only real users can create or run workflows.")
+
+        if payload is None or len(ways_to_create.intersection(payload)) == 0:
+            message = f"One parameter among - {', '.join(ways_to_create)} - must be specified"
             raise exceptions.RequestParameterMissingException(message)
 
         if len(ways_to_create.intersection(payload)) > 1:
-            message = "Only one parameter among - %s - must be specified" % ", ".join(ways_to_create)
+            message = f"Only one parameter among - {', '.join(ways_to_create)} - must be specified"
             raise exceptions.RequestParameterInvalidException(message)
 
-        if 'installed_repository_file' in payload:
-            if not trans.user_is_admin:
-                raise exceptions.AdminRequiredException()
-            installed_repository_file = payload.get('installed_repository_file', '')
-            if not os.path.exists(installed_repository_file):
-                raise exceptions.RequestParameterInvalidException("Workflow file '%s' not found" % installed_repository_file)
-            elif os.path.getsize(os.path.abspath(installed_repository_file)) > 0:
-                with io.open(installed_repository_file, encoding='utf-8') as f:
-                    workflow_data = f.read()
-                return self.__api_import_from_archive(trans, workflow_data)
-            else:
-                raise exceptions.MessageException("You attempted to open an empty file.")
-
-        if 'archive_source' in payload:
-            archive_source = payload['archive_source']
-            archive_file = payload.get('archive_file')
+        if "archive_source" in payload or "archive_file" in payload:
+            archive_source = payload.get("archive_source")
+            archive_file = payload.get("archive_file")
             archive_data = None
             if archive_source:
+                validate_uri_access(archive_source, trans.user_is_admin, trans.app.config.fetch_url_allowlist_ips)
                 if archive_source.startswith("file://"):
-                    if not trans.user_is_admin:
-                        raise exceptions.AdminRequiredException()
-                    workflow_src = {"src": "from_path", "path": archive_source[len("file://"):]}
+                    workflow_src = {"src": "from_path", "path": archive_source[len("file://") :]}
                     payload["workflow"] = workflow_src
                     return self.__api_import_new_workflow(trans, payload, **kwd)
+                elif archive_source == "trs_tool":
+                    server = None
+                    trs_tool_id = None
+                    trs_version_id = None
+                    import_source = None
+                    if "trs_url" in payload:
+                        parts = self.app.trs_proxy.match_url(payload["trs_url"])
+                        if parts:
+                            server = self.app.trs_proxy.server_from_url(parts["trs_base_url"])
+                            trs_tool_id = parts["tool_id"]
+                            trs_version_id = parts["version_id"]
+                            payload["trs_tool_id"] = trs_tool_id
+                            payload["trs_version_id"] = trs_version_id
+                        else:
+                            raise exceptions.MessageException("Invalid TRS URL.")
+                    else:
+                        trs_server = payload.get("trs_server")
+                        server = self.app.trs_proxy.get_server(trs_server)
+                        trs_tool_id = payload.get("trs_tool_id")
+                        trs_version_id = payload.get("trs_version_id")
+
+                    archive_data = server.get_version_descriptor(trs_tool_id, trs_version_id)
                 else:
                     try:
-                        archive_data = requests.get(archive_source).text
+                        archive_data = stream_url_to_str(
+                            archive_source, trans.app.file_sources, prefix="gx_workflow_download"
+                        )
+                        import_source = "URL"
                     except Exception:
-                        raise exceptions.MessageException("Failed to open URL '%s'." % escape(archive_source))
-            elif hasattr(archive_file, 'file'):
+                        raise exceptions.MessageException(f"Failed to open URL '{escape(archive_source)}'.")
+            elif hasattr(archive_file, "file"):
                 uploaded_file = archive_file.file
                 uploaded_file_name = uploaded_file.name
                 if os.path.getsize(os.path.abspath(uploaded_file_name)) > 0:
                     archive_data = util.unicodify(uploaded_file.read())
+                    import_source = "uploaded file"
                 else:
                     raise exceptions.MessageException("You attempted to upload an empty file.")
             else:
                 raise exceptions.MessageException("Please provide a URL or file.")
-            return self.__api_import_from_archive(trans, archive_data, "uploaded file")
+            return self.__api_import_from_archive(trans, archive_data, import_source, payload=payload)
 
-        if 'from_history_id' in payload:
-            from_history_id = payload.get('from_history_id')
+        if "from_history_id" in payload:
+            from_history_id = payload.get("from_history_id")
             from_history_id = self.decode_id(from_history_id)
             history = self.history_manager.get_accessible(from_history_id, trans.user, current_history=trans.history)
 
-            job_ids = [self.decode_id(_) for _ in payload.get('job_ids', [])]
-            dataset_ids = payload.get('dataset_ids', [])
-            dataset_collection_ids = payload.get('dataset_collection_ids', [])
-            workflow_name = payload['workflow_name']
+            job_ids = [self.decode_id(_) for _ in payload.get("job_ids", [])]
+            dataset_ids = payload.get("dataset_ids", [])
+            dataset_collection_ids = payload.get("dataset_collection_ids", [])
+            workflow_name = payload["workflow_name"]
             stored_workflow = extract_workflow(
                 trans=trans,
-                user=trans.get_user(),
+                user=trans.user,
                 history=history,
                 job_ids=job_ids,
                 dataset_ids=dataset_ids,
                 dataset_collection_ids=dataset_collection_ids,
                 workflow_name=workflow_name,
             )
-            item = stored_workflow.to_dict(value_mapper={'id': trans.security.encode_id})
-            item['url'] = url_for('workflow', id=item['id'])
+            item = stored_workflow.to_dict(value_mapper={"id": trans.security.encode_id})
+            item["url"] = url_for("workflow", id=item["id"])
             return item
 
-        if 'from_path' in payload:
-            from_path = payload.get('from_path')
+        if "from_path" in payload:
+            from_path = payload.get("from_path")
             object_id = payload.get("object_id")
             workflow_src = {"src": "from_path", "path": from_path}
             if object_id is not None:
                 workflow_src["object_id"] = object_id
             payload["workflow"] = workflow_src
             return self.__api_import_new_workflow(trans, payload, **kwd)
 
-        if 'shared_workflow_id' in payload:
-            workflow_id = payload['shared_workflow_id']
+        if "shared_workflow_id" in payload:
+            workflow_id = payload["shared_workflow_id"]
             return self.__api_import_shared_workflow(trans, workflow_id, payload)
 
-        if 'workflow' in payload:
+        if "workflow" in payload:
             return self.__api_import_new_workflow(trans, payload, **kwd)
 
-        workflow_id = payload.get('workflow_id', None)
-        if not workflow_id:
-            message = "Invalid workflow_id specified."
-            raise exceptions.RequestParameterInvalidException(message)
-
-        # Get workflow + accessibility check.
-        stored_workflow = self.__get_stored_accessible_workflow(trans, workflow_id)
-        workflow = stored_workflow.latest_workflow
-
-        run_configs = build_workflow_run_configs(trans, workflow, payload)
-        assert len(run_configs) == 1
-        run_config = run_configs[0]
-        history = run_config.target_history
-
-        # invoke may throw MessageExceptions on tool erors, failure
-        # to match up inputs, etc...
-        outputs, invocation = invoke(
-            trans=trans,
-            workflow=workflow,
-            workflow_run_config=run_config,
-            populate_state=True,
-        )
-        trans.sa_session.flush()
-
-        # Build legacy output - should probably include more information from
-        # outputs.
-        rval = {}
-        rval['history'] = trans.security.encode_id(history.id)
-        rval['outputs'] = []
-        if outputs:
-            # Newer outputs don't necessarily fill outputs (?)
-            for step in workflow.steps:
-                if step.type == 'tool' or step.type is None:
-                    for v in outputs[step.id].values():
-                        rval['outputs'].append(trans.security.encode_id(v.id))
-
-        # Newer version of this API just returns the invocation as a dict, to
-        # facilitate migration - produce the newer style response and blend in
-        # the older information.
-        invocation_response = self.__encode_invocation(invocation, **kwd)
-        invocation_response.update(rval)
-        return invocation_response
+        # This was already raised above, but just in case...
+        raise exceptions.RequestParameterMissingException("No method for workflow creation supplied.")
 
     @expose_api_raw_anonymous_and_sessionless
-    def workflow_dict(self, trans, workflow_id, **kwd):
+    def workflow_dict(self, trans: GalaxyWebTransaction, workflow_id, **kwd):
         """
         GET /api/workflows/{encoded_workflow_id}/download
 
         Returns a selected workflow.
 
         :type   style:  str
         :param  style:  Style of export. The default is 'export', which is the meant to be used
@@ -463,438 +368,498 @@
         :param  instance:                 true if fetch by Workflow ID instead of StoredWorkflow id, false
                                           by default.
         :type   instance:                 boolean
         """
         stored_workflow = self.__get_stored_accessible_workflow(trans, workflow_id, **kwd)
 
         style = kwd.get("style", "export")
-        download_format = kwd.get('format')
-        version = kwd.get('version')
-        ret_dict = self.workflow_contents_manager.workflow_to_dict(trans, stored_workflow, style=style, version=version)
-        if download_format == 'json-download':
+        download_format = kwd.get("format")
+        version = kwd.get("version")
+        history_id = kwd.get("history_id")
+        history = None
+        if history_id:
+            history = self.history_manager.get_accessible(
+                self.decode_id(history_id), trans.user, current_history=trans.history
+            )
+        ret_dict = self.workflow_contents_manager.workflow_to_dict(
+            trans, stored_workflow, style=style, version=version, history=history
+        )
+        if download_format == "json-download":
             sname = stored_workflow.name
-            sname = ''.join(c in util.FILENAME_VALID_CHARS and c or '_' for c in sname)[0:150]
+            sname = "".join(c in util.FILENAME_VALID_CHARS and c or "_" for c in sname)[0:150]
             if ret_dict.get("format-version", None) == "0.1":
                 extension = "ga"
             else:
                 extension = "gxwf.json"
-            trans.response.headers["Content-Disposition"] = 'attachment; filename="Galaxy-Workflow-%s.%s"' % (sname, extension)
-            trans.response.set_content_type('application/galaxy-archive')
+            trans.response.headers[
+                "Content-Disposition"
+            ] = f'attachment; filename="Galaxy-Workflow-{sname}.{extension}"'
+            trans.response.set_content_type("application/galaxy-archive")
 
-        if style == "format2" and download_format != 'json-download':
+        if style == "format2" and download_format != "json-download":
             return ordered_dump(ret_dict)
         else:
             return format_return_as_json(ret_dict, pretty=True)
 
     @expose_api
-    def delete(self, trans, id, **kwd):
-        """
-        DELETE /api/workflows/{encoded_workflow_id}
-        Deletes a specified workflow
-        Author: rpark
-
-        copied from galaxy.web.controllers.workflows.py (delete)
-        """
-        workflow_id = id
-
-        try:
-            stored_workflow = trans.sa_session.query(self.app.model.StoredWorkflow).get(self.decode_id(workflow_id))
-        except Exception as e:
-            trans.response.status = 400
-            return ("Workflow with ID='%s' can not be found\n Exception: %s") % (workflow_id, util.unicodify(e))
-
-        # check to see if user has permissions to selected workflow
-        if stored_workflow.user != trans.user and not trans.user_is_admin:
-            trans.response.status = 403
-            return("Workflow is not owned by current user")
-
-        # Mark a workflow as deleted
-        stored_workflow.deleted = True
-        trans.sa_session.flush()
-
-        # TODO: Unsure of response message to let api know that a workflow was successfully deleted
-        return ("Workflow '%s' successfully deleted" % stored_workflow.name)
-
-    @expose_api
-    def import_new_workflow_deprecated(self, trans, payload, **kwd):
+    def import_new_workflow_deprecated(self, trans: GalaxyWebTransaction, payload, **kwd):
         """
         POST /api/workflows/upload
         Importing dynamic workflows from the api. Return newly generated workflow id.
         Author: rpark
 
         # currently assumes payload['workflow'] is a json representation of a workflow to be inserted into the database
 
         Deprecated in favor to POST /api/workflows with encoded 'workflow' in
         payload the same way.
         """
         return self.__api_import_new_workflow(trans, payload, **kwd)
 
     @expose_api
-    def update(self, trans, id, payload, **kwds):
+    def update(self, trans: GalaxyWebTransaction, id, payload, **kwds):
         """
-        * PUT /api/workflows/{id}
-            updates the workflow stored with ``id``
+        PUT /api/workflows/{id}
+
+        Update the workflow stored with ``id``.
 
         :type   id:      str
         :param  id:      the encoded id of the workflow to update
-        :param  instance:                 true if fetch by Workflow ID instead of StoredWorkflow id, false
-                                          by default.
-        :type   instance:                 boolean
+        :param  instance: true if fetch by Workflow ID instead of StoredWorkflow id, false by default.
+        :type   instance: boolean
         :type   payload: dict
         :param  payload: a dictionary containing any or all the
-            * workflow   the json description of the workflow as would be
-                         produced by GET workflows/<id>/download or
-                         given to `POST workflows`
-
-                         The workflow contents will be updated to target
-                         this.
-
-            * name       optional string name for the workflow, if not present in payload,
-                         name defaults to existing name
-            * annotation optional string annotation for the workflow, if not present in payload,
-                         annotation defaults to existing annotation
-            * menu_entry optional boolean marking if the workflow should appear in the user's menu,
-                         if not present, workflow menu entries are not modified
-            * from_tool_form True iff encoded state coming in is encoded for the tool form.
+
+            :workflow:
+
+                the json description of the workflow as would be
+                produced by GET workflows/<id>/download or
+                given to `POST workflows`
+
+                The workflow contents will be updated to target this.
+
+            :name:
+
+                optional string name for the workflow, if not present in payload,
+                name defaults to existing name
+
+            :annotation:
+
+                optional string annotation for the workflow, if not present in payload,
+                annotation defaults to existing annotation
+
+            :menu_entry:
+
+                optional boolean marking if the workflow should appear in the user\'s menu,
+                if not present, workflow menu entries are not modified
+
+            :tags:
+
+                optional list containing list of tags to add to the workflow (overwriting
+                existing tags), if not present, tags are not modified
+
+            :from_tool_form:
+
+                True iff encoded state coming in is encoded for the tool form.
+
 
         :rtype:     dict
         :returns:   serialized version of the workflow
         """
         stored_workflow = self.__get_stored_workflow(trans, id, **kwds)
-        workflow_dict = payload.get('workflow') or payload
+        workflow_dict = payload.get("workflow", {})
+        workflow_dict.update({k: v for k, v in payload.items() if k not in workflow_dict})
         if workflow_dict:
+            require_flush = False
             raw_workflow_description = self.__normalize_workflow(trans, workflow_dict)
             workflow_dict = raw_workflow_description.as_dict
-            new_workflow_name = workflow_dict.get('name')
-            if new_workflow_name and new_workflow_name != stored_workflow.name:
-                sanitized_name = sanitize_html(new_workflow_name)
-                workflow = stored_workflow.latest_workflow.copy()
+            new_workflow_name = workflow_dict.get("name")
+            old_workflow = stored_workflow.latest_workflow
+            name_updated = new_workflow_name and new_workflow_name != stored_workflow.name
+            steps_updated = "steps" in workflow_dict
+            if name_updated and not steps_updated:
+                sanitized_name = sanitize_html(new_workflow_name or old_workflow.name)
+                workflow = old_workflow.copy(user=trans.user)
                 workflow.stored_workflow = stored_workflow
                 workflow.name = sanitized_name
                 stored_workflow.name = sanitized_name
                 stored_workflow.latest_workflow = workflow
                 trans.sa_session.add(workflow, stored_workflow)
-                trans.sa_session.flush()
-
-            if 'annotation' in workflow_dict:
-                newAnnotation = sanitize_html(workflow_dict['annotation'])
-                self.add_item_annotation(trans.sa_session, trans.get_user(), stored_workflow, newAnnotation)
-                trans.sa_session.flush()
+                require_flush = True
 
-            if 'menu_entry' in workflow_dict or 'show_in_tool_panel' in workflow_dict:
-                if workflow_dict.get('menu_entry') or workflow_dict.get('show_in_tool_panel'):
-                    menuEntry = model.StoredWorkflowMenuEntry()
-                    menuEntry.stored_workflow = stored_workflow
-                    trans.get_user().stored_workflow_menu_entries.append(menuEntry)
+            if "hidden" in workflow_dict and stored_workflow.hidden != workflow_dict["hidden"]:
+                stored_workflow.hidden = workflow_dict["hidden"]
+                require_flush = True
+
+            if "published" in workflow_dict and stored_workflow.published != workflow_dict["published"]:
+                stored_workflow.published = workflow_dict["published"]
+                require_flush = True
+
+            if "importable" in workflow_dict and stored_workflow.importable != workflow_dict["importable"]:
+                stored_workflow.importable = workflow_dict["importable"]
+                require_flush = True
+
+            if "annotation" in workflow_dict and not steps_updated:
+                newAnnotation = sanitize_html(workflow_dict["annotation"])
+                self.add_item_annotation(trans.sa_session, trans.user, stored_workflow, newAnnotation)
+                require_flush = True
+
+            if "menu_entry" in workflow_dict or "show_in_tool_panel" in workflow_dict:
+                show_in_panel = workflow_dict.get("menu_entry") or workflow_dict.get("show_in_tool_panel")
+                stored_workflow_menu_entries = trans.user.stored_workflow_menu_entries
+                decoded_id = trans.security.decode_id(id)
+                if show_in_panel:
+                    workflow_ids = [wf.stored_workflow_id for wf in stored_workflow_menu_entries]
+                    if decoded_id not in workflow_ids:
+                        menu_entry = model.StoredWorkflowMenuEntry()
+                        menu_entry.stored_workflow = stored_workflow
+                        stored_workflow_menu_entries.append(menu_entry)
+                        trans.sa_session.add(menu_entry)
+                        require_flush = True
                 else:
                     # remove if in list
-                    entries = {x.stored_workflow_id: x for x in trans.get_user().stored_workflow_menu_entries}
-                    if trans.security.decode_id(id) in entries:
-                        trans.get_user().stored_workflow_menu_entries.remove(entries[trans.security.decode_id(id)])
+                    entries = {x.stored_workflow_id: x for x in stored_workflow_menu_entries}
+                    if decoded_id in entries:
+                        stored_workflow_menu_entries.remove(entries[decoded_id])
+                        require_flush = True
             # set tags
-            if 'tags' in workflow_dict:
-                trans.app.tag_handler.set_tags_from_list(user=trans.user, item=stored_workflow, new_tags_list=workflow_dict['tags'])
+            if "tags" in workflow_dict:
+                trans.app.tag_handler.set_tags_from_list(
+                    user=trans.user, item=stored_workflow, new_tags_list=workflow_dict["tags"]
+                )
 
-            if 'steps' in workflow_dict:
+            if require_flush:
+                trans.sa_session.flush()
+
+            if "steps" in workflow_dict:
                 try:
-                    from_dict_kwds = self.__import_or_update_kwds(payload)
+                    workflow_update_options = WorkflowUpdateOptions(**payload)
                     workflow, errors = self.workflow_contents_manager.update_workflow_from_raw_description(
                         trans,
                         stored_workflow,
                         raw_workflow_description,
-                        **from_dict_kwds
+                        workflow_update_options,
+                    )
+                except MissingToolsException:
+                    raise exceptions.MessageException(
+                        "This workflow contains missing tools. It cannot be saved until they have been removed from the workflow or installed."
                     )
-                except workflows.MissingToolsException:
-                    raise exceptions.MessageException("This workflow contains missing tools. It cannot be saved until they have been removed from the workflow or installed.")
+
         else:
             message = "Updating workflow requires dictionary containing 'workflow' attribute with new JSON description."
             raise exceptions.RequestParameterInvalidException(message)
         return self.workflow_contents_manager.workflow_to_dict(trans, stored_workflow, style="instance")
 
     @expose_api
-    def build_module(self, trans, payload={}):
+    def refactor(self, trans, id, payload, **kwds):
+        """
+        * PUT /api/workflows/{id}/refactor
+            updates the workflow stored with ``id``
+
+        :type   id:      str
+        :param  id:      the encoded id of the workflow to update
+        :param  instance:                 true if fetch by Workflow ID instead of StoredWorkflow id, false
+                                          by default.
+        :type   instance:                 boolean
+        :type   payload: dict
+        :param  payload: a dictionary containing list of actions to apply.
+        :rtype:     dict
+        :returns:   serialized version of the workflow
+        """
+        stored_workflow = self.__get_stored_workflow(trans, id, **kwds)
+        refactor_request = RefactorRequest(**payload)
+        return self.workflow_contents_manager.refactor(trans, stored_workflow, refactor_request)
+
+    @expose_api
+    def build_module(self, trans: GalaxyWebTransaction, payload=None):
         """
         POST /api/workflows/build_module
         Builds module models for the workflow editor.
         """
-        inputs = payload.get('inputs', {})
+        if payload is None:
+            payload = {}
+        inputs = payload.get("inputs", {})
         trans.workflow_building_mode = workflow_building_modes.ENABLED
         module = module_factory.from_dict(trans, payload, from_tool_form=True)
-        if 'tool_state' not in payload:
-            module_state = {}
+        if "tool_state" not in payload:
+            module_state: Dict[str, Any] = {}
             populate_state(trans, module.get_inputs(), inputs, module_state, check=False)
             module.recover_state(module_state, from_tool_form=True)
         return {
-            'label'             : inputs.get('__label', ''),
-            'annotation'        : inputs.get('__annotation', ''),
-            'name'              : module.get_name(),
-            'tool_state'        : module.get_state(),
-            'inputs'            : module.get_all_inputs(connectable_only=True),
-            'outputs'           : module.get_all_outputs(),
-            'config_form'       : module.get_config_form(),
-            'post_job_actions'  : module.get_post_job_actions(inputs)
+            "label": inputs.get("__label", ""),
+            "annotation": inputs.get("__annotation", ""),
+            "name": module.get_name(),
+            "tool_state": module.get_state(),
+            "content_id": module.get_content_id(),
+            "inputs": module.get_all_inputs(connectable_only=True),
+            "outputs": module.get_all_outputs(),
+            "config_form": module.get_config_form(),
         }
 
     @expose_api
-    def get_tool_predictions(self, trans, payload, **kwd):
+    def get_tool_predictions(self, trans: ProvidesUserContext, payload, **kwd):
         """
         POST /api/workflows/get_tool_predictions
+
         Fetch predicted tools for a workflow
+
         :type   payload: dict
-        :param  payload: a dictionary containing two parameters:
-                         'tool_sequence' - comma separated sequence of tool ids
-                         'remote_model_url' - (optional) path to the deep learning model
-        """
-        remote_model_url = payload.get('remote_model_url', trans.app.config.tool_recommendation_model_path)
-        tool_sequence = payload.get('tool_sequence', "")
-        if 'tool_sequence' not in payload or remote_model_url is None:
+        :param  payload:
+
+            a dictionary containing two parameters
+            'tool_sequence' - comma separated sequence of tool ids
+            'remote_model_url' - (optional) path to the deep learning model
+        """
+        remote_model_url = payload.get("remote_model_url", trans.app.config.tool_recommendation_model_path)
+        tool_sequence = payload.get("tool_sequence", "")
+        if "tool_sequence" not in payload or remote_model_url is None:
             return
-        tool_sequence, recommended_tools = self.tool_recommendations.get_predictions(trans, tool_sequence, remote_model_url)
-        return {
-            "current_tool": tool_sequence,
-            "predicted_data": recommended_tools
-        }
+        tool_sequence, recommended_tools = self.tool_recommendations.get_predictions(
+            trans, tool_sequence, remote_model_url
+        )
+        return {"current_tool": tool_sequence, "predicted_data": recommended_tools}
 
     #
     # -- Helper methods --
     #
-    def __api_import_from_archive(self, trans, archive_data, source=None):
+    def __api_import_from_archive(self, trans: GalaxyWebTransaction, archive_data, source=None, payload=None):
+        payload = payload or {}
         try:
             data = json.loads(archive_data)
         except Exception:
             if "GalaxyWorkflow" in archive_data:
                 data = {"yaml_content": archive_data}
             else:
                 raise exceptions.MessageException("The data content does not appear to be a valid workflow.")
         if not data:
             raise exceptions.MessageException("The data content is missing.")
         raw_workflow_description = self.__normalize_workflow(trans, data)
-        workflow, missing_tool_tups = self._workflow_from_dict(trans, raw_workflow_description, source=source)
+        workflow_create_options = WorkflowCreateOptions(**payload)
+        workflow, missing_tool_tups = self._workflow_from_dict(
+            trans, raw_workflow_description, workflow_create_options, source=source
+        )
+        workflow_id = workflow.id
         workflow = workflow.latest_workflow
+
+        response = {
+            "message": f"Workflow '{escape(workflow.name)}' imported successfully.",
+            "status": "success",
+            "id": trans.security.encode_id(workflow_id),
+        }
         if workflow.has_errors:
-            return {"message": "Imported, but some steps in this workflow have validation errors.", "status": "error"}
+            response["message"] = "Imported, but some steps in this workflow have validation errors."
+            response["status"] = "error"
         elif len(workflow.steps) == 0:
-            return {"message": "Imported, but this workflow has no steps.", "status": "error"}
+            response["message"] = "Imported, but this workflow has no steps."
+            response["status"] = "error"
         elif workflow.has_cycles:
-            return {"message": "Imported, but this workflow contains cycles.", "status": "error"}
-        return {"message": "Workflow '%s' imported successfully." % escape(workflow.name), "status": "success"}
+            response["message"] = "Imported, but this workflow contains cycles."
+            response["status"] = "error"
+        return response
 
-    def __api_import_new_workflow(self, trans, payload, **kwd):
-        data = payload['workflow']
+    def __api_import_new_workflow(self, trans: GalaxyWebTransaction, payload, **kwd):
+        data = payload["workflow"]
         raw_workflow_description = self.__normalize_workflow(trans, data)
-        data = raw_workflow_description.as_dict
-        import_tools = util.string_as_bool(payload.get("import_tools", False))
-        if import_tools and not trans.user_is_admin:
-            raise exceptions.AdminRequiredException()
-
-        from_dict_kwds = self.__import_or_update_kwds(payload)
-
-        publish = util.string_as_bool(payload.get("publish", False))
-        # If 'publish' set, default to importable.
-        importable = util.string_as_bool(payload.get("importable", publish))
-
-        if publish and not importable:
-            raise exceptions.RequestParameterInvalidException("Published workflow must be importable.")
-
-        from_dict_kwds["publish"] = publish
-        workflow, missing_tool_tups = self._workflow_from_dict(trans, raw_workflow_description, **from_dict_kwds)
-        if importable:
-            self._make_item_accessible(trans.sa_session, workflow)
-            trans.sa_session.flush()
+        workflow_create_options = WorkflowCreateOptions(**payload)
+        workflow, missing_tool_tups = self._workflow_from_dict(
+            trans,
+            raw_workflow_description,
+            workflow_create_options,
+        )
         # galaxy workflow newly created id
         workflow_id = workflow.id
         # api encoded, id
         encoded_id = trans.security.encode_id(workflow_id)
-        item = workflow.to_dict(value_mapper={'id': trans.security.encode_id})
-        item['annotations'] = [x.annotation for x in workflow.annotations]
-        item['url'] = url_for('workflow', id=encoded_id)
-        item['owner'] = workflow.user.username
-        item['number_of_steps'] = len(workflow.latest_workflow.steps)
-        if import_tools:
-            tools = {}
-            for key in data['steps']:
-                item = data['steps'][key]
-                if item is not None:
-                    if 'tool_shed_repository' in item:
-                        tool_shed_repository = item['tool_shed_repository']
-                        if 'owner' in tool_shed_repository and 'changeset_revision' in tool_shed_repository and 'name' in tool_shed_repository and 'tool_shed' in tool_shed_repository:
-                            toolstr = tool_shed_repository['owner'] \
-                                + tool_shed_repository['changeset_revision'] \
-                                + tool_shed_repository['name'] \
-                                + tool_shed_repository['tool_shed']
-                            tools[toolstr] = tool_shed_repository
-            irm = InstallRepositoryManager(self.app)
-            for k in tools:
-                item = tools[k]
-                tool_shed_url = 'https://' + item['tool_shed'] + '/'
-                name = item['name']
-                owner = item['owner']
-                changeset_revision = item['changeset_revision']
-                irm.install(tool_shed_url,
-                            name,
-                            owner,
-                            changeset_revision,
-                            payload)
+        item = workflow.to_dict(value_mapper={"id": trans.security.encode_id})
+        item["annotations"] = [x.annotation for x in workflow.annotations]
+        item["url"] = url_for("workflow", id=encoded_id)
+        item["owner"] = workflow.user.username
+        item["number_of_steps"] = len(workflow.latest_workflow.steps)
         return item
 
-    def __import_or_update_kwds(self, payload):
-        # Galaxy will try to upgrade tool versions that don't match exactly during import,
-        # this prevents that.
-        exact_tools = util.string_as_bool(payload.get("exact_tools", True))
-
-        # Fill in missing tool state for hand built so the workflow can run, default of this
-        # should become True at some point in the future I imagine.
-        fill_defaults = util.string_as_bool(payload.get("fill_defaults", False))
-        from_tool_form = payload.get("from_tool_form", False)
-        return {
-            'exact_tools': exact_tools,
-            'fill_defaults': fill_defaults,
-            'from_tool_form': from_tool_form,
-        }
-
-    def __normalize_workflow(self, trans, as_dict):
+    def __normalize_workflow(self, trans: GalaxyWebTransaction, as_dict):
         return self.workflow_contents_manager.normalize_workflow_format(trans, as_dict)
 
     @expose_api
-    def import_shared_workflow_deprecated(self, trans, payload, **kwd):
+    def import_shared_workflow_deprecated(self, trans: GalaxyWebTransaction, payload, **kwd):
         """
         POST /api/workflows/import
         Import a workflow shared by other users.
 
         :param  workflow_id:      the workflow id (required)
         :type   workflow_id:      str
 
         :raises: exceptions.MessageException, exceptions.ObjectNotFound
         """
         # Pull parameters out of payload.
-        workflow_id = payload.get('workflow_id', None)
+        workflow_id = payload.get("workflow_id", None)
         if workflow_id is None:
             raise exceptions.ObjectAttributeMissingException("Missing required parameter 'workflow_id'.")
         self.__api_import_shared_workflow(trans, workflow_id, payload)
 
-    def __api_import_shared_workflow(self, trans, workflow_id, payload, **kwd):
+    def __api_import_shared_workflow(self, trans: GalaxyWebTransaction, workflow_id, payload, **kwd):
         try:
             stored_workflow = self.get_stored_workflow(trans, workflow_id, check_ownership=False)
         except Exception:
-            raise exceptions.ObjectNotFound("Malformed workflow id ( %s ) specified." % workflow_id)
+            raise exceptions.ObjectNotFound("Malformed workflow id specified.")
         if stored_workflow.importable is False:
-            raise exceptions.ItemAccessibilityException('The owner of this workflow has disabled imports via this link.')
+            raise exceptions.ItemAccessibilityException(
+                "The owner of this workflow has disabled imports via this link."
+            )
         elif stored_workflow.deleted:
             raise exceptions.ItemDeletionException("You can't import this workflow because it has been deleted.")
         imported_workflow = self._import_shared_workflow(trans, stored_workflow)
-        item = imported_workflow.to_dict(value_mapper={'id': trans.security.encode_id})
+        item = imported_workflow.to_dict(value_mapper={"id": trans.security.encode_id})
         encoded_id = trans.security.encode_id(imported_workflow.id)
-        item['url'] = url_for('workflow', id=encoded_id)
+        item["url"] = url_for("workflow", id=encoded_id)
         return item
 
     @expose_api
-    def invoke(self, trans, workflow_id, payload, **kwd):
+    def invoke(self, trans: GalaxyWebTransaction, workflow_id, payload, **kwd):
         """
         POST /api/workflows/{encoded_workflow_id}/invocations
 
         Schedule the workflow specified by `workflow_id` to run.
 
         .. note:: This method takes the same arguments as
             :func:`galaxy.webapps.galaxy.api.workflows.WorkflowsAPIController.create` above.
+
+        :raises: exceptions.MessageException, exceptions.RequestParameterInvalidException
         """
         # Get workflow + accessibility check.
-        stored_workflow = self.__get_stored_accessible_workflow(trans, workflow_id)
+        stored_workflow = self.__get_stored_accessible_workflow(trans, workflow_id, instance=kwd.get("instance", False))
         workflow = stored_workflow.latest_workflow
         run_configs = build_workflow_run_configs(trans, workflow, payload)
-        is_batch = payload.get('batch')
+        is_batch = payload.get("batch")
         if not is_batch and len(run_configs) != 1:
             raise exceptions.RequestParameterInvalidException("Must specify 'batch' to use batch parameters.")
 
+        require_exact_tool_versions = util.string_as_bool(payload.get("require_exact_tool_versions", "true"))
+        tools = self.workflow_contents_manager.get_all_tools(workflow)
+        missing_tools = [
+            tool
+            for tool in tools
+            if not self.app.toolbox.has_tool(
+                tool["tool_id"], tool_version=tool["tool_version"], exact=require_exact_tool_versions
+            )
+        ]
+        if missing_tools:
+            missing_tools_message = "Workflow was not invoked; the following required tools are not installed: "
+            if require_exact_tool_versions:
+                missing_tools_message += ", ".join(
+                    [f"{tool['tool_id']} (version {tool['tool_version']})" for tool in missing_tools]
+                )
+            else:
+                missing_tools_message += ", ".join([tool["tool_id"] for tool in missing_tools])
+            raise exceptions.MessageException(missing_tools_message)
+
         invocations = []
         for run_config in run_configs:
-            workflow_scheduler_id = payload.get('scheduler', None)
+            workflow_scheduler_id = payload.get("scheduler", None)
             # TODO: workflow scheduler hints
             work_request_params = dict(scheduler=workflow_scheduler_id)
             workflow_invocation = queue_invoke(
                 trans=trans,
                 workflow=workflow,
                 workflow_run_config=run_config,
-                request_params=work_request_params
+                request_params=work_request_params,
+                flush=False,
             )
-            invocation = self.encode_all_ids(trans, workflow_invocation.to_dict(), recursive=True)
-            invocations.append(invocation)
+            invocations.append(workflow_invocation)
+
+        trans.sa_session.flush()
+        encoded_invocations = []
+        for invocation in invocations:
+            as_dict = workflow_invocation.to_dict()
+            as_dict = self.encode_all_ids(trans, as_dict, recursive=True)
+            as_dict["messages"] = [
+                InvocationMessageResponseModel.parse_obj(message).__root__.dict() for message in invocation.messages
+            ]
+            encoded_invocations.append(as_dict)
 
         if is_batch:
-            return invocations
+            return encoded_invocations
         else:
-            return invocations[0]
+            return encoded_invocations[0]
 
     @expose_api
-    def index_invocations(self, trans, workflow_id=None, **kwd):
+    def index_invocations(self, trans: GalaxyWebTransaction, **kwd):
         """
         GET /api/workflows/{workflow_id}/invocations
         GET /api/invocations
 
         Get the list of a user's workflow invocations. If workflow_id is supplied
         (either via URL or query parameter) it should be an encoded StoredWorkflow id
         and returned invocations will be restricted to that workflow. history_id (an encoded
         History id) can be used to further restrict the query. If neither a workflow_id or
         history_id is supplied, all the current user's workflow invocations will be indexed
         (as determined by the invocation being executed on one of the user's histories).
 
         :param  workflow_id:      an encoded stored workflow id to restrict query to
         :type   workflow_id:      str
 
+        :param  instance:         true if fetch by Workflow ID instead of StoredWorkflow id, false
+                                  by default.
+        :type   instance:         boolean
+
         :param  history_id:       an encoded history id to restrict query to
         :type   history_id:       str
 
+        :param  job_id:           an encoded job id to restrict query to
+        :type   job_id:           str
+
         :param  user_id:          an encoded user id to restrict query to, must be own id if not admin user
         :type   user_id:          str
 
         :param  view:             level of detail to return per invocation 'element' or 'collection'.
         :type   view:             str
 
         :param  step_details:     If 'view' is 'element', also include details on individual steps.
         :type   step_details:     bool
 
         :raises: exceptions.MessageException, exceptions.ObjectNotFound
         """
-        if workflow_id is not None:
-            stored_workflow_id = self.__get_stored_workflow(trans, workflow_id).id
-        else:
-            stored_workflow_id = None
-
-        encoded_history_id = kwd.get("history_id", None)
-        if encoded_history_id:
-            history = self.history_manager.get_accessible(self.decode_id(encoded_history_id), trans.user, current_history=trans.history)
-            history_id = history.id
-        else:
-            history_id = None
-
-        encoded_user_id = kwd.get("user_id", None)
-        if encoded_user_id:
-            target_user_id = self.decode_id(encoded_user_id)
-        else:
-            target_user_id = None
-
-        if not trans.user_is_admin:
-            # We restrict the query to the current users' invocations
-            user_id = trans.user.id
-            if target_user_id and user_id != target_user_id:
-                raise exceptions.AdminRequiredException("Only admins can index the invocations of others")
-        else:
-            # Get all invocation if user is admin
-            user_id = target_user_id
-
-        include_terminal = util.string_as_bool(kwd.get("include_terminal", True))
-        limit = kwd.get("limit", None)
-        if limit is not None:
-            limit = int(limit)
-        invocations = self.workflow_manager.build_invocations_query(
-            trans, stored_workflow_id=stored_workflow_id, history_id=history_id, user_id=user_id, include_terminal=include_terminal, limit=limit
+        invocation_payload = InvocationIndexPayload(**kwd)
+        serialization_params = InvocationSerializationParams(**kwd)
+        invocations, total_matches = self.invocations_service.index(trans, invocation_payload, serialization_params)
+        trans.response.headers["total_matches"] = total_matches
+        return invocations
+
+    @expose_api_anonymous
+    def create_invocations_from_store(self, trans, payload, **kwd):
+        """
+        POST /api/invocations/from_store
+
+        Create invocation(s) from a supplied model store.
+
+        Input can be an archive describing a Galaxy model store containing an
+        workflow invocation - for instance one created with with write_store
+        or prepare_store_download endpoint.
+        """
+        create_payload = CreateInvocationFromStore(**payload)
+        serialization_params = InvocationSerializationParams(**payload)
+        # refactor into a service...
+        return self._create_from_store(trans, create_payload, serialization_params)
+
+    def _create_from_store(
+        self, trans, payload: CreateInvocationFromStore, serialization_params: InvocationSerializationParams
+    ):
+        history = self.history_manager.get_owned(
+            self.decode_id(payload.history_id), trans.user, current_history=trans.history
+        )
+        object_tracker = self.create_objects_from_store(
+            trans,
+            payload,
+            history=history,
+        )
+        return self.invocations_service.serialize_workflow_invocations(
+            object_tracker.invocations_by_key.values(), serialization_params
         )
-        return self.workflow_manager.serialize_workflow_invocations(invocations, **kwd)
 
     @expose_api
-    def show_invocation(self, trans, invocation_id, **kwd):
+    def show_invocation(self, trans: GalaxyWebTransaction, invocation_id, **kwd):
         """
         GET /api/workflows/{workflow_id}/invocations/{invocation_id}
         GET /api/invocations/{invocation_id}
 
         Get detailed description of workflow invocation
 
         :param  invocation_id:      the invocation id (required)
@@ -915,23 +880,22 @@
                                     step outputs but the individual job outputs
                                     when this is set - at least for now.
         :type   legacy_job_state:   bool
 
         :raises: exceptions.MessageException, exceptions.ObjectNotFound
         """
         decoded_workflow_invocation_id = self.decode_id(invocation_id)
-        workflow_invocation = self.workflow_manager.get_invocation(trans, decoded_workflow_invocation_id)
-        if workflow_invocation:
-            step_details = util.string_as_bool(kwd.get('step_details', 'False'))
-            legacy_job_state = util.string_as_bool(kwd.get('legacy_job_state', 'False'))
-            return self.__encode_invocation(workflow_invocation, step_details=step_details, legacy_job_state=legacy_job_state)
-        return None
+        workflow_invocation = self.workflow_manager.get_invocation(trans, decoded_workflow_invocation_id, eager=True)
+        if not workflow_invocation:
+            raise exceptions.ObjectNotFound()
+
+        return self.__encode_invocation(workflow_invocation, **kwd)
 
     @expose_api
-    def cancel_invocation(self, trans, invocation_id, **kwd):
+    def cancel_invocation(self, trans: ProvidesUserContext, invocation_id, **kwd):
         """
         DELETE /api/workflows/{workflow_id}/invocations/{invocation_id}
         DELETE /api/invocations/{invocation_id}
         Cancel the specified workflow invocation.
 
         :param  invocation_id:      the usage id (required)
         :type   invocation_id:      str
@@ -939,51 +903,35 @@
         :raises: exceptions.MessageException, exceptions.ObjectNotFound
         """
         decoded_workflow_invocation_id = self.decode_id(invocation_id)
         workflow_invocation = self.workflow_manager.cancel_invocation(trans, decoded_workflow_invocation_id)
         return self.__encode_invocation(workflow_invocation, **kwd)
 
     @expose_api
-    def show_invocation_report(self, trans, invocation_id, **kwd):
+    def show_invocation_report(self, trans: GalaxyWebTransaction, invocation_id, **kwd):
         """
         GET /api/workflows/{workflow_id}/invocations/{invocation_id}/report
         GET /api/invocations/{invocation_id}/report
 
         Get JSON summarizing invocation for reporting.
         """
         kwd["format"] = "json"
-        return self._generate_report(trans, invocation_id, **kwd)
+        return self.workflow_manager.get_invocation_report(trans, invocation_id, **kwd)
 
     @expose_api_raw
-    def show_invocation_report_pdf(self, trans, invocation_id, **kwd):
+    def show_invocation_report_pdf(self, trans: GalaxyWebTransaction, invocation_id, **kwd):
         """
         GET /api/workflows/{workflow_id}/invocations/{invocation_id}/report.pdf
         GET /api/invocations/{invocation_id}/report.pdf
 
         Get JSON summarizing invocation for reporting.
         """
         kwd["format"] = "pdf"
         trans.response.set_content_type("application/pdf")
-        return self._generate_report(trans, invocation_id, **kwd)
-
-    def _generate_report(self, trans, invocation_id, **kwd):
-        decoded_workflow_invocation_id = self.decode_id(invocation_id)
-        workflow_invocation = self.workflow_manager.get_invocation(trans, decoded_workflow_invocation_id)
-        generator_plugin_type = kwd.get("generator_plugin_type")
-        runtime_report_config_json = kwd.get("runtime_report_config_json")
-        invocation_markdown = kwd.get("invocation_markdown", None)
-        target_format = kwd.get("format", "json")
-        if invocation_markdown:
-            runtime_report_config_json = {"markdown": invocation_markdown}
-        return generate_report(
-            trans, workflow_invocation,
-            runtime_report_config_json=runtime_report_config_json,
-            plugin_type=generator_plugin_type,
-            target_format=target_format,
-        )
+        return self.workflow_manager.get_invocation_report(trans, invocation_id, **kwd)
 
     @expose_api
     def invocation_step(self, trans, invocation_id, step_id, **kwd):
         """
         GET /api/workflows/{workflow_id}/invocations/{invocation_id}/steps/{step_id}
         GET /api/invocations/{invocation_id}/steps/{step_id}
 
@@ -995,26 +943,24 @@
 
         :param  payload:       payload containing update action information
                                for running workflow.
 
         :raises: exceptions.MessageException, exceptions.ObjectNotFound
         """
         decoded_invocation_step_id = self.decode_id(step_id)
-        invocation_step = self.workflow_manager.get_invocation_step(
-            trans,
-            decoded_invocation_step_id
-        )
+        invocation_step = self.workflow_manager.get_invocation_step(trans, decoded_invocation_step_id)
         return self.__encode_invocation_step(trans, invocation_step)
 
     @expose_api_anonymous_and_sessionless
-    def invocation_step_jobs_summary(self, trans, invocation_id, **kwd):
+    def invocation_step_jobs_summary(self, trans: GalaxyWebTransaction, invocation_id, **kwd):
         """
-        * GET /api/workflows/{workflow_id}/invocations/{invocation_id}/step_jobs_summary
-          GET /api/invocations/{invocation_id}/step_jobs_summary
-            return job state summary info aggregated across per step of the workflow invocation
+        GET /api/workflows/{workflow_id}/invocations/{invocation_id}/step_jobs_summary
+        GET /api/invocations/{invocation_id}/step_jobs_summary
+
+        return job state summary info aggregated across per step of the workflow invocation
 
         Warning: We allow anyone to fetch job state information about any object they
         can guess an encoded ID for - it isn't considered protected data. This keeps
         polling IDs as part of state calculation for large histories and collections as
         efficient as possible.
 
         :param  invocation_id:    the invocation id (required)
@@ -1022,25 +968,26 @@
 
         :rtype:     dict[]
         :returns:   an array of job summary object dictionaries for each step
         """
         decoded_invocation_id = self.decode_id(invocation_id)
         ids = []
         types = []
-        for (job_source_type, job_source_id, _) in invocation_job_source_iter(trans.sa_session, decoded_invocation_id):
+        for job_source_type, job_source_id, _ in invocation_job_source_iter(trans.sa_session, decoded_invocation_id):
             ids.append(job_source_id)
             types.append(job_source_type)
         return [self.encode_all_ids(trans, s) for s in fetch_job_states(trans.sa_session, ids, types)]
 
     @expose_api_anonymous_and_sessionless
-    def invocation_jobs_summary(self, trans, invocation_id, **kwd):
+    def invocation_jobs_summary(self, trans: GalaxyWebTransaction, invocation_id, **kwd):
         """
-        * GET /api/workflows/{workflow_id}/invocations/{invocation_id}/jobs_summary
-          GET /api/invocations/{invocation_id}/jobs_summary
-            return job state summary info aggregated across all current jobs of workflow invocation
+        GET /api/workflows/{workflow_id}/invocations/{invocation_id}/jobs_summary
+        GET /api/invocations/{invocation_id}/jobs_summary
+
+        return job state summary info aggregated across all current jobs of workflow invocation
 
         Warning: We allow anyone to fetch job state information about any object they
         can guess an encoded ID for - it isn't considered protected data. This keeps
         polling IDs as part of state calculation for large histories and collections as
         efficient as possible.
 
         :param  invocation_id:    the invocation id (required)
@@ -1050,15 +997,15 @@
         :returns:   a job summary object merged for all steps in workflow invocation
         """
         ids = [self.decode_id(invocation_id)]
         types = ["WorkflowInvocation"]
         return [self.encode_all_ids(trans, s) for s in fetch_job_states(trans.sa_session, ids, types)][0]
 
     @expose_api
-    def update_invocation_step(self, trans, invocation_id, step_id, payload, **kwd):
+    def update_invocation_step(self, trans: GalaxyWebTransaction, invocation_id, step_id, payload, **kwd):
         """
         PUT /api/workflows/{workflow_id}/invocations/{invocation_id}/steps/{step_id}
         PUT /api/invocations/{invocation_id}/steps/{step_id}
 
         Update state of running workflow step invocation - still very nebulous
         but this would be for stuff like confirming paused steps can proceed
         etc....
@@ -1077,24 +1024,483 @@
         invocation_step = self.workflow_manager.update_invocation_step(
             trans,
             decoded_invocation_step_id,
             action=action,
         )
         return self.__encode_invocation_step(trans, invocation_step)
 
-    def __encode_invocation_step(self, trans, invocation_step):
-        return self.encode_all_ids(
+    def _workflow_from_dict(self, trans, data, workflow_create_options, source=None):
+        """Creates a workflow from a dict.
+
+        Created workflow is stored in the database and returned.
+        """
+        publish = workflow_create_options.publish
+        importable = workflow_create_options.is_importable
+        if publish and not importable:
+            raise exceptions.RequestParameterInvalidException("Published workflow must be importable.")
+
+        workflow_contents_manager = self.app.workflow_contents_manager
+        raw_workflow_description = workflow_contents_manager.ensure_raw_description(data)
+        created_workflow = workflow_contents_manager.build_workflow_from_raw_description(
             trans,
-            invocation_step.to_dict('element'),
-            True
+            raw_workflow_description,
+            workflow_create_options,
+            source=source,
         )
+        if importable:
+            self._make_item_accessible(trans.sa_session, created_workflow.stored_workflow)
+            trans.sa_session.flush()
+
+        self._import_tools_if_needed(trans, workflow_create_options, raw_workflow_description)
+        return created_workflow.stored_workflow, created_workflow.missing_tools
+
+    def _import_tools_if_needed(self, trans, workflow_create_options, raw_workflow_description):
+        if not workflow_create_options.import_tools:
+            return
+
+        if not trans.user_is_admin:
+            raise exceptions.AdminRequiredException()
+
+        data = raw_workflow_description.as_dict
+
+        tools = {}
+        for key in data["steps"]:
+            item = data["steps"][key]
+            if item is not None:
+                if "tool_shed_repository" in item:
+                    tool_shed_repository = item["tool_shed_repository"]
+                    if (
+                        "owner" in tool_shed_repository
+                        and "changeset_revision" in tool_shed_repository
+                        and "name" in tool_shed_repository
+                        and "tool_shed" in tool_shed_repository
+                    ):
+                        toolstr = (
+                            tool_shed_repository["owner"]
+                            + tool_shed_repository["changeset_revision"]
+                            + tool_shed_repository["name"]
+                            + tool_shed_repository["tool_shed"]
+                        )
+                        tools[toolstr] = tool_shed_repository
+
+        irm = InstallRepositoryManager(self.app)
+        install_options = workflow_create_options.install_options
+        for k in tools:
+            item = tools[k]
+            tool_shed_url = f"https://{item['tool_shed']}/"
+            name = item["name"]
+            owner = item["owner"]
+            changeset_revision = item["changeset_revision"]
+            irm.install(tool_shed_url, name, owner, changeset_revision, install_options)
+
+    def __encode_invocation_step(self, trans: ProvidesUserContext, invocation_step):
+        return self.encode_all_ids(trans, invocation_step.to_dict("element"), True)
 
     def __get_stored_accessible_workflow(self, trans, workflow_id, **kwd):
         instance = util.string_as_bool(kwd.get("instance", "false"))
         return self.workflow_manager.get_stored_accessible_workflow(trans, workflow_id, by_stored_id=not instance)
 
     def __get_stored_workflow(self, trans, workflow_id, **kwd):
         instance = util.string_as_bool(kwd.get("instance", "false"))
         return self.workflow_manager.get_stored_workflow(trans, workflow_id, by_stored_id=not instance)
 
     def __encode_invocation(self, invocation, **kwd):
-        return self.workflow_manager.serialize_workflow_invocation(invocation, **kwd)
+        params = InvocationSerializationParams(**kwd)
+        return self.invocations_service.serialize_workflow_invocation(invocation, params)
+
+
+StoredWorkflowIDPathParam: DecodedDatabaseIdField = Path(
+    ..., title="Stored Workflow ID", description="The encoded database identifier of the Stored Workflow."
+)
+
+InvocationIDPathParam: DecodedDatabaseIdField = Path(
+    ..., title="Invocation ID", description="The encoded database identifier of the Invocation."
+)
+
+DeletedQueryParam: bool = Query(
+    default=False, title="Display deleted", description="Whether to restrict result to deleted workflows."
+)
+
+HiddenQueryParam: bool = Query(
+    default=False, title="Display hidden", description="Whether to restrict result to hidden workflows."
+)
+
+MissingToolsQueryParam: bool = Query(
+    default=False,
+    title="Display missing tools",
+    description="Whether to include a list of missing tools per workflow entry",
+)
+
+ShowPublishedQueryParam: Optional[bool] = Query(default=None, title="Include published workflows.", description="")
+
+ShowSharedQueryParam: Optional[bool] = Query(
+    default=None, title="Include workflows shared with authenticated user.", description=""
+)
+
+SortByQueryParam: Optional[WorkflowSortByEnum] = Query(
+    default=None,
+    title="Sort workflow index by this attribute",
+    description="In unspecified, default ordering depends on other parameters but generally the user's own workflows appear first based on update time",
+)
+
+SortDescQueryParam: Optional[bool] = Query(
+    default=None,
+    title="Sort Descending",
+    description="Sort in descending order?",
+)
+
+LimitQueryParam: Optional[int] = Query(default=None, title="Limit number of queries.")
+
+OffsetQueryParam: Optional[int] = Query(
+    default=0,
+    title="Number of workflows to skip in sorted query (to enable pagination).",
+)
+
+InstanceQueryParam: Optional[bool] = Query(
+    default=False, title="True when fetching by Workflow ID, False when fetching by StoredWorkflow ID."
+)
+
+query_tags = [
+    IndexQueryTag("name", "The stored workflow's name.", "n"),
+    IndexQueryTag(
+        "tag",
+        "The workflow's tag, if the tag contains a colon an approach will be made to match the key and value of the tag separately.",
+        "t",
+    ),
+    IndexQueryTag("user", "The stored workflow's owner's username.", "u"),
+    IndexQueryTag(
+        "is:published",
+        "Include only published workflows in the final result. Be sure the the query parameter `show_published` is set to `true` if to include all published workflows and not just the requesting user's.",
+    ),
+    IndexQueryTag(
+        "is:share_with_me",
+        "Include only workflows shared with the requesting user.  Be sure the the query parameter `show_shared` is set to `true` if to include shared workflows.",
+    ),
+]
+
+SearchQueryParam: Optional[str] = search_query_param(
+    model_name="Stored Workflow",
+    tags=query_tags,
+    free_text_fields=["name", "tag", "user"],
+)
+
+SkipStepCountsQueryParam: bool = Query(
+    default=False,
+    title="Skip step counts.",
+    description="Set this to true to skip joining workflow step counts and optimize the resulting index query. Response objects will not contain step counts.",
+)
+
+
+@router.cbv
+class FastAPIWorkflows:
+    service: WorkflowsService = depends(WorkflowsService)
+
+    @router.get(
+        "/api/workflows",
+        summary="Lists stored workflows viewable by the user.",
+        response_description="A list with summary stored workflow information per viewable entry.",
+    )
+    def index(
+        self,
+        response: Response,
+        trans: ProvidesUserContext = DependsOnTrans,
+        show_deleted: bool = DeletedQueryParam,
+        show_hidden: bool = HiddenQueryParam,
+        missing_tools: bool = MissingToolsQueryParam,
+        show_published: Optional[bool] = ShowPublishedQueryParam,
+        show_shared: Optional[bool] = ShowSharedQueryParam,
+        sort_by: Optional[WorkflowSortByEnum] = SortByQueryParam,
+        sort_desc: Optional[bool] = SortDescQueryParam,
+        limit: Optional[int] = LimitQueryParam,
+        offset: Optional[int] = OffsetQueryParam,
+        search: Optional[str] = SearchQueryParam,
+        skip_step_counts: bool = SkipStepCountsQueryParam,
+    ) -> List[Dict[str, Any]]:
+        """Lists stored workflows viewable by the user."""
+        payload = WorkflowIndexPayload.construct(
+            show_published=show_published,
+            show_hidden=show_hidden,
+            show_deleted=show_deleted,
+            show_shared=show_shared,
+            missing_tools=missing_tools,
+            sort_by=sort_by,
+            sort_desc=sort_desc,
+            limit=limit,
+            offset=offset,
+            search=search,
+            skip_step_counts=skip_step_counts,
+        )
+        workflows, total_matches = self.service.index(trans, payload, include_total_count=True)
+        response.headers["total_matches"] = str(total_matches)
+        return workflows
+
+    @router.get(
+        "/api/workflows/{id}/sharing",
+        summary="Get the current sharing status of the given item.",
+    )
+    def sharing(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+    ) -> SharingStatus:
+        """Return the sharing status of the item."""
+        return self.service.shareable_service.sharing(trans, id)
+
+    @router.put(
+        "/api/workflows/{id}/enable_link_access",
+        summary="Makes this item accessible by a URL link.",
+    )
+    def enable_link_access(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+    ) -> SharingStatus:
+        """Makes this item accessible by a URL link and return the current sharing status."""
+        return self.service.shareable_service.enable_link_access(trans, id)
+
+    @router.put(
+        "/api/workflows/{id}/disable_link_access",
+        summary="Makes this item inaccessible by a URL link.",
+    )
+    def disable_link_access(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+    ) -> SharingStatus:
+        """Makes this item inaccessible by a URL link and return the current sharing status."""
+        return self.service.shareable_service.disable_link_access(trans, id)
+
+    @router.put(
+        "/api/workflows/{id}/publish",
+        summary="Makes this item public and accessible by a URL link.",
+    )
+    def publish(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+    ) -> SharingStatus:
+        """Makes this item publicly available by a URL link and return the current sharing status."""
+        return self.service.shareable_service.publish(trans, id)
+
+    @router.put(
+        "/api/workflows/{id}/unpublish",
+        summary="Removes this item from the published list.",
+    )
+    def unpublish(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+    ) -> SharingStatus:
+        """Removes this item from the published list and return the current sharing status."""
+        return self.service.shareable_service.unpublish(trans, id)
+
+    @router.put(
+        "/api/workflows/{id}/share_with_users",
+        summary="Share this item with specific users.",
+    )
+    def share_with_users(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+        payload: ShareWithPayload = Body(...),
+    ) -> ShareWithStatus:
+        """Shares this item with specific users and return the current sharing status."""
+        return self.service.shareable_service.share_with_users(trans, id, payload)
+
+    @router.put(
+        "/api/workflows/{id}/slug",
+        summary="Set a new slug for this shared item.",
+        status_code=status.HTTP_204_NO_CONTENT,
+    )
+    def set_slug(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+        payload: SetSlugPayload = Body(...),
+    ):
+        """Sets a new slug to access this item by URL. The new slug must be unique."""
+        self.service.shareable_service.set_slug(trans, id, payload)
+        return Response(status_code=status.HTTP_204_NO_CONTENT)
+
+    @router.delete(
+        "/api/workflows/{workflow_id}",
+        summary="Add the deleted flag to a workflow.",
+    )
+    def delete_workflow(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        workflow_id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+    ):
+        self.service.delete(trans, workflow_id)
+        return Response(status_code=status.HTTP_204_NO_CONTENT)
+
+    @router.post(
+        "/api/workflows/{workflow_id}/undelete",
+        summary="Remove the deleted flag from a workflow.",
+    )
+    def undelete_workflow(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        workflow_id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+    ):
+        self.service.undelete(trans, workflow_id)
+        return Response(status_code=status.HTTP_204_NO_CONTENT)
+
+    @router.get(
+        "/api/workflows/{workflow_id}/versions",
+        summary="List all versions of a workflow.",
+    )
+    def show_versions(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        workflow_id: DecodedDatabaseIdField = StoredWorkflowIDPathParam,
+        instance: Optional[bool] = InstanceQueryParam,
+    ):
+        return self.service.get_versions(trans, workflow_id, instance)
+
+    @router.get(
+        "/api/workflows/menu",
+        summary="Get workflows present in the tools panel.",
+    )
+    def get_workflow_menu(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        show_deleted: Optional[bool] = DeletedQueryParam,
+        show_hidden: Optional[bool] = HiddenQueryParam,
+        missing_tools: Optional[bool] = MissingToolsQueryParam,
+        show_published: Optional[bool] = ShowPublishedQueryParam,
+        show_shared: Optional[bool] = ShowSharedQueryParam,
+    ):
+        payload = WorkflowIndexPayload(
+            show_published=show_published,
+            show_hidden=show_hidden,
+            show_deleted=show_deleted,
+            show_shared=show_shared,
+            missing_tools=missing_tools,
+        )
+        return self.service.get_workflow_menu(
+            trans,
+            payload=payload,
+        )
+
+
+@router.cbv
+class FastAPIInvocations:
+    invocations_service: InvocationsService = depends(InvocationsService)
+
+    @router.post(
+        "/api/invocations/{invocation_id}/prepare_store_download",
+        summary="Prepare a workflow invocation export-style download.",
+    )
+    def prepare_store_download(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        invocation_id: DecodedDatabaseIdField = InvocationIDPathParam,
+        payload: PrepareStoreDownloadPayload = Body(...),
+    ) -> AsyncFile:
+        return self.invocations_service.prepare_store_download(
+            trans,
+            invocation_id,
+            payload,
+        )
+
+    @router.post(
+        "/api/invocations/{invocation_id}/write_store",
+        summary="Prepare a workflow invocation export-style download and write to supplied URI.",
+    )
+    def write_store(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        invocation_id: DecodedDatabaseIdField = InvocationIDPathParam,
+        payload: WriteInvocationStoreToPayload = Body(...),
+    ) -> AsyncTaskResultSummary:
+        rval = self.invocations_service.write_store(
+            trans,
+            invocation_id,
+            payload,
+        )
+        return rval
+
+    # TODO: remove this endpoint after 23.1 release
+    @router.get(
+        "/api/invocations/{invocation_id}/biocompute",
+        summary="Return a BioCompute Object for the workflow invocation.",
+        deprecated=True,
+    )
+    def export_invocation_bco(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        invocation_id: DecodedDatabaseIdField = InvocationIDPathParam,
+        merge_history_metadata: Optional[bool] = Query(default=False),
+    ):
+        """
+        The BioCompute Object endpoints are in beta - important details such
+        as how inputs and outputs are represented, how the workflow is encoded,
+        and how author and version information is encoded, and how URLs are
+        generated will very likely change in important ways over time.
+
+        **Deprecation Notice**: please use the asynchronous short_term_storage export system instead.
+
+        1. call POST `api/invocations/{id}/prepare_store_download` with payload:
+            ```
+            {
+                model_store_format: bco.json
+            }
+            ```
+        2. Get `storageRequestId` from response and poll GET `api/short_term_storage/${storageRequestId}/ready` until `SUCCESS`
+
+        3. Get the resulting file with `api/short_term_storage/${storageRequestId}`
+        """
+        bco = self._deprecated_generate_bco(trans, invocation_id, merge_history_metadata)
+        return json.loads(bco)
+
+    # TODO: remove this endpoint after 23.1 release
+    @router.get(
+        "/api/invocations/{invocation_id}/biocompute/download",
+        summary="Return a BioCompute Object for the workflow invocation as a file for download.",
+        response_class=StreamingResponse,
+        deprecated=True,
+    )
+    def download_invocation_bco(
+        self,
+        trans: ProvidesUserContext = DependsOnTrans,
+        invocation_id: DecodedDatabaseIdField = InvocationIDPathParam,
+        merge_history_metadata: Optional[bool] = Query(default=False),
+    ):
+        """
+        The BioCompute Object endpoints are in beta - important details such
+        as how inputs and outputs are represented, how the workflow is encoded,
+        and how author and version information is encoded, and how URLs are
+        generated will very likely change in important ways over time.
+
+        **Deprecation Notice**: please use the asynchronous short_term_storage export system instead.
+
+        1. call POST `api/invocations/{id}/prepare_store_download` with payload:
+            ```
+            {
+                model_store_format: bco.json
+            }
+            ```
+        2. Get `storageRequestId` from response and poll GET `api/short_term_storage/${storageRequestId}/ready` until `SUCCESS`
+
+        3. Get the resulting file with `api/short_term_storage/${storageRequestId}`
+        """
+        bco = self._deprecated_generate_bco(trans, invocation_id, merge_history_metadata)
+        return StreamingResponse(
+            content=BytesIO(bco),
+            media_type="application/json",
+            headers={
+                "Content-Disposition": f'attachment; filename="bco_{trans.security.encode_id(invocation_id)}.json"',
+                "Access-Control-Expose-Headers": "Content-Disposition",
+            },
+        )
+
+    # TODO: remove this after 23.1 release
+    def _deprecated_generate_bco(
+        self, trans, invocation_id: DecodedDatabaseIdField, merge_history_metadata: Optional[bool]
+    ):
+        export_options = BcoExportOptions(
+            galaxy_url=trans.request.url_path,
+            galaxy_version=VERSION,
+            merge_history_metadata=merge_history_metadata or False,
+        )
+        return self.invocations_service.deprecated_generate_invocation_bco(trans, invocation_id, export_options)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/admin.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/admin.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,54 +1,55 @@
-import imp
 import logging
-import os
-from collections import OrderedDict
-from datetime import datetime, timedelta
+from typing import Set
 
-import six
-from sqlalchemy import and_, false, or_
+from sqlalchemy import (
+    false,
+    func,
+)
+from typing_extensions import TypedDict
 
 from galaxy import (
     model,
     util,
-    web
+    web,
+)
+from galaxy.exceptions import (
+    ActionInputError,
+    MessageException,
 )
-from galaxy.actions.admin import AdminActions
-from galaxy.exceptions import ActionInputError, MessageException
+from galaxy.managers.quotas import QuotaManager
 from galaxy.model import tool_shed_install as install_model
 from galaxy.security.validate_user_input import validate_password
-from galaxy.tool_shed.util.repository_util import get_ids_of_tool_shed_repositories_being_installed
+from galaxy.structured_app import StructuredApp
 from galaxy.util import (
     nice_size,
+    pretty_print_time_interval,
     sanitize_text,
-    url_get
 )
-from galaxy.util.tool_shed import common_util, encoding_util
 from galaxy.web import url_for
-from galaxy.web.framework.helpers import grids, time_ago
-from galaxy.web.params import QuotaParamParser
+from galaxy.web.framework.helpers import (
+    grids,
+    time_ago,
+)
 from galaxy.webapps.base import controller
-from galaxy.webapps.base.controller import UsesQuotaMixin
 from tool_shed.util.web_util import escape
 
-
 log = logging.getLogger(__name__)
 
 
 class UserListGrid(grids.Grid):
-
     class EmailColumn(grids.TextColumn):
         def get_value(self, trans, grid, user):
             return escape(user.email)
 
     class UserNameColumn(grids.TextColumn):
         def get_value(self, trans, grid, user):
             if user.username:
                 return escape(user.username)
-            return 'not set'
+            return "not set"
 
     class StatusColumn(grids.GridColumn):
         def get_value(self, trans, grid, user):
             if user.purged:
                 return "purged"
             elif user.deleted:
                 return "deleted"
@@ -65,127 +66,153 @@
             if user.roles:
                 return len(user.roles)
             return 0
 
     class ExternalColumn(grids.GridColumn):
         def get_value(self, trans, grid, user):
             if user.external:
-                return 'yes'
-            return 'no'
+                return "yes"
+            return "no"
 
     class LastLoginColumn(grids.GridColumn):
         def get_value(self, trans, grid, user):
             if user.galaxy_sessions:
                 return self.format(user.galaxy_sessions[0].update_time)
-            return 'never'
+            return "never"
+
+        def sort(self, trans, query, ascending, column_name=None):
+            last_login_subquery = (
+                trans.sa_session.query(
+                    model.GalaxySession.table.c.user_id,
+                    func.max(model.GalaxySession.table.c.update_time).label("last_login"),
+                )
+                .group_by(model.GalaxySession.table.c.user_id)
+                .subquery()
+            )
+            query = query.outerjoin((last_login_subquery, model.User.table.c.id == last_login_subquery.c.user_id))
+
+            if not ascending:
+                query = query.order_by((last_login_subquery.c.last_login).desc().nullslast())
+            else:
+                query = query.order_by((last_login_subquery.c.last_login).asc().nullsfirst())
+            return query
 
     class TimeCreatedColumn(grids.GridColumn):
         def get_value(self, trans, grid, user):
-            return user.create_time.strftime('%x')
+            return user.create_time.strftime("%x")
 
     class ActivatedColumn(grids.GridColumn):
         def get_value(self, trans, grid, user):
             if user.active:
-                return 'Y'
+                return "Y"
             else:
-                return 'N'
+                return "N"
 
-    class APIKeyColumn(grids.GridColumn):
+    class DiskUsageColumn(grids.GridColumn):
         def get_value(self, trans, grid, user):
-            if user.api_keys:
-                return user.api_keys[0].key
+            return user.get_disk_usage(nice_size=True)
+
+        def sort(self, trans, query, ascending, column_name=None):
+            if column_name is None:
+                column_name = self.key
+            column = self.model_class.table.c.get(column_name)
+            if column is None:
+                column = getattr(self.model_class, column_name)
+            if ascending:
+                query = query.order_by(func.coalesce(column, 0).asc())
             else:
-                return ""
+                query = query.order_by(func.coalesce(column, 0).desc())
+            return query
 
     # Grid definition
     title = "Users"
     title_id = "users-grid"
     model_class = model.User
     default_sort_key = "email"
     columns = [
-        EmailColumn("Email",
-                    key="email",
-                    model_class=model.User,
-                    link=(lambda item: dict(controller="user", action="information", id=item.id, webapp="galaxy")),
-                    attach_popup=True,
-                    filterable="advanced",
-                    target="top"),
-        UserNameColumn("User Name",
-                       key="username",
-                       model_class=model.User,
-                       attach_popup=False,
-                       filterable="advanced"),
+        EmailColumn(
+            "Email",
+            key="email",
+            link=(lambda item: dict(controller="user", action="information", id=item.id, webapp="galaxy")),
+            attach_popup=True,
+            filterable="advanced",
+            target="top",
+        ),
+        UserNameColumn("User Name", key="username", attach_popup=False, filterable="advanced"),
+        LastLoginColumn("Last Login", format=time_ago, key="last_login", sortable=True),
+        DiskUsageColumn("Disk Usage", key="disk_usage", attach_popup=False),
+        StatusColumn("Status", attach_popup=False, key="deleted"),
+        TimeCreatedColumn("Created", attach_popup=False, key="create_time"),
+        ActivatedColumn("Activated", attach_popup=False, key="active"),
         GroupsColumn("Groups", attach_popup=False),
         RolesColumn("Roles", attach_popup=False),
-        ExternalColumn("External", attach_popup=False),
-        LastLoginColumn("Last Login", format=time_ago),
-        StatusColumn("Status", attach_popup=False),
-        TimeCreatedColumn("Created", attach_popup=False),
-        ActivatedColumn("Activated", attach_popup=False),
-        APIKeyColumn("API Key", attach_popup=False),
+        ExternalColumn("External", attach_popup=False, key="external"),
         # Columns that are valid for filtering but are not visible.
         grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
-        grids.PurgedColumn("Purged", key="purged", visible=False, filterable="advanced")
-    ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0], columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    global_actions = [
-        grids.GridAction("Create new user", url_args=dict(action="users/create"))
+        grids.PurgedColumn("Purged", key="purged", visible=False, filterable="advanced"),
     ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
+    global_actions = [grids.GridAction("Create new user", url_args=dict(action="users/create"))]
     operations = [
-        grids.GridOperation("Manage Information",
-                            condition=(lambda item: not item.deleted),
-                            allow_multiple=False,
-                            url_args=dict(controller="user", action="information", webapp="galaxy")),
-        grids.GridOperation("Manage Roles and Groups",
-                            condition=(lambda item: not item.deleted),
-                            allow_multiple=False,
-                            url_args=dict(action="form/manage_roles_and_groups_for_user")),
-        grids.GridOperation("Reset Password",
-                            condition=(lambda item: not item.deleted),
-                            allow_multiple=True,
-                            url_args=dict(action="form/reset_user_password"),
-                            target="top"),
-        grids.GridOperation("Recalculate Disk Usage",
-                            condition=(lambda item: not item.deleted),
-                            allow_multiple=False),
-        grids.GridOperation("Generate New API Key",
-                            allow_multiple=False,
-                            async_compatible=True)
+        grids.GridOperation(
+            "Manage Information",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(controller="user", action="information", webapp="galaxy"),
+        ),
+        grids.GridOperation(
+            "Manage Roles and Groups",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/manage_roles_and_groups_for_user"),
+        ),
+        grids.GridOperation(
+            "Reset Password",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=True,
+            url_args=dict(action="form/reset_user_password"),
+            target="top",
+        ),
+        grids.GridOperation("Recalculate Disk Usage", condition=(lambda item: not item.deleted), allow_multiple=False),
+        grids.GridOperation("Generate New API Key", allow_multiple=False, async_compatible=True),
     ]
 
     standard_filters = [
         grids.GridColumnFilter("Active", args=dict(deleted=False)),
         grids.GridColumnFilter("Deleted", args=dict(deleted=True, purged=False)),
         grids.GridColumnFilter("Purged", args=dict(purged=True)),
-        grids.GridColumnFilter("All", args=dict(deleted='All'))
+        grids.GridColumnFilter("All", args=dict(deleted="All")),
     ]
     num_rows_per_page = 50
     use_paging = True
     default_filter = dict(purged="False")
     use_default_filter = True
 
     def get_current_item(self, trans, **kwargs):
         return trans.user
 
 
 class RoleListGrid(grids.Grid):
-
     class NameColumn(grids.TextColumn):
         def get_value(self, trans, grid, role):
             return escape(role.name)
 
     class DescriptionColumn(grids.TextColumn):
         def get_value(self, trans, grid, role):
             if role.description:
                 return escape(role.description)
-            return ''
+            return ""
 
     class TypeColumn(grids.TextColumn):
         def get_value(self, trans, grid, role):
             return role.type
 
     class StatusColumn(grids.GridColumn):
         def get_value(self, trans, grid, role):
@@ -207,77 +234,74 @@
 
     # Grid definition
     title = "Roles"
     title_id = "roles-grid"
     model_class = model.Role
     default_sort_key = "name"
     columns = [
-        NameColumn("Name",
-                   key="name",
-                   link=(lambda item: dict(action="form/manage_users_and_groups_for_role", id=item.id, webapp="galaxy")),
-                   model_class=model.Role,
-                   attach_popup=True,
-                   filterable="advanced",
-                   target="top"),
-        DescriptionColumn("Description",
-                          key='description',
-                          model_class=model.Role,
-                          attach_popup=False,
-                          filterable="advanced"),
-        TypeColumn("Type",
-                   key='type',
-                   model_class=model.Role,
-                   attach_popup=False,
-                   filterable="advanced"),
+        NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(action="form/manage_users_and_groups_for_role", id=item.id, webapp="galaxy")),
+            model_class=model.Role,
+            attach_popup=True,
+            filterable="advanced",
+            target="top",
+        ),
+        DescriptionColumn(
+            "Description", key="description", model_class=model.Role, attach_popup=False, filterable="advanced"
+        ),
+        TypeColumn("Type", key="type", model_class=model.Role, attach_popup=False, filterable="advanced"),
         GroupsColumn("Groups", attach_popup=False),
         UsersColumn("Users", attach_popup=False),
         StatusColumn("Status", attach_popup=False),
         # Columns that are valid for filtering but are not visible.
         grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
-        grids.GridColumn("Last Updated", key="update_time", format=time_ago)
+        grids.GridColumn("Last Updated", key="update_time"),
     ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0], columns[1], columns[2]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    global_actions = [
-        grids.GridAction("Add new role", url_args=dict(action="form/create_role"))
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search",
+            cols_to_filter=[columns[0], columns[1], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
+    global_actions = [grids.GridAction("Add new role", url_args=dict(action="form/create_role"))]
+    operations = [
+        grids.GridOperation(
+            "Edit Name/Description",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/rename_role"),
+        ),
+        grids.GridOperation(
+            "Edit Permissions",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/manage_users_and_groups_for_role", webapp="galaxy"),
+        ),
+        grids.GridOperation("Delete", condition=(lambda item: not item.deleted), allow_multiple=True),
+        grids.GridOperation("Undelete", condition=(lambda item: item.deleted), allow_multiple=True),
+        grids.GridOperation("Purge", condition=(lambda item: item.deleted), allow_multiple=True),
     ]
-    operations = [grids.GridOperation("Edit Name/Description",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/rename_role")),
-                  grids.GridOperation("Edit Permissions",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/manage_users_and_groups_for_role", webapp="galaxy")),
-                  grids.GridOperation("Delete",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=True),
-                  grids.GridOperation("Undelete",
-                                      condition=(lambda item: item.deleted),
-                                      allow_multiple=True),
-                  grids.GridOperation("Purge",
-                                      condition=(lambda item: item.deleted),
-                                      allow_multiple=True)]
     standard_filters = [
         grids.GridColumnFilter("Active", args=dict(deleted=False)),
         grids.GridColumnFilter("Deleted", args=dict(deleted=True)),
-        grids.GridColumnFilter("All", args=dict(deleted='All'))
+        grids.GridColumnFilter("All", args=dict(deleted="All")),
     ]
     num_rows_per_page = 50
     use_paging = True
 
     def apply_query_filter(self, trans, query, **kwargs):
         return query.filter(model.Role.type != model.Role.types.PRIVATE)
 
 
 class GroupListGrid(grids.Grid):
-
     class NameColumn(grids.TextColumn):
         def get_value(self, trans, grid, group):
             return escape(group.name)
 
     class StatusColumn(grids.GridColumn):
         def get_value(self, trans, grid, group):
             if group.deleted:
@@ -288,93 +312,92 @@
         def get_value(self, trans, grid, group):
             if group.roles:
                 return len(group.roles)
             return 0
 
     class UsersColumn(grids.GridColumn):
         def get_value(self, trans, grid, group):
-            if group.members:
-                return len(group.members)
+            if group.users:
+                return len(group.users)
             return 0
 
     # Grid definition
     title = "Groups"
     title_id = "groups-grid"
     model_class = model.Group
     default_sort_key = "name"
     columns = [
-        NameColumn("Name",
-                   key="name",
-                   link=(lambda item: dict(action="form/manage_users_and_roles_for_group", id=item.id, webapp="galaxy")),
-                   model_class=model.Group,
-                   attach_popup=True,
-                   filterable="advanced"),
+        NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(action="form/manage_users_and_roles_for_group", id=item.id, webapp="galaxy")),
+            model_class=model.Group,
+            attach_popup=True,
+            filterable="advanced",
+        ),
         UsersColumn("Users", attach_popup=False),
         RolesColumn("Roles", attach_popup=False),
         StatusColumn("Status", attach_popup=False),
         # Columns that are valid for filtering but are not visible.
         grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
-        grids.GridColumn("Last Updated", key="update_time", format=time_ago)
+        grids.GridColumn("Last Updated", key="update_time", format=pretty_print_time_interval),
     ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    global_actions = [
-        grids.GridAction("Add new group", url_args=dict(action="form/create_group"))
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search", cols_to_filter=[columns[0]], key="free-text-search", visible=False, filterable="standard"
+        )
+    )
+    global_actions = [grids.GridAction("Add new group", url_args=dict(action="form/create_group"))]
+    operations = [
+        grids.GridOperation(
+            "Edit Name",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/rename_group"),
+        ),
+        grids.GridOperation(
+            "Edit Permissions",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/manage_users_and_roles_for_group", webapp="galaxy"),
+        ),
+        grids.GridOperation("Delete", condition=(lambda item: not item.deleted), allow_multiple=True),
+        grids.GridOperation("Undelete", condition=(lambda item: item.deleted), allow_multiple=True),
+        grids.GridOperation("Purge", condition=(lambda item: item.deleted), allow_multiple=True),
     ]
-    operations = [grids.GridOperation("Edit Name",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/rename_group")),
-                  grids.GridOperation("Edit Permissions",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/manage_users_and_roles_for_group", webapp="galaxy")),
-                  grids.GridOperation("Delete",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=True),
-                  grids.GridOperation("Undelete",
-                                      condition=(lambda item: item.deleted),
-                                      allow_multiple=True),
-                  grids.GridOperation("Purge",
-                                      condition=(lambda item: item.deleted),
-                                      allow_multiple=True)]
     standard_filters = [
         grids.GridColumnFilter("Active", args=dict(deleted=False)),
         grids.GridColumnFilter("Deleted", args=dict(deleted=True)),
-        grids.GridColumnFilter("All", args=dict(deleted='All'))
+        grids.GridColumnFilter("All", args=dict(deleted="All")),
     ]
     num_rows_per_page = 50
     use_paging = True
 
 
 class QuotaListGrid(grids.Grid):
-
     class NameColumn(grids.TextColumn):
         def get_value(self, trans, grid, quota):
             return escape(quota.name)
 
     class DescriptionColumn(grids.TextColumn):
         def get_value(self, trans, grid, quota):
             if quota.description:
                 return escape(quota.description)
-            return ''
+            return ""
 
     class AmountColumn(grids.TextColumn):
         def get_value(self, trans, grid, quota):
             return quota.operation + quota.display_amount
 
     class StatusColumn(grids.GridColumn):
         def get_value(self, trans, grid, quota):
             if quota.deleted:
                 return "deleted"
             elif quota.default:
-                return "<strong>default for %s users</strong>" % quota.default[0].type
+                return f"<strong>default for {quota.default[0].type} users</strong>"
             return ""
 
     class UsersColumn(grids.GridColumn):
         def get_value(self, trans, grid, quota):
             if quota.users:
                 return len(quota.users)
             return 0
@@ -386,247 +409,245 @@
             return 0
 
     # Grid definition
     title = "Quotas"
     model_class = model.Quota
     default_sort_key = "name"
     columns = [
-        NameColumn("Name",
-                   key="name",
-                   link=(lambda item: dict(action="form/edit_quota", id=item.id)),
-                   model_class=model.Quota,
-                   attach_popup=True,
-                   filterable="advanced"),
-        DescriptionColumn("Description",
-                          key='description',
-                          model_class=model.Quota,
-                          attach_popup=False,
-                          filterable="advanced"),
-        AmountColumn("Amount",
-                     key='amount',
-                     model_class=model.Quota,
-                     attach_popup=False),
+        NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(action="form/edit_quota", id=item.id)),
+            model_class=model.Quota,
+            attach_popup=True,
+            filterable="advanced",
+        ),
+        DescriptionColumn(
+            "Description", key="description", model_class=model.Quota, attach_popup=False, filterable="advanced"
+        ),
+        AmountColumn("Amount", key="amount", model_class=model.Quota, attach_popup=False),
         UsersColumn("Users", attach_popup=False),
         GroupsColumn("Groups", attach_popup=False),
         StatusColumn("Status", attach_popup=False),
         # Columns that are valid for filtering but are not visible.
-        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced")
+        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
     ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0], columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    global_actions = [
-        grids.GridAction("Add new quota", dict(action="form/create_quota"))
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
+    global_actions = [grids.GridAction("Add new quota", dict(action="form/create_quota"))]
+    operations = [
+        grids.GridOperation(
+            "Rename",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/rename_quota"),
+        ),
+        grids.GridOperation(
+            "Change amount",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/edit_quota"),
+        ),
+        grids.GridOperation(
+            "Manage users and groups",
+            condition=(lambda item: not item.default and not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/manage_users_and_groups_for_quota"),
+        ),
+        grids.GridOperation(
+            "Set as different type of default",
+            condition=(lambda item: item.default),
+            allow_multiple=False,
+            url_args=dict(action="form/set_quota_default"),
+        ),
+        grids.GridOperation(
+            "Set as default",
+            condition=(lambda item: not item.default and not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="form/set_quota_default"),
+        ),
+        grids.GridOperation(
+            "Unset as default", condition=(lambda item: item.default and not item.deleted), allow_multiple=False
+        ),
+        grids.GridOperation(
+            "Delete", condition=(lambda item: not item.deleted and not item.default), allow_multiple=True
+        ),
+        grids.GridOperation("Undelete", condition=(lambda item: item.deleted), allow_multiple=True),
+        grids.GridOperation("Purge", condition=(lambda item: item.deleted), allow_multiple=True),
     ]
-    operations = [grids.GridOperation("Rename",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/rename_quota")),
-                  grids.GridOperation("Change amount",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/edit_quota")),
-                  grids.GridOperation("Manage users and groups",
-                                      condition=(lambda item: not item.default and not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/manage_users_and_groups_for_quota")),
-                  grids.GridOperation("Set as different type of default",
-                                      condition=(lambda item: item.default),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/set_quota_default")),
-                  grids.GridOperation("Set as default",
-                                      condition=(lambda item: not item.default and not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="form/set_quota_default")),
-                  grids.GridOperation("Unset as default",
-                                      condition=(lambda item: item.default and not item.deleted),
-                                      allow_multiple=False),
-                  grids.GridOperation("Delete",
-                                      condition=(lambda item: not item.deleted and not item.default),
-                                      allow_multiple=True),
-                  grids.GridOperation("Undelete",
-                                      condition=(lambda item: item.deleted),
-                                      allow_multiple=True),
-                  grids.GridOperation("Purge",
-                                      condition=(lambda item: item.deleted),
-                                      allow_multiple=True)]
     standard_filters = [
         grids.GridColumnFilter("Active", args=dict(deleted=False)),
         grids.GridColumnFilter("Deleted", args=dict(deleted=True)),
         grids.GridColumnFilter("Purged", args=dict(purged=True)),
-        grids.GridColumnFilter("All", args=dict(deleted='All'))
+        grids.GridColumnFilter("All", args=dict(deleted="All")),
     ]
     num_rows_per_page = 50
     use_paging = True
 
 
 class ToolVersionListGrid(grids.Grid):
-
     class ToolIdColumn(grids.TextColumn):
         def get_value(self, trans, grid, tool_version):
             toolbox = trans.app.toolbox
             if toolbox.has_tool(tool_version.tool_id, exact=True):
-                link = url_for(controller='tool_runner', tool_id=tool_version.tool_id)
-                link_str = '<a target="_blank" href="%s">' % link
-                return '<div class="count-box state-color-ok">%s%s</a></div>' % (link_str, tool_version.tool_id)
+                link = url_for(controller="tool_runner", tool_id=tool_version.tool_id)
+                link_str = f'<a target="_blank" href="{link}">'
+                return f'<div class="count-box state-color-ok">{link_str}{tool_version.tool_id}</a></div>'
             return tool_version.tool_id
 
     class ToolVersionsColumn(grids.TextColumn):
         def get_value(self, trans, grid, tool_version):
-            tool_ids_str = ''
+            tool_ids_str = ""
             toolbox = trans.app.toolbox
             tool = toolbox._tools_by_id.get(tool_version.tool_id)
             if tool:
                 for tool_id in tool.lineage.tool_ids:
                     if toolbox.has_tool(tool_id, exact=True):
-                        link = url_for(controller='tool_runner', tool_id=tool_id)
-                        link_str = '<a target="_blank" href="%s">' % link
-                        tool_ids_str += '<div class="count-box state-color-ok">%s%s</a></div><br/>' % (link_str, tool_id)
+                        link = url_for(controller="tool_runner", tool_id=tool_id)
+                        link_str = f'<a target="_blank" href="{link}">'
+                        tool_ids_str += f'<div class="count-box state-color-ok">{link_str}{tool_id}</a></div><br/>'
                     else:
-                        tool_ids_str += '%s<br/>' % tool_version.tool_id
+                        tool_ids_str += f"{tool_version.tool_id}<br/>"
             else:
-                tool_ids_str += '%s<br/>' % tool_version.tool_id
+                tool_ids_str += f"{tool_version.tool_id}<br/>"
             return tool_ids_str
 
     # Grid definition
     title = "Tool versions"
     model_class = install_model.ToolVersion
     default_sort_key = "tool_id"
     columns = [
-        ToolIdColumn("Tool id",
-                     key='tool_id',
-                     attach_popup=False),
-        ToolVersionsColumn("Version lineage by tool id (parent/child ordered)")
+        ToolIdColumn("Tool id", key="tool_id", attach_popup=False),
+        ToolVersionsColumn("Version lineage by tool id (parent/child ordered)"),
     ]
-    columns.append(grids.MulticolFilterColumn("Search tool id",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    global_actions = []
-    operations = []
-    standard_filters = []
-    default_filter = {}
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search tool id", cols_to_filter=[columns[0]], key="free-text-search", visible=False, filterable="standard"
+        )
+    )
     num_rows_per_page = 50
     use_paging = True
 
     def build_initial_query(self, trans, **kwd):
         return trans.install_model.context.query(self.model_class)
 
 
-class AdminGalaxy(controller.JSAppLauncher, AdminActions, UsesQuotaMixin, QuotaParamParser):
+# TODO: Convert admin UI to use the API and drop this.
+class DatatypesEntryT(TypedDict):
+    status: str
+    keys: list
+    data: list
+    message: str
+
 
+class AdminGalaxy(controller.JSAppLauncher):
     user_list_grid = UserListGrid()
     role_list_grid = RoleListGrid()
     group_list_grid = GroupListGrid()
     quota_list_grid = QuotaListGrid()
     tool_version_list_grid = ToolVersionListGrid()
-    delete_operation = grids.GridOperation("Delete", condition=(lambda item: not item.deleted and not item.purged), allow_multiple=True)
-    undelete_operation = grids.GridOperation("Undelete", condition=(lambda item: item.deleted and not item.purged), allow_multiple=True)
-    purge_operation = grids.GridOperation("Purge", condition=(lambda item: item.deleted and not item.purged), allow_multiple=True)
+    delete_operation = grids.GridOperation(
+        "Delete", condition=(lambda item: not item.deleted and not item.purged), allow_multiple=True
+    )
+    undelete_operation = grids.GridOperation(
+        "Undelete", condition=(lambda item: item.deleted and not item.purged), allow_multiple=True
+    )
+    purge_operation = grids.GridOperation(
+        "Purge", condition=(lambda item: item.deleted and not item.purged), allow_multiple=True
+    )
     impersonate_operation = grids.GridOperation(
         "Impersonate",
         url_args=dict(controller="admin", action="impersonate"),
         condition=(lambda item: not item.deleted and not item.purged),
-        allow_multiple=False
+        allow_multiple=False,
+    )
+    activate_operation = grids.GridOperation(
+        "Activate User", condition=(lambda item: not item.active), allow_multiple=False
+    )
+    resend_activation_email = grids.GridOperation(
+        "Resend Activation Email", condition=(lambda item: not item.active), allow_multiple=False
     )
-    activate_operation = grids.GridOperation("Activate User", condition=(lambda item: not item.active), allow_multiple=False)
-    resend_activation_email = grids.GridOperation("Resend Activation Email", condition=(lambda item: not item.active), allow_multiple=False)
-
-    @web.expose
-    @web.require_admin
-    def index(self, trans, **kwd):
-        return self.client(trans, **kwd)
 
-    @web.expose
-    @web.require_admin
-    def client(self, trans, **kwd):
-        """
-        Endpoint for admin clientside routes.
-        """
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        settings = {
-            'is_repo_installed': trans.install_model.context.query(trans.install_model.ToolShedRepository).first() is not None,
-            'installing_repository_ids': get_ids_of_tool_shed_repositories_being_installed(trans.app, as_string=True),
-            'is_tool_shed_installed': bool(trans.app.tool_shed_registry and trans.app.tool_shed_registry.tool_sheds)
-        }
-        return self._bootstrapped_client(trans, app_name='admin', settings=settings, message=message, status=status)
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
+        self.quota_manager: QuotaManager = QuotaManager(app)
 
     @web.expose
     @web.json
     @web.require_admin
     def data_tables_list(self, trans, **kwd):
         data = []
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'done')
-        sorted_data_tables = sorted(
-            trans.app.tool_data_tables.get_tables().items()
-        )
+        message = kwd.get("message", "")
+        status = kwd.get("status", "done")
+        sorted_data_tables = sorted(trans.app.tool_data_tables.get_tables().items())
 
-        for data_table_elem_name, data_table in sorted_data_tables:
+        for _data_table_elem_name, data_table in sorted_data_tables:
             for filename, file_dict in data_table.filenames.items():
-                file_missing = ['file missing'] \
-                    if not file_dict.get('found') else []
-                data.append({
-                    'name': data_table.name,
-                    'filename': filename,
-                    'tool_data_path': file_dict.get('tool_data_path'),
-                    'errors': ', '.join(file_missing + [
-                        error for error in file_dict.get('errors', [])
-                    ]),
-                })
+                file_missing = ["file missing"] if not file_dict.get("found") else []
+                data.append(
+                    {
+                        "name": data_table.name,
+                        "filename": filename,
+                        "tool_data_path": file_dict.get("tool_data_path"),
+                        "errors": ", ".join(file_missing + [error for error in file_dict.get("errors", [])]),
+                    }
+                )
 
-        return {'data': data, 'message': message, 'status': status}
+        return {"data": data, "message": message, "status": status}
 
     @web.expose
     @web.json
     @web.require_admin
-    def data_types_list(self, trans, **kwd):
+    def data_types_list(self, trans, **kwd) -> DatatypesEntryT:
         datatypes = []
-        keys = set()
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'done')
-        for dtype in sorted(trans.app.datatypes_registry.datatype_elems,
-                           key=lambda dtype: dtype.get('extension')):
-            datatypes.append(dtype.attrib)
-            keys |= set(dtype.attrib)
-        return {'keys': list(keys), 'data': datatypes, 'message': message, 'status': status}
+        keys: Set[str] = set()
+        message = kwd.get("message", "")
+        status = kwd.get("status", "done")
+        for dtype in sorted(trans.app.datatypes_registry.datatype_elems, key=lambda dt: dt.get("extension")):
+            attrib = dict(dtype.attrib)
+            datatypes.append(attrib)
+            keys |= set(attrib.keys())
+        return {"keys": list(keys), "data": datatypes, "message": message, "status": status}
 
     @web.expose
     @web.json
     @web.require_admin
     def users_list(self, trans, **kwd):
-        message = kwd.get('message', '')
-        status = kwd.get('status', '')
-        if 'operation' in kwd:
-            id = kwd.get('id')
+        message = kwd.get("message", "")
+        status = kwd.get("status", "")
+        if "operation" in kwd:
+            id = kwd.get("id")
             if not id:
-                message, status = ('Invalid user id (%s) received.' % str(id), 'error')
+                message, status = (f"Invalid user id ({str(id)}) received.", "error")
             ids = util.listify(id)
-            operation = kwd['operation'].lower()
-            if operation == 'delete':
+            operation = kwd["operation"].lower()
+            if operation == "delete":
                 message, status = self._delete_user(trans, ids)
-            elif operation == 'undelete':
+            elif operation == "undelete":
                 message, status = self._undelete_user(trans, ids)
-            elif operation == 'purge':
+            elif operation == "purge":
                 message, status = self._purge_user(trans, ids)
-            elif operation == 'recalculate disk usage':
+            elif operation == "recalculate disk usage":
                 message, status = self._recalculate_user(trans, id)
-            elif operation == 'generate new api key':
+            elif operation == "generate new api key":
                 message, status = self._new_user_apikey(trans, id)
-            elif operation == 'activate user':
+            elif operation == "activate user":
                 message, status = self._activate_user(trans, id)
-            elif operation == 'resend activation email':
+            elif operation == "resend activation email":
                 message, status = self._resend_activation_email(trans, id)
         if message and status:
-            kwd['message'] = util.sanitize_text(message)
-            kwd['status'] = status
+            kwd["message"] = util.sanitize_text(message)
+            kwd["status"] = status
         if trans.app.config.allow_user_deletion:
             if self.delete_operation not in self.user_list_grid.operations:
                 self.user_list_grid.operations.append(self.delete_operation)
             if self.undelete_operation not in self.user_list_grid.operations:
                 self.user_list_grid.operations.append(self.undelete_operation)
             if self.purge_operation not in self.user_list_grid.operations:
                 self.user_list_grid.operations.append(self.purge_operation)
@@ -638,458 +659,446 @@
                 self.user_list_grid.operations.append(self.activate_operation)
                 self.user_list_grid.operations.append(self.resend_activation_email)
         return self.user_list_grid(trans, **kwd)
 
     @web.legacy_expose_api
     @web.require_admin
     def quotas_list(self, trans, payload=None, **kwargs):
-        message = kwargs.get('message', '')
-        status = kwargs.get('status', '')
-        if 'operation' in kwargs:
-            id = kwargs.get('id')
+        message = kwargs.get("message", "")
+        status = kwargs.get("status", "")
+        if "operation" in kwargs:
+            id = kwargs.get("id")
             if not id:
-                return self.message_exception(trans, 'Invalid quota id (%s) received.' % str(id))
+                return self.message_exception(trans, f"Invalid quota id ({str(id)}) received.")
             quotas = []
             for quota_id in util.listify(id):
                 try:
                     quotas.append(get_quota(trans, quota_id))
                 except MessageException as e:
                     return self.message_exception(trans, util.unicodify(e))
-            operation = kwargs.pop('operation').lower()
+            operation = kwargs.pop("operation").lower()
             try:
-                if operation == 'delete':
-                    message = self._delete_quota(quotas)
-                elif operation == 'undelete':
-                    message = self._undelete_quota(quotas)
-                elif operation == 'purge':
-                    message = self._purge_quota(quotas)
-                elif operation == 'unset as default':
-                    message = self._unset_quota_default(quotas[0])
+                if operation == "delete":
+                    message = self.quota_manager.delete_quota(quotas)
+                elif operation == "undelete":
+                    message = self.quota_manager.undelete_quota(quotas)
+                elif operation == "purge":
+                    message = self.quota_manager.purge_quota(quotas)
+                elif operation == "unset as default":
+                    message = self.quota_manager.unset_quota_default(quotas[0])
             except ActionInputError as e:
-                message, status = (e.err_msg, 'error')
+                message, status = (e.err_msg, "error")
         if message:
-            kwargs['message'] = util.sanitize_text(message)
-            kwargs['status'] = status or 'done'
+            kwargs["message"] = util.sanitize_text(message)
+            kwargs["status"] = status or "done"
         return self.quota_list_grid(trans, **kwargs)
 
     @web.legacy_expose_api
     @web.require_admin
     def create_quota(self, trans, payload=None, **kwd):
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             all_users = []
             all_groups = []
-            for user in trans.sa_session.query(trans.app.model.User) \
-                                        .filter(trans.app.model.User.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.User.table.c.email):
+            for user in (
+                trans.sa_session.query(trans.app.model.User)
+                .filter(trans.app.model.User.table.c.deleted == false())
+                .order_by(trans.app.model.User.table.c.email)
+            ):
                 all_users.append((user.email, trans.security.encode_id(user.id)))
-            for group in trans.sa_session.query(trans.app.model.Group) \
-                                         .filter(trans.app.model.Group.table.c.deleted == false()) \
-                                         .order_by(trans.app.model.Group.table.c.name):
+            for group in (
+                trans.sa_session.query(trans.app.model.Group)
+                .filter(trans.app.model.Group.deleted == false())
+                .order_by(trans.app.model.Group.name)
+            ):
                 all_groups.append((group.name, trans.security.encode_id(group.id)))
-            default_options = [('No', 'no')]
-            for typ in trans.app.model.DefaultQuotaAssociation.types.__dict__.values():
-                default_options.append(('Yes, ' + typ, typ))
-            return {'title'  : 'Create Quota',
-                    'inputs' : [
-                        {
-                            'name'    : 'name',
-                            'label'   : 'Name'
-                        }, {
-                            'name'    : 'description',
-                            'label'   : 'Description'
-                        }, {
-                            'name'    : 'amount',
-                            'label'   : 'Amount',
-                            'help'    : 'Examples: "10000MB", "99 gb", "0.2T", "unlimited"'
-                        }, {
-                            'name'    : 'operation',
-                            'label'   : 'Assign, increase by amount, or decrease by amount?',
-                            'options' : [('=', '='), ('+', '+'), ('-', '-')]
-                        }, {
-                            'name'    : 'default',
-                            'label'   : 'Is this quota a default for a class of users (if yes, what type)?',
-                            'options' : default_options,
-                            'help'    : 'Warning: Any users or groups associated with this quota will be disassociated.'
-                        },
-                        build_select_input('in_groups', 'Groups', all_groups, []),
-                        build_select_input('in_users', 'Users', all_users, [])]}
+            default_options = [("No", "no")]
+            for type_ in trans.app.model.DefaultQuotaAssociation.types:
+                default_options.append((f"Yes, {type_}", type_))
+            return {
+                "title": "Create Quota",
+                "inputs": [
+                    {"name": "name", "label": "Name"},
+                    {"name": "description", "label": "Description"},
+                    {"name": "amount", "label": "Amount", "help": 'Examples: "10000MB", "99 gb", "0.2T", "unlimited"'},
+                    {
+                        "name": "operation",
+                        "label": "Assign, increase by amount, or decrease by amount?",
+                        "options": [("=", "="), ("+", "+"), ("-", "-")],
+                    },
+                    {
+                        "name": "default",
+                        "label": "Is this quota a default for a class of users (if yes, what type)?",
+                        "options": default_options,
+                        "help": "Warning: Any users or groups associated with this quota will be disassociated.",
+                    },
+                    build_select_input("in_groups", "Groups", all_groups, []),
+                    build_select_input("in_users", "Users", all_users, []),
+                ],
+            }
         else:
             try:
-                quota, message = self._create_quota(util.Params(payload), decode_id=trans.security.decode_id)
-                return {'message': message}
+                quota, message = self.quota_manager.create_quota(payload, decode_id=trans.security.decode_id)
+                return {"message": message}
             except ActionInputError as e:
                 return self.message_exception(trans, e.err_msg)
 
     @web.legacy_expose_api
     @web.require_admin
     def rename_quota(self, trans, payload=None, **kwd):
-        id = kwd.get('id')
+        id = kwd.get("id")
         if not id:
-            return self.message_exception(trans, 'No quota id received for renaming.')
+            return self.message_exception(trans, "No quota id received for renaming.")
         quota = get_quota(trans, id)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             return {
-                'title'  : 'Change quota name and description for \'%s\'' % util.sanitize_text(quota.name),
-                'inputs' : [{
-                    'name'  : 'name',
-                    'label' : 'Name',
-                    'value' : quota.name
-                }, {
-                    'name'  : 'description',
-                    'label' : 'Description',
-                    'value' : quota.description
-                }]
+                "title": "Change quota name and description for '%s'" % util.sanitize_text(quota.name),
+                "inputs": [
+                    {"name": "name", "label": "Name", "value": quota.name},
+                    {"name": "description", "label": "Description", "value": quota.description},
+                ],
             }
         else:
             try:
-                return {'message': self._rename_quota(quota, util.Params(payload))}
+                return {"message": self.quota_manager.rename_quota(quota, util.Params(payload))}
             except ActionInputError as e:
                 return self.message_exception(trans, e.err_msg)
 
     @web.legacy_expose_api
     @web.require_admin
     def manage_users_and_groups_for_quota(self, trans, payload=None, **kwd):
-        quota_id = kwd.get('id')
+        quota_id = kwd.get("id")
         if not quota_id:
-            return self.message_exception(trans, 'Invalid quota id (%s) received' % str(quota_id))
+            return self.message_exception(trans, f"Invalid quota id ({str(quota_id)}) received")
         quota = get_quota(trans, quota_id)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             in_users = []
             all_users = []
             in_groups = []
             all_groups = []
-            for user in trans.sa_session.query(trans.app.model.User) \
-                                        .filter(trans.app.model.User.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.User.table.c.email):
+            for user in (
+                trans.sa_session.query(trans.app.model.User)
+                .filter(trans.app.model.User.table.c.deleted == false())
+                .order_by(trans.app.model.User.table.c.email)
+            ):
                 if user in [x.user for x in quota.users]:
                     in_users.append(trans.security.encode_id(user.id))
                 all_users.append((user.email, trans.security.encode_id(user.id)))
-            for group in trans.sa_session.query(trans.app.model.Group) \
-                                         .filter(trans.app.model.Group.table.c.deleted == false()) \
-                                         .order_by(trans.app.model.Group.table.c.name):
+            for group in (
+                trans.sa_session.query(trans.app.model.Group)
+                .filter(trans.app.model.Group.deleted == false())
+                .order_by(trans.app.model.Group.name)
+            ):
                 if group in [x.group for x in quota.groups]:
                     in_groups.append(trans.security.encode_id(group.id))
                 all_groups.append((group.name, trans.security.encode_id(group.id)))
-            return {'title'  : 'Quota \'%s\'' % quota.name,
-                    'message': 'Quota \'%s\' is currently associated with %d user(s) and %d group(s).' %
-                    (quota.name, len(in_users), len(in_groups)),
-                    'status' : 'info',
-                    'inputs' : [build_select_input('in_groups', 'Groups', all_groups, in_groups),
-                                build_select_input('in_users', 'Users', all_users, in_users)]}
+            return {
+                "title": "Quota '%s'" % quota.name,
+                "message": "Quota '%s' is currently associated with %d user(s) and %d group(s)."
+                % (quota.name, len(in_users), len(in_groups)),
+                "status": "info",
+                "inputs": [
+                    build_select_input("in_groups", "Groups", all_groups, in_groups),
+                    build_select_input("in_users", "Users", all_users, in_users),
+                ],
+            }
         else:
             try:
-                return {'message': self._manage_users_and_groups_for_quota(quota, util.Params(payload), decode_id=trans.security.decode_id)}
+                return {
+                    "message": self.quota_manager.manage_users_and_groups_for_quota(
+                        quota, util.Params(payload), decode_id=trans.security.decode_id
+                    )
+                }
             except ActionInputError as e:
                 return self.message_exception(trans, e.err_msg)
 
     @web.legacy_expose_api
     @web.require_admin
     def edit_quota(self, trans, payload=None, **kwd):
-        id = kwd.get('id')
+        id = kwd.get("id")
         if not id:
-            return self.message_exception(trans, 'No quota id received for renaming.')
+            return self.message_exception(trans, "No quota id received for renaming.")
         quota = get_quota(trans, id)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             return {
-                'title'  : 'Edit quota size for \'%s\'' % util.sanitize_text(quota.name),
-                'inputs' : [{
-                    'name'    : 'amount',
-                    'label'   : 'Amount',
-                    'value'   : quota.display_amount,
-                    'help'    : 'Examples: "10000MB", "99 gb", "0.2T", "unlimited"'
-                }, {
-                    'name'    : 'operation',
-                    'label'   : 'Assign, increase by amount, or decrease by amount?',
-                    'options' : [('=', '='), ('+', '+'), ('-', '-')],
-                    'value'   : quota.operation
-                }]
+                "title": "Edit quota size for '%s'" % util.sanitize_text(quota.name),
+                "inputs": [
+                    {
+                        "name": "amount",
+                        "label": "Amount",
+                        "value": quota.display_amount,
+                        "help": 'Examples: "10000MB", "99 gb", "0.2T", "unlimited"',
+                    },
+                    {
+                        "name": "operation",
+                        "label": "Assign, increase by amount, or decrease by amount?",
+                        "options": [("=", "="), ("+", "+"), ("-", "-")],
+                        "value": quota.operation,
+                    },
+                ],
             }
         else:
             try:
-                return {'message': self._edit_quota(quota, util.Params(payload))}
+                return {"message": self.quota_manager.edit_quota(quota, util.Params(payload))}
             except ActionInputError as e:
                 return self.message_exception(trans, e.err_msg)
 
     @web.legacy_expose_api
     @web.require_admin
     def set_quota_default(self, trans, payload=None, **kwd):
-        id = kwd.get('id')
+        id = kwd.get("id")
         if not id:
-            return self.message_exception(trans, 'No quota id received for renaming.')
+            return self.message_exception(trans, "No quota id received for renaming.")
         quota = get_quota(trans, id)
-        if trans.request.method == 'GET':
-            default_value = quota.default[0].type if quota.default else 'no'
-            default_options = [('No', 'no')]
-            for typ in trans.app.model.DefaultQuotaAssociation.types.__dict__.values():
-                default_options.append(('Yes, ' + typ, typ))
+        if trans.request.method == "GET":
+            default_value = quota.default[0].type if quota.default else "no"
+            default_options = [("No", "no")]
+            for typ in trans.app.model.DefaultQuotaAssociation.types.__members__.values():
+                default_options.append((f"Yes, {typ}", typ))
             return {
-                'title'  : 'Set quota default for \'%s\'' % util.sanitize_text(quota.name),
-                'inputs' : [{
-                    'name'    : 'default',
-                    'label'   : 'Assign, increase by amount, or decrease by amount?',
-                    'options' : default_options,
-                    'value'   : default_value,
-                    'help'    : 'Warning: Any users or groups associated with this quota will be disassociated.'
-                }]
+                "title": "Set quota default for '%s'" % util.sanitize_text(quota.name),
+                "inputs": [
+                    {
+                        "name": "default",
+                        "label": "Is this quota a default for a class of users (if yes, what type)?",
+                        "options": default_options,
+                        "value": default_value,
+                        "help": "Warning: Any users or groups associated with this quota will be disassociated.",
+                    }
+                ],
             }
         else:
             try:
-                return {'message': self._set_quota_default(quota, util.Params(payload))}
+                return {"message": self.quota_manager.set_quota_default(quota, util.Params(payload))}
             except ActionInputError as e:
                 return self.message_exception(trans, e.err_msg)
 
     @web.expose
     @web.require_admin
     def impersonate(self, trans, **kwd):
         if not trans.app.config.allow_user_impersonation:
             return trans.show_error_message("User impersonation is not enabled in this instance of Galaxy.")
         user = None
-        user_id = kwd.get('id', None)
+        user_id = kwd.get("id", None)
         if user_id is not None:
             try:
                 user = trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(user_id))
                 if user:
                     trans.handle_user_logout()
                     trans.handle_user_login(user)
-                    return trans.show_message('You are now logged in as %s, <a target="_top" href="%s">return to the home page</a>' % (user.email, url_for(controller='root')), use_panels=True)
+                    return trans.show_message(
+                        f"You are now logged in as {user.email}, <a target=\"_top\" href=\"{url_for(controller='root')}\">return to the home page</a>",
+                        use_panels=True,
+                    )
             except Exception:
                 log.exception("Error fetching user for impersonation")
-        return trans.response.send_redirect(web.url_for(controller='admin',
-                                                        action='users',
-                                                        message="Invalid user selected", status="error"))
-
-    def check_for_tool_dependencies(self, trans, migration_stage):
-        # Get the 000x_tools.xml file associated with migration_stage.
-        tools_xml_file_path = os.path.abspath(os.path.join(common_util.TOOL_MIGRATION_SCRIPTS_DIR, '%04d_tools.xml' % migration_stage))
-        tree = util.parse_xml(tools_xml_file_path)
-        root = tree.getroot()
-        tool_shed = root.get('name')
-        shed_url = common_util.get_tool_shed_url_from_tool_shed_registry(trans.app, tool_shed)
-        repo_name_dependency_tups = []
-        if shed_url:
-            for elem in root:
-                if elem.tag == 'repository':
-                    tool_dependencies = []
-                    tool_dependencies_dict = {}
-                    repository_name = elem.get('name')
-                    changeset_revision = elem.get('changeset_revision')
-                    params = dict(name=repository_name, owner='devteam', changeset_revision=changeset_revision)
-                    pathspec = ['repository', 'get_tool_dependencies']
-                    text = url_get(shed_url, auth=self.app.tool_shed_registry.url_auth(shed_url), pathspec=pathspec, params=params)
-                    if text:
-                        tool_dependencies_dict = encoding_util.tool_shed_decode(text)
-                        for dependency_key, requirements_dict in tool_dependencies_dict.items():
-                            tool_dependency_name = requirements_dict['name']
-                            tool_dependency_version = requirements_dict['version']
-                            tool_dependency_type = requirements_dict['type']
-                            tool_dependency_readme = requirements_dict.get('readme', '')
-                            tool_dependencies.append((tool_dependency_name, tool_dependency_version, tool_dependency_type, tool_dependency_readme))
-                    repo_name_dependency_tups.append((repository_name, tool_dependencies))
-        return repo_name_dependency_tups
-
-    @web.expose
-    @web.require_admin
-    def review_tool_migration_stages(self, trans, **kwd):
-        message = escape(util.restore_text(kwd.get('message', '')))
-        status = util.restore_text(kwd.get('status', 'done'))
-        migration_stages_dict = OrderedDict()
-        # FIXME: this isn't valid in an installed context
-        migration_scripts_dir = os.path.abspath(os.path.join(trans.app.config.root, 'lib', 'galaxy', 'tool_shed', 'galaxy_install', 'migrate', 'versions'))
-        modules = os.listdir(migration_scripts_dir)
-        modules.sort()
-        modules.reverse()
-        for item in modules:
-            if not item.endswith('_tools.py') or item.startswith('0001_tools'):
-                continue
-            module = item.replace('.py', '')
-            migration_stage = int(module.replace('_tools', ''))
-            repo_name_dependency_tups = self.check_for_tool_dependencies(trans, migration_stage)
-            open_file_obj, file_name, description = imp.find_module(module, [migration_scripts_dir])
-            imported_module = imp.load_module('upgrade', open_file_obj, file_name, description)
-            migration_info = imported_module.__doc__
-            open_file_obj.close()
-            migration_stages_dict[migration_stage] = (migration_info, repo_name_dependency_tups)
-        return trans.fill_template('admin/review_tool_migration_stages.mako',
-                                   migration_stages_dict=migration_stages_dict,
-                                   message=message,
-                                   status=status)
+        return trans.response.send_redirect(
+            web.url_for(controller="admin", action="users", message="Invalid user selected", status="error")
+        )
 
     @web.legacy_expose_api
     @web.require_admin
     def tool_versions_list(self, trans, **kwd):
         return self.tool_version_list_grid(trans, **kwd)
 
     @web.expose
     @web.json
     @web.require_admin
     def roles_list(self, trans, **kwargs):
-        message = kwargs.get('message')
-        status = kwargs.get('status')
-        if 'operation' in kwargs:
-            id = kwargs.get('id', None)
+        message = kwargs.get("message")
+        status = kwargs.get("status")
+        if "operation" in kwargs:
+            id = kwargs.get("id", None)
             if not id:
-                message, status = ('Invalid role id (%s) received.' % str(id), 'error')
+                message, status = (f"Invalid role id ({str(id)}) received.", "error")
             ids = util.listify(id)
-            operation = kwargs['operation'].lower().replace('+', ' ')
-            if operation == 'delete':
+            operation = kwargs["operation"].lower().replace("+", " ")
+            if operation == "delete":
                 message, status = self._delete_role(trans, ids)
-            elif operation == 'undelete':
+            elif operation == "undelete":
                 message, status = self._undelete_role(trans, ids)
-            elif operation == 'purge':
+            elif operation == "purge":
                 message, status = self._purge_role(trans, ids)
         if message and status:
-            kwargs['message'] = util.sanitize_text(message)
-            kwargs['status'] = status
+            kwargs["message"] = util.sanitize_text(message)
+            kwargs["status"] = status
         return self.role_list_grid(trans, **kwargs)
 
     @web.legacy_expose_api
     @web.require_admin
     def create_role(self, trans, payload=None, **kwd):
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             all_users = []
             all_groups = []
-            for user in trans.sa_session.query(trans.app.model.User) \
-                                        .filter(trans.app.model.User.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.User.table.c.email):
+            for user in (
+                trans.sa_session.query(trans.app.model.User)
+                .filter(trans.app.model.User.table.c.deleted == false())
+                .order_by(trans.app.model.User.table.c.email)
+            ):
                 all_users.append((user.email, trans.security.encode_id(user.id)))
-            for group in trans.sa_session.query(trans.app.model.Group) \
-                                         .filter(trans.app.model.Group.table.c.deleted == false()) \
-                                         .order_by(trans.app.model.Group.table.c.name):
+            for group in (
+                trans.sa_session.query(trans.app.model.Group)
+                .filter(trans.app.model.Group.deleted == false())
+                .order_by(trans.app.model.Group.name)
+            ):
                 all_groups.append((group.name, trans.security.encode_id(group.id)))
             return {
-                'title'  : 'Create Role',
-                'inputs' : [{
-                    'name'  : 'name',
-                    'label' : 'Name'
-                }, {
-                    'name'  : 'description',
-                    'label' : 'Description'
-                },
-                    build_select_input('in_groups', 'Groups', all_groups, []),
-                    build_select_input('in_users', 'Users', all_users, []), {
-                    'name'  : 'auto_create',
-                    'label' : 'Create a new group of the same name for this role:',
-                    'type'  : 'boolean'
-                }]}
+                "title": "Create Role",
+                "inputs": [
+                    {"name": "name", "label": "Name"},
+                    {"name": "description", "label": "Description"},
+                    build_select_input("in_groups", "Groups", all_groups, []),
+                    build_select_input("in_users", "Users", all_users, []),
+                    {
+                        "name": "auto_create",
+                        "label": "Create a new group of the same name for this role:",
+                        "type": "boolean",
+                        "optional": True,
+                    },
+                ],
+            }
         else:
-            name = util.restore_text(payload.get('name', ''))
-            description = util.restore_text(payload.get('description', ''))
-            auto_create_checked = payload.get('auto_create') == 'true'
-            in_users = [trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_users'))]
-            in_groups = [trans.sa_session.query(trans.app.model.Group).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_groups'))]
+            name = util.restore_text(payload.get("name", ""))
+            description = util.restore_text(payload.get("description", ""))
+            auto_create_checked = payload.get("auto_create")
+            in_users = [
+                trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_users"))
+            ]
+            in_groups = [
+                trans.sa_session.query(trans.app.model.Group).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_groups"))
+            ]
             if not name or not description:
-                return self.message_exception(trans, 'Enter a valid name and a description.')
-            elif trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.table.c.name == name).first():
-                return self.message_exception(trans, 'Role names must be unique and a role with that name already exists, so choose another name.')
+                return self.message_exception(trans, "Enter a valid name and a description.")
+            elif trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.name == name).first():
+                return self.message_exception(
+                    trans, "Role names must be unique and a role with that name already exists, so choose another name."
+                )
             elif None in in_users or None in in_groups:
-                return self.message_exception(trans, 'One or more invalid user/group id has been provided.')
+                return self.message_exception(trans, "One or more invalid user/group id has been provided.")
             else:
                 # Create the role
                 role = trans.app.model.Role(name=name, description=description, type=trans.app.model.Role.types.ADMIN)
                 trans.sa_session.add(role)
                 # Create the UserRoleAssociations
                 for user in in_users:
                     ura = trans.app.model.UserRoleAssociation(user, role)
                     trans.sa_session.add(ura)
                 # Create the GroupRoleAssociations
                 for group in in_groups:
                     gra = trans.app.model.GroupRoleAssociation(group, role)
                     trans.sa_session.add(gra)
                 if auto_create_checked:
                     # Check if role with same name already exists
-                    if trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.name == name).first():
-                        return self.message_exception(trans, 'A group with that name already exists, so choose another name or disable group creation.')
+                    if trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.name == name).first():
+                        return self.message_exception(
+                            trans,
+                            "A group with that name already exists, so choose another name or disable group creation.",
+                        )
                     # Create the group
                     group = trans.app.model.Group(name=name)
                     trans.sa_session.add(group)
                     # Associate the group with the role
                     gra = trans.model.GroupRoleAssociation(group, role)
                     trans.sa_session.add(gra)
                     num_in_groups = len(in_groups) + 1
                 else:
                     num_in_groups = len(in_groups)
                 trans.sa_session.flush()
-                message = 'Role \'%s\' has been created with %d associated users and %d associated groups.' % (role.name, len(in_users), num_in_groups)
+                message = f"Role '{role.name}' has been created with {len(in_users)} associated users and {num_in_groups} associated groups."
                 if auto_create_checked:
-                    message += 'One of the groups associated with this role is the newly created group with the same name.'
-                return {'message' : message}
+                    message += (
+                        "One of the groups associated with this role is the newly created group with the same name."
+                    )
+                return {"message": message}
 
     @web.legacy_expose_api
     @web.require_admin
     def rename_role(self, trans, payload=None, **kwd):
-        id = kwd.get('id')
+        id = kwd.get("id")
         if not id:
-            return self.message_exception(trans, 'No role id received for renaming.')
+            return self.message_exception(trans, "No role id received for renaming.")
         role = get_role(trans, id)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             return {
-                'title'  : 'Change role name and description for \'%s\'' % util.sanitize_text(role.name),
-                'inputs' : [{
-                    'name'  : 'name',
-                    'label' : 'Name',
-                    'value' : role.name
-                }, {
-                    'name'  : 'description',
-                    'label' : 'Description',
-                    'value' : role.description
-                }]
+                "title": "Change role name and description for '%s'" % util.sanitize_text(role.name),
+                "inputs": [
+                    {"name": "name", "label": "Name", "value": role.name},
+                    {"name": "description", "label": "Description", "value": role.description},
+                ],
             }
         else:
             old_name = role.name
-            new_name = util.restore_text(payload.get('name'))
-            new_description = util.restore_text(payload.get('description'))
+            new_name = util.restore_text(payload.get("name"))
+            new_description = util.restore_text(payload.get("description"))
             if not new_name:
-                return self.message_exception(trans, 'Enter a valid role name.')
+                return self.message_exception(trans, "Enter a valid role name.")
             else:
-                existing_role = trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.table.c.name == new_name).first()
+                existing_role = (
+                    trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.name == new_name).first()
+                )
                 if existing_role and existing_role.id != role.id:
-                    return self.message_exception(trans, 'A role with that name already exists.')
+                    return self.message_exception(trans, "A role with that name already exists.")
                 else:
                     if not (role.name == new_name and role.description == new_description):
                         role.name = new_name
                         role.description = new_description
                         trans.sa_session.add(role)
                         trans.sa_session.flush()
-            return {'message': 'Role \'%s\' has been renamed to \'%s\'.' % (old_name, new_name)}
+            return {"message": f"Role '{old_name}' has been renamed to '{new_name}'."}
 
     @web.legacy_expose_api
     @web.require_admin
     def manage_users_and_groups_for_role(self, trans, payload=None, **kwd):
-        role_id = kwd.get('id')
+        role_id = kwd.get("id")
         if not role_id:
-            return self.message_exception(trans, 'Invalid role id (%s) received' % str(role_id))
+            return self.message_exception(trans, f"Invalid role id ({str(role_id)}) received")
         role = get_role(trans, role_id)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             in_users = []
             all_users = []
             in_groups = []
             all_groups = []
-            for user in trans.sa_session.query(trans.app.model.User) \
-                                        .filter(trans.app.model.User.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.User.table.c.email):
+            for user in (
+                trans.sa_session.query(trans.app.model.User)
+                .filter(trans.app.model.User.table.c.deleted == false())
+                .order_by(trans.app.model.User.table.c.email)
+            ):
                 if user in [x.user for x in role.users]:
                     in_users.append(trans.security.encode_id(user.id))
                 all_users.append((user.email, trans.security.encode_id(user.id)))
-            for group in trans.sa_session.query(trans.app.model.Group) \
-                                         .filter(trans.app.model.Group.table.c.deleted == false()) \
-                                         .order_by(trans.app.model.Group.table.c.name):
+            for group in (
+                trans.sa_session.query(trans.app.model.Group)
+                .filter(trans.app.model.Group.deleted == false())
+                .order_by(trans.app.model.Group.name)
+            ):
                 if group in [x.group for x in role.groups]:
                     in_groups.append(trans.security.encode_id(group.id))
                 all_groups.append((group.name, trans.security.encode_id(group.id)))
-            return {'title'  : 'Role \'%s\'' % role.name,
-                    'message': 'Role \'%s\' is currently associated with %d user(s) and %d group(s).' %
-                    (role.name, len(in_users), len(in_groups)),
-                    'status' : 'info',
-                    'inputs' : [build_select_input('in_groups', 'Groups', all_groups, in_groups),
-                                build_select_input('in_users', 'Users', all_users, in_users)]}
+            return {
+                "title": "Role '%s'" % role.name,
+                "message": "Role '%s' is currently associated with %d user(s) and %d group(s)."
+                % (role.name, len(in_users), len(in_groups)),
+                "status": "info",
+                "inputs": [
+                    build_select_input("in_groups", "Groups", all_groups, in_groups),
+                    build_select_input("in_users", "Users", all_users, in_users),
+                ],
+            }
         else:
-            in_users = [trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_users'))]
-            in_groups = [trans.sa_session.query(trans.app.model.Group).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_groups'))]
+            in_users = [
+                trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_users"))
+            ]
+            in_groups = [
+                trans.sa_session.query(trans.app.model.Group).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_groups"))
+            ]
             if None in in_users or None in in_groups:
-                return self.message_exception(trans, 'One or more invalid user/group id has been provided.')
+                return self.message_exception(trans, "One or more invalid user/group id has been provided.")
             for ura in role.users:
                 user = trans.sa_session.query(trans.app.model.User).get(ura.user_id)
                 if user not in in_users:
                     # Delete DefaultUserPermissions for previously associated users that have been removed from the role
                     for dup in user.default_permissions:
                         if role == dup.role:
                             trans.sa_session.delete(dup)
@@ -1097,53 +1106,55 @@
                     for history in user.histories:
                         for dhp in history.default_permissions:
                             if role == dhp.role:
                                 trans.sa_session.delete(dhp)
                     trans.sa_session.flush()
             trans.app.security_agent.set_entity_role_associations(roles=[role], users=in_users, groups=in_groups)
             trans.sa_session.refresh(role)
-            return {'message' : 'Role \'%s\' has been updated with %d associated users and %d associated groups.' % (role.name, len(in_users), len(in_groups))}
+            return {
+                "message": f"Role '{role.name}' has been updated with {len(in_users)} associated users and {len(in_groups)} associated groups."
+            }
 
     def _delete_role(self, trans, ids):
-        message = 'Deleted %d roles: ' % len(ids)
+        message = "Deleted %d roles: " % len(ids)
         for role_id in ids:
             role = get_role(trans, role_id)
             role.deleted = True
             trans.sa_session.add(role)
             trans.sa_session.flush()
-            message += ' %s ' % role.name
-        return (message, 'done')
+            message += f" {role.name} "
+        return (message, "done")
 
     def _undelete_role(self, trans, ids):
         count = 0
         undeleted_roles = ""
         for role_id in ids:
             role = get_role(trans, role_id)
             if not role.deleted:
-                return ("Role '%s' has not been deleted, so it cannot be undeleted." % role.name, "error")
+                return (f"Role '{role.name}' has not been deleted, so it cannot be undeleted.", "error")
             role.deleted = False
             trans.sa_session.add(role)
             trans.sa_session.flush()
             count += 1
-            undeleted_roles += " %s" % role.name
+            undeleted_roles += f" {role.name}"
         return ("Undeleted %d roles: %s" % (count, undeleted_roles), "done")
 
     def _purge_role(self, trans, ids):
         # This method should only be called for a Role that has previously been deleted.
         # Purging a deleted Role deletes all of the following from the database:
         # - UserRoleAssociations where role_id == Role.id
         # - DefaultUserPermissions where role_id == Role.id
         # - DefaultHistoryPermissions where role_id == Role.id
         # - GroupRoleAssociations where role_id == Role.id
         # - DatasetPermissionss where role_id == Role.id
         message = "Purged %d roles: " % len(ids)
         for role_id in ids:
             role = get_role(trans, role_id)
             if not role.deleted:
-                return ("Role '%s' has not been deleted, so it cannot be purged." % role.name, "error")
+                return (f"Role '{role.name}' has not been deleted, so it cannot be purged.", "error")
             # Delete UserRoleAssociations
             for ura in role.users:
                 user = trans.sa_session.query(trans.app.model.User).get(ura.user_id)
                 # Delete DefaultUserPermissions for associated users
                 for dup in user.default_permissions:
                     if role == dup.role:
                         trans.sa_session.delete(dup)
@@ -1156,529 +1167,479 @@
             # Delete GroupRoleAssociations
             for gra in role.groups:
                 trans.sa_session.delete(gra)
             # Delete DatasetPermissionss
             for dp in role.dataset_actions:
                 trans.sa_session.delete(dp)
             trans.sa_session.flush()
-            message += " %s " % role.name
+            message += f" {role.name} "
         return (message, "done")
 
     @web.legacy_expose_api
     @web.require_admin
     def groups_list(self, trans, **kwargs):
-        message = kwargs.get('message')
-        status = kwargs.get('status')
-        if 'operation' in kwargs:
-            id = kwargs.get('id')
+        message = kwargs.get("message")
+        status = kwargs.get("status")
+        if "operation" in kwargs:
+            id = kwargs.get("id")
             if not id:
-                return self.message_exception(trans, 'Invalid group id (%s) received.' % str(id))
+                return self.message_exception(trans, f"Invalid group id ({str(id)}) received.")
             ids = util.listify(id)
-            operation = kwargs['operation'].lower().replace('+', ' ')
-            if operation == 'delete':
+            operation = kwargs["operation"].lower().replace("+", " ")
+            if operation == "delete":
                 message, status = self._delete_group(trans, ids)
-            elif operation == 'undelete':
+            elif operation == "undelete":
                 message, status = self._undelete_group(trans, ids)
-            elif operation == 'purge':
+            elif operation == "purge":
                 message, status = self._purge_group(trans, ids)
         if message and status:
-            kwargs['message'] = util.sanitize_text(message)
-            kwargs['status'] = status
+            kwargs["message"] = util.sanitize_text(message)
+            kwargs["status"] = status
         return self.group_list_grid(trans, **kwargs)
 
     @web.legacy_expose_api
     @web.require_admin
     def rename_group(self, trans, payload=None, **kwd):
-        id = kwd.get('id')
+        id = kwd.get("id")
         if not id:
-            return self.message_exception(trans, 'No group id received for renaming.')
+            return self.message_exception(trans, "No group id received for renaming.")
         group = get_group(trans, id)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             return {
-                'title'  : 'Change group name for \'%s\'' % util.sanitize_text(group.name),
-                'inputs' : [{
-                    'name'  : 'name',
-                    'label' : 'Name',
-                    'value' : group.name
-                }]
+                "title": "Change group name for '%s'" % util.sanitize_text(group.name),
+                "inputs": [{"name": "name", "label": "Name", "value": group.name}],
             }
         else:
             old_name = group.name
-            new_name = util.restore_text(payload.get('name'))
+            new_name = util.restore_text(payload.get("name"))
             if not new_name:
-                return self.message_exception(trans, 'Enter a valid group name.')
+                return self.message_exception(trans, "Enter a valid group name.")
             else:
-                existing_group = trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.name == new_name).first()
+                existing_group = (
+                    trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.name == new_name).first()
+                )
                 if existing_group and existing_group.id != group.id:
-                    return self.message_exception(trans, 'A group with that name already exists.')
+                    return self.message_exception(trans, "A group with that name already exists.")
                 else:
                     if not (group.name == new_name):
                         group.name = new_name
                         trans.sa_session.add(group)
                         trans.sa_session.flush()
-            return {'message': 'Group \'%s\' has been renamed to \'%s\'.' % (old_name, new_name)}
+            return {"message": f"Group '{old_name}' has been renamed to '{new_name}'."}
 
     @web.legacy_expose_api
     @web.require_admin
     def manage_users_and_roles_for_group(self, trans, payload=None, **kwd):
-        group_id = kwd.get('id')
+        group_id = kwd.get("id")
         if not group_id:
-            return self.message_exception(trans, 'Invalid group id (%s) received' % str(group_id))
+            return self.message_exception(trans, f"Invalid group id ({str(group_id)}) received")
         group = get_group(trans, group_id)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             in_users = []
             all_users = []
             in_roles = []
             all_roles = []
-            for user in trans.sa_session.query(trans.app.model.User) \
-                                        .filter(trans.app.model.User.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.User.table.c.email):
+            for user in (
+                trans.sa_session.query(trans.app.model.User)
+                .filter(trans.app.model.User.table.c.deleted == false())
+                .order_by(trans.app.model.User.table.c.email)
+            ):
                 if user in [x.user for x in group.users]:
                     in_users.append(trans.security.encode_id(user.id))
                 all_users.append((user.email, trans.security.encode_id(user.id)))
-            for role in trans.sa_session.query(trans.app.model.Role) \
-                                        .filter(trans.app.model.Role.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.Role.table.c.name):
+            for role in (
+                trans.sa_session.query(trans.app.model.Role)
+                .filter(trans.app.model.Role.deleted == false())
+                .order_by(trans.app.model.Role.name)
+            ):
                 if role in [x.role for x in group.roles]:
                     in_roles.append(trans.security.encode_id(role.id))
                 all_roles.append((role.name, trans.security.encode_id(role.id)))
-            return {'title'  : 'Group \'%s\'' % group.name,
-                    'message': 'Group \'%s\' is currently associated with %d user(s) and %d role(s).' %
-                    (group.name, len(in_users), len(in_roles)),
-                    'status' : 'info',
-                    'inputs' : [build_select_input('in_roles', 'Roles', all_roles, in_roles),
-                                build_select_input('in_users', 'Users', all_users, in_users)]}
+            return {
+                "title": "Group '%s'" % group.name,
+                "message": "Group '%s' is currently associated with %d user(s) and %d role(s)."
+                % (group.name, len(in_users), len(in_roles)),
+                "status": "info",
+                "inputs": [
+                    build_select_input("in_roles", "Roles", all_roles, in_roles),
+                    build_select_input("in_users", "Users", all_users, in_users),
+                ],
+            }
         else:
-            in_users = [trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_users'))]
-            in_roles = [trans.sa_session.query(trans.app.model.Role).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_roles'))]
+            in_users = [
+                trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_users"))
+            ]
+            in_roles = [
+                trans.sa_session.query(trans.app.model.Role).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_roles"))
+            ]
             if None in in_users or None in in_roles:
-                return self.message_exception(trans, 'One or more invalid user/role id has been provided.')
+                return self.message_exception(trans, "One or more invalid user/role id has been provided.")
             trans.app.security_agent.set_entity_group_associations(groups=[group], users=in_users, roles=in_roles)
             trans.sa_session.refresh(group)
-            return {'message' : 'Group \'%s\' has been updated with %d associated users and %d associated roles.' % (group.name, len(in_users), len(in_roles))}
+            return {
+                "message": f"Group '{group.name}' has been updated with {len(in_users)} associated users and {len(in_roles)} associated roles."
+            }
 
     @web.legacy_expose_api
     @web.require_admin
     def create_group(self, trans, payload=None, **kwd):
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             all_users = []
             all_roles = []
-            for user in trans.sa_session.query(trans.app.model.User) \
-                                        .filter(trans.app.model.User.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.User.table.c.email):
+            for user in (
+                trans.sa_session.query(trans.app.model.User)
+                .filter(trans.app.model.User.table.c.deleted == false())
+                .order_by(trans.app.model.User.table.c.email)
+            ):
                 all_users.append((user.email, trans.security.encode_id(user.id)))
-            for role in trans.sa_session.query(trans.app.model.Role) \
-                                        .filter(trans.app.model.Role.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.Role.table.c.name):
+            for role in (
+                trans.sa_session.query(trans.app.model.Role)
+                .filter(trans.app.model.Role.deleted == false())
+                .order_by(trans.app.model.Role.name)
+            ):
                 all_roles.append((role.name, trans.security.encode_id(role.id)))
             return {
-                'title'    : 'Create Group',
-                'title_id' : 'create-group',
-                'inputs' : [{
-                    'name'  : 'name',
-                    'label' : 'Name'
-                },
-                    build_select_input('in_roles', 'Roles', all_roles, []),
-                    build_select_input('in_users', 'Users', all_users, []), {
-                    'name'  : 'auto_create',
-                    'label' : 'Create a new role of the same name for this group:',
-                    'type'  : 'boolean'
-                }]
+                "title": "Create Group",
+                "title_id": "create-group",
+                "inputs": [
+                    {"name": "name", "label": "Name"},
+                    build_select_input("in_roles", "Roles", all_roles, []),
+                    build_select_input("in_users", "Users", all_users, []),
+                    {
+                        "name": "auto_create",
+                        "label": "Create a new role of the same name for this group:",
+                        "type": "boolean",
+                        "optional": True,
+                    },
+                ],
             }
         else:
-            name = util.restore_text(payload.get('name', ''))
-            auto_create_checked = payload.get('auto_create') == 'true'
-            in_users = [trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_users'))]
-            in_roles = [trans.sa_session.query(trans.app.model.Role).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_roles'))]
+            name = util.restore_text(payload.get("name", ""))
+            auto_create_checked = payload.get("auto_create")
+            in_users = [
+                trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_users"))
+            ]
+            in_roles = [
+                trans.sa_session.query(trans.app.model.Role).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_roles"))
+            ]
             if not name:
-                return self.message_exception(trans, 'Enter a valid name.')
-            elif trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.name == name).first():
-                return self.message_exception(trans, 'Group names must be unique and a group with that name already exists, so choose another name.')
+                return self.message_exception(trans, "Enter a valid name.")
+            elif trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.name == name).first():
+                return self.message_exception(
+                    trans,
+                    "Group names must be unique and a group with that name already exists, so choose another name.",
+                )
             elif None in in_users or None in in_roles:
-                return self.message_exception(trans, 'One or more invalid user/role id has been provided.')
+                return self.message_exception(trans, "One or more invalid user/role id has been provided.")
             else:
                 # Create the role
                 group = trans.app.model.Group(name=name)
                 trans.sa_session.add(group)
                 # Create the UserRoleAssociations
                 for user in in_users:
                     uga = trans.app.model.UserGroupAssociation(user, group)
                     trans.sa_session.add(uga)
                 # Create the GroupRoleAssociations
                 for role in in_roles:
                     gra = trans.app.model.GroupRoleAssociation(group, role)
                     trans.sa_session.add(gra)
                 if auto_create_checked:
                     # Check if role with same name already exists
-                    if trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.table.c.name == name).first():
-                        return self.message_exception(trans, 'A role with that name already exists, so choose another name or disable role creation.')
+                    if trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.name == name).first():
+                        return self.message_exception(
+                            trans,
+                            "A role with that name already exists, so choose another name or disable role creation.",
+                        )
                     # Create the role
-                    role = trans.app.model.Role(name=name, description='Role for group %s' % name)
+                    role = trans.app.model.Role(name=name, description=f"Role for group {name}")
                     trans.sa_session.add(role)
                     # Associate the group with the role
                     gra = trans.model.GroupRoleAssociation(group, role)
                     trans.sa_session.add(gra)
                     num_in_roles = len(in_roles) + 1
                 else:
                     num_in_roles = len(in_roles)
                 trans.sa_session.flush()
-                message = 'Group \'%s\' has been created with %d associated users and %d associated roles.' % (group.name, len(in_users), num_in_roles)
+                message = "Group '%s' has been created with %d associated users and %d associated roles." % (
+                    group.name,
+                    len(in_users),
+                    num_in_roles,
+                )
                 if auto_create_checked:
-                    message += 'One of the roles associated with this group is the newly created role with the same name.'
-                return {'message' : message}
+                    message += (
+                        "One of the roles associated with this group is the newly created role with the same name."
+                    )
+                return {"message": message}
 
     def _delete_group(self, trans, ids):
-        message = 'Deleted %d groups: ' % len(ids)
+        message = "Deleted %d groups: " % len(ids)
         for group_id in ids:
             group = get_group(trans, group_id)
             group.deleted = True
             trans.sa_session.add(group)
             trans.sa_session.flush()
-            message += ' %s ' % group.name
-        return (message, 'done')
+            message += f" {group.name} "
+        return (message, "done")
 
     def _undelete_group(self, trans, ids):
         count = 0
         undeleted_groups = ""
         for group_id in ids:
             group = get_group(trans, group_id)
             if not group.deleted:
-                return ("Group '%s' has not been deleted, so it cannot be undeleted." % group.name, "error")
+                return (f"Group '{group.name}' has not been deleted, so it cannot be undeleted.", "error")
             group.deleted = False
             trans.sa_session.add(group)
             trans.sa_session.flush()
             count += 1
-            undeleted_groups += " %s" % group.name
+            undeleted_groups += f" {group.name}"
         return ("Undeleted %d groups: %s" % (count, undeleted_groups), "done")
 
     def _purge_group(self, trans, ids):
         message = "Purged %d groups: " % len(ids)
         for group_id in ids:
             group = get_group(trans, group_id)
             if not group.deleted:
-                return ("Group '%s' has not been deleted, so it cannot be purged." % group.name, "error")
+                return (f"Group '{group.name}' has not been deleted, so it cannot be purged.", "error")
             # Delete UserGroupAssociations
             for uga in group.users:
                 trans.sa_session.delete(uga)
             # Delete GroupRoleAssociations
             for gra in group.roles:
                 trans.sa_session.delete(gra)
             trans.sa_session.flush()
-            message += " %s " % group.name
+            message += f" {group.name} "
         return (message, "done")
 
     @web.expose
     @web.require_admin
     def create_new_user(self, trans, **kwd):
-        return trans.response.send_redirect(web.url_for(controller='user',
-                                                        action='create',
-                                                        cntrller='admin'))
+        return trans.response.send_redirect(web.url_for(controller="user", action="create", cntrller="admin"))
 
     @web.legacy_expose_api
     @web.require_admin
     def reset_user_password(self, trans, payload=None, **kwd):
-        users = {user_id: get_user(trans, user_id) for user_id in util.listify(kwd.get('id'))}
+        users = {user_id: get_user(trans, user_id) for user_id in util.listify(kwd.get("id"))}
         if users:
-            if trans.request.method == 'GET':
+            if trans.request.method == "GET":
                 return {
-                    'message': 'Changes password(s) for: %s.' % ', '.join(user.email for user in users.values()),
-                    'status' : 'info',
-                    'inputs' : [{'name' : 'password', 'label' : 'New password', 'type' : 'password'},
-                                {'name' : 'confirm', 'label' : 'Confirm password', 'type' : 'password'}]
+                    "message": f"Changes password(s) for: {', '.join(user.email for user in users.values())}.",
+                    "status": "info",
+                    "inputs": [
+                        {"name": "password", "label": "New password", "type": "password"},
+                        {"name": "confirm", "label": "Confirm password", "type": "password"},
+                    ],
                 }
             else:
-                password = payload.get('password')
-                confirm = payload.get('confirm')
+                password = payload.get("password")
+                confirm = payload.get("confirm")
                 message = validate_password(trans, password, confirm)
                 if message:
                     return self.message_exception(trans, message)
                 for user in users.values():
                     user.set_password_cleartext(password)
                     trans.sa_session.add(user)
                     trans.sa_session.flush()
-                return {'message': 'Passwords reset for %d user(s).' % len(users)}
+                return {"message": "Passwords reset for %d user(s)." % len(users)}
         else:
-            return self.message_exception(trans, 'Please specify user ids.')
+            return self.message_exception(trans, "Please specify user ids.")
 
     def _delete_user(self, trans, ids):
-        message = 'Deleted %d users: ' % len(ids)
+        message = "Deleted %d users: " % len(ids)
         for user_id in ids:
             user = get_user(trans, user_id)
             # Actually do the delete
             self.user_manager.delete(user)
             # Accumulate messages for the return message
-            message += ' %s ' % user.email
-        return (message, 'done')
+            message += f" {user.email} "
+        return (message, "done")
 
     def _undelete_user(self, trans, ids):
         count = 0
         undeleted_users = ""
         for user_id in ids:
             user = get_user(trans, user_id)
             # Actually do the undelete
             self.user_manager.undelete(user)
             # Count and accumulate messages to return to the admin panel
             count += 1
-            undeleted_users += ' %s' % user.email
-        message = 'Undeleted %d users: %s' % (count, undeleted_users)
-        return (message, 'done')
+            undeleted_users += f" {user.email}"
+        message = "Undeleted %d users: %s" % (count, undeleted_users)
+        return (message, "done")
 
     def _purge_user(self, trans, ids):
         # This method should only be called for a User that has previously been deleted.
         # We keep the User in the database ( marked as purged ), and stuff associated
         # with the user's private role in case we want the ability to unpurge the user
         # some time in the future.
         # Purging a deleted User deletes all of the following:
         # - History where user_id = User.id
         #    - HistoryDatasetAssociation where history_id = History.id
         # - UserGroupAssociation where user_id == User.id
         # - UserRoleAssociation where user_id == User.id EXCEPT FOR THE PRIVATE ROLE
         # - UserAddress where user_id == User.id
         # Purging Histories and Datasets must be handled via the cleanup_datasets.py script
-        message = 'Purged %d users: ' % len(ids)
+        message = "Purged %d users: " % len(ids)
         for user_id in ids:
             user = get_user(trans, user_id)
             self.user_manager.purge(user)
-            message += '\t%s\n ' % user.email
-        return (message, 'done')
+            message += f"\t{user.email}\n "
+        return (message, "done")
 
     def _recalculate_user(self, trans, user_id):
         user = trans.sa_session.query(trans.model.User).get(trans.security.decode_id(user_id))
         if not user:
-            return ('User not found for id (%s)' % sanitize_text(str(user_id)), 'error')
+            return (f"User not found for id ({sanitize_text(str(user_id))})", "error")
         current = user.get_disk_usage()
         user.calculate_and_set_disk_usage()
         new = user.get_disk_usage()
         if new in (current, None):
-            message = 'Usage is unchanged at %s.' % nice_size(current)
+            message = f"Usage is unchanged at {nice_size(current)}."
         else:
-            message = 'Usage has changed by %s to %s.' % (nice_size(new - current), nice_size(new))
-        return (message, 'done')
+            message = f"Usage has changed by {nice_size(new - current)} to {nice_size(new)}."
+        return (message, "done")
 
     def _new_user_apikey(self, trans, user_id):
         user = trans.sa_session.query(trans.model.User).get(trans.security.decode_id(user_id))
         if not user:
-            return ('User not found for id (%s)' % sanitize_text(str(user_id)), 'error')
+            return (f"User not found for id ({sanitize_text(str(user_id))})", "error")
         new_key = trans.app.model.APIKeys(
-            user_id=trans.security.decode_id(user_id),
-            key=trans.app.security.get_new_guid()
+            user_id=trans.security.decode_id(user_id), key=trans.app.security.get_new_guid()
         )
         trans.sa_session.add(new_key)
         trans.sa_session.flush()
-        return ("New key '%s' generated for requested user '%s'." % (new_key.key, user.email), "done")
+        return (f"New key '{new_key.key}' generated for requested user '{user.email}'.", "done")
 
     def _activate_user(self, trans, user_id):
         user = trans.sa_session.query(trans.model.User).get(trans.security.decode_id(user_id))
         if not user:
-            return ('User not found for id (%s)' % sanitize_text(str(user_id)), 'error')
+            return (f"User not found for id ({sanitize_text(str(user_id))})", "error")
         self.user_manager.activate(user)
-        return ('Activated user: %s.' % user.email, 'done')
+        return (f"Activated user: {user.email}.", "done")
 
     def _resend_activation_email(self, trans, user_id):
         user = trans.sa_session.query(trans.model.User).get(trans.security.decode_id(user_id))
         if not user:
-            return ('User not found for id (%s)' % sanitize_text(str(user_id)), 'error')
+            return (f"User not found for id ({sanitize_text(str(user_id))})", "error")
         if self.user_manager.send_activation_email(trans, user.email, user.username):
-            return ('Activation email has been sent to user: %s.' % user.email, 'done')
+            return (f"Activation email has been sent to user: {user.email}.", "done")
         else:
-            return ('Unable to send activation email to user: %s.' % user.email, 'error')
+            return (f"Unable to send activation email to user: {user.email}.", "error")
 
     @web.legacy_expose_api
     @web.require_admin
     def manage_roles_and_groups_for_user(self, trans, payload=None, **kwd):
-        user_id = kwd.get('id')
+        user_id = kwd.get("id")
         if not user_id:
-            return self.message_exception(trans, 'Invalid user id (%s) received' % str(user_id))
+            return self.message_exception(trans, f"Invalid user id ({str(user_id)}) received")
         user = get_user(trans, user_id)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             in_roles = []
             all_roles = []
             in_groups = []
             all_groups = []
-            for role in trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.table.c.deleted == false()) \
-                    .order_by(trans.app.model.Role.table.c.name):
+            for role in (
+                trans.sa_session.query(trans.app.model.Role)
+                .filter(trans.app.model.Role.deleted == false())
+                .order_by(trans.app.model.Role.name)
+            ):
                 if role in [x.role for x in user.roles]:
                     in_roles.append(trans.security.encode_id(role.id))
                 if role.type != trans.app.model.Role.types.PRIVATE:
                     # There is a 1 to 1 mapping between a user and a PRIVATE role, so private roles should
                     # not be listed in the roles form fields, except for the currently selected user's private
                     # role, which should always be in in_roles.  The check above is added as an additional
                     # precaution, since for a period of time we were including private roles in the form fields.
                     all_roles.append((role.name, trans.security.encode_id(role.id)))
-            for group in trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.deleted == false()) \
-                    .order_by(trans.app.model.Group.table.c.name):
+            for group in (
+                trans.sa_session.query(trans.app.model.Group)
+                .filter(trans.app.model.Group.deleted == false())
+                .order_by(trans.app.model.Group.name)
+            ):
                 if group in [x.group for x in user.groups]:
                     in_groups.append(trans.security.encode_id(group.id))
                 all_groups.append((group.name, trans.security.encode_id(group.id)))
-            return {'title'  : 'Roles and groups for \'%s\'' % user.email,
-                    'message': 'User \'%s\' is currently associated with %d role(s) and is a member of %d group(s).' %
-                    (user.email, len(in_roles) - 1, len(in_groups)),
-                    'status' : 'info',
-                    'inputs' : [build_select_input('in_roles', 'Roles', all_roles, in_roles),
-                                build_select_input('in_groups', 'Groups', all_groups, in_groups)]}
+            return {
+                "title": f"Roles and groups for '{user.email}'",
+                "message": f"User '{user.email}' is currently associated with {len(in_roles) - 1} role(s) and is a member of {len(in_groups)} group(s).",
+                "status": "info",
+                "inputs": [
+                    build_select_input("in_roles", "Roles", all_roles, in_roles),
+                    build_select_input("in_groups", "Groups", all_groups, in_groups),
+                ],
+            }
         else:
-            in_roles = [trans.sa_session.query(trans.app.model.Role).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_roles'))]
-            in_groups = [trans.sa_session.query(trans.app.model.Group).get(trans.security.decode_id(x)) for x in util.listify(payload.get('in_groups'))]
+            in_roles = [
+                trans.sa_session.query(trans.app.model.Role).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_roles"))
+            ]
+            in_groups = [
+                trans.sa_session.query(trans.app.model.Group).get(trans.security.decode_id(x))
+                for x in util.listify(payload.get("in_groups"))
+            ]
             if None in in_groups or None in in_roles:
-                return self.message_exception(trans, 'One or more invalid role/group id has been provided.')
+                return self.message_exception(trans, "One or more invalid role/group id has been provided.")
 
             # make sure the user is not dis-associating himself from his private role
             private_role = trans.app.security_agent.get_private_user_role(user)
             if private_role not in in_roles:
                 in_roles.append(private_role)
 
             trans.app.security_agent.set_entity_user_associations(users=[user], roles=in_roles, groups=in_groups)
             trans.sa_session.refresh(user)
-            return {'message' : 'User \'%s\' has been updated with %d associated roles and %d associated groups (private roles are not displayed).' % (user.email, len(in_roles) - 1, len(in_groups))}
-
-    @web.expose
-    @web.json
-    @web.require_admin
-    def jobs_list(self, trans, cutoff=180, **kwd):
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'info')
-        cutoff_time = datetime.utcnow() - timedelta(seconds=int(cutoff))
-        jobs = trans.sa_session.query(trans.app.model.Job) \
-                               .filter(and_(trans.app.model.Job.table.c.update_time < cutoff_time,
-                                            or_(trans.app.model.Job.state == trans.app.model.Job.states.NEW,
-                                                trans.app.model.Job.state == trans.app.model.Job.states.QUEUED,
-                                                trans.app.model.Job.state == trans.app.model.Job.states.RUNNING,
-                                                trans.app.model.Job.state == trans.app.model.Job.states.UPLOAD))) \
-                               .order_by(trans.app.model.Job.table.c.update_time.desc()).all()
-        recent_jobs = trans.sa_session.query(trans.app.model.Job) \
-            .filter(and_(trans.app.model.Job.table.c.update_time > cutoff_time,
-                         or_(trans.app.model.Job.state == trans.app.model.Job.states.ERROR,
-                             trans.app.model.Job.state == trans.app.model.Job.states.OK))) \
-            .order_by(trans.app.model.Job.table.c.update_time.desc()).all()
-
-        def prepare_jobs_list(jobs):
-            res = []
-            for job in jobs:
-                res.append({
-                    'job_info': {
-                        'id': job.id,
-                    },
-                    'id': trans.security.encode_id(job.id),
-                    'user': job.history.user.email if job.history and job.history.user else 'anonymous',
-                    'update_time': job.update_time.isoformat(),
-                    'tool_id': job.tool_id,
-                    'state': job.state,
-                    'command_line': job.command_line,
-                    'job_runner_name': job.job_runner_name,
-                    'job_runner_external_id': job.job_runner_external_id
-                })
-            return res
-        return {'jobs': prepare_jobs_list(jobs),
-                'recent_jobs': prepare_jobs_list(recent_jobs),
-                'cutoff': cutoff,
-                'message': message,
-                'status': status}
-
-    @web.expose
-    @web.require_admin
-    def job_info(self, trans, jobid=None):
-        job = None
-        if jobid is not None:
-            job = trans.sa_session.query(trans.app.model.Job).get(jobid)
-        return trans.fill_template('/webapps/reports/job_info.mako',
-                                   job=job)
-
-    @web.expose
-    @web.require_admin
-    def manage_tool_dependencies(self,
-                                 trans,
-                                 install_dependencies=False,
-                                 uninstall_dependencies=False,
-                                 remove_unused_dependencies=False,
-                                 selected_tool_ids=None,
-                                 selected_environments_to_uninstall=None,
-                                 viewkey='View tool-centric dependencies'):
-        if not selected_tool_ids:
-            selected_tool_ids = []
-        if not selected_environments_to_uninstall:
-            selected_environments_to_uninstall = []
-        tools_by_id = trans.app.toolbox.tools_by_id.copy()
-        view = six.next(six.itervalues(trans.app.toolbox.tools_by_id))._view
-        if selected_tool_ids:
-            # install the dependencies for the tools in the selected_tool_ids list
-            if not isinstance(selected_tool_ids, list):
-                selected_tool_ids = [selected_tool_ids]
-            requirements = {tools_by_id[tid].tool_requirements for tid in selected_tool_ids}
-            if install_dependencies:
-                [view.install_dependencies(r) for r in requirements]
-            elif uninstall_dependencies:
-                [view.uninstall_dependencies(index=None, requirements=r) for r in requirements]
-        if selected_environments_to_uninstall and remove_unused_dependencies:
-            if not isinstance(selected_environments_to_uninstall, list):
-                selected_environments_to_uninstall = [selected_environments_to_uninstall]
-            view.remove_unused_dependency_paths(selected_environments_to_uninstall)
-        return trans.fill_template('/webapps/galaxy/admin/manage_dependencies.mako',
-                                   tools=tools_by_id,
-                                   requirements_status=view.toolbox_requirements_status,
-                                   tool_ids_by_requirements=view.tool_ids_by_requirements,
-                                   unused_environments=view.unused_dependency_paths,
-                                   viewkey=viewkey)
-
-    @web.expose
-    @web.require_admin
-    def sanitize_whitelist(self, trans, submit_whitelist=False, tools_to_whitelist=[]):
-        if submit_whitelist:
-            # write the configured sanitize_whitelist_file with new whitelist
-            # and update in-memory list.
-            with open(trans.app.config.sanitize_whitelist_file, 'wt') as f:
-                if isinstance(tools_to_whitelist, six.string_types):
-                    tools_to_whitelist = [tools_to_whitelist]
-                new_whitelist = sorted([tid for tid in tools_to_whitelist if tid in trans.app.toolbox.tools_by_id])
-                f.write("\n".join(new_whitelist))
-            trans.app.config.sanitize_whitelist = new_whitelist
-            trans.app.queue_worker.send_control_task('reload_sanitize_whitelist', noop_self=True)
-            # dispatch a message to reload list for other processes
-        return trans.fill_template('/webapps/galaxy/admin/sanitize_whitelist.mako',
-                                   sanitize_all=trans.app.config.sanitize_all_html,
-                                   tools=trans.app.toolbox.tools_by_id)
+            return {
+                "message": f"User '{user.email}' has been updated with {len(in_roles) - 1} associated roles and {len(in_groups)} associated groups (private roles are not displayed)."
+            }
 
 
 # ---- Utility methods -------------------------------------------------------
 
+
 def build_select_input(name, label, options, value):
-    return {'type'      : 'select',
-            'multiple'  : True,
-            'optional'  : True,
-            'individual': True,
-            'name'      : name,
-            'label'     : label,
-            'options'   : options,
-            'value'     : value}
+    return {
+        "type": "select",
+        "multiple": True,
+        "optional": True,
+        "individual": True,
+        "name": name,
+        "label": label,
+        "options": options,
+        "value": value,
+    }
 
 
 def get_user(trans, user_id):
     """Get a User from the database by id."""
     user = trans.sa_session.query(trans.model.User).get(trans.security.decode_id(user_id))
     if not user:
-        return trans.show_error_message("User not found for id (%s)" % str(user_id))
+        return trans.show_error_message(f"User not found for id ({str(user_id)})")
     return user
 
 
 def get_role(trans, id):
     """Get a Role from the database by id."""
     # Load user from database
     id = trans.security.decode_id(id)
     role = trans.sa_session.query(trans.model.Role).get(id)
     if not role:
-        return trans.show_error_message("Role not found for id (%s)" % str(id))
+        return trans.show_error_message(f"Role not found for id ({str(id)})")
     return role
 
 
 def get_group(trans, id):
     """Get a Group from the database by id."""
     # Load user from database
     id = trans.security.decode_id(id)
     group = trans.sa_session.query(trans.model.Group).get(id)
     if not group:
-        return trans.show_error_message("Group not found for id (%s)" % str(id))
+        return trans.show_error_message(f"Group not found for id ({str(id)})")
     return group
 
 
 def get_quota(trans, id):
     """Get a Quota from the database by id."""
     # Load user from database
     id = trans.security.decode_id(id)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/async.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/async.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,31 +1,29 @@
 """
 Upload class
 """
 
-from __future__ import absolute_import
-
 import logging
+from urllib.parse import urlencode
 
 import requests
-from six.moves.urllib.parse import urlencode
 
-from galaxy import jobs, web
+from galaxy import web
 from galaxy.util import (
+    DEFAULT_SOCKET_TIMEOUT,
     Params,
     unicodify,
 )
 from galaxy.util.hash_util import hmac_new
 from galaxy.webapps.base.controller import BaseUIController
 
 log = logging.getLogger(__name__)
 
 
 class ASync(BaseUIController):
-
     @web.expose
     def default(self, trans, tool_id=None, data_id=None, data_secret=None, **kwd):
         """Catches the tool id and redirects as needed"""
         return self.index(trans, tool_id=tool_id, data_id=data_id, data_secret=data_secret, **kwd)
 
     @web.expose
     def index(self, trans, tool_id=None, data_secret=None, **kwd):
@@ -40,45 +38,47 @@
             return trans.response.send_redirect("/index")
 
         params = Params(kwd, sanitize=False)
         STATUS = params.STATUS
         URL = params.URL
         data_id = params.data_id
 
-        log.debug('async dataid -> %s' % data_id)
-        trans.log_event('Async dataid -> %s' % str(data_id))
+        log.debug(f"async dataid -> {data_id}")
+        trans.log_event(f"Async dataid -> {str(data_id)}")
 
         # initialize the tool
         toolbox = self.get_toolbox()
         tool = toolbox.get_tool(tool_id)
         if not tool:
-            return "Tool with id %s not found" % tool_id
+            return f"Tool with id {tool_id} not found"
 
         #
         # we have an incoming data_id
         #
         if data_id:
             if not URL:
-                return "No URL parameter was submitted for data %s" % data_id
+                return f"No URL parameter was submitted for data {data_id}"
             data = trans.sa_session.query(trans.model.HistoryDatasetAssociation).get(data_id)
 
             if not data:
-                return "Data %s does not exist or has already been deleted" % data_id
+                return f"Data {data_id} does not exist or has already been deleted"
 
-            if STATUS == 'OK':
+            if STATUS == "OK":
                 key = hmac_new(trans.app.config.tool_secret, "%d:%d" % (data.id, data.history_id))
                 if key != data_secret:
-                    return "You do not have permission to alter data %s." % data_id
+                    return f"You do not have permission to alter data {data_id}."
                 # push the job into the queue
                 data.state = data.blurb = data.states.RUNNING
-                log.debug('executing tool %s' % tool.id)
-                trans.log_event('Async executing tool %s' % tool.id, tool_id=tool.id)
-                galaxy_url = trans.request.base + '/async/%s/%s/%s' % (tool_id, data.id, key)
+                log.debug(f"executing tool {tool.id}")
+                trans.log_event(f"Async executing tool {tool.id}", tool_id=tool.id)
+                galaxy_url = f"{trans.request.url_path}/async/{tool_id}/{data.id}/{key}"
                 galaxy_url = params.get("GALAXY_URL", galaxy_url)
-                params = dict(URL=URL, GALAXY_URL=galaxy_url, name=data.name, info=data.info, dbkey=data.dbkey, data_type=data.ext)
+                params = dict(
+                    URL=URL, GALAXY_URL=galaxy_url, name=data.name, info=data.info, dbkey=data.dbkey, data_type=data.ext
+                )
 
                 # Assume there is exactly one output file possible
                 TOOL_OUTPUT_TYPE = None
                 for key, obj in tool.outputs.items():
                     try:
                         TOOL_OUTPUT_TYPE = obj.format
                         params[key] = data.id
@@ -86,33 +86,34 @@
                     except Exception:
                         # exclude outputs different from ToolOutput (e.g. collections) from the previous assumption
                         continue
                 if TOOL_OUTPUT_TYPE is None:
                     raise Exception("Error: ToolOutput object not found")
 
                 original_history = trans.sa_session.query(trans.app.model.History).get(data.history_id)
-                tool.execute(trans, incoming=params, history=original_history)
+                job, *_ = tool.execute(trans, incoming=params, history=original_history)
+                trans.app.job_manager.enqueue(job, tool=tool)
             else:
-                log.debug('async error -> %s' % STATUS)
-                trans.log_event('Async error -> %s' % STATUS)
-                data.state = data.blurb = jobs.JOB_ERROR
-                data.info = "Error -> %s" % STATUS
+                log.debug(f"async error -> {STATUS}")
+                trans.log_event(f"Async error -> {STATUS}")
+                data.state = data.blurb = "error"
+                data.info = f"Error -> {STATUS}"
 
             trans.sa_session.flush()
 
-            return "Data %s with status %s received. OK" % (data_id, STATUS)
+            return f"Data {data_id} with status {STATUS} received. OK"
         else:
             #
             # no data_id must be parameter submission
             #
             GALAXY_TYPE = None
             if params.data_type:
                 GALAXY_TYPE = params.data_type
-            elif params.galaxyFileFormat == 'wig':  # this is an undocumented legacy special case
-                GALAXY_TYPE = 'wig'
+            elif params.galaxyFileFormat == "wig":  # this is an undocumented legacy special case
+                GALAXY_TYPE = "wig"
             elif params.GALAXY_TYPE:
                 GALAXY_TYPE = params.GALAXY_TYPE
             else:
                 # Assume there is exactly one output
                 outputs_count = 0
                 for obj in tool.outputs.values():
                     try:
@@ -124,60 +125,66 @@
                         continue
                 if outputs_count > 1:
                     raise Exception("Error: the tool should have just one output")
 
             if GALAXY_TYPE is None:
                 raise Exception("Error: ToolOutput object not found")
 
-            GALAXY_NAME = params.name or params.GALAXY_NAME or '%s query' % tool.name
-            GALAXY_INFO = params.info or params.GALAXY_INFO or params.galaxyDescription or ''
-            GALAXY_BUILD = params.dbkey or params.GALAXY_BUILD or params.galaxyFreeze or '?'
+            GALAXY_NAME = params.name or params.GALAXY_NAME or f"{tool.name} query"
+            GALAXY_INFO = params.info or params.GALAXY_INFO or params.galaxyDescription or ""
+            GALAXY_BUILD = params.dbkey or params.GALAXY_BUILD or params.galaxyFreeze or "?"
 
             # data = datatypes.factory(ext=GALAXY_TYPE)()
             # data.ext   = GALAXY_TYPE
             # data.name  = GALAXY_NAME
             # data.info  = GALAXY_INFO
             # data.dbkey = GALAXY_BUILD
             # data.state = jobs.JOB_OK
             # history.datasets.add_dataset( data )
 
-            data = trans.app.model.HistoryDatasetAssociation(create_dataset=True, sa_session=trans.sa_session, extension=GALAXY_TYPE)
-            trans.app.security_agent.set_all_dataset_permissions(data.dataset, trans.app.security_agent.history_get_default_permissions(trans.history))
+            data = trans.app.model.HistoryDatasetAssociation(
+                create_dataset=True, sa_session=trans.sa_session, extension=GALAXY_TYPE
+            )
+            trans.app.security_agent.set_all_dataset_permissions(
+                data.dataset, trans.app.security_agent.history_get_default_permissions(trans.history)
+            )
             data.name = GALAXY_NAME
             data.dbkey = GALAXY_BUILD
             data.info = GALAXY_INFO
-            trans.sa_session.add(data)  # Need to add data to session before setting state (setting state requires that the data object is in the session, but this may change)
+            trans.sa_session.add(
+                data
+            )  # Need to add data to session before setting state (setting state requires that the data object is in the session, but this may change)
             data.state = data.states.NEW
             trans.history.add_dataset(data, genome_build=GALAXY_BUILD)
             trans.sa_session.add(trans.history)
             trans.sa_session.flush()
             # Need to explicitly create the file
-            data.dataset.object_store.create(data)
+            data.dataset.object_store.create(data.dataset)
             trans.log_event("Added dataset %d to history %d" % (data.id, trans.history.id), tool_id=tool_id)
 
             try:
                 key = hmac_new(trans.app.config.tool_secret, "%d:%d" % (data.id, data.history_id))
-                galaxy_url = trans.request.base + '/async/%s/%s/%s' % (tool_id, data.id, key)
-                params.update({'GALAXY_URL': galaxy_url})
-                params.update({'data_id': data.id})
+                galaxy_url = f"{trans.request.url_path}/async/{tool_id}/{data.id}/{key}"
+                params.update({"GALAXY_URL": galaxy_url})
+                params.update({"data_id": data.id})
 
                 # Use provided URL or fallback to tool action
                 url = URL or tool.action
                 # Does url already have query params?
-                if '?' in url:
-                    url_join_char = '&'
+                if "?" in url:
+                    url_join_char = "&"
                 else:
-                    url_join_char = '?'
-                url = "%s%s%s" % (url, url_join_char, urlencode(params.flatten()))
-                log.debug("connecting to -> %s" % url)
-                trans.log_event("Async connecting to -> %s" % url)
-                text = requests.get(url).text.strip()
-                if not text.endswith('OK'):
+                    url_join_char = "?"
+                url = f"{url}{url_join_char}{urlencode(params.flatten())}"
+                log.debug(f"connecting to -> {url}")
+                trans.log_event(f"Async connecting to -> {url}")
+                text = requests.get(url, timeout=DEFAULT_SOCKET_TIMEOUT).text.strip()
+                if not text.endswith("OK"):
                     raise Exception(text)
                 data.state = data.blurb = data.states.RUNNING
             except Exception as e:
                 data.info = unicodify(e)
                 data.state = data.blurb = data.states.ERROR
 
             trans.sa_session.flush()
 
-        return trans.fill_template('root/tool_runner.mako', out_data={}, num_jobs=1, job_errors=[])
+        return trans.fill_template("root/tool_runner.mako", out_data={}, num_jobs=1, job_errors=[])
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/data_manager.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/data_manager.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,246 +1,263 @@
 import logging
 from json import loads
 
 import paste.httpexceptions
-from six import string_types
 
 from galaxy import web
-from galaxy.util import nice_size, unicodify
+from galaxy.util import (
+    nice_size,
+    unicodify,
+)
 from galaxy.webapps.base.controller import BaseUIController
 
 log = logging.getLogger(__name__)
 
 
 class DataManager(BaseUIController):
-
     @web.expose
     @web.json
     def data_managers_list(self, trans, **kwd):
         not_is_admin = not trans.user_is_admin
         if not_is_admin and not trans.app.config.enable_data_manager_user_view:
-            raise paste.httpexceptions.HTTPUnauthorized("This Galaxy instance is not configured to allow non-admins to view the data manager.")
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'info')
+            raise paste.httpexceptions.HTTPUnauthorized(
+                "This Galaxy instance is not configured to allow non-admins to view the data manager."
+            )
+        message = kwd.get("message", "")
+        status = kwd.get("status", "info")
         data_managers = []
-        for data_manager_id, data_manager in sorted(trans.app.data_managers.data_managers.items(),
-                                                    key=lambda data_manager: data_manager[1].name):
-            data_managers.append({'toolUrl': web.url_for(controller='root',
-                                                         tool_id=data_manager.tool.id),
-                                  'id': data_manager_id,
-                                  'name': data_manager.name,
-                                  'description': data_manager.description.lower()})
+        for data_manager_id, data_manager in sorted(
+            trans.app.data_managers.data_managers.items(), key=lambda dm: dm[1].name
+        ):
+            data_managers.append(
+                {
+                    "toolUrl": web.url_for(controller="root", tool_id=data_manager.tool.id),
+                    "id": data_manager_id,
+                    "name": data_manager.name,
+                    "description": data_manager.description.lower(),
+                }
+            )
         data_tables = []
         managed_table_names = trans.app.data_managers.managed_data_tables.keys()
         for table_name in sorted(trans.app.tool_data_tables.get_tables().keys()):
-            data_tables.append({'name': table_name,
-                                'managed': True if table_name in managed_table_names else False})
+            data_tables.append({"name": table_name, "managed": True if table_name in managed_table_names else False})
 
-        return {'dataManagers': data_managers,
-                'dataTables': data_tables,
-                'viewOnly': not_is_admin,
-                'message': message,
-                'status': status}
+        return {
+            "dataManagers": data_managers,
+            "dataTables": data_tables,
+            "viewOnly": not_is_admin,
+            "message": message,
+            "status": status,
+        }
 
     @web.expose
     @web.json
     def jobs_list(self, trans, **kwd):
         not_is_admin = not trans.user_is_admin
         if not_is_admin and not trans.app.config.enable_data_manager_user_view:
-            raise paste.httpexceptions.HTTPUnauthorized("This Galaxy instance is not configured to allow non-admins to view the data manager.")
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'info')
-        data_manager_id = kwd.get('id', None)
+            raise paste.httpexceptions.HTTPUnauthorized(
+                "This Galaxy instance is not configured to allow non-admins to view the data manager."
+            )
+        message = kwd.get("message", "")
+        status = kwd.get("status", "info")
+        data_manager_id = kwd.get("id", None)
         data_manager = trans.app.data_managers.get_manager(data_manager_id)
         if data_manager is None:
-            return {'message': "Invalid Data Manager (%s) was requested" % data_manager_id,
-                    'status': "error"}
+            return {"message": f"Invalid Data Manager ({data_manager_id}) was requested", "status": "error"}
         jobs = []
-        for assoc in trans.sa_session.query(trans.app.model.DataManagerJobAssociation).filter_by(data_manager_id=data_manager_id):
+        for assoc in trans.sa_session.query(trans.app.model.DataManagerJobAssociation).filter_by(
+            data_manager_id=data_manager_id
+        ):
             j = assoc.job
-            jobs.append({
-                'id': j.id,
-                'encId': trans.security.encode_id(j.id),
-                'runUrl': web.url_for(controller="tool_runner", action="rerun", job_id=trans.security.encode_id(j.id)),
-                'user': j.history.user.email if j.history and j.history.user else "anonymous",
-                'updateTime': j.update_time.isoformat(),
-                'state': j.state,
-                'commandLine': j.command_line,
-                'jobRunnerName': j.job_runner_name,
-                'jobRunnerExternalId': j.job_runner_external_id
-            })
+            jobs.append(
+                {
+                    "id": j.id,
+                    "encId": trans.security.encode_id(j.id),
+                    "runUrl": web.url_for(
+                        controller="tool_runner", action="rerun", job_id=trans.security.encode_id(j.id)
+                    ),
+                    "user": j.history.user.email if j.history and j.history.user else "anonymous",
+                    "updateTime": j.update_time.isoformat(),
+                    "state": j.state,
+                    "commandLine": j.command_line,
+                    "jobRunnerName": j.job_runner_name,
+                    "jobRunnerExternalId": j.job_runner_external_id,
+                }
+            )
         jobs.reverse()
-        return {'dataManager': {'name': data_manager.name,
-                                'description': data_manager.description.lower(),
-                                'toolUrl': web.url_for(controller='root',
-                                                       tool_id=data_manager.tool.id)},
-                'jobs': jobs,
-                'viewOnly': not_is_admin,
-                'message': message,
-                'status': status}
+        return {
+            "dataManager": {
+                "name": data_manager.name,
+                "description": data_manager.description.lower(),
+                "toolUrl": web.url_for(controller="root", tool_id=data_manager.tool.id),
+            },
+            "jobs": jobs,
+            "viewOnly": not_is_admin,
+            "message": message,
+            "status": status,
+        }
 
     @web.expose
     @web.json
     def job_info(self, trans, **kwd):
         not_is_admin = not trans.user_is_admin
         if not_is_admin and not trans.app.config.enable_data_manager_user_view:
-            raise paste.httpexceptions.HTTPUnauthorized("This Galaxy instance is not configured to allow non-admins to view the data manager.")
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'info')
-        job_id = kwd.get('id', None)
+            raise paste.httpexceptions.HTTPUnauthorized(
+                "This Galaxy instance is not configured to allow non-admins to view the data manager."
+            )
+        message = kwd.get("message", "")
+        status = kwd.get("status", "info")
+        job_id = kwd.get("id", None)
         try:
             job_id = trans.security.decode_id(job_id)
             job = trans.sa_session.query(trans.app.model.Job).get(job_id)
         except Exception as e:
             job = None
-            log.error("Bad job id (%s) passed to job_info: %s" % (job_id, e))
+            log.error(f"Bad job id ({job_id}) passed to job_info: {e}")
         if not job:
-            return {'message': "Invalid job (%s) was requested" % job_id,
-                    'status': "error"}
+            return {"message": f"Invalid job ({job_id}) was requested", "status": "error"}
         data_manager_id = job.data_manager_association.data_manager_id
         data_manager = trans.app.data_managers.get_manager(data_manager_id)
         hdas = [assoc.dataset for assoc in job.get_output_datasets()]
         hda_info = []
         data_manager_output = []
         error_messages = []
         for hda in hdas:
-            hda_info.append({'id': hda.id,
-                             'encId': trans.security.encode_id(hda.id),
-                             'name': hda.name,
-                             'created': unicodify(hda.create_time.strftime(trans.app.config.pretty_datetime_format)),
-                             'fileSize': nice_size(hda.dataset.file_size),
-                             'fileName': hda.file_name,
-                             'infoUrl': web.url_for(controller='dataset',
-                                                    action='show_params',
-                                                    dataset_id=trans.security.encode_id(hda.id))})
+            hda_info.append(
+                {
+                    "id": hda.id,
+                    "encId": trans.security.encode_id(hda.id),
+                    "name": hda.name,
+                    "created": unicodify(hda.create_time.strftime(trans.app.config.pretty_datetime_format)),
+                    "fileSize": nice_size(hda.dataset.file_size),
+                    "fileName": hda.file_name,
+                    "infoUrl": web.url_for(
+                        controller="dataset", action="show_params", dataset_id=trans.security.encode_id(hda.id)
+                    ),
+                }
+            )
             try:
                 data_manager_json = loads(open(hda.get_file_name()).read())
             except Exception as e:
                 data_manager_json = {}
-                error_messages.append("Unable to obtain data_table info for hda (%s): %s" % (hda.id, e))
+                error_messages.append(f"Unable to obtain data_table info for hda ({hda.id}): {e}")
             values = []
-            for key, value in data_manager_json.get('data_tables', {}).items():
+            for key, value in data_manager_json.get("data_tables", {}).items():
                 values.append((key, value))
             data_manager_output.append(values)
-        return {'jobId': job_id,
-                'exitCode': job.exit_code,
-                'runUrl': web.url_for(controller="tool_runner",
-                                      action="rerun",
-                                      job_id=trans.security.encode_id(job.id)),
-                'commandLine': job.command_line,
-                'dataManager': {'id': data_manager_id,
-                                'name': data_manager.name,
-                                'description': data_manager.description.lower(),
-                                'toolUrl': web.url_for(controller='root',
-                                                       tool_id=data_manager.tool.id)},
-                'hdaInfo': hda_info,
-                'dataManagerOutput': data_manager_output,
-                'errorMessages': error_messages,
-                'viewOnly': not_is_admin,
-                'message': message,
-                'status': status}
+        return {
+            "jobId": job_id,
+            "exitCode": job.exit_code,
+            "runUrl": web.url_for(controller="tool_runner", action="rerun", job_id=trans.security.encode_id(job.id)),
+            "commandLine": job.command_line,
+            "dataManager": {
+                "id": data_manager_id,
+                "name": data_manager.name,
+                "description": data_manager.description.lower(),
+                "toolUrl": web.url_for(controller="root", tool_id=data_manager.tool.id),
+            },
+            "hdaInfo": hda_info,
+            "dataManagerOutput": data_manager_output,
+            "errorMessages": error_messages,
+            "viewOnly": not_is_admin,
+            "message": message,
+            "status": status,
+        }
 
     @web.expose
     @web.json
     def tool_data_table_info(self, trans, **kwd):
         return self.tool_data_table_info_1(trans, **kwd)
 
     def tool_data_table_info_1(self, trans, **kwd):
         not_is_admin = not trans.user_is_admin
         if not_is_admin and not trans.app.config.enable_data_manager_user_view:
-            raise paste.httpexceptions.HTTPUnauthorized("This Galaxy instance is not configured to allow non-admins to view the data manager.")
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'info')
-        data_table_name = kwd.get('table_name', None)
+            raise paste.httpexceptions.HTTPUnauthorized(
+                "This Galaxy instance is not configured to allow non-admins to view the data manager."
+            )
+        message = kwd.get("message", "")
+        status = kwd.get("status", "info")
+        data_table_name = kwd.get("table_name", None)
         if not data_table_name:
-            return {'message': "No data table was requested.",
-                    'status': "error"}
+            return {"message": "No data table was requested.", "status": "error"}
         data_table = trans.app.tool_data_tables.get(data_table_name, None)
         if data_table is None:
-            return {'message': "Invalid data table '%s' was requested." % data_table_name,
-                    'status': "error"}
-        return {'dataTable': {'name': data_table.name,
-                              'columns': data_table.get_column_name_list(),
-                              'data': data_table.data},
-                'viewOnly': not_is_admin,
-                'message': message,
-                'status': status}
+            return {"message": f"Invalid data table '{data_table_name}' was requested.", "status": "error"}
+        return {
+            "dataTable": {
+                "name": data_table.name,
+                "columns": data_table.get_column_name_list(),
+                "data": data_table.data,
+            },
+            "viewOnly": not_is_admin,
+            "message": message,
+            "status": status,
+        }
 
     @web.expose
     @web.json
     @web.require_admin
     def reload_tool_data_tables(self, trans, table_name=None, **kwd):
-        if table_name and isinstance(table_name, string_types):
+        if table_name and isinstance(table_name, str):
             table_name = table_name.split(",")
         # Reload the tool data tables
         table_names = self.app.tool_data_tables.reload_tables(table_names=table_name)
         trans.app.queue_worker.send_control_task(
-            'reload_tool_data_tables',
-            noop_self=True,
-            kwargs={'table_name': table_name}
+            "reload_tool_data_tables", noop_self=True, kwargs={"table_name": table_name}
         )
         data = None
         if table_names:
-            message = "Reloaded data table%s '%s'." % ('s'[len(table_names) == 1:],
-                                                       ', '.join(table_names))
-            data = self.tool_data_table_info_1(trans,
-                                               table_name=table_names[0],
-                                               message=message,
-                                               status="done")
+            message = "Reloaded data table{} '{}'.".format("s"[len(table_names) == 1 :], ", ".join(table_names))
+            data = self.tool_data_table_info_1(trans, table_name=table_names[0], message=message, status="done")
         else:
-            data = {'message': "No data tables have been reloaded.",
-                    'status': "error"}
+            data = {"message": "No data tables have been reloaded.", "status": "error"}
         return data
 
     @web.expose
     @web.json
     @web.require_admin
     def tool_data_table_items(self, trans, **kwd):
-        data = {'columns': [], 'items': []}
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'info')
-        table_name = kwd.get('table_name', None)
+        data = {"columns": [], "items": []}
+        message = kwd.get("message", "")
+        status = kwd.get("status", "info")
+        table_name = kwd.get("table_name", None)
 
         if not table_name:
             return {
-                'data': data,
-                'message': 'No Data table name provided.',
-                'status': 'warning',
+                "data": data,
+                "message": "No Data table name provided.",
+                "status": "warning",
             }
 
         data_table = trans.app.tool_data_tables.get(table_name, None)
 
         if data_table is None:
-            return {
-                'data': data,
-                'message': 'Invalid Data table (%s) was requested' % table_name,
-                'status': 'error'
-            }
+            return {"data": data, "message": f"Invalid Data table ({table_name}) was requested", "status": "error"}
 
         columns = data_table.get_column_name_list()
         rows = [dict(zip(columns, table_row)) for table_row in data_table.data]
-        data['columns'] = columns
-        data['items'] = rows
+        data["columns"] = columns
+        data["items"] = rows
 
-        return {'data': data, 'message': message, 'status': status}
+        return {"data": data, "message": message, "status": status}
 
     @web.expose
     @web.json
     @web.require_admin
     def reload_tool_data_table(self, trans, **kwd):
-        table_name = kwd.get('table_name', None)
+        table_name = kwd.get("table_name", None)
 
         if not table_name:
             return {
-                'message': 'No data table has been reloaded.',
-                'status': 'error',
+                "message": "No data table has been reloaded.",
+                "status": "error",
             }
 
         redirect_url = web.url_for(
-            controller='data_manager',
-            action='tool_data_table_items',
+            controller="data_manager",
+            action="tool_data_table_items",
             table_name=table_name,
-            message='The data table "%s" has been reloaded.' % table_name,
-            status='done',
+            message=f'The data table "{table_name}" has been reloaded.',
+            status="done",
         )
 
         return trans.response.send_redirect(redirect_url)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/forms.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/forms.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,17 +1,27 @@
 import copy
+import csv
 import logging
 import re
 
-import six
 from markupsafe import escape
 
-from galaxy import model, util
-from galaxy.web.framework.helpers import grids, iff, time_ago
-from galaxy.webapps.base.controller import BaseUIController, web
+from galaxy import (
+    model,
+    util,
+)
+from galaxy.web.framework.helpers import (
+    grids,
+    iff,
+    time_ago,
+)
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    web,
+)
 
 log = logging.getLogger(__name__)
 
 VALID_FIELDNAME_RE = re.compile(r"^[a-zA-Z0-9\_]+$")
 
 
 class FormsGrid(grids.Grid):
@@ -38,269 +48,254 @@
     title = "Forms"
     model_class = model.FormDefinitionCurrent
     default_sort_key = "-update_time"
     num_rows_per_page = 50
     use_paging = True
     default_filter = dict(deleted="False")
     columns = [
-        NameColumn("Name",
-                   key="name",
-                   model_class=model.FormDefinition,
-                   link=(lambda item: iff(item.deleted, None, dict(controller="admin", action="form/edit_form", id=item.id))),
-                   attach_popup=True,
-                   filterable="advanced"),
-        DescriptionColumn("Description",
-                          key="desc",
-                          model_class=model.FormDefinition,
-                          filterable="advanced"),
+        NameColumn(
+            "Name",
+            key="name",
+            model_class=model.FormDefinition,
+            link=(lambda item: iff(item.deleted, None, dict(controller="admin", action="form/edit_form", id=item.id))),
+            attach_popup=True,
+            filterable="advanced",
+        ),
+        DescriptionColumn("Description", key="desc", model_class=model.FormDefinition, filterable="advanced"),
         TypeColumn("Type"),
         grids.GridColumn("Last Updated", key="update_time", format=time_ago),
         StatusColumn("Status"),
-        grids.DeletedColumn("Deleted",
-                            key="deleted",
-                            visible=False,
-                            filterable="advanced")
+        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
     ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0], columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
     operations = [
         grids.GridOperation("Delete", allow_multiple=True, condition=(lambda item: not item.deleted)),
         grids.GridOperation("Undelete", condition=(lambda item: item.deleted)),
     ]
-    global_actions = [
-        grids.GridAction("Create new form", dict(controller="admin", action="form/create_form"))
-    ]
+    global_actions = [grids.GridAction("Create new form", dict(controller="admin", action="form/create_form"))]
 
     def build_initial_query(self, trans, **kwargs):
-        return trans.sa_session.query(self.model_class).join(model.FormDefinition, self.model_class.latest_form_id == model.FormDefinition.id)
+        return trans.sa_session.query(self.model_class).join(
+            model.FormDefinition, self.model_class.latest_form_id == model.FormDefinition.id
+        )
 
 
 class Forms(BaseUIController):
     forms_grid = FormsGrid()
 
     @web.legacy_expose_api
     @web.require_admin
     def forms_list(self, trans, payload=None, **kwd):
-        message = kwd.get('message', '')
-        status = kwd.get('status', '')
-        if 'operation' in kwd:
-            id = kwd.get('id')
+        message = kwd.get("message", "")
+        status = kwd.get("status", "")
+        if "operation" in kwd:
+            id = kwd.get("id")
             if not id:
-                return self.message_exception(trans, 'Invalid form id (%s) received.' % str(id))
+                return self.message_exception(trans, f"Invalid form id ({str(id)}) received.")
             ids = util.listify(id)
-            operation = kwd['operation'].lower()
-            if operation == 'delete':
+            operation = kwd["operation"].lower()
+            if operation == "delete":
                 message, status = self._delete_form(trans, ids)
-            elif operation == 'undelete':
+            elif operation == "undelete":
                 message, status = self._undelete_form(trans, ids)
         if message and status:
-            kwd['message'] = util.sanitize_text(message)
-            kwd['status'] = status
+            kwd["message"] = util.sanitize_text(message)
+            kwd["status"] = status
         return self.forms_grid(trans, **kwd)
 
     @web.legacy_expose_api
     @web.require_admin
     def create_form(self, trans, payload=None, **kwd):
-        if trans.request.method == 'GET':
-            fd_types = sorted(trans.app.model.FormDefinition.types.items())
+        if trans.request.method == "GET":
+            fd_types = sorted(trans.app.model.FormDefinition.types.__members__.items())
             return {
-                'title'         : 'Create new form',
-                'submit_title'  : 'Create',
-                'inputs'        : [{
-                    'name'    : 'name',
-                    'label'   : 'Name'
-                }, {
-                    'name'    : 'desc',
-                    'label'   : 'Description'
-                }, {
-                    'name'    : 'type',
-                    'type'    : 'select',
-                    'options' : [('None', 'none')] + [(ft[1], ft[1]) for ft in fd_types],
-                    'label'   : 'Type'
-                }, {
-                    'name'    : 'csv_file',
-                    'label'   : 'Import from CSV',
-                    'type'    : 'upload',
-                    'help'    : 'Import fields from CSV-file with the following format: Label, Help, Type, Value, Options, Required=True/False.'
-                }]
+                "title": "Create new form",
+                "inputs": [
+                    {"name": "name", "label": "Name"},
+                    {"name": "desc", "label": "Description"},
+                    {
+                        "name": "type",
+                        "type": "select",
+                        "options": [(ft[1], ft[1]) for ft in fd_types],
+                        "label": "Type",
+                    },
+                    {
+                        "name": "csv_file",
+                        "label": "Import from CSV",
+                        "type": "upload",
+                        "help": "Import fields from CSV-file with the following format: Label, Help, Type, Value, Options, Required=True/False.",
+                        "optional": True,
+                    },
+                ],
             }
         else:
             # csv-file format: label, helptext, type, default, selectlist, required '''
-            csv_file = payload.get('csv_file')
+            csv_file = payload.get("csv_file")
             index = 0
             if csv_file:
                 lines = csv_file.splitlines()
-                for line in lines:
-                    row = line.split(',')
+                rows = csv.reader(lines)
+                for row in rows:
                     if len(row) >= 6:
-                        prefix = 'fields_%i|' % index
-                        payload['%s%s' % (prefix, 'name')] = '%i_imported_field' % (index + 1)
-                        payload['%s%s' % (prefix, 'label')] = row[0]
-                        payload['%s%s' % (prefix, 'helptext')] = row[1]
-                        payload['%s%s' % (prefix, 'type')] = row[2]
-                        payload['%s%s' % (prefix, 'default')] = row[3]
-                        payload['%s%s' % (prefix, 'selectlist')] = row[4].split(',')
-                        payload['%s%s' % (prefix, 'required')] = row[5].lower() == 'true'
+                        for column in range(len(row)):
+                            row[column] = str(row[column]).strip('"')
+                        prefix = "fields_%i|" % index
+                        payload[f"{prefix}name"] = "%i_imported_field" % (index + 1)
+                        payload[f"{prefix}label"] = row[0]
+                        payload[f"{prefix}helptext"] = row[1]
+                        payload[f"{prefix}type"] = row[2]
+                        payload[f"{prefix}default"] = row[3]
+                        payload[f"{prefix}selectlist"] = row[4]
+                        payload[f"{prefix}required"] = row[5].lower() == "true"
                     index = index + 1
             new_form, message = self.save_form_definition(trans, None, payload)
             if new_form is None:
                 return self.message_exception(trans, message)
-            imported = (' with %i imported fields' % index) if index > 0 else ''
-            message = 'The form \'%s\' has been created%s.' % (payload.get('name'), imported)
-            return {'message': util.sanitize_text(message)}
+            imported = (" with %i imported fields" % index) if index > 0 else ""
+            message = f"The form '{payload.get('name')}' has been created{imported}."
+            return {"message": util.sanitize_text(message)}
 
     @web.legacy_expose_api
     @web.require_admin
     def edit_form(self, trans, payload=None, **kwd):
-        id = kwd.get('id')
+        id = kwd.get("id")
         if not id:
-            return self.message_exception(trans, 'No form id received for editing.')
+            return self.message_exception(trans, "No form id received for editing.")
         form = get_form(trans, id)
         latest_form = form.latest_form
-        if trans.request.method == 'GET':
-            fd_types = sorted(trans.app.model.FormDefinition.types.items())
-            ff_types = [(t.__name__.replace('Field', ''), t.__name__) for t in trans.model.FormDefinition.supported_field_types]
+        if trans.request.method == "GET":
+            fd_types = sorted(trans.app.model.FormDefinition.types.__members__.items())
+            ff_types = [(t.__name__, t.__name__) for t in trans.model.FormDefinition.supported_field_types]
             field_cache = []
-            field_inputs = [{
-                'name'    : 'name',
-                'label'   : 'Name',
-                'value'   : 'field_name',
-                'help'    : 'The field name must be unique for each field and must contain only alphanumeric characters and underscore.'
-            }, {
-                'name'    : 'label',
-                'label'   : 'Label',
-                'value'   : 'Field label'
-            }, {
-                'name'    : 'helptext',
-                'label'   : 'Help text'
-            }, {
-                'name'    : 'type',
-                'label'   : 'Type',
-                'type'    : 'select',
-                'options' : ff_types
-            }, {
-                'name'    : 'default',
-                'label'   : 'Default value'
-            }, {
-                'name'    : 'selectlist',
-                'label'   : 'Options',
-                'help'    : '*Only for fields which allow multiple selections, provide comma-separated values.'
-            }, {
-                'name'    : 'required',
-                'label'   : 'Required',
-                'type'    : 'boolean',
-                'value'   : 'false'
-            }]
+            field_inputs = [
+                {
+                    "name": "name",
+                    "label": "Name",
+                    "value": "field_name",
+                    "help": "The field name must be unique for each field and must contain only alphanumeric characters and underscore.",
+                },
+                {"name": "label", "label": "Label", "value": "Field label"},
+                {"name": "helptext", "label": "Help text"},
+                {"name": "type", "label": "Type", "type": "select", "options": ff_types},
+                {"name": "default", "label": "Default value"},
+                {
+                    "name": "selectlist",
+                    "label": "Options",
+                    "help": "*Only for fields which allow multiple selections, provide comma-separated values.",
+                },
+                {"name": "required", "label": "Required", "type": "boolean", "value": False},
+            ]
             form_dict = {
-                'title'  : 'Edit form for \'%s\'' % (util.sanitize_text(latest_form.name)),
-                'inputs' : [{
-                    'name'    : 'name',
-                    'label'   : 'Name',
-                    'value'   : latest_form.name
-                }, {
-                    'name'    : 'desc',
-                    'label'   : 'Description',
-                    'value'   : latest_form.desc
-                }, {
-                    'name'    : 'type',
-                    'type'    : 'select',
-                    'options' : [('None', 'none')] + [(ft[1], ft[1]) for ft in fd_types],
-                    'label'   : 'Type',
-                    'value'   : latest_form.type
-                }, {
-                    'name'    : 'fields',
-                    'title'   : 'Field',
-                    'type'    : 'repeat',
-                    'cache'   : field_cache,
-                    'inputs'  : field_inputs
-                }]
+                "title": "Edit form for '%s'" % (util.sanitize_text(latest_form.name)),
+                "inputs": [
+                    {"name": "name", "label": "Name", "value": latest_form.name},
+                    {"name": "desc", "label": "Description", "value": latest_form.desc},
+                    {
+                        "name": "type",
+                        "type": "select",
+                        "options": [(ft[1], ft[1]) for ft in fd_types],
+                        "label": "Type",
+                        "value": latest_form.type,
+                    },
+                    {
+                        "name": "fields",
+                        "title": "Field",
+                        "type": "repeat",
+                        "cache": field_cache,
+                        "inputs": field_inputs,
+                    },
+                ],
             }
             for field in latest_form.fields:
                 new_field = copy.deepcopy(field_inputs)
                 for field_input in new_field:
-                    field_value = field.get(field_input['name'])
+                    field_value = field.get(field_input["name"])
                     if field_value:
                         if isinstance(field_value, list):
-                            field_value = ','.join(field_value)
-                        field_input['value'] = str(field_value)
+                            field_value = ",".join(field_value)
+                        field_input["value"] = str(field_value)
                 field_cache.append(new_field)
             return form_dict
         else:
             new_form, message = self.save_form_definition(trans, id, payload)
             if new_form is None:
                 return self.message_exception(trans, message)
-            message = 'The form \'%s\' has been updated.' % payload.get('name')
-            return {'message': util.sanitize_text(message)}
+            message = f"The form '{payload.get('name')}' has been updated."
+            return {"message": util.sanitize_text(message)}
 
     def get_current_form(self, trans, payload=None, **kwd):
-        '''
+        """
         This method gets all the unsaved user-entered form details and returns a
         dictionary containing the name, desc, type, layout & fields of the form
-        '''
-        name = payload.get('name')
-        desc = payload.get('desc') or ''
-        type = payload.get('type')
+        """
+        name = payload.get("name")
+        desc = payload.get("desc") or ""
+        type = payload.get("type")
         fields = []
         index = 0
         while True:
-            prefix = 'fields_%i|' % index
-            if '%s%s' % (prefix, 'label') in payload:
-                field_attributes = ['name', 'label', 'helptext', 'required', 'type', 'selectlist', 'default']
-                field_dict = {attr: payload.get('%s%s' % (prefix, attr)) for attr in field_attributes}
-                field_dict['visible'] = True
-                field_dict['required'] = field_dict['required'] == 'true'
-                if isinstance(field_dict['selectlist'], six.string_types):
-                    field_dict['selectlist'] = field_dict['selectlist'].split(',')
+            prefix = "fields_%i|" % index
+            if f"{prefix}label" in payload:
+                field_attributes = ["name", "label", "helptext", "required", "type", "selectlist", "default"]
+                field_dict = {attr: payload.get(f"{prefix}{attr}") for attr in field_attributes}
+                field_dict["visible"] = True
+                if isinstance(field_dict["selectlist"], str):
+                    field_dict["selectlist"] = field_dict["selectlist"].split(",")
                 else:
-                    field_dict['selectlist'] = []
+                    field_dict["selectlist"] = []
                 fields.append(field_dict)
                 index = index + 1
             else:
                 break
-        return dict(name=name,
-                    desc=desc,
-                    type=type,
-                    layout=[],
-                    fields=fields)
+        return dict(name=name, desc=desc, type=type, layout=[], fields=fields)
 
     def save_form_definition(self, trans, form_id=None, payload=None, **kwd):
-        '''
+        """
         This method saves a form given an id
-        '''
-        if not payload.get('name'):
-            return None, 'Please provide a form name.'
-        if payload.get('type') == 'none':
-            return None, 'Please select a form type.'
+        """
+        if not payload.get("name"):
+            return None, "Please provide a form name."
+        if payload.get("type") == "none":
+            return None, "Please select a form type."
         current_form = self.get_current_form(trans, payload)
         # validate fields
         field_names_dict = {}
-        for field in current_form['fields']:
-            if not field['label']:
-                return None, 'All the field labels must be completed.'
-            if not VALID_FIELDNAME_RE.match(field['name']):
-                return None, '%s is not a valid field name.' % field['name']
-            if field['name'] in field_names_dict:
-                return None, 'Each field name must be unique in the form definition. %s is not unique.' % field['name']
+        for field in current_form["fields"]:
+            if not field["label"]:
+                return None, "All the field labels must be completed."
+            if not VALID_FIELDNAME_RE.match(field["name"]):
+                return None, f"{field['name']} is not a valid field name."
+            if field["name"] in field_names_dict:
+                return None, f"Each field name must be unique in the form definition. {field['name']} is not unique."
             else:
-                field_names_dict[field['name']] = 1
+                field_names_dict[field["name"]] = 1
         # create a new form definition
-        form_definition = trans.app.model.FormDefinition(name=current_form['name'],
-                                                         desc=current_form['desc'],
-                                                         fields=current_form['fields'],
-                                                         form_definition_current=None,
-                                                         form_type=current_form['type'],
-                                                         layout=current_form['layout'])
+        form_definition = trans.app.model.FormDefinition(
+            name=current_form["name"],
+            desc=current_form["desc"],
+            fields=current_form["fields"],
+            form_definition_current=None,
+            type=current_form["type"],
+            layout=current_form["layout"],
+        )
         # save changes to the existing form
         if form_id:
-            form_definition_current = trans.sa_session.query(trans.app.model.FormDefinitionCurrent).get(trans.security.decode_id(form_id))
+            form_definition_current = trans.sa_session.query(trans.app.model.FormDefinitionCurrent).get(
+                trans.security.decode_id(form_id)
+            )
             if form_definition_current is None:
-                return None, 'Invalid form id (%s) provided. Cannot save form.' % form_id
+                return None, f"Invalid form id ({form_id}) provided. Cannot save form."
         else:
             form_definition_current = trans.app.model.FormDefinitionCurrent()
         # create corresponding row in the form_definition_current table
         form_definition.form_definition_current = form_definition_current
         form_definition_current.latest_form = form_definition
         trans.sa_session.add(form_definition_current)
         trans.sa_session.flush()
@@ -310,28 +305,29 @@
     @web.require_admin
     def _delete_form(self, trans, ids):
         for form_id in ids:
             form = get_form(trans, form_id)
             form.deleted = True
             trans.sa_session.add(form)
             trans.sa_session.flush()
-        return ('Deleted %i form(s).' % len(ids), 'done')
+        return ("Deleted %i form(s)." % len(ids), "done")
 
     @web.expose
     @web.require_admin
     def _undelete_form(self, trans, ids):
         for form_id in ids:
             form = get_form(trans, form_id)
             form.deleted = False
             trans.sa_session.add(form)
             trans.sa_session.flush()
-        return ('Undeleted %i form(s).' % len(ids), 'done')
+        return ("Undeleted %i form(s)." % len(ids), "done")
+
 
 # ---- Utility methods -------------------------------------------------------
 
 
 def get_form(trans, form_id):
     """Get a FormDefinition from the database by id."""
     form = trans.sa_session.query(trans.app.model.FormDefinitionCurrent).get(trans.security.decode_id(form_id))
     if not form:
-        return trans.show_error_message("Form not found for id (%s)" % str(form_id))
+        return trans.show_error_message(f"Form not found for id ({str(form_id)})")
     return form
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/page.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/workflow.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,692 +1,735 @@
+import base64
+import json
+import logging
+from html.parser import HTMLParser
+from http.client import HTTPConnection
 
 from markupsafe import escape
-from sqlalchemy import (
-    and_,
-    desc,
-    false,
-    true
-)
+from sqlalchemy import desc
 from sqlalchemy.orm import (
-    eagerload,
-    undefer
+    joinedload,
+    lazyload,
+    undefer,
 )
+from sqlalchemy.sql import expression
 
 from galaxy import (
-    exceptions,
     model,
     util,
-    web
+    web,
 )
-from galaxy.managers.hdas import HDAManager
-from galaxy.managers.histories import HistoryManager, HistorySerializer
-from galaxy.managers.pages import (
-    get_page_identifiers,
-    PageContentProcessor,
-    PageManager,
+from galaxy.managers.sharable import SlugBuilder
+from galaxy.managers.workflows import (
+    MissingToolsException,
+    WorkflowUpdateOptions,
 )
 from galaxy.model.item_attrs import UsesItemRatings
-from galaxy.util import unicodify
-from galaxy.util.sanitize_html import sanitize_html
-from galaxy.web import (
-    error,
-    url_for
+from galaxy.tools.parameters.basic import workflow_building_modes
+from galaxy.util import (
+    FILENAME_VALID_CHARS,
+    unicodify,
 )
+from galaxy.util.sanitize_html import sanitize_html
+from galaxy.web import url_for
 from galaxy.web.framework.helpers import (
     grids,
-    time_ago
+    time_ago,
 )
 from galaxy.webapps.base.controller import (
     BaseUIController,
     SharableMixin,
     UsesStoredWorkflowMixin,
-    UsesVisualizationMixin
+)
+from galaxy.workflow.extract import (
+    extract_workflow,
+    summarize,
+)
+from galaxy.workflow.modules import (
+    load_module_sections,
+    module_factory,
+)
+from galaxy.workflow.render import (
+    STANDALONE_SVG_TEMPLATE,
+    WorkflowCanvas,
 )
 
-
-def format_bool(b):
-    if b:
-        return "yes"
-    else:
-        return ""
+log = logging.getLogger(__name__)
 
 
-class PageListGrid(grids.Grid):
-    # Custom column.
-    class URLColumn(grids.PublicURLColumn):
-        def get_value(self, trans, grid, item):
-            return url_for(controller='page', action='display_by_username_and_slug', username=item.user.username, slug=item.slug)
+class StoredWorkflowListGrid(grids.Grid):
+    class StepsColumn(grids.GridColumn):
+        def get_value(self, trans, grid, workflow):
+            return len(workflow.latest_workflow.steps)
 
     # Grid definition
     use_panels = True
-    title = "Pages"
-    model_class = model.Page
-    default_filter = {"published": "All", "tags": "All", "title": "All", "sharing": "All"}
+    title = "Saved Workflows"
+    model_class = model.StoredWorkflow
+    default_filter = {"name": "All", "tags": "All"}
     default_sort_key = "-update_time"
     columns = [
-        grids.TextColumn("Title", key="title", attach_popup=True, filterable="advanced", link=(lambda item: dict(action="display_by_username_and_slug", username=item.user.username, slug=item.slug))),
-        URLColumn("Public URL"),
-        grids.OwnerAnnotationColumn("Annotation", key="annotation", model_annotation_association_class=model.PageAnnotationAssociation, filterable="advanced"),
-        grids.IndividualTagsColumn("Tags", key="tags", model_tag_association_class=model.PageTagAssociation, filterable="advanced", grid_name="PageListGrid"),
-        grids.SharingStatusColumn("Sharing", key="sharing", filterable="advanced", sortable=False),
+        grids.TextColumn("Name", key="name", attach_popup=True, filterable="advanced"),
+        grids.IndividualTagsColumn(
+            "Tags",
+            "tags",
+            model_tag_association_class=model.StoredWorkflowTagAssociation,
+            filterable="advanced",
+            grid_name="StoredWorkflowListGrid",
+        ),
+        StepsColumn("Steps"),
         grids.GridColumn("Created", key="create_time", format=time_ago),
         grids.GridColumn("Last Updated", key="update_time", format=time_ago),
     ]
-    columns.append(grids.MulticolFilterColumn(
-        "Search",
-        cols_to_filter=[columns[0], columns[2]],
-        key="free-text-search", visible=False, filterable="standard"))
-    global_actions = [
-        grids.GridAction("Add new page", dict(controller="", action="pages/create"))
-    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
     operations = [
-        grids.DisplayByUsernameAndSlugGridOperation("View", allow_multiple=False),
-        grids.GridOperation("Edit content", allow_multiple=False, url_args=dict(action="edit_content")),
-        grids.GridOperation("Edit attributes", allow_multiple=False, url_args=dict(controller="", action="pages/edit")),
-        grids.GridOperation("Share or Publish", allow_multiple=False, condition=(lambda item: not item.deleted), url_args=dict(controller="", action="pages/sharing")),
-        grids.GridOperation("Delete", confirm="Are you sure you want to delete this page?"),
+        grids.GridOperation(
+            "Edit", allow_multiple=False, condition=(lambda item: not item.deleted), async_compatible=False
+        ),
+        grids.GridOperation("Run", condition=(lambda item: not item.deleted), async_compatible=False),
+        grids.GridOperation("Copy", condition=(lambda item: not item.deleted), async_compatible=False),
+        grids.GridOperation("Rename", condition=(lambda item: not item.deleted), async_compatible=False),
+        grids.GridOperation("Sharing", condition=(lambda item: not item.deleted), async_compatible=False),
+        grids.GridOperation("Delete", condition=(lambda item: item.deleted), async_compatible=True),
     ]
 
     def apply_query_filter(self, trans, query, **kwargs):
         return query.filter_by(user=trans.user, deleted=False)
 
 
-class PageAllPublishedGrid(grids.Grid):
-    # Grid definition
-    use_panels = True
-    title = "Published Pages"
-    model_class = model.Page
+class StoredWorkflowAllPublishedGrid(grids.Grid):
+    title = "Published Workflows"
+    model_class = model.StoredWorkflow
     default_sort_key = "update_time"
-    default_filter = dict(title="All", username="All")
+    default_filter = dict(public_url="All", username="All", tags="All")
     columns = [
-        grids.PublicURLColumn("Title", key="title", filterable="advanced"),
-        grids.OwnerAnnotationColumn("Annotation", key="annotation", model_annotation_association_class=model.PageAnnotationAssociation, filterable="advanced"),
+        grids.PublicURLColumn("Name", key="name", filterable="advanced", attach_popup=True),
+        grids.OwnerAnnotationColumn(
+            "Annotation",
+            key="annotation",
+            model_annotation_association_class=model.StoredWorkflowAnnotationAssociation,
+            filterable="advanced",
+        ),
         grids.OwnerColumn("Owner", key="username", model_class=model.User, filterable="advanced"),
         grids.CommunityRatingColumn("Community Rating", key="rating"),
-        grids.CommunityTagsColumn("Community Tags", key="tags", model_tag_association_class=model.PageTagAssociation, filterable="advanced", grid_name="PageAllPublishedGrid"),
-        grids.ReverseSortColumn("Last Updated", key="update_time", format=time_ago)
+        grids.CommunityTagsColumn(
+            "Community Tags",
+            key="tags",
+            model_tag_association_class=model.StoredWorkflowTagAssociation,
+            filterable="advanced",
+            grid_name="PublicWorkflowListGrid",
+        ),
+        grids.ReverseSortColumn("Last Updated", key="update_time", format=time_ago),
     ]
     columns.append(
         grids.MulticolFilterColumn(
-            "Search title, annotation, owner, and tags",
+            "Search name, annotation, owner, and tags",
             cols_to_filter=[columns[0], columns[1], columns[2], columns[4]],
-            key="free-text-search", visible=False, filterable="standard")
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
     )
+    operations = [
+        grids.GridOperation(
+            "Run",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(controller="workflows", action="run"),
+        ),
+        grids.GridOperation(
+            "Import", condition=(lambda item: not item.deleted), allow_multiple=False, url_args=dict(action="imp")
+        ),
+        grids.GridOperation(
+            "Save as File",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="export_to_file"),
+        ),
+    ]
+    num_rows_per_page = 50
+    use_paging = True
 
     def build_initial_query(self, trans, **kwargs):
         # See optimization description comments and TODO for tags in matching public histories query.
-        return trans.sa_session.query(self.model_class).join("user").options(eagerload("user").load_only("username"), eagerload("annotations"), undefer("average_rating"))
+        # In addition to that - be sure to lazyload the latest_workflow - it isn't needed and it causes all
+        # of its steps to be eagerly loaded.
+        return (
+            trans.sa_session.query(self.model_class)
+            .join(self.model_class.user)
+            .options(
+                lazyload(self.model_class.latest_workflow),
+                joinedload(self.model_class.user).load_only(model.User.username),
+                joinedload(self.model_class.annotations),
+                undefer(self.model_class.average_rating),
+            )
+        )
 
     def apply_query_filter(self, trans, query, **kwargs):
-        return query.filter(self.model_class.deleted == false()).filter(self.model_class.published == true())
+        # A public workflow is published, has a slug, and is not deleted.
+        return (
+            query.filter(self.model_class.published == expression.true())
+            .filter(self.model_class.slug.isnot(None))
+            .filter(self.model_class.deleted == expression.false())
+        )
 
 
-class ItemSelectionGrid(grids.Grid):
-    """ Base class for pages' item selection grids. """
-    # Custom columns.
-    class NameColumn(grids.TextColumn):
-        def get_value(self, trans, grid, item):
-            if hasattr(item, "get_display_name"):
-                return escape(item.get_display_name())
-            else:
-                return escape(item.name)
-
-    # Grid definition.
-    show_item_checkboxes = True
-    default_filter = {"deleted": "False", "sharing": "All"}
-    default_sort_key = "-update_time"
-    use_paging = True
-    num_rows_per_page = 10
+# Simple HTML parser to get all content in a single tag.
+class SingleTagContentsParser(HTMLParser):
+    def __init__(self, target_tag):
+        # Cannot use super() because HTMLParser is an old-style class in Python2
+        HTMLParser.__init__(self)
+        self.target_tag = target_tag
+        self.cur_tag = None
+        self.tag_content = ""
+
+    def handle_starttag(self, tag, attrs):
+        """Called for each start tag."""
+        self.cur_tag = tag
+
+    def handle_data(self, text):
+        """Called for each block of plain text."""
+        if self.cur_tag == self.target_tag:
+            self.tag_content += text
+
+
+class WorkflowController(BaseUIController, SharableMixin, UsesStoredWorkflowMixin, UsesItemRatings):
+    stored_list_grid = StoredWorkflowListGrid()
+    published_list_grid = StoredWorkflowAllPublishedGrid()
+    slug_builder = SlugBuilder()
+
+    @web.expose
+    @web.require_login("use Galaxy workflows")
+    def list_grid(self, trans, **kwargs):
+        """List user's stored workflows."""
+        # status = message = None
+        if "operation" in kwargs:
+            operation = kwargs["operation"].lower()
+            if operation == "rename":
+                return self.rename(trans, **kwargs)
+            workflow_ids = util.listify(kwargs.get("id", []))
+            if operation == "sharing":
+                return self.sharing(trans, id=workflow_ids)
+        return self.stored_list_grid(trans, **kwargs)
 
-    def apply_query_filter(self, trans, query, **kwargs):
-        return query.filter_by(user=trans.user)
+    @web.expose
+    @web.json
+    def list_published(self, trans, **kwargs):
+        return self.published_list_grid(trans, **kwargs)
 
+    @web.expose
+    def display_by_username_and_slug(self, trans, username, slug, format="html", **kwargs):
+        """
+        Display workflow based on a username and slug. Format can be html, json, or json-download.
+        """
 
-class HistorySelectionGrid(ItemSelectionGrid):
-    """ Grid for selecting histories. """
-    # Grid definition.
-    title = "Saved Histories"
-    model_class = model.History
-    columns = [
-        ItemSelectionGrid.NameColumn("Name", key="name", filterable="advanced"),
-        grids.IndividualTagsColumn("Tags", key="tags", model_tag_association_class=model.HistoryTagAssociation, filterable="advanced"),
-        grids.GridColumn("Last Updated", key="update_time", format=time_ago),
-        # Columns that are valid for filtering but are not visible.
-        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
-        grids.SharingStatusColumn("Sharing", key="sharing", filterable="advanced", sortable=False, visible=False),
-    ]
-    columns.append(
-        grids.MulticolFilterColumn(
-            "Search",
-            cols_to_filter=[columns[0], columns[1]],
-            key="free-text-search", visible=False, filterable="standard")
-    )
+        # Get workflow by username and slug. Security is handled by the display methods below.
+        session = trans.sa_session
+        user = session.query(model.User).filter_by(username=username).first()
+        if not user:
+            raise web.httpexceptions.HTTPNotFound()
+        stored_workflow = (
+            trans.sa_session.query(model.StoredWorkflow).filter_by(user=user, slug=slug, deleted=False).first()
+        )
+        if not stored_workflow:
+            raise web.httpexceptions.HTTPNotFound()
+        encoded_id = trans.security.encode_id(stored_workflow.id)
 
-    def apply_query_filter(self, trans, query, **kwargs):
-        return query.filter_by(user=trans.user, purged=False)
+        # Display workflow in requested format.
+        if format == "html":
+            return self._display(trans, stored_workflow)
+        elif format == "json":
+            return self.for_direct_import(trans, encoded_id)
+        elif format == "json-download":
+            return self.export_to_file(trans, encoded_id)
+
+    def _display(self, trans, stored_workflow):
+        """Diplay workflow in client."""
+        if stored_workflow is None:
+            raise web.httpexceptions.HTTPNotFound()
 
+        # Security check raises error if user cannot access workflow.
+        self.security_check(trans, stored_workflow, False, True)
 
-class HistoryDatasetAssociationSelectionGrid(ItemSelectionGrid):
-    """ Grid for selecting HDAs. """
-    # Grid definition.
-    title = "Saved Datasets"
-    model_class = model.HistoryDatasetAssociation
-    columns = [
-        ItemSelectionGrid.NameColumn("Name", key="name", filterable="advanced"),
-        grids.IndividualTagsColumn("Tags", key="tags", model_tag_association_class=model.HistoryDatasetAssociationTagAssociation, filterable="advanced"),
-        grids.GridColumn("Last Updated", key="update_time", format=time_ago),
-        # Columns that are valid for filtering but are not visible.
-        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
-        grids.SharingStatusColumn("Sharing", key="sharing", filterable="advanced", sortable=False, visible=False),
-    ]
-    columns.append(
-        grids.MulticolFilterColumn(
-            "Search",
-            cols_to_filter=[columns[0], columns[1]],
-            key="free-text-search", visible=False, filterable="standard")
-    )
+        # Get data for workflow's steps.
+        self.get_stored_workflow_steps(trans, stored_workflow)
 
-    def apply_query_filter(self, trans, query, **kwargs):
-        # To filter HDAs by user, need to join HDA and History table and then filter histories by user. This is necessary because HDAs do not have
-        # a user relation.
-        return query.select_from(model.HistoryDatasetAssociation.table.join(model.History.table)).filter(model.History.user == trans.user)
+        # Get annotations.
+        stored_workflow.annotation = self.get_item_annotation_str(
+            trans.sa_session, stored_workflow.user, stored_workflow
+        )
+        for step in stored_workflow.latest_workflow.steps:
+            step.annotation = self.get_item_annotation_str(trans.sa_session, stored_workflow.user, step)
 
+        # Encode page identifier.
+        workflow_id = trans.security.encode_id(stored_workflow.id)
 
-class WorkflowSelectionGrid(ItemSelectionGrid):
-    """ Grid for selecting workflows. """
-    # Grid definition.
-    title = "Saved Workflows"
-    model_class = model.StoredWorkflow
-    columns = [
-        ItemSelectionGrid.NameColumn("Name", key="name", filterable="advanced"),
-        grids.IndividualTagsColumn("Tags", key="tags", model_tag_association_class=model.StoredWorkflowTagAssociation, filterable="advanced"),
-        grids.GridColumn("Last Updated", key="update_time", format=time_ago),
-        # Columns that are valid for filtering but are not visible.
-        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
-        grids.SharingStatusColumn("Sharing", key="sharing", filterable="advanced", sortable=False, visible=False),
-    ]
-    columns.append(
-        grids.MulticolFilterColumn(
-            "Search",
-            cols_to_filter=[columns[0], columns[1]],
-            key="free-text-search", visible=False, filterable="standard")
-    )
+        # Redirect to client.
+        return trans.response.send_redirect(
+            web.url_for(
+                controller="published",
+                action="workflow",
+                id=workflow_id,
+            )
+        )
 
+    @web.expose
+    @web.require_login("to import a workflow", use_panels=True)
+    def imp(self, trans, id, **kwargs):
+        """Imports a workflow shared by other users."""
+        # Set referer message.
+        referer = trans.request.referer
+        if referer and not referer.startswith(f"{trans.request.application_url}{url_for('/login')}"):
+            referer_message = f"<a href='{escape(referer)}'>return to the previous page</a>"
+        else:
+            referer_message = f"<a href='{url_for('/')}'>go to Galaxy's start page</a>"
 
-class PageSelectionGrid(ItemSelectionGrid):
-    """ Grid for selecting pages. """
-    # Grid definition.
-    title = "Saved Pages"
-    model_class = model.Page
-    columns = [
-        grids.TextColumn("Title", key="title", filterable="advanced"),
-        grids.IndividualTagsColumn("Tags", key="tags", model_tag_association_class=model.PageTagAssociation, filterable="advanced"),
-        grids.GridColumn("Last Updated", key="update_time", format=time_ago),
-        # Columns that are valid for filtering but are not visible.
-        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
-        grids.SharingStatusColumn("Sharing", key="sharing", filterable="advanced", sortable=False, visible=False),
-    ]
-    columns.append(
-        grids.MulticolFilterColumn(
-            "Search",
-            cols_to_filter=[columns[0], columns[1]],
-            key="free-text-search", visible=False, filterable="standard")
-    )
+        # Do import.
+        stored = self.get_stored_workflow(trans, id, check_ownership=False)
+        if stored.importable is False:
+            return trans.show_error_message(
+                f"The owner of this workflow has disabled imports via this link.<br>You can {referer_message}",
+                use_panels=True,
+            )
+        elif stored.deleted:
+            return trans.show_error_message(
+                f"You can't import this workflow because it has been deleted.<br>You can {referer_message}",
+                use_panels=True,
+            )
+        self._import_shared_workflow(trans, stored)
+
+        # Redirect to load galaxy frames.
+        return trans.show_ok_message(
+            message="""Workflow "%s" has been imported. <br>You can <a href="%s">start using this workflow</a> or %s."""
+            % (stored.name, web.url_for("/workflows/list"), referer_message)
+        )
 
+    @web.expose
+    @web.require_login("use Galaxy workflows")
+    def rename_async(self, trans, id, new_name=None, **kwargs):
+        stored = self.get_stored_workflow(trans, id)
+        if new_name:
+            san_new_name = sanitize_html(new_name)
+            stored.name = san_new_name
+            stored.latest_workflow.name = san_new_name
+            trans.sa_session.flush()
+            return stored.name
 
-class VisualizationSelectionGrid(ItemSelectionGrid):
-    """ Grid for selecting visualizations. """
-    # Grid definition.
-    title = "Saved Visualizations"
-    model_class = model.Visualization
-    columns = [
-        grids.TextColumn("Title", key="title", filterable="advanced"),
-        grids.TextColumn("Type", key="type"),
-        grids.IndividualTagsColumn("Tags", key="tags", model_tag_association_class=model.VisualizationTagAssociation, filterable="advanced", grid_name="VisualizationListGrid"),
-        grids.SharingStatusColumn("Sharing", key="sharing", filterable="advanced", sortable=False),
-        grids.GridColumn("Last Updated", key="update_time", format=time_ago),
-    ]
-    columns.append(
-        grids.MulticolFilterColumn(
-            "Search",
-            cols_to_filter=[columns[0], columns[2]],
-            key="free-text-search", visible=False, filterable="standard")
-    )
+    @web.expose
+    @web.require_login("use Galaxy workflows")
+    def annotate_async(self, trans, id, new_annotation=None, **kwargs):
+        stored = self.get_stored_workflow(trans, id)
+        if new_annotation:
+            # Sanitize annotation before adding it.
+            new_annotation = sanitize_html(new_annotation)
+            self.add_item_annotation(trans.sa_session, trans.get_user(), stored, new_annotation)
+            trans.sa_session.flush()
+            return new_annotation
 
+    @web.expose
+    def get_embed_html_async(self, trans, id):
+        """Returns HTML for embedding a workflow in a page."""
 
-# Adapted from the _BaseHTMLProcessor class of https://github.com/kurtmckee/feedparser
-class PageController(BaseUIController, SharableMixin,
-                     UsesStoredWorkflowMixin, UsesVisualizationMixin, UsesItemRatings):
-
-    _page_list = PageListGrid()
-    _all_published_list = PageAllPublishedGrid()
-    _history_selection_grid = HistorySelectionGrid()
-    _workflow_selection_grid = WorkflowSelectionGrid()
-    _datasets_selection_grid = HistoryDatasetAssociationSelectionGrid()
-    _page_selection_grid = PageSelectionGrid()
-    _visualization_selection_grid = VisualizationSelectionGrid()
-
-    def __init__(self, app):
-        super(PageController, self).__init__(app)
-        self.page_manager = PageManager(app)
-        self.history_manager = HistoryManager(app)
-        self.history_serializer = HistorySerializer(self.app)
-        self.hda_manager = HDAManager(app)
+        # TODO: user should be able to embed any item he has access to. see display_by_username_and_slug for security code.
+        stored = self.get_stored_workflow(trans, id)
+        if stored:
+            return f"Embedded Workflow '{stored.name}'"
 
     @web.expose
     @web.json
-    @web.require_login()
-    def list(self, trans, *args, **kwargs):
-        """ List user's pages. """
-        # Handle operation
-        if 'operation' in kwargs and 'id' in kwargs:
-            session = trans.sa_session
-            operation = kwargs['operation'].lower()
-            ids = util.listify(kwargs['id'])
-            for id in ids:
-                item = session.query(model.Page).get(self.decode_id(id))
-                if operation == "delete":
-                    item.deleted = True
-            session.flush()
-
-        # Build grid dictionary.
-        grid = self._page_list(trans, *args, **kwargs)
-        grid['shared_by_others'] = self._get_shared(trans)
-        return grid
+    @web.require_login("use Galaxy workflows")
+    def get_name_and_link_async(self, trans, id=None):
+        """Returns workflow's name and link."""
+        stored = self.get_stored_workflow(trans, id)
+        return_dict = {
+            "name": stored.name,
+            "link": url_for(
+                controller="workflow",
+                action="display_by_username_and_slug",
+                username=stored.user.username,
+                slug=stored.slug,
+            ),
+        }
+        return return_dict
 
     @web.expose
-    @web.json
-    def list_published(self, trans, *args, **kwargs):
-        grid = self._all_published_list(trans, *args, **kwargs)
-        grid['shared_by_others'] = self._get_shared(trans)
-        return grid
-
-    def _get_shared(self, trans):
-        """Identify shared pages"""
-        shared_by_others = trans.sa_session \
-            .query(model.PageUserShareAssociation) \
-            .filter_by(user=trans.get_user()) \
-            .join(model.Page.table) \
-            .filter(model.Page.deleted == false()) \
-            .order_by(desc(model.Page.update_time)) \
-            .all()
-        return [{'username' : p.page.user.username,
-                 'slug'     : p.page.slug,
-                 'title'    : p.page.title} for p in shared_by_others]
+    @web.require_login("use Galaxy workflows")
+    def gen_image(self, trans, id):
+        stored = self.get_stored_workflow(trans, id, check_ownership=True)
+        try:
+            svg = self._workflow_to_svg_canvas(trans, stored)
+        except Exception:
+            message = (
+                "Galaxy is unable to create the SVG image. Please check your workflow, there might be missing tools."
+            )
+            return trans.show_error_message(message)
+        trans.response.set_content_type("image/svg+xml")
+        s = STANDALONE_SVG_TEMPLATE % svg.tostring()
+        return s.encode("utf-8")
 
     @web.legacy_expose_api
-    @web.require_login("create pages")
     def create(self, trans, payload=None, **kwd):
-        """
-        Create a new page.
-        """
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             return {
-                'title'  : 'Create a new page',
-                'inputs' : [{
-                    'name'      : 'title',
-                    'label'     : 'Name'
-                }, {
-                    'name'      : 'slug',
-                    'label'     : 'Identifier',
-                    'help'      : 'A unique identifier that will be used for public links to this page. This field can only contain lowercase letters, numbers, and dashes (-).'
-                }, {
-                    'name'      : 'annotation',
-                    'label'     : 'Annotation',
-                    'help'      : 'A description of the page. The annotation is shown alongside published pages.'
-                }, {
-                    'name'      : 'content_format',
-                    'label'     : 'Content Format',
-                    'type'      : 'select',
-                    'options'   : [('HTML', 'html'), ('Markdown', 'markdown')],
-                    'help'      : 'Use the traditional rich HTML editor or the newer experimental Markdown editor to create the page content. The HTML editor has several known bugs, is unmaintained and pages created with it will be read-only in future releases of Galaxy.'
-                }]
+                "title": "Create Workflow",
+                "inputs": [
+                    {"name": "workflow_name", "label": "Name", "value": "Unnamed workflow"},
+                    {
+                        "name": "workflow_annotation",
+                        "label": "Annotation",
+                        "help": "A description of the workflow; annotation is shown alongside shared or published workflows.",
+                    },
+                ],
             }
         else:
-            try:
-                page = self.page_manager.create(trans, payload)
-            except exceptions.MessageException as e:
-                return self.message_exception(trans, unicodify(e))
-            return {'message': 'Page \'%s\' successfully created.' % page.title, 'status': 'success'}
+            user = trans.get_user()
+            workflow_name = payload.get("workflow_name")
+            workflow_annotation = payload.get("workflow_annotation")
+            if not workflow_name:
+                return self.message_exception(trans, "Please provide a workflow name.")
+            # Create the new stored workflow
+            stored_workflow = model.StoredWorkflow()
+            stored_workflow.name = workflow_name
+            stored_workflow.user = user
+            self.slug_builder.create_item_slug(trans.sa_session, stored_workflow)
+            # And the first (empty) workflow revision
+            workflow = model.Workflow()
+            workflow.name = workflow_name
+            workflow.stored_workflow = stored_workflow
+            stored_workflow.latest_workflow = workflow
+            # Add annotation.
+            workflow_annotation = sanitize_html(workflow_annotation)
+            self.add_item_annotation(trans.sa_session, trans.get_user(), stored_workflow, workflow_annotation)
+            # Persist
+            session = trans.sa_session
+            session.add(stored_workflow)
+            session.flush()
+            return {
+                "id": trans.security.encode_id(stored_workflow.id),
+                "message": f"Workflow {workflow_name} has been created.",
+            }
 
-    @web.legacy_expose_api
-    @web.require_login("edit pages")
-    def edit(self, trans, payload=None, **kwd):
+    @web.json
+    def save_workflow_as(self, trans, workflow_name, workflow_data, workflow_annotation="", from_tool_form=False):
         """
-        Edit a page's attributes.
+        Creates a new workflow based on Save As command. It is a new workflow, but
+        is created with workflow_data already present.
         """
-        id = kwd.get('id')
-        if not id:
-            return self.message_exception(trans, 'No page id received for editing.')
-        decoded_id = self.decode_id(id)
         user = trans.get_user()
-        p = trans.sa_session.query(model.Page).get(decoded_id)
-        if trans.request.method == 'GET':
-            if p.slug is None:
-                self.create_item_slug(trans.sa_session, p)
-            return {
-                'title'  : 'Edit page attributes',
-                'inputs' : [{
-                    'name'      : 'title',
-                    'label'     : 'Name',
-                    'value'     : p.title
-                }, {
-                    'name'      : 'slug',
-                    'label'     : 'Identifier',
-                    'value'     : p.slug,
-                    'help'      : 'A unique identifier that will be used for public links to this page. This field can only contain lowercase letters, numbers, and dashes (-).'
-                }, {
-                    'name'      : 'annotation',
-                    'label'     : 'Annotation',
-                    'value'     : self.get_item_annotation_str(trans.sa_session, user, p),
-                    'help'      : 'A description of the page. The annotation is shown alongside published pages.'
-                }]
-            }
-        else:
-            p_title = payload.get('title')
-            p_slug = payload.get('slug')
-            p_annotation = payload.get('annotation')
-            if not p_title:
-                return self.message_exception(trans, 'Please provide a page name is required.')
-            elif not p_slug:
-                return self.message_exception(trans, 'Please provide a unique identifier.')
-            elif not self._is_valid_slug(p_slug):
-                return self.message_exception(trans, 'Page identifier can only contain lowercase letters, numbers, and dashes (-).')
-            elif p_slug != p.slug and trans.sa_session.query(model.Page).filter_by(user=p.user, slug=p_slug, deleted=False).first():
-                return self.message_exception(trans, 'Page id must be unique.')
-            else:
-                p.title = p_title
-                p.slug = p_slug
-                if p_annotation:
-                    p_annotation = sanitize_html(p_annotation)
-                    self.add_item_annotation(trans.sa_session, user, p, p_annotation)
-                trans.sa_session.add(p)
-                trans.sa_session.flush()
-            return {'message': 'Attributes of \'%s\' successfully saved.' % p.title, 'status': 'success'}
-
-    @web.expose
-    @web.require_login("edit pages")
-    def edit_content(self, trans, id):
-        """
-        Render the main page editor interface.
-        """
-        id = self.decode_id(id)
-        page = trans.sa_session.query(model.Page).get(id)
-        assert page.user == trans.user
-        return trans.fill_template("page/editor.mako", page=page)
-
-    @web.expose
-    @web.require_login("use Galaxy pages")
-    def share(self, trans, id, email="", use_panels=False):
-        """ Handle sharing with an individual user. """
-        msg = mtype = None
-        page = trans.sa_session.query(model.Page).get(self.decode_id(id))
-        if email:
-            other = trans.sa_session.query(model.User) \
-                                    .filter(and_(model.User.table.c.email == email,
-                                                 model.User.table.c.deleted == false())) \
-                                    .first()
-            if not other:
-                mtype = "error"
-                msg = ("User '%s' does not exist" % escape(email))
-            elif other == trans.get_user():
-                mtype = "error"
-                msg = ("You cannot share a page with yourself")
-            elif trans.sa_session.query(model.PageUserShareAssociation) \
-                    .filter_by(user=other, page=page).count() > 0:
-                mtype = "error"
-                msg = ("Page already shared with '%s'" % escape(email))
-            else:
-                share = model.PageUserShareAssociation()
-                share.page = page
-                share.user = other
-                session = trans.sa_session
-                session.add(share)
-                self.create_item_slug(session, page)
-                session.flush()
-                page_title = escape(page.title)
-                other_email = escape(other.email)
-                trans.set_message("Page '%s' shared with user '%s'" % (page_title, other_email))
-                return trans.response.send_redirect(url_for("/pages/sharing?id=%s" % id))
-        return trans.fill_template("/ind_share_base.mako",
-                                   message=msg,
-                                   messagetype=mtype,
-                                   item=page,
-                                   email=email,
-                                   use_panels=use_panels)
-
-    @web.expose
-    @web.require_login()
-    def display(self, trans, id):
-        id = self.decode_id(id)
-        page = trans.sa_session.query(model.Page).get(id)
-        if not page:
-            raise web.httpexceptions.HTTPNotFound()
-        return self.display_by_username_and_slug(trans, page.user.username, page.slug)
+        if workflow_name is not None:
+            workflow_contents_manager = self.app.workflow_contents_manager
+            stored_workflow = model.StoredWorkflow()
+            stored_workflow.name = workflow_name
+            stored_workflow.user = user
+            self.slug_builder.create_item_slug(trans.sa_session, stored_workflow)
+            workflow = model.Workflow()
+            workflow.name = workflow_name
+            workflow.stored_workflow = stored_workflow
+            stored_workflow.latest_workflow = workflow
+            # Add annotation.
+            workflow_annotation = sanitize_html(workflow_annotation)
+            self.add_item_annotation(trans.sa_session, trans.get_user(), stored_workflow, workflow_annotation)
 
-    @web.expose
-    def display_by_username_and_slug(self, trans, username, slug):
-        """ Display page based on a username and slug. """
-
-        # Get page.
-        session = trans.sa_session
-        user = session.query(model.User).filter_by(username=username).first()
-        page = trans.sa_session.query(model.Page).filter_by(user=user, slug=slug, deleted=False).first()
-        if page is None:
-            raise web.httpexceptions.HTTPNotFound()
-        # Security check raises error if user cannot access page.
-        self.security_check(trans, page, False, True)
-
-        latest_revision = page.latest_revision
-        if latest_revision.content_format == "html":
-            # Process page content.
-            processor = PageContentProcessor(trans, self._get_embed_html)
-            processor.feed(page.latest_revision.content)
-            # Output is string, so convert to unicode for display.
-            page_content = unicodify(processor.output(), 'utf-8')
-            template = "page/display.mako"
+            # Persist
+            session = trans.sa_session
+            session.add(stored_workflow)
+            session.flush()
+            workflow_update_options = WorkflowUpdateOptions(
+                update_stored_workflow_attributes=False,  # taken care of above
+                from_tool_form=from_tool_form,
+            )
+            try:
+                workflow, errors = workflow_contents_manager.update_workflow_from_raw_description(
+                    trans,
+                    stored_workflow,
+                    workflow_data,
+                    workflow_update_options,
+                )
+            except MissingToolsException as e:
+                return dict(
+                    name=e.workflow.name,
+                    message=(
+                        "This workflow includes missing or invalid tools. "
+                        "It cannot be saved until the following steps are removed or the missing tools are enabled."
+                    ),
+                    errors=e.errors,
+                )
+            return trans.security.encode_id(stored_workflow.id)
         else:
-            page_content = trans.security.encode_id(page.id)
-            template = "page/display_markdown.mako"
-
-        # Get rating data.
-        user_item_rating = 0
-        if trans.get_user():
-            user_item_rating = self.get_user_item_rating(trans.sa_session, trans.get_user(), page)
-            if user_item_rating:
-                user_item_rating = user_item_rating.rating
-            else:
-                user_item_rating = 0
-        ave_item_rating, num_ratings = self.get_ave_item_rating_data(trans.sa_session, page)
-
-        return trans.fill_template_mako(template, item=page,
-                                        item_data=page_content,
-                                        user_item_rating=user_item_rating,
-                                        ave_item_rating=ave_item_rating,
-                                        num_ratings=num_ratings,
-                                        content_only=True)
-
-    @web.expose
-    @web.require_login("use Galaxy pages")
-    def set_accessible_async(self, trans, id=None, accessible=False):
-        """ Set page's importable attribute and slug. """
-        page = self.get_page(trans, id)
-
-        # Only set if importable value would change; this prevents a change in the update_time unless attribute really changed.
-        importable = accessible in ['True', 'true', 't', 'T']
-        if page.importable != importable:
-            if importable:
-                self._make_item_accessible(trans.sa_session, page)
-            else:
-                page.importable = importable
-            trans.sa_session.flush()
-        return
+            # This is an error state, 'save as' must have a workflow_name
+            log.exception("Error in Save As workflow: no name.")
 
     @web.expose
-    @web.require_login("rate items")
     @web.json
-    def rate_async(self, trans, id, rating):
-        """ Rate a page asynchronously and return updated community data. """
-
-        page = self.get_page(trans, id, check_ownership=False, check_accessible=True)
-        if not page:
-            return trans.show_error_message("The specified page does not exist.")
-
-        # Rate page.
-        self.rate_item(trans.sa_session, trans.get_user(), page, rating)
-
-        return self.get_ave_item_rating_data(trans.sa_session, page)
-
-    @web.expose
-    def get_embed_html_async(self, trans, id):
-        """ Returns HTML for embedding a workflow in a page. """
-
-        # TODO: user should be able to embed any item he has access to. see display_by_username_and_slug for security code.
-        page = self.get_page(trans, id)
-        if page:
-            return "Embedded Page '%s'" % page.title
+    @web.require_login("edit workflows")
+    def editor(self, trans, id=None, workflow_id=None, version=None):
+        """
+        Render the main workflow editor interface. The canvas is embedded as
+        an iframe (necessary for scrolling to work properly), which is
+        rendered by `editor_canvas`.
+        """
+        if not id:
+            if workflow_id:
+                stored_workflow = self.app.workflow_manager.get_stored_workflow(trans, workflow_id, by_stored_id=False)
+                self.security_check(trans, stored_workflow, True, False)
+                id = trans.security.encode_id(stored_workflow.id)
+        stored = self.get_stored_workflow(trans, id)
+        # The following query loads all user-owned workflows,
+        # So that they can be copied or inserted in the workflow editor.
+        workflows = (
+            trans.sa_session.query(model.StoredWorkflow)
+            .filter_by(user=trans.user, deleted=False, hidden=False)
+            .order_by(desc(model.StoredWorkflow.table.c.update_time))
+            .options(joinedload(model.StoredWorkflow.latest_workflow).joinedload(model.Workflow.steps))
+            .all()
+        )
+        if version is None:
+            version = len(stored.workflows) - 1
+        else:
+            version = int(version)
 
-    @web.expose
-    @web.json
-    @web.require_login("use Galaxy pages")
-    def get_name_and_link_async(self, trans, id=None):
-        """ Returns page's name and link. """
-        page = self.get_page(trans, id)
+        # create workflow module models
+        module_sections = []
+        for module_section in load_module_sections(trans).values():
+            module_sections.append(
+                {
+                    "title": module_section.get("title"),
+                    "name": module_section.get("name"),
+                    "elems": [
+                        {"name": elem.get("name"), "title": elem.get("title"), "description": elem.get("description")}
+                        for elem in module_section.get("modules")
+                    ],
+                }
+            )
+
+        # create data manager tool models
+        data_managers = []
+        if trans.user_is_admin and trans.app.data_managers.data_managers:
+            for data_manager_val in trans.app.data_managers.data_managers.values():
+                tool = data_manager_val.tool
+                if not tool.hidden:
+                    data_managers.append(
+                        {
+                            "id": tool.id,
+                            "name": tool.name,
+                            "hidden": tool.hidden,
+                            "description": tool.description,
+                            "is_workflow_compatible": tool.is_workflow_compatible,
+                        }
+                    )
+
+        # create workflow models
+        workflows = [
+            {
+                "id": trans.security.encode_id(workflow.id),
+                "latest_id": trans.security.encode_id(workflow.latest_workflow.id),
+                "step_count": len(workflow.latest_workflow.steps),
+                "name": workflow.name,
+            }
+            for workflow in workflows
+            if workflow.id != stored.id
+        ]
+
+        # identify item tags
+        item_tags = stored.make_tag_string_list()
+
+        # build workflow editor model
+        editor_config = {
+            "id": trans.security.encode_id(stored.id),
+            "name": stored.name,
+            "tags": item_tags,
+            "initialVersion": version,
+            "annotation": self.get_item_annotation_str(trans.sa_session, trans.user, stored),
+            "moduleSections": module_sections,
+            "dataManagers": data_managers,
+            "workflows": workflows,
+        }
 
-        if self.create_item_slug(trans.sa_session, page):
-            trans.sa_session.flush()
-        return_dict = {"name": page.title, "link": url_for(controller='page',
-                                                           action="display_by_username_and_slug",
-                                                           username=page.user.username,
-                                                           slug=page.slug)}
-        return return_dict
+        # parse to mako
+        return editor_config
 
-    @web.expose
     @web.json
-    @web.require_login("select a history from saved histories")
-    def list_histories_for_selection(self, trans, **kwargs):
-        """ Returns HTML that enables a user to select one or more histories. """
-        return self._history_selection_grid(trans, **kwargs)
+    def load_workflow(self, trans, id, version=None):
+        """
+        Get the latest Workflow for the StoredWorkflow identified by `id` and
+        encode it as a json string that can be read by the workflow editor
+        web interface.
+        """
+        trans.workflow_building_mode = workflow_building_modes.ENABLED
+        stored = self.get_stored_workflow(trans, id, check_ownership=True, check_accessible=False)
+        workflow_contents_manager = self.app.workflow_contents_manager
+        return workflow_contents_manager.workflow_to_dict(trans, stored, style="editor", version=version)
 
     @web.expose
-    @web.json
-    @web.require_login("select a workflow from saved workflows")
-    def list_workflows_for_selection(self, trans, **kwargs):
-        """ Returns HTML that enables a user to select one or more workflows. """
-        return self._workflow_selection_grid(trans, **kwargs)
+    @web.require_login("use workflows")
+    def export_to_myexp(self, trans, id, myexp_username, myexp_password):
+        """
+        Exports a workflow to myExperiment website.
+        """
+        trans.workflow_building_mode = workflow_building_modes.ENABLED
+        stored = self.get_stored_workflow(trans, id, check_ownership=False, check_accessible=True)
 
-    @web.expose
-    @web.json
-    @web.require_login("select a visualization from saved visualizations")
-    def list_visualizations_for_selection(self, trans, **kwargs):
-        """ Returns HTML that enables a user to select one or more visualizations. """
-        return self._visualization_selection_grid(trans, **kwargs)
+        # Convert workflow to dict.
+        workflow_dict = self._workflow_to_dict(trans, stored)
 
-    @web.expose
-    @web.json
-    @web.require_login("select a page from saved pages")
-    def list_pages_for_selection(self, trans, **kwargs):
-        """ Returns HTML that enables a user to select one or more pages. """
-        return self._page_selection_grid(trans, **kwargs)
+        #
+        # Create and submit workflow myExperiment request.
+        #
+
+        # Create workflow content JSON.
+        workflow_content = json.dumps(workflow_dict, indent=4, sort_keys=True)
+
+        # Create myExperiment request.
+        request_raw = trans.fill_template(
+            "workflow/myexp_export.mako",
+            workflow_name=workflow_dict["name"],
+            workflow_description=workflow_dict["annotation"],
+            workflow_content=workflow_content,
+            workflow_svg=self._workflow_to_svg_canvas(trans, stored).tostring(),
+        )
+        # strip() b/c myExperiment XML parser doesn't allow white space before XML; utf-8 handles unicode characters.
+        request = unicodify(request_raw.strip(), "utf-8")
 
-    @web.expose
-    @web.json
-    @web.require_login("select a dataset from saved datasets")
-    def list_datasets_for_selection(self, trans, **kwargs):
-        """ Returns HTML that enables a user to select one or more datasets. """
-        return self._datasets_selection_grid(trans, **kwargs)
-
-    @web.expose
-    def get_editor_iframe(self, trans):
-        """ Returns the document for the page editor's iframe. """
-        return trans.fill_template("page/wymiframe.mako")
-
-    def get_page(self, trans, id, check_ownership=True, check_accessible=False):
-        """Get a page from the database by id."""
-        # Load history from database
-        id = self.decode_id(id)
-        page = trans.sa_session.query(model.Page).get(id)
-        if not page:
-            error("Page not found")
+        # Do request and get result.
+        auth_header = base64.b64encode(f"{myexp_username}:{myexp_password}")
+        headers = {"Content-type": "text/xml", "Accept": "text/xml", "Authorization": f"Basic {auth_header}"}
+        myexp_url = trans.app.config.myexperiment_target_url
+        conn = HTTPConnection(myexp_url)
+        # NOTE: blocks web thread.
+        conn.request("POST", "/workflow.xml", request, headers)
+        response = conn.getresponse()
+        response_data = response.read()
+        conn.close()
+
+        # Do simple parse of response to see if export successful and provide user feedback.
+        parser = SingleTagContentsParser("id")
+        parser.feed(response_data)
+        myexp_workflow_id = parser.tag_content
+        workflow_list_str = f" <br>Return to <a href='{url_for(controller='workflows', action='list')}'>workflow list."
+        if myexp_workflow_id:
+            return trans.show_message(
+                """Workflow '{}' successfully exported to myExperiment. <br/>
+                <a href="http://{}/workflows/{}">Click here to view the workflow on myExperiment</a> {}
+                """.format(
+                    stored.name, myexp_url, myexp_workflow_id, workflow_list_str
+                ),
+                use_panels=True,
+            )
         else:
-            return self.security_check(trans, page, check_ownership, check_accessible)
+            return trans.show_error_message(
+                "Workflow '%s' could not be exported to myExperiment. Error: %s %s"
+                % (stored.name, response_data, workflow_list_str),
+                use_panels=True,
+            )
 
-    def get_item(self, trans, id):
-        return self.get_page(trans, id)
+    @web.json_pretty
+    def for_direct_import(self, trans, id):
+        """
+        Get the latest Workflow for the StoredWorkflow identified by `id` and
+        encode it as a json string that can be imported back into Galaxy
 
-    def _get_embedded_history_html(self, trans, decoded_id):
+        This has slightly different information than the above. In particular,
+        it does not attempt to decode forms and build UIs, it just stores
+        the raw state.
         """
-        Returns html suitable for embedding in another page.
+        stored = self.get_stored_workflow(trans, id, check_ownership=False, check_accessible=True)
+        return self._workflow_to_dict(trans, stored)
+
+    @web.json_pretty
+    def export_to_file(self, trans, id):
         """
-        # histories embedded in pages are set to importable when embedded, check for access here
-        history = self.history_manager.get_accessible(decoded_id, trans.user, current_history=trans.history)
+        Get the latest Workflow for the StoredWorkflow identified by `id` and
+        encode it as a json string that can be imported back into Galaxy
 
-        # create ownership flag for template, dictify models
-        # note: adding original annotation since this is published - get_dict returns user-based annos
-        user_is_owner = trans.user == history.user
-        history.annotation = self.get_item_annotation_str(trans.sa_session, history.user, history)
+        This has slightly different information than the above. In particular,
+        it does not attempt to decode forms and build UIs, it just stores
+        the raw state.
+        """
 
-        # include all datasets: hidden, deleted, and purged
-        history_dictionary = self.history_serializer.serialize_to_view(
-            history, view='detailed', user=trans.user, trans=trans
-        )
-        contents = self.history_serializer.serialize_contents(history, 'contents', trans=trans, user=trans.user)
-        history_dictionary['annotation'] = history.annotation
+        # Get workflow.
+        stored = self.get_stored_workflow(trans, id, check_ownership=False, check_accessible=True)
 
-        filled = trans.fill_template("history/embed.mako",
-                                     item=history,
-                                     user_is_owner=user_is_owner,
-                                     history_dict=history_dictionary,
-                                     content_dicts=contents)
-        return filled
-
-    def _get_embedded_visualization_html(self, trans, encoded_id):
-        """
-        Returns html suitable for embedding visualizations in another page.
-        """
-        visualization = self.get_visualization(trans, encoded_id, False, True)
-        visualization.annotation = self.get_item_annotation_str(trans.sa_session, visualization.user, visualization)
-        if not visualization:
-            return None
-
-        # Fork to template based on visualization.type (registry or builtin).
-        if((trans.app.visualizations_registry and visualization.type in trans.app.visualizations_registry.plugins) and
-                (visualization.type not in trans.app.visualizations_registry.BUILT_IN_VISUALIZATIONS)):
-            # if a registry visualization, load a version into an iframe :(
-            # TODO: simplest path from A to B but not optimal - will be difficult to do reg visualizations any other way
-            # TODO: this will load the visualization twice (once above, once when the iframe src calls 'saved')
-            encoded_visualization_id = trans.security.encode_id(visualization.id)
-            return trans.fill_template('visualization/embed_in_frame.mako',
-                                       item=visualization,
-                                       encoded_visualization_id=encoded_visualization_id,
-                                       content_only=True)
-
-        return trans.fill_template("visualization/embed.mako", item=visualization, item_data=None)
-
-    def _get_embed_html(self, trans, item_class, item_id):
-        """ Returns HTML for embedding an item in a page. """
-        item_class = self.get_class(item_class)
-        encoded_id, decoded_id = get_page_identifiers(item_id, trans.app)
-        if item_class == model.History:
-            return self._get_embedded_history_html(trans, decoded_id)
-
-        elif item_class == model.HistoryDatasetAssociation:
-            dataset = self.hda_manager.get_accessible(decoded_id, trans.user)
-            dataset = self.hda_manager.error_if_uploading(dataset)
-
-            dataset.annotation = self.get_item_annotation_str(trans.sa_session, dataset.history.user, dataset)
-            if dataset:
-                data = self.hda_manager.text_data(dataset)
-                return trans.fill_template("dataset/embed.mako", item=dataset, item_data=data)
-
-        elif item_class == model.StoredWorkflow:
-            workflow = self.get_stored_workflow(trans, encoded_id, False, True)
-            workflow.annotation = self.get_item_annotation_str(trans.sa_session, workflow.user, workflow)
-            if workflow:
-                self.get_stored_workflow_steps(trans, workflow)
-                return trans.fill_template("workflow/embed.mako", item=workflow, item_data=workflow.latest_workflow.steps)
+        # Stream workflow to file.
+        stored_dict = self._workflow_to_dict(trans, stored)
+        if not stored_dict:
+            # This workflow has a tool that's missing from the distribution
+            trans.response.status = 400
+            return "Workflow cannot be exported due to missing tools."
+        sname = stored.name
+        sname = "".join(c in FILENAME_VALID_CHARS and c or "_" for c in sname)[0:150]
+        trans.response.headers["Content-Disposition"] = f'attachment; filename="Galaxy-Workflow-{sname}.ga"'
+        trans.response.set_content_type("application/galaxy-archive")
+        return stored_dict
+
+    @web.expose
+    def build_from_current_history(
+        self,
+        trans,
+        job_ids=None,
+        dataset_ids=None,
+        dataset_collection_ids=None,
+        workflow_name=None,
+        dataset_names=None,
+        dataset_collection_names=None,
+    ):
+        user = trans.get_user()
+        history = trans.get_history()
+        if not user:
+            return trans.show_error_message("Must be logged in to create workflows")
+        if (job_ids is None and dataset_ids is None) or workflow_name is None:
+            jobs, warnings = summarize(trans)
+            # Render
+            return trans.fill_template(
+                "workflow/build_from_current_history.mako", jobs=jobs, warnings=warnings, history=history
+            )
+        else:
+            # If there is just one dataset name selected or one dataset collection, these
+            # come through as string types instead of lists. xref #3247.
+            dataset_names = util.listify(dataset_names)
+            dataset_collection_names = util.listify(dataset_collection_names)
+            stored_workflow = extract_workflow(
+                trans,
+                user=user,
+                job_ids=job_ids,
+                dataset_ids=dataset_ids,
+                dataset_collection_ids=dataset_collection_ids,
+                workflow_name=workflow_name,
+                dataset_names=dataset_names,
+                dataset_collection_names=dataset_collection_names,
+            )
+            # Index page with message
+            workflow_id = trans.security.encode_id(stored_workflow.id)
+            edit_url = url_for(f"/workflows/edit?id={workflow_id}")
+            run_url = url_for(f"/workflows/run?id={workflow_id}")
+            return trans.show_message(
+                f'Workflow "{escape(workflow_name)}" created from current history. '
+                f'You can <a href="{edit_url}" target="_parent">edit</a> or <a href="{run_url}" target="_parent">run</a> the workflow.'
+            )
 
-        elif item_class == model.Visualization:
-            return self._get_embedded_visualization_html(trans, encoded_id)
+    def get_item(self, trans, id):
+        return self.get_stored_workflow(trans, id)
 
-        elif item_class == model.Page:
-            pass
+    def _workflow_to_svg_canvas(self, trans, stored):
+        workflow = stored.latest_workflow
+        workflow_canvas = WorkflowCanvas()
+        for step in workflow.steps:
+            # Load from database representation
+            module = module_factory.from_workflow_step(trans, step)
+            module_name = module.get_name()
+            module_data_inputs = module.get_data_inputs()
+            module_data_outputs = module.get_data_outputs()
+            workflow_canvas.populate_data_for_step(
+                step,
+                module_name,
+                module_data_inputs,
+                module_data_outputs,
+            )
+        workflow_canvas.add_steps()
+        return workflow_canvas.finish()
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/shed_tool_static.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/shed_tool_static.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,56 +1,58 @@
 import logging
 import os
 
 from galaxy import web
 from galaxy.exceptions import RequestParameterInvalidException
-from galaxy.util.path import join, safe_contains
+from galaxy.util.path import (
+    join,
+    safe_contains,
+)
 from galaxy.webapps.base.controller import BaseUIController
 
 log = logging.getLogger(__name__)
 
 
 def _asset_exists_and_is_safe(repo_path, asset_path):
     if not safe_contains(repo_path, asset_path):
         raise RequestParameterInvalidException()
     return os.path.exists(asset_path)
 
 
 class ShedToolStatic(BaseUIController):
-
     @web.expose
     def index(self, trans, shed, owner, repo, tool, version, image_file):
         """
         Open an image file that is contained in an installed tool shed repository or that is referenced by a URL for display.  The
         image can be defined in either a README.rst file contained in the repository or the help section of a Galaxy tool config that
         is contained in the repository.  The following image definitions are all supported.  The former $PATH_TO_IMAGES is no longer
         required, and is now ignored.
         .. image:: https://raw.github.com/galaxy/some_image.png
         .. image:: $PATH_TO_IMAGES/some_image.png
         .. image:: /static/images/some_image.gif
         .. image:: some_image.jpg
         .. image:: /deep/some_image.png
         """
-        guid = '/'.join((shed, 'repos', owner, repo, tool, version))
+        guid = "/".join((shed, "repos", owner, repo, tool, version))
         tool = trans.app.toolbox.get_tool(guid)
         repo_path = os.path.abspath(tool._repository_dir)
         found_path = None
 
-        if 'static/images' not in image_file:
-            asset_path = os.path.abspath(join(repo_path, 'static', 'images', image_file))
+        if "static/images" not in image_file:
+            asset_path = os.path.abspath(join(repo_path, "static", "images", image_file))
             if _asset_exists_and_is_safe(repo_path, asset_path):
                 found_path = asset_path
 
         if not found_path:
             asset_path = os.path.abspath(join(repo_path, image_file))
             if _asset_exists_and_is_safe(repo_path, asset_path):
                 found_path = asset_path
 
         if found_path:
-            ext = os.path.splitext(image_file)[-1].lstrip('.')
+            ext = os.path.splitext(image_file)[-1].lstrip(".")
             if ext:
                 mime = trans.app.datatypes_registry.get_mimetype_by_extension(ext)
                 if mime:
                     trans.response.set_content_type(mime)
-            return open(found_path, 'rb')
+            return open(found_path, "rb")
         else:
             raise RequestParameterInvalidException()
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/tag.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/tag.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,71 +1,77 @@
 """
 Tags Controller: handles tagging/untagging of entities
 and provides autocomplete support.
 """
 import logging
 
-from six import text_type
 from sqlalchemy.sql import select
-from sqlalchemy.sql.expression import and_, func
+from sqlalchemy.sql.expression import (
+    and_,
+    func,
+)
 
 from galaxy import web
-from galaxy.webapps.base.controller import BaseUIController, UsesTagsMixin
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    UsesTagsMixin,
+)
 
 log = logging.getLogger(__name__)
 
 
 class TagsController(BaseUIController, UsesTagsMixin):
-
     @web.expose
     @web.require_login("edit item tags")
     def get_tagging_elt_async(self, trans, item_id, item_class, elt_context=""):
         """
         Returns HTML for editing an item's tags.
         """
         item = self._get_item(trans, item_class, trans.security.decode_id(item_id))
         if not item:
-            return trans.show_error_message("No item of class %s with id %s " % (item_class, item_id))
-        return trans.fill_template("/tagging_common.mako",
-                                   tag_type="individual",
-                                   user=trans.user,
-                                   tagged_item=item,
-                                   elt_context=elt_context,
-                                   tag_click_fn="default_tag_click_fn",
-                                   use_toggle_link=False)
+            return trans.show_error_message(f"No item of class {item_class} with id {item_id} ")
+        return trans.fill_template(
+            "/tagging_common.mako",
+            tag_type="individual",
+            user=trans.user,
+            tagged_item=item,
+            elt_context=elt_context,
+            tag_click_fn="default_tag_click_fn",
+            use_toggle_link=False,
+        )
 
     @web.expose
     @web.require_login("add tag to an item")
     def add_tag_async(self, trans, item_id=None, item_class=None, new_tag=None, context=None):
         """
         Add tag to an item.
         """
         # Apply tag.
         item = self._get_item(trans, item_class, trans.security.decode_id(item_id))
         user = trans.user
         self.get_tag_handler(trans).apply_item_tags(user, item, new_tag)
         trans.sa_session.flush()
         # Log.
         params = dict(item_id=item.id, item_class=item_class, tag=new_tag)
-        trans.log_action(user, text_type("tag"), context, params)
+        trans.log_action(user, "tag", context, params)
 
     @web.expose
     @web.require_login("remove tag from an item")
     def remove_tag_async(self, trans, item_id=None, item_class=None, tag_name=None, context=None):
         """
         Remove tag from an item.
         """
         # Remove tag.
         item = self._get_item(trans, item_class, trans.security.decode_id(item_id))
         user = trans.user
         self.get_tag_handler(trans).remove_item_tag(user, item, tag_name)
         trans.sa_session.flush()
         # Log.
         params = dict(item_id=item.id, item_class=item_class, tag=tag_name)
-        trans.log_action(user, text_type("untag"), context, params)
+        trans.log_action(user, "untag", context, params)
 
     # Retag an item. All previous tags are deleted and new tags are applied.
     @web.expose
     @web.require_login("Apply a new set of tags to an item; previous tags are deleted.")
     def retag_async(self, trans, item_id=None, item_class=None, new_tags=None):
         """
         Apply a new set of tags to an item; previous tags are deleted.
@@ -85,15 +91,15 @@
         """
         # Get item, do security check, and get autocomplete data.
         item = None
         if item_id is not None:
             item = self._get_item(trans, item_class, trans.security.decode_id(item_id))
         user = trans.user
         item_class = self.get_class(item_class)
-        q = '' if q is None else q
+        q = "" if q is None else q
         if q.find(":") == -1:
             return self._get_tag_autocomplete_names(trans, q, limit, timestamp, user, item, item_class)
         else:
             return self._get_tag_autocomplete_values(trans, q, limit, timestamp, user, item, item_class)
 
     def _get_tag_autocomplete_names(self, trans, q, limit, timestamp, user=None, item=None, item_class=None):
         """
@@ -104,39 +110,42 @@
         # Get item's class object and item-tag association class.
         if item is None and item_class is None:
             raise RuntimeError("Both item and item_class cannot be None")
         elif item is not None:
             item_class = item.__class__
         item_tag_assoc_class = self.get_tag_handler(trans).get_tag_assoc_class(item_class)
         # Build select statement.
-        cols_to_select = [item_tag_assoc_class.table.c.tag_id, func.count('*')]
+        cols_to_select = [item_tag_assoc_class.table.c.tag_id, func.count("*")]
         from_obj = item_tag_assoc_class.table.join(item_class.table).join(trans.app.model.Tag.table)
-        where_clause = and_(trans.app.model.Tag.table.c.name.like(q + "%"),
-                            item_tag_assoc_class.table.c.user_id == user.id)
+        where_clause = and_(
+            trans.app.model.Tag.table.c.name.like(f"{q}%"), item_tag_assoc_class.table.c.user_id == user.id
+        )
         order_by = [func.count("*").desc()]
         group_by = item_tag_assoc_class.table.c.tag_id
         # Do query and get result set.
-        query = select(columns=cols_to_select,
-                       from_obj=from_obj,
-                       whereclause=where_clause,
-                       group_by=group_by,
-                       order_by=order_by,
-                       limit=limit)
+        query = select(
+            columns=cols_to_select,
+            from_obj=from_obj,
+            whereclause=where_clause,
+            group_by=group_by,
+            order_by=order_by,
+            limit=limit,
+        )
         result_set = trans.sa_session.execute(query)
         # Create and return autocomplete data.
         ac_data = "#Header|Your Tags\n"
         for row in result_set:
             tag = self.get_tag_handler(trans).get_tag_by_id(row[0])
             # Exclude tags that are already applied to the item.
             if (item is not None) and (self.get_tag_handler(trans).item_has_tag(trans.user, item, tag)):
                 continue
             # Add tag to autocomplete data. Use the most frequent name that user
             # has employed for the tag.
             tag_names = self._get_usernames_for_tag(trans, trans.user, tag, item_class, item_tag_assoc_class)
-            ac_data += tag_names[0] + "|" + tag_names[0] + "\n"
+            ac_data += f"{tag_names[0]}|{tag_names[0]}\n"
         return ac_data
 
     def _get_tag_autocomplete_values(self, trans, q, limit, timestamp, user=None, item=None, item_class=None):
         """
         Returns autocomplete data for tag values ordered from most frequently used to
         least frequently used.
         """
@@ -150,52 +159,54 @@
         # Get item's class object and item-tag association class.
         if item is None and item_class is None:
             raise RuntimeError("Both item and item_class cannot be None")
         elif item is not None:
             item_class = item.__class__
         item_tag_assoc_class = self.get_tag_handler(trans).get_tag_assoc_class(item_class)
         # Build select statement.
-        cols_to_select = [item_tag_assoc_class.table.c.value, func.count('*')]
+        cols_to_select = [item_tag_assoc_class.table.c.value, func.count("*")]
         from_obj = item_tag_assoc_class.table.join(item_class.table).join(trans.app.model.Tag.table)
-        where_clause = and_(item_tag_assoc_class.table.c.user_id == user.id,
-                            trans.app.model.Tag.table.c.id == tag.id,
-                            item_tag_assoc_class.table.c.value.like(tag_value + "%"))
+        where_clause = and_(
+            item_tag_assoc_class.table.c.user_id == user.id,
+            trans.app.model.Tag.table.c.id == tag.id,
+            item_tag_assoc_class.table.c.value.like(f"{tag_value}%"),
+        )
         order_by = [func.count("*").desc(), item_tag_assoc_class.table.c.value]
         group_by = item_tag_assoc_class.table.c.value
         # Do query and get result set.
-        query = select(columns=cols_to_select,
-                       from_obj=from_obj,
-                       whereclause=where_clause,
-                       group_by=group_by,
-                       order_by=order_by,
-                       limit=limit)
+        query = select(
+            columns=cols_to_select,
+            from_obj=from_obj,
+            whereclause=where_clause,
+            group_by=group_by,
+            order_by=order_by,
+            limit=limit,
+        )
         result_set = trans.sa_session.execute(query)
         # Create and return autocomplete data.
-        ac_data = "#Header|Your Values for '%s'\n" % (tag_name)
+        ac_data = f"#Header|Your Values for '{tag_name}'\n"
         tag_uname = self._get_usernames_for_tag(trans, trans.user, tag, item_class, item_tag_assoc_class)[0]
         for row in result_set:
-            ac_data += tag_uname + ":" + row[0] + "|" + row[0] + "\n"
+            ac_data += f"{tag_uname}:{row[0]}|{row[0]}\n"
         return ac_data
 
     def _get_usernames_for_tag(self, trans, user, tag, item_class, item_tag_assoc_class):
         """
         Returns an ordered list of the user names for a tag; list is ordered from
         most popular to least popular name.
         """
         # Build select stmt.
-        cols_to_select = [item_tag_assoc_class.table.c.user_tname, func.count('*')]
-        where_clause = and_(item_tag_assoc_class.table.c.user_id == user.id,
-                            item_tag_assoc_class.table.c.tag_id == tag.id)
+        cols_to_select = [item_tag_assoc_class.table.c.user_tname, func.count("*")]
+        where_clause = and_(
+            item_tag_assoc_class.table.c.user_id == user.id, item_tag_assoc_class.table.c.tag_id == tag.id
+        )
         group_by = item_tag_assoc_class.table.c.user_tname
         order_by = [func.count("*").desc()]
         # Do query and get result set.
-        query = select(columns=cols_to_select,
-                       whereclause=where_clause,
-                       group_by=group_by,
-                       order_by=order_by)
+        query = select(columns=cols_to_select, whereclause=where_clause, group_by=group_by, order_by=order_by)
         result_set = trans.sa_session.execute(query)
         user_tag_names = list()
         for row in result_set:
             user_tag_names.append(row[0])
         return user_tag_names
 
     def _get_item(self, trans, item_class_name, id):
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/tool_runner.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/tool_runner.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,88 +4,97 @@
 import logging
 
 from markupsafe import escape
 
 import galaxy.util
 from galaxy import web
 from galaxy.tools import DataSourceTool
-from galaxy.web import error, url_for
+from galaxy.web import (
+    error,
+    url_for,
+)
 from galaxy.webapps.base.controller import BaseUIController
 
 log = logging.getLogger(__name__)
 
 
 class ToolRunner(BaseUIController):
-
     # Hack to get biomart to work, ideally, we could pass tool_id to biomart and receive it back
     @web.expose
-    def biomart(self, trans, tool_id='biomart', **kwd):
+    def biomart(self, trans, tool_id="biomart", **kwd):
         """Catches the tool id and redirects as needed"""
         return self.index(trans, tool_id=tool_id, **kwd)
 
     # test to get hapmap to work, ideally, we could pass tool_id to hapmap biomart and receive it back
     @web.expose
-    def hapmapmart(self, trans, tool_id='hapmapmart', **kwd):
+    def hapmapmart(self, trans, tool_id="hapmapmart", **kwd):
         """Catches the tool id and redirects as needed"""
         return self.index(trans, tool_id=tool_id, **kwd)
 
     @web.expose
     def default(self, trans, tool_id=None, **kwd):
         """Catches the tool id and redirects as needed"""
         return self.index(trans, tool_id=tool_id, **kwd)
 
     def __get_tool(self, tool_id, tool_version=None, get_loaded_tools_by_lineage=False, set_selected=False):
-        tool_version_select_field, tools, tool = self.get_toolbox().get_tool_components(tool_id, tool_version, get_loaded_tools_by_lineage, set_selected)
+        tool_version_select_field, tools, tool = self.get_toolbox().get_tool_components(
+            tool_id, tool_version, get_loaded_tools_by_lineage, set_selected
+        )
         return tool
 
     @web.expose
     def index(self, trans, tool_id=None, from_noframe=None, **kwd):
         def __tool_404__():
-            log.error('index called with tool id \'%s\' but no such tool exists', tool_id)
-            trans.log_event('Tool id \'%s\' does not exist' % tool_id)
+            log.debug("index called with tool id '%s' but no such tool exists", tool_id)
+            trans.log_event("Tool id '%s' does not exist" % tool_id)
             trans.response.status = 404
-            return trans.show_error_message('Tool \'%s\' does not exist.' % (escape(tool_id)))
+            return trans.show_error_message("Tool '%s' does not exist." % (escape(tool_id)))
+
         # tool id not available, redirect to main page
         if tool_id is None:
-            return trans.response.send_redirect(url_for(controller='root', action='welcome'))
+            return trans.response.send_redirect(url_for(controller="root", action="welcome"))
         tool = self.__get_tool(tool_id)
         # tool id is not matching, display an error
         if not tool:
             return __tool_404__()
         if tool.require_login and not trans.user:
-            redirect = url_for(controller='tool_runner', action='index', tool_id=tool_id, **kwd)
-            return trans.response.send_redirect(url_for(controller='user',
-                                                        action='login',
-                                                        cntrller='user',
-                                                        status='info',
-                                                        message='You must be logged in to use this tool.',
-                                                        redirect=redirect))
+            redirect = url_for(controller="tool_runner", action="index", tool_id=tool_id, **kwd)
+            return trans.response.send_redirect(
+                url_for(
+                    controller="user",
+                    action="login",
+                    cntrller="user",
+                    status="info",
+                    message="You must be logged in to use this tool.",
+                    redirect=redirect,
+                )
+            )
         if not tool.allow_user_access(trans.user):
             return __tool_404__()
         # FIXME: Tool class should define behavior
-        if tool.tool_type in ['default', 'interactivetool']:
-            return trans.response.send_redirect(url_for(controller='root', tool_id=tool_id))
+        if tool.tool_type in ["default", "interactivetool"]:
+            return trans.response.send_redirect(url_for(controller="root", tool_id=tool_id))
 
         # execute tool without displaying form (used for datasource tools)
         params = galaxy.util.Params(kwd, sanitize=False)
         # do param translation here, used by datasource tools
         if tool.input_translator:
             tool.input_translator.translate(params)
-        if 'runtool_btn' not in params.__dict__ and 'URL' not in params.__dict__:
-            error('Tool execution through the `tool_runner` requires a `runtool_btn` flag or `URL` parameter.')
+        if "runtool_btn" not in params.__dict__ and "URL" not in params.__dict__:
+            error("Tool execution through the `tool_runner` requires a `runtool_btn` flag or `URL` parameter.")
         # We may be visiting Galaxy for the first time ( e.g., sending data from UCSC ),
         # so make sure to create a new history if we've never had one before.
         history = tool.get_default_history_by_trans(trans, create=True)
         try:
             vars = tool.handle_input(trans, params.__dict__, history=history)
         except Exception as e:
             error(galaxy.util.unicodify(e))
         if len(params) > 0:
-            trans.log_event('Tool params: %s' % (str(params)), tool_id=tool_id)
-        return trans.fill_template('root/tool_runner.mako', **vars)
+            trans.log_event(f"Tool params: {str(params)}", tool_id=tool_id)
+        return trans.fill_template("root/tool_runner.mako", **vars)
 
     @web.expose
     def rerun(self, trans, id=None, job_id=None, **kwd):
         """
         Given a HistoryDatasetAssociation id, find the job and that created
         the dataset, extract the parameters, and display the appropriate tool
         form with parameters already filled in.
@@ -100,15 +109,18 @@
                 try:
                     id = trans.security.decode_id(id)
                 except Exception:
                     error("Invalid value for 'id' parameter")
             # Get the dataset object
             data = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(id)
             # only allow rerunning if user is allowed access to the dataset.
-            if not (trans.user_is_admin or trans.app.security_agent.can_access_dataset(trans.get_current_user_roles(), data.dataset)):
+            if not (
+                trans.user_is_admin
+                or trans.app.security_agent.can_access_dataset(trans.get_current_user_roles(), data.dataset)
+            ):
                 error("You are not allowed to access this dataset")
             # Get the associated job, if any.
             job = data.creating_job
             if job:
                 job_id = trans.security.encode_id(job.id)
             else:
                 raise Exception("Failed to get job information for dataset hid %d" % data.hid)
@@ -126,23 +138,16 @@
         """
         if tool_id is None:
             return trans.response.send_redirect(url_for(controller="root", action="welcome"))
         tool = self.__get_tool(tool_id)
         # No tool matching the tool id, display an error (shouldn't happen)
         if not tool:
             log.error("data_source_redirect called with tool id '%s' but no such tool exists", tool_id)
-            trans.log_event("Tool id '%s' does not exist" % tool_id)
+            trans.log_event(f"Tool id '{tool_id}' does not exist")
             trans.response.status = 404
-            return trans.show_error_message("Tool '%s' does not exist." % (escape(tool_id)))
+            return trans.show_error_message(f"Tool '{escape(tool_id)}' does not exist.")
 
         if isinstance(tool, DataSourceTool):
             link = url_for(tool.action, **tool.get_static_param_values(trans))
         else:
-            link = url_for(controller='tool_runner', tool_id=tool.id)
+            link = url_for(controller="tool_runner", tool_id=tool.id)
         return trans.response.send_redirect(link)
-
-    @web.expose
-    def redirect(self, trans, redirect_url=None, **kwd):
-        if not redirect_url:
-            return trans.show_error_message("Required URL for redirection missing")
-        trans.log_event("Redirecting to: %s" % redirect_url)
-        return trans.fill_template('root/redirect.mako', redirect_url=redirect_url)
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/user.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/user.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,78 +1,93 @@
 """
 Contains the user interface in the Universe class
 """
 
 import logging
-from datetime import datetime, timedelta
+from datetime import (
+    datetime,
+    timedelta,
+)
+from urllib.parse import unquote
 
 from markupsafe import escape
-from six.moves.urllib.parse import unquote
-from sqlalchemy import (
-    func,
-    or_
-)
 from sqlalchemy.orm.exc import NoResultFound
 
 from galaxy import (
     util,
-    web
+    web,
 )
 from galaxy.exceptions import Conflict
 from galaxy.managers import users
-from galaxy.queue_worker import send_local_control_task
 from galaxy.security.validate_user_input import (
     validate_email,
-    validate_publicname
+    validate_publicname,
+)
+from galaxy.structured_app import StructuredApp
+from galaxy.web import (
+    expose_api_anonymous_and_sessionless,
+    url_for,
 )
-from galaxy.web import expose_api_anonymous_and_sessionless
-from galaxy.web import url_for
 from galaxy.webapps.base.controller import (
     BaseUIController,
-    CreatesApiKeysMixin,
-    UsesFormDefinitionsMixin
+    UsesFormDefinitionsMixin,
 )
+from ..api import depends
 
 log = logging.getLogger(__name__)
 
 
 def _filtered_registration_params_dict(payload):
-    return {k: v for (k, v) in payload.items() if k in ['email', 'username', 'password', 'confirm', 'subscribe']}
+    return {k: v for (k, v) in payload.items() if k in ["email", "username", "password", "confirm", "subscribe"]}
 
 
-class User(BaseUIController, UsesFormDefinitionsMixin, CreatesApiKeysMixin):
+class User(BaseUIController, UsesFormDefinitionsMixin):
+    user_manager: users.UserManager = depends(users.UserManager)
     installed_len_files = None
 
-    def __init__(self, app):
-        super(User, self).__init__(app)
-        self.user_manager = users.UserManager(app)
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
 
-    def __handle_role_and_group_auto_creation(self, trans, user, roles, auto_create_roles=False,
-                                              auto_create_groups=False, auto_assign_roles_to_groups_only=False):
+    def __handle_role_and_group_auto_creation(
+        self,
+        trans,
+        user,
+        roles,
+        auto_create_roles=False,
+        auto_create_groups=False,
+        auto_assign_roles_to_groups_only=False,
+    ):
         for role_name in roles:
             role = None
             group = None
             if auto_create_roles:
                 try:
                     # first try to find the role
                     role = trans.app.security_agent.get_role(role_name)
                 except NoResultFound:
                     # or create it
                     role, num_in_groups = trans.app.security_agent.create_role(
-                        role_name, "Auto created upon user registration", [], [],
-                        create_group_for_role=auto_create_groups)
+                        role_name,
+                        "Auto created upon user registration",
+                        [],
+                        [],
+                        create_group_for_role=auto_create_groups,
+                    )
                     if auto_create_groups:
                         trans.log_event("Created role and group for auto-registered user.")
                     else:
                         trans.log_event("Created role for auto-registered user.")
             if auto_create_groups:
                 # only create a group if not existing yet
                 try:
-                    group = self.sa_session.query(trans.app.model.Group).filter(
-                        trans.app.model.Group.table.c.name == role_name).first()
+                    group = (
+                        self.sa_session.query(trans.app.model.Group)
+                        .filter(trans.app.model.Group.name == role_name)
+                        .first()
+                    )
                 except NoResultFound:
                     group = self.model.Group(name=role_name)
                     self.sa_session.add(group)
                 trans.app.security_agent.associate_user_group(user, group)
 
             if auto_assign_roles_to_groups_only and group and role:
                 trans.log_event("Assigning role to group only")
@@ -84,107 +99,113 @@
     def __autoregistration(self, trans, login, password):
         """
         Does the autoregistration if enabled. Returns a message
         """
         try:
             autoreg = trans.app.auth_manager.check_auto_registration(trans, login, password)
         except Conflict as conflict:
-            return "Auto-registration failed, {}".format(conflict), None
+            return f"Auto-registration failed, {conflict}", None
         user = None
         if autoreg["auto_reg"]:
             email = autoreg["email"]
             username = autoreg["username"]
-            message = " ".join((validate_email(trans, email, allow_empty=True),
-                                validate_publicname(trans, username))).rstrip()
+            message = " ".join(
+                (validate_email(trans, email, allow_empty=True), validate_publicname(trans, username))
+            ).rstrip()
             if not message:
                 user = self.user_manager.create(email=email, username=username, password="")
                 if trans.app.config.user_activation_on:
                     self.user_manager.send_activation_email(trans, email, username)
                 # The handle_user_login() method has a call to the history_set_default_permissions() method
                 # (needed when logging in with a history), user needs to have default permissions set before logging in
                 if not trans.user_is_admin:
                     trans.handle_user_login(user)
                     trans.log_event("User (auto) created a new account")
                     trans.log_event("User logged in")
                 if "attributes" in autoreg and "roles" in autoreg["attributes"]:
                     self.__handle_role_and_group_auto_creation(
-                        trans, user, autoreg["attributes"]["roles"],
+                        trans,
+                        user,
+                        autoreg["attributes"]["roles"],
                         auto_create_groups=autoreg["auto_create_groups"],
                         auto_create_roles=autoreg["auto_create_roles"],
-                        auto_assign_roles_to_groups_only=autoreg["auto_assign_roles_to_groups_only"])
+                        auto_assign_roles_to_groups_only=autoreg["auto_assign_roles_to_groups_only"],
+                    )
             else:
-                message = "Auto-registration failed, contact your local Galaxy administrator. %s" % message
+                message = f"Auto-registration failed, contact your local Galaxy administrator. {message}"
         else:
             message = "No such user or invalid password."
         return message, user
 
     @expose_api_anonymous_and_sessionless
-    def login(self, trans, payload={}, **kwd):
+    def login(self, trans, payload=None, **kwd):
+        payload = payload or {}
         return self.__validate_login(trans, payload, **kwd)
 
-    def __validate_login(self, trans, payload={}, **kwd):
-        '''Handle Galaxy Log in'''
+    def __validate_login(self, trans, payload=None, **kwd):
+        """Handle Galaxy Log in"""
         if not payload:
             payload = kwd
         message = trans.check_csrf_token(payload)
         if message:
             return self.message_exception(trans, message)
         login = payload.get("login")
         password = payload.get("password")
         redirect = payload.get("redirect")
         status = None
         if not login or not password:
             return self.message_exception(trans, "Please specify a username and password.")
-        user = trans.sa_session.query(trans.app.model.User).filter(or_(
-            trans.app.model.User.table.c.email == login,
-            trans.app.model.User.table.c.username == login
-        )).first()
-        if not user and login.lower() != login:
-            user = trans.sa_session.query(trans.app.model.User).filter(
-                func.lower(trans.app.model.User.table.c.email) == login.lower()
-            ).first()
-        log.debug("trans.app.config.auth_config_file: %s" % trans.app.config.auth_config_file)
+        user = self.user_manager.get_user_by_identity(login)
+        log.debug(f"trans.app.config.auth_config_file: {trans.app.config.auth_config_file}")
         if user is None:
             message, user = self.__autoregistration(trans, login, password)
             if message:
                 return self.message_exception(trans, message)
         elif user.deleted:
-            message = "This account has been marked deleted, contact your local Galaxy administrator to restore the account."
+            message = (
+                "This account has been marked deleted, contact your local Galaxy administrator to restore the account."
+            )
             if trans.app.config.error_email_to is not None:
-                message += " Contact: %s." % trans.app.config.error_email_to
+                message += f" Contact: {trans.app.config.error_email_to}."
             return self.message_exception(trans, message, sanitize=False)
         elif user.external:
             message = "This account was created for use with an external authentication method, contact your local Galaxy administrator to activate it."
             if trans.app.config.error_email_to is not None:
-                message += " Contact: %s." % trans.app.config.error_email_to
+                message += f" Contact: {trans.app.config.error_email_to}."
             return self.message_exception(trans, message, sanitize=False)
-        elif not trans.app.auth_manager.check_password(user, password):
+        elif not trans.app.auth_manager.check_password(user, password, trans.request):
             return self.message_exception(trans, "Invalid password.")
         elif trans.app.config.user_activation_on and not user.active:  # activation is ON and the user is INACTIVE
-            if (trans.app.config.activation_grace_period != 0):  # grace period is ON
-                if self.is_outside_grace_period(trans, user.create_time):  # User is outside the grace period. Login is disabled and he will have the activation email resent.
+            if trans.app.config.activation_grace_period != 0:  # grace period is ON
+                if self.is_outside_grace_period(
+                    trans, user.create_time
+                ):  # User is outside the grace period. Login is disabled and he will have the activation email resent.
                     message, status = self.resend_activation_email(trans, user.email, user.username)
                     return self.message_exception(trans, message, sanitize=False)
                 else:  # User is within the grace period, let him log in.
                     trans.handle_user_login(user)
                     trans.log_event("User logged in")
             else:  # Grace period is off. Login is disabled and user will have the activation email resent.
                 message, status = self.resend_activation_email(trans, user.email, user.username)
                 return self.message_exception(trans, message, sanitize=False)
         else:  # activation is OFF
             pw_expires = trans.app.config.password_expiration_period
             if pw_expires and user.last_password_change < datetime.today() - pw_expires:
                 # Password is expired, we don't log them in.
-                return {"message": "Your password has expired. Please reset or change it to access Galaxy.", "status": "warning", "expired_user": trans.security.encode_id(user.id)}
+                return {
+                    "message": "Your password has expired. Please reset or change it to access Galaxy.",
+                    "status": "warning",
+                    "expired_user": trans.security.encode_id(user.id),
+                }
             trans.handle_user_login(user)
             trans.log_event("User logged in")
             if pw_expires and user.last_password_change < datetime.today() - timedelta(days=pw_expires.days / 10):
                 # If password is about to expire, modify message to state that.
                 expiredate = datetime.today() - user.last_password_change + pw_expires
-                return {"message": "Your password will expire in %s day(s)." % expiredate.days, "status": "warning"}
+                return {"message": f"Your password will expire in {expiredate.days} day(s).", "status": "warning"}
         return {"message": "Success.", "redirect": self.__get_redirect_url(redirect)}
 
     @web.expose
     def resend_verification(self, trans):
         """
         Exposed function for use outside of the class. E.g. when user click on the resend link in the masthead.
         """
@@ -196,55 +217,53 @@
 
     def resend_activation_email(self, trans, email, username):
         """
         Function resends the verification email in case user wants to log in with an inactive account or he clicks the resend link.
         """
         if email is None:  # User is coming from outside registration form, load email from trans
             if not trans.user:
-                trans.show_error_message("No session found, cannot send activation email.")
+                return "No session found, cannot send activation email.", None
             email = trans.user.email
         if username is None:  # User is coming from outside registration form, load email from trans
             username = trans.user.username
         is_activation_sent = self.user_manager.send_activation_email(trans, email, username)
         if is_activation_sent:
-            message = 'This account has not been activated yet. The activation link has been sent again. Please check your email address <b>%s</b> including the spam/trash folder. <a target="_top" href="%s">Return to the home page</a>.' % (escape(email), url_for('/'))
+            message = f"This account has not been activated yet. The activation link has been sent again. Please check your email address <b>{escape(email)}</b> including the spam/trash folder. <a target=\"_top\" href=\"{url_for('/')}\">Return to the home page</a>."
         else:
-            message = 'This account has not been activated yet but we are unable to send the activation link. Please contact your local Galaxy administrator. <a target="_top" href="%s">Return to the home page</a>.' % url_for('/')
+            message = f"This account has not been activated yet but we are unable to send the activation link. Please contact your local Galaxy administrator. <a target=\"_top\" href=\"{url_for('/')}\">Return to the home page</a>."
             if trans.app.config.error_email_to is not None:
-                message += ' Error contact: %s.' % trans.app.config.error_email_to
+                message += f" Error contact: {trans.app.config.error_email_to}."
         return message, is_activation_sent
 
     def is_outside_grace_period(self, trans, create_time):
         """
         Function checks whether the user is outside the config-defined grace period for inactive accounts.
         """
         #  Activation is forced and the user is not active yet. Check the grace period.
         activation_grace_period = trans.app.config.activation_grace_period
         delta = timedelta(hours=int(activation_grace_period))
         time_difference = datetime.utcnow() - create_time
-        return (time_difference > delta or activation_grace_period == 0)
+        return time_difference > delta or activation_grace_period == 0
 
     @web.expose
+    @web.json
     def logout(self, trans, logout_all=False, **kwd):
         message = trans.check_csrf_token(kwd)
         if message:
             return self.message_exception(trans, message)
-        if trans.user:
-            # Queue a quota recalculation (async) task -- this takes a
-            # while sometimes, so we don't want to block on logout.
-            send_local_control_task(trans.app,
-                                    "recalculate_user_disk_usage",
-                                    kwargs={"user_id": trans.security.encode_id(trans.user.id)})
         # Since logging an event requires a session, we'll log prior to ending the session
         trans.log_event("User logged out")
         trans.handle_user_logout(logout_all=logout_all)
-        return {"message": "Success."}
+        success_response = {"message": "Success."}  # This is a little weird as a response.
+        if trans.app.config.use_remote_user and trans.app.config.remote_user_logout_href:
+            success_response["redirect_uri"] = trans.app.config.remote_user_logout_href
+        return success_response
 
     @expose_api_anonymous_and_sessionless
-    def create(self, trans, payload={}, **kwd):
+    def create(self, trans, payload=None, **kwd):
         if not payload:
             payload = kwd
         message = trans.check_csrf_token(payload)
         if message:
             return self.message_exception(trans, message)
         user, message = self.user_manager.register(trans, **_filtered_registration_params_dict(payload))
         if message:
@@ -257,72 +276,88 @@
 
     @web.expose
     def activate(self, trans, **kwd):
         """
         Check whether token fits the user and then activate the user's account.
         """
         params = util.Params(kwd, sanitize=False)
-        email = params.get('email', None)
+        email = params.get("email", None)
         if email is not None:
             email = unquote(email)
-        activation_token = params.get('activation_token', None)
+        activation_token = params.get("activation_token", None)
+        index_url = web.url_for(controller="root", action="index")
 
         if email is None or activation_token is None:
             #  We don't have the email or activation_token, show error.
-            return trans.show_error_message("You are using an invalid activation link. Try to log in and we will send you a new activation email. <br><a href='%s'>Go to login page.</a>") % web.url_for(controller="root", action="index")
+            return trans.show_error_message(
+                f"You are using an invalid activation link. Try to log in and we will send you a new activation email. <br><a href='{index_url}'>Go to login page.</a>"
+            )
         else:
             # Find the user
-            user = trans.sa_session.query(trans.app.model.User).filter(trans.app.model.User.table.c.email == email).first()
+            user = (
+                trans.sa_session.query(trans.app.model.User).filter(trans.app.model.User.table.c.email == email).first()
+            )
             if not user:
                 # Probably wrong email address
-                return trans.show_error_message("You are using an invalid activation link. Try to log in and we will send you a new activation email. <br><a href='%s'>Go to login page.</a>") % web.url_for(controller="root", action="index")
+                return trans.show_error_message(
+                    f"You are using an invalid activation link. Try to log in and we will send you a new activation email. <br><a href='{index_url}'>Go to login page.</a>"
+                )
             # If the user is active already don't try to activate
             if user.active is True:
-                return trans.show_ok_message("Your account is already active. Nothing has changed. <br><a href='%s'>Go to login page.</a>") % web.url_for(controller='root', action='index')
-            if user.activation_token == activation_token:
+                return trans.show_ok_message(
+                    f"Your account is already active. Nothing has changed. <br><a href='{index_url}'>Go to login page.</a>"
+                )
+            if user.activation_token == activation_token[:64]:
                 user.activation_token = None
                 self.user_manager.activate(user)
-                return trans.show_ok_message("Your account has been successfully activated! <br><a href='%s'>Go to login page.</a>") % web.url_for(controller='root', action='index')
+                return trans.show_ok_message(
+                    f"Your account has been successfully activated! <br><a href='{index_url}'>Go to login page.</a>"
+                )
             else:
                 #  Tokens don't match. Activation is denied.
-                return trans.show_error_message("You are using an invalid activation link. Try to log in and we will send you a new activation email. <br><a href='%s'>Go to login page.</a>") % web.url_for(controller='root', action='index')
-        return
+                return trans.show_error_message(
+                    f"You are using an invalid activation link. Try to log in and we will send you a new activation email. <br><a href='{index_url}'>Go to login page.</a>"
+                )
 
     @expose_api_anonymous_and_sessionless
-    def change_password(self, trans, payload={}, **kwd):
+    def change_password(self, trans, payload=None, **kwd):
         """
         Allows to change own password.
 
         :type   payload: dict
         :param  payload: dictionary structure containing:
             * id:               encoded user id
             * current:          current user password
             * token:            temporary token to change password (instead of id and current)
             * password:         new password
             * confirm:          new password (confirmation)
         """
+        payload = payload or {}
         user, message = self.user_manager.change_password(trans, **payload)
         if user is None:
             return self.message_exception(trans, message)
         trans.handle_user_login(user)
         return {"message": "Password has been changed."}
 
     @expose_api_anonymous_and_sessionless
-    def reset_password(self, trans, payload={}, **kwd):
+    def reset_password(self, trans, payload=None, **kwd):
         """Reset the user's password. Send an email with token that allows a password change."""
+        payload = payload or {}
         message = self.user_manager.send_reset_email(trans, payload)
         if message:
             return self.message_exception(trans, message)
         return {"message": "Reset link has been sent to your email."}
 
     def __get_redirect_url(self, redirect):
         if not redirect or redirect == "None":
             return None
-        root_url = url_for('/', qualified=True)
+        root_url = url_for("/", qualified=True)
         # compare urls, to prevent a redirect from pointing (directly) outside of galaxy
         # or to enter a logout/login loop
-        if not util.compare_urls(root_url, redirect, compare_path=False) or util.compare_urls(url_for(controller='user', action='logout', qualified=True), redirect):
-            log.warning('Redirect URL is outside of Galaxy, will redirect to Galaxy root instead: %s', redirect)
+        if not util.compare_urls(root_url, redirect, compare_path=False) or util.compare_urls(
+            url_for(controller="user", action="logout", qualified=True), redirect
+        ):
+            log.warning("Redirect URL is outside of Galaxy, will redirect to Galaxy root instead: %s", redirect)
             redirect = root_url
-        elif util.compare_urls(url_for(controller='user', action='logout', qualified=True), redirect):
+        elif util.compare_urls(url_for(controller="user", action="logout", qualified=True), redirect):
             redirect = root_url
         return redirect
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/userskeys.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/userskeys.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,32 +2,35 @@
 Contains the user interface in the Universe class
 """
 
 from sqlalchemy import false
 
 from galaxy import (
     util,
-    web
+    web,
+)
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    UsesFormDefinitionsMixin,
 )
-from galaxy.webapps.base.controller import BaseUIController, UsesFormDefinitionsMixin
 
 
 class User(BaseUIController, UsesFormDefinitionsMixin):
     @web.expose
     @web.require_login()
     @web.require_admin
     def index(self, trans, cntrller, **kwd):
         return self.get_all_users(trans)
 
     @web.expose
     @web.require_login()
     @web.require_admin
     def admin_api_keys(self, trans, uid, **kwd):
         params = util.Params(kwd)
-        uid = params.get('uid', uid)
+        uid = params.get("uid", uid)
         new_key = trans.app.model.APIKeys()
         new_key.user_id = trans.security.decode_id(uid)
         new_key.key = trans.app.security.get_new_guid()
         trans.sa_session.add(new_key)
         trans.sa_session.flush()
         return self.get_all_users(trans)
 
@@ -36,17 +39,20 @@
     @web.require_admin
     def all_users(self, trans, **kwd):
         return self.get_all_users(trans)
 
     @web.json
     def get_all_users(self, trans):
         users = []
-        for user in trans.sa_session.query(trans.app.model.User) \
-                                    .filter(trans.app.model.User.table.c.deleted == false()) \
-                                    .order_by(trans.app.model.User.table.c.email):
+        for user in (
+            trans.sa_session.query(trans.app.model.User)
+            .filter(trans.app.model.User.table.c.deleted == false())
+            .order_by(trans.app.model.User.table.c.email)
+        ):
             uid = int(user.id)
             userkey = ""
-            for api_user in trans.sa_session.query(trans.app.model.APIKeys) \
-                    .filter(trans.app.model.APIKeys.user_id == uid):
+            for api_user in trans.sa_session.query(trans.app.model.APIKeys).filter(
+                trans.app.model.APIKeys.user_id == uid
+            ):
                 userkey = api_user.key
-            users.append({'uid': trans.security.encode_id(uid), 'email': user.email, 'key': userkey})
+            users.append({"uid": trans.security.encode_id(uid), "email": user.email, "key": userkey})
         return users
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/galaxy/controllers/visualization.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/galaxy/controllers/visualization.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,62 +1,76 @@
-from __future__ import absolute_import
-
 import logging
-import os
 from json import loads
 
-import yaml
 from markupsafe import escape
 from paste.httpexceptions import (
     HTTPBadRequest,
-    HTTPNotFound
+    HTTPNotFound,
 )
-from six import string_types
 from sqlalchemy import (
-    and_,
     desc,
     false,
     or_,
     text,
     true,
 )
-from sqlalchemy.orm import eagerload, undefer
+from sqlalchemy.orm import (
+    joinedload,
+    undefer,
+)
 
-from galaxy import managers, model, util, web
-from galaxy.datatypes.interval import Bed
-from galaxy.model.item_attrs import UsesAnnotations, UsesItemRatings
-from galaxy.util import sanitize_text, unicodify
+from galaxy import (
+    model,
+    util,
+    web,
+)
+from galaxy.managers.hdas import HDAManager
+from galaxy.managers.sharable import SlugBuilder
+from galaxy.model.item_attrs import (
+    UsesAnnotations,
+    UsesItemRatings,
+)
+from galaxy.structured_app import StructuredApp
+from galaxy.util import (
+    sanitize_text,
+    unicodify,
+)
 from galaxy.util.sanitize_html import sanitize_html
-from galaxy.visualization.data_providers.genome import RawBedDataProvider
 from galaxy.visualization.data_providers.phyloviz import PhylovizDataProvider
-from galaxy.visualization.genomes import decode_dbkey
-from galaxy.visualization.genomes import GenomeRegion
+from galaxy.visualization.genomes import (
+    decode_dbkey,
+    GenomeRegion,
+)
 from galaxy.visualization.plugins import registry
-from galaxy.web.framework.helpers import grids, time_ago
+from galaxy.web.framework.helpers import (
+    grids,
+    time_ago,
+)
 from galaxy.webapps.base.controller import (
     BaseUIController,
     SharableMixin,
-    UsesVisualizationMixin
+    UsesVisualizationMixin,
 )
+from ..api import depends
 
 log = logging.getLogger(__name__)
 
 
 #
 # -- Grids --
 #
 class HistoryDatasetsSelectionGrid(grids.Grid):
-
     class DbKeyColumn(grids.GridColumn):
         def filter(self, trans, user, query, dbkey):
-            """ Filter by dbkey through a raw SQL b/c metadata is a BLOB. """
+            """Filter by dbkey through a raw SQL b/c metadata is a BLOB."""
             dbkey_user, dbkey = decode_dbkey(dbkey)
             dbkey = dbkey.replace("'", "\\'")
-            return query.filter(or_(text("metadata like '%%\"dbkey\": [\"%s\"]%%'" % dbkey,
-                                         "metadata like '%%\"dbkey\": \"%s\"%%'" % dbkey)))
+            return query.filter(
+                or_(text(f'metadata like \'%"dbkey": ["{dbkey}"]%\'', f'metadata like \'%"dbkey": "{dbkey}"%\''))
+            )
 
     class HistoryColumn(grids.GridColumn):
         def get_value(self, trans, grid, hda):
             return escape(hda.history.name)
 
         def sort(self, trans, query, ascending, column_name=None):
             """Sort query using this column."""
@@ -69,132 +83,168 @@
     default_sort_key = "-hid"
     columns = [
         grids.GridColumn("Id", key="hid"),
         grids.TextColumn("Name", key="name", model_class=model.HistoryDatasetAssociation),
         grids.TextColumn("Type", key="extension", model_class=model.HistoryDatasetAssociation),
         grids.TextColumn("history_id", key="history_id", model_class=model.HistoryDatasetAssociation, visible=False),
         HistoryColumn("History", key="history", visible=True),
-        DbKeyColumn("Build", key="dbkey", model_class=model.HistoryDatasetAssociation, visible=True, sortable=False)
+        DbKeyColumn("Build", key="dbkey", model_class=model.HistoryDatasetAssociation, visible=True, sortable=False),
     ]
     columns.append(
-        grids.MulticolFilterColumn("Search name and filetype", cols_to_filter=[columns[1], columns[2]],
-                                   key="free-text-search", visible=False, filterable="standard")
+        grids.MulticolFilterColumn(
+            "Search name and filetype",
+            cols_to_filter=[columns[1], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
     )
 
     def build_initial_query(self, trans, **kwargs):
         return trans.sa_session.query(self.model_class).join(model.History.table).join(model.Dataset.table)
 
     def apply_query_filter(self, trans, query, **kwargs):
         if self.available_tracks is None:
             self.available_tracks = trans.app.datatypes_registry.get_available_tracks()
-        return query.filter(model.History.user == trans.user) \
-                    .filter(model.HistoryDatasetAssociation.extension.in_(self.available_tracks)) \
-                    .filter(model.Dataset.state == model.Dataset.states.OK) \
-                    .filter(model.HistoryDatasetAssociation.deleted == false()) \
-                    .filter(model.HistoryDatasetAssociation.visible == true())
+        return (
+            query.filter(model.History.user == trans.user)
+            .filter(model.HistoryDatasetAssociation.extension.in_(self.available_tracks))
+            .filter(model.Dataset.state == model.Dataset.states.OK)
+            .filter(model.HistoryDatasetAssociation.deleted == false())
+            .filter(model.HistoryDatasetAssociation.visible == true())
+        )
 
 
 class LibraryDatasetsSelectionGrid(grids.Grid):
     available_tracks = None
     title = "Add Datasets"
     model_class = model.LibraryDatasetDatasetAssociation
     default_filter = {"deleted": "False"}
     default_sort_key = "-id"
     columns = [
         grids.GridColumn("Id", key="id"),
         grids.TextColumn("Name", key="name", model_class=model.LibraryDatasetDatasetAssociation),
         grids.TextColumn("Type", key="extension", model_class=model.LibraryDatasetDatasetAssociation),
     ]
     columns.append(
-        grids.MulticolFilterColumn("Search name and filetype", cols_to_filter=[columns[1], columns[2]],
-                                   key="free-text-search", visible=False, filterable="standard")
+        grids.MulticolFilterColumn(
+            "Search name and filetype",
+            cols_to_filter=[columns[1], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
     )
 
     def build_initial_query(self, trans, **kwargs):
         return trans.sa_session.query(self.model_class).join(model.Dataset.table)
 
     def apply_query_filter(self, trans, query, **kwargs):
         if self.available_tracks is None:
             self.available_tracks = trans.app.datatypes_registry.get_available_tracks()
-        return query.filter(model.LibraryDatasetDatasetAssociation.user == trans.user) \
-                    .filter(model.LibraryDatasetDatasetAssociation.extension.in_(self.available_tracks)) \
-                    .filter(model.Dataset.state == model.Dataset.states.OK) \
-                    .filter(model.LibraryDatasetDatasetAssociation.deleted == false()) \
-                    .filter(model.LibraryDatasetDatasetAssociation.visible == true())
+        return (
+            query.filter(model.LibraryDatasetDatasetAssociation.user == trans.user)
+            .filter(model.LibraryDatasetDatasetAssociation.extension.in_(self.available_tracks))
+            .filter(model.Dataset.state == model.Dataset.states.OK)
+            .filter(model.LibraryDatasetDatasetAssociation.deleted == false())
+            .filter(model.LibraryDatasetDatasetAssociation.visible == true())
+        )
 
 
 class TracksterSelectionGrid(grids.Grid):
     title = "Insert into visualization"
     model_class = model.Visualization
     default_sort_key = "-update_time"
     use_paging = False
     show_item_checkboxes = True
     columns = [
         grids.TextColumn("Title", key="title", model_class=model.Visualization, filterable="standard"),
         grids.TextColumn("Build", key="dbkey", model_class=model.Visualization),
-        grids.GridColumn("Last Updated", key="update_time", format=time_ago)
+        grids.GridColumn("Last Updated", key="update_time", format=time_ago),
     ]
 
     def build_initial_query(self, trans, **kwargs):
         return trans.sa_session.query(self.model_class)
 
     def apply_query_filter(self, trans, query, **kwargs):
-        return query.filter(self.model_class.user_id == trans.user.id) \
-                    .filter(self.model_class.deleted == false()) \
-                    .filter(self.model_class.type == "trackster")
+        return (
+            query.filter(self.model_class.user_id == trans.user.id)
+            .filter(self.model_class.deleted == false())
+            .filter(self.model_class.type == "trackster")
+        )
 
 
 class VisualizationListGrid(grids.Grid):
     def get_url_args(item):
         """
         Returns dictionary used to create item link.
         """
-        url_kwargs = dict(controller='visualization', id=item.id)
+        url_kwargs = dict(controller="visualization", id=item.id)
         # TODO: hack to build link to saved visualization - need trans in this function instead in order to do
         # link_data = trans.app.visualizations_registry.get_visualizations( trans, item )
         if item.type in registry.VisualizationsRegistry.BUILT_IN_VISUALIZATIONS:
-            url_kwargs['action'] = item.type
+            url_kwargs["action"] = item.type
         else:
-            url_kwargs['__route_name__'] = 'saved_visualization'
-            url_kwargs['visualization_name'] = item.type
-            url_kwargs['action'] = 'saved'
+            url_kwargs["__route_name__"] = "saved_visualization"
+            url_kwargs["visualization_name"] = item.type
+            url_kwargs["action"] = "saved"
         return url_kwargs
 
     def get_display_name(self, trans, item):
         if trans.app.visualizations_registry and item.type in trans.app.visualizations_registry.plugins:
             plugin = trans.app.visualizations_registry.plugins[item.type]
-            return plugin.config.get('name', item.type)
+            return plugin.config.get("name", item.type)
         return item.type
 
     # Grid definition
     title = "Saved Visualizations"
     model_class = model.Visualization
     default_sort_key = "-update_time"
     default_filter = dict(title="All", deleted="False", tags="All", sharing="All")
     columns = [
         grids.TextColumn("Title", key="title", attach_popup=True, link=get_url_args),
-        grids.TextColumn("Type", method='get_display_name'),
+        grids.TextColumn("Type", method="get_display_name"),
         grids.TextColumn("Build", key="dbkey"),
-        grids.IndividualTagsColumn("Tags", key="tags", model_tag_association_class=model.VisualizationTagAssociation, filterable="advanced", grid_name="VisualizationListGrid"),
+        grids.IndividualTagsColumn(
+            "Tags",
+            key="tags",
+            model_tag_association_class=model.VisualizationTagAssociation,
+            filterable="advanced",
+            grid_name="VisualizationListGrid",
+        ),
         grids.SharingStatusColumn("Sharing", key="sharing", filterable="advanced", sortable=False),
         grids.GridColumn("Created", key="create_time", format=time_ago),
         grids.GridColumn("Last Updated", key="update_time", format=time_ago),
     ]
     columns.append(
         grids.MulticolFilterColumn(
             "Search",
             cols_to_filter=[columns[0], columns[2]],
-            key="free-text-search", visible=False, filterable="standard")
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
     )
     operations = [
         grids.GridOperation("Open", allow_multiple=False, url_args=get_url_args),
-        grids.GridOperation("Edit Attributes", allow_multiple=False, url_args=dict(controller="", action='visualizations/edit')),
+        grids.GridOperation(
+            "Edit Attributes", allow_multiple=False, url_args=dict(controller="", action="visualizations/edit")
+        ),
         grids.GridOperation("Copy", allow_multiple=False, condition=(lambda item: not item.deleted)),
-        grids.GridOperation("Share or Publish", allow_multiple=False, condition=(lambda item: not item.deleted), url_args=dict(controller="", action="visualizations/sharing")),
-        grids.GridOperation("Delete", condition=(lambda item: not item.deleted), confirm="Are you sure you want to delete this visualization?"),
+        grids.GridOperation(
+            "Share or Publish",
+            allow_multiple=False,
+            condition=(lambda item: not item.deleted),
+            url_args=dict(controller="", action="visualizations/sharing"),
+        ),
+        grids.GridOperation(
+            "Delete",
+            condition=(lambda item: not item.deleted),
+            confirm="Are you sure you want to delete this visualization?",
+        ),
     ]
 
     def apply_query_filter(self, trans, query, **kwargs):
         return query.filter_by(user=trans.user, deleted=False)
 
 
 class VisualizationAllPublishedGrid(grids.Grid):
@@ -202,46 +252,70 @@
     use_panels = True
     title = "Published Visualizations"
     model_class = model.Visualization
     default_sort_key = "update_time"
     default_filter = dict(title="All", username="All")
     columns = [
         grids.PublicURLColumn("Title", key="title", filterable="advanced"),
-        grids.OwnerAnnotationColumn("Annotation", key="annotation", model_annotation_association_class=model.VisualizationAnnotationAssociation, filterable="advanced"),
+        grids.OwnerAnnotationColumn(
+            "Annotation",
+            key="annotation",
+            model_annotation_association_class=model.VisualizationAnnotationAssociation,
+            filterable="advanced",
+        ),
         grids.OwnerColumn("Owner", key="username", model_class=model.User, filterable="advanced"),
         grids.CommunityRatingColumn("Community Rating", key="rating"),
-        grids.CommunityTagsColumn("Community Tags", key="tags", model_tag_association_class=model.VisualizationTagAssociation, filterable="advanced", grid_name="VisualizationAllPublishedGrid"),
-        grids.ReverseSortColumn("Last Updated", key="update_time", format=time_ago)
+        grids.CommunityTagsColumn(
+            "Community Tags",
+            key="tags",
+            model_tag_association_class=model.VisualizationTagAssociation,
+            filterable="advanced",
+            grid_name="VisualizationAllPublishedGrid",
+        ),
+        grids.ReverseSortColumn("Last Updated", key="update_time", format=time_ago),
     ]
     columns.append(
         grids.MulticolFilterColumn(
             "Search title, annotation, owner, and tags",
             cols_to_filter=[columns[0], columns[1], columns[2], columns[4]],
-            key="free-text-search", visible=False, filterable="standard")
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
     )
 
     def build_initial_query(self, trans, **kwargs):
         # See optimization description comments and TODO for tags in matching public histories query.
-        return trans.sa_session.query(self.model_class).join("user").options(eagerload("user").load_only("username"), eagerload("annotations"), undefer("average_rating"))
+        return (
+            trans.sa_session.query(self.model_class)
+            .join(self.model_class.user)
+            .options(
+                joinedload(self.model_class.user).load_only("username"),
+                joinedload(self.model_class.annotations),
+                undefer("average_rating"),
+            )
+        )
 
     def apply_query_filter(self, trans, query, **kwargs):
         return query.filter(self.model_class.deleted == false()).filter(self.model_class.published == true())
 
 
-class VisualizationController(BaseUIController, SharableMixin, UsesVisualizationMixin,
-                              UsesAnnotations, UsesItemRatings):
+class VisualizationController(
+    BaseUIController, SharableMixin, UsesVisualizationMixin, UsesAnnotations, UsesItemRatings
+):
     _visualization_list_grid = VisualizationListGrid()
     _published_list_grid = VisualizationAllPublishedGrid()
     _history_datasets_grid = HistoryDatasetsSelectionGrid()
     _library_datasets_grid = LibraryDatasetsSelectionGrid()
     _tracks_grid = TracksterSelectionGrid()
+    hda_manager: HDAManager = depends(HDAManager)
+    slug_builder: SlugBuilder = depends(SlugBuilder)
 
-    def __init__(self, app):
-        super(VisualizationController, self).__init__(app)
-        self.hda_manager = managers.hdas.HDAManager(app)
+    def __init__(self, app: StructuredApp):
+        super().__init__(app)
 
     #
     # -- Functions for listing visualizations. --
     #
 
     @web.expose
     @web.json
@@ -251,356 +325,300 @@
         return self._libraries_grid(trans, **kwargs)
 
     @web.expose
     @web.json
     @web.require_login("see a history's datasets that can added to this visualization")
     def list_history_datasets(self, trans, **kwargs):
         """List a history's datasets that can be added to a visualization."""
-        kwargs['show_item_checkboxes'] = 'True'
+        kwargs["show_item_checkboxes"] = "True"
         return self._history_datasets_grid(trans, **kwargs)
 
     @web.expose
     @web.json
     @web.require_login("see a history's datasets that can added to this visualization")
     def list_library_datasets(self, trans, **kwargs):
         """List a library's datasets that can be added to a visualization."""
-        kwargs['show_item_checkboxes'] = 'True'
+        kwargs["show_item_checkboxes"] = "True"
         return self._library_datasets_grid(trans, **kwargs)
 
     @web.expose
     @web.json
     def list_tracks(self, trans, **kwargs):
         return self._tracks_grid(trans, **kwargs)
 
     @web.expose
     @web.json
     def list_published(self, trans, *args, **kwargs):
         grid = self._published_list_grid(trans, **kwargs)
-        grid['shared_by_others'] = self._get_shared(trans)
+        grid["shared_by_others"] = self._get_shared(trans)
         return grid
 
     @web.legacy_expose_api
     @web.require_login("use Galaxy visualizations", use_panels=True)
     def list(self, trans, **kwargs):
-        message = kwargs.get('message')
-        status = kwargs.get('status')
-        if 'operation' in kwargs and 'id' in kwargs:
+        message = kwargs.get("message")
+        status = kwargs.get("status")
+        if "operation" in kwargs and "id" in kwargs:
             session = trans.sa_session
-            operation = kwargs['operation'].lower()
-            ids = util.listify(kwargs['id'])
+            operation = kwargs["operation"].lower()
+            ids = util.listify(kwargs["id"])
             for id in ids:
-                item = session.query(model.Visualization).get(self.decode_id(id))
                 if operation == "delete":
+                    item = self.get_visualization(trans, id)
                     item.deleted = True
                 if operation == "copy":
                     self.copy(trans, **kwargs)
             session.flush()
-        kwargs['embedded'] = True
+        kwargs["embedded"] = True
         if message and status:
-            kwargs['message'] = sanitize_text(message)
-            kwargs['status'] = status
+            kwargs["message"] = sanitize_text(message)
+            kwargs["status"] = status
         grid = self._visualization_list_grid(trans, **kwargs)
-        grid['shared_by_others'] = self._get_shared(trans)
+        grid["shared_by_others"] = self._get_shared(trans)
         return grid
 
     def _get_shared(self, trans):
         """Identify shared visualizations"""
-        shared_by_others = trans.sa_session \
-            .query(model.VisualizationUserShareAssociation) \
-            .filter_by(user=trans.get_user()) \
-            .join(model.Visualization.table) \
-            .filter(model.Visualization.deleted == false()) \
-            .order_by(desc(model.Visualization.update_time)) \
+        shared_by_others = (
+            trans.sa_session.query(model.VisualizationUserShareAssociation)
+            .filter_by(user=trans.get_user())
+            .join(model.Visualization.table)
+            .filter(model.Visualization.deleted == false())
+            .order_by(desc(model.Visualization.update_time))
             .all()
-        return [{'username' : v.visualization.user.username,
-                 'slug'     : v.visualization.slug,
-                 'title'    : v.visualization.title} for v in shared_by_others]
+        )
+        return [
+            {"username": v.visualization.user.username, "slug": v.visualization.slug, "title": v.visualization.title}
+            for v in shared_by_others
+        ]
 
     #
     # -- Functions for operating on visualizations. --
     #
 
     @web.expose
     @web.require_login("use Galaxy visualizations", use_panels=True)
     def index(self, trans, *args, **kwargs):
-        """ Lists user's saved visualizations. """
+        """Lists user's saved visualizations."""
         return self.list(trans, *args, **kwargs)
 
     @web.expose
     @web.require_login()
     def copy(self, trans, id, **kwargs):
-        visualization = self.get_visualization(trans, id, check_ownership=False)
+        visualization = self.get_visualization(trans, id, check_ownership=False, check_accessible=True)
         user = trans.get_user()
-        owner = (visualization.user == user)
-        new_title = "Copy of '%s'" % visualization.title
+        owner = visualization.user == user
+        new_title = f"Copy of '{visualization.title}'"
         if not owner:
-            new_title += " shared by %s" % visualization.user.email
+            new_title += f" shared by {visualization.user.email}"
 
         copied_viz = visualization.copy(user=trans.user, title=new_title)
 
         # Persist
         session = trans.sa_session
         session.add(copied_viz)
         session.flush()
 
         # Display the management page
-        trans.set_message('Created new visualization with name "%s"' % copied_viz.title)
+        trans.set_message(f'Created new visualization with name "{copied_viz.title}"')
         return
 
     @web.expose
     @web.require_login("use Galaxy visualizations")
     def set_accessible_async(self, trans, id=None, accessible=False):
-        """ Set visualization's importable attribute and slug. """
+        """Set visualization's importable attribute and slug."""
         visualization = self.get_visualization(trans, id)
 
         # Only set if importable value would change; this prevents a change in the update_time unless attribute really changed.
-        importable = accessible in ['True', 'true', 't', 'T']
+        importable = accessible in ["True", "true", "t", "T"]
         if visualization and visualization.importable != importable:
             if importable:
                 self._make_item_accessible(trans.sa_session, visualization)
             else:
                 visualization.importable = importable
             trans.sa_session.flush()
 
         return
 
     @web.expose
-    @web.require_login("rate items")
-    @web.json
-    def rate_async(self, trans, id, rating):
-        """ Rate a visualization asynchronously and return updated community data. """
-
-        visualization = self.get_visualization(trans, id, check_ownership=False, check_accessible=True)
-        if not visualization:
-            return trans.show_error_message("The specified visualization does not exist.")
-
-        # Rate visualization.
-        self.rate_item(trans.sa_session, trans.get_user(), visualization, rating)
-
-        return self.get_ave_item_rating_data(trans.sa_session, visualization)
-
-    @web.expose
     @web.require_login("share Galaxy visualizations")
     def imp(self, trans, id):
-        """ Import a visualization into user's workspace. """
+        """Import a visualization into user's workspace."""
         # Set referer message.
         referer = trans.request.referer
-        if referer:
-            referer_message = "<a href='%s'>return to the previous page</a>" % escape(referer)
+        if referer and not referer.startswith(f"{trans.request.application_url}{web.url_for('/login')}"):
+            referer_message = f"<a href='{escape(referer)}'>return to the previous page</a>"
         else:
-            referer_message = "<a href='%s'>go to Galaxy's start page</a>" % web.url_for('/')
+            referer_message = f"<a href='{web.url_for('/')}'>go to Galaxy's start page</a>"
 
         # Do import.
         session = trans.sa_session
-        visualization = self.get_visualization(trans, id, check_ownership=False)
+        visualization = self.get_visualization(trans, id, check_ownership=False, check_accessible=True)
         if visualization.importable is False:
-            return trans.show_error_message("The owner of this visualization has disabled imports via this link.<br>You can %s" % referer_message, use_panels=True)
+            return trans.show_error_message(
+                f"The owner of this visualization has disabled imports via this link.<br>You can {referer_message}",
+                use_panels=True,
+            )
         elif visualization.deleted:
-            return trans.show_error_message("You can't import this visualization because it has been deleted.<br>You can %s" % referer_message, use_panels=True)
+            return trans.show_error_message(
+                f"You can't import this visualization because it has been deleted.<br>You can {referer_message}",
+                use_panels=True,
+            )
         else:
             # Create imported visualization via copy.
             #   TODO: need to handle custom db keys.
 
-            imported_visualization = visualization.copy(user=trans.user, title="imported: " + visualization.title)
+            imported_visualization = visualization.copy(user=trans.user, title=f"imported: {visualization.title}")
 
             # Persist
             session = trans.sa_session
             session.add(imported_visualization)
             session.flush()
 
             # Redirect to load galaxy frames.
             return trans.show_ok_message(
                 message="""Visualization "%s" has been imported. <br>You can <a href="%s">start using this visualization</a> or %s."""
-                % (visualization.title, web.url_for('/visualizations/list'), referer_message), use_panels=True)
-
-    @web.expose
-    @web.require_login("share Galaxy visualizations")
-    def share(self, trans, id=None, email="", use_panels=False):
-        """ Handle sharing a visualization with a particular user. """
-        msg = mtype = None
-        visualization = self.get_visualization(trans, id, check_ownership=True)
-        if email:
-            other = trans.sa_session.query(model.User) \
-                                    .filter(and_(model.User.table.c.email == email,
-                                                 model.User.table.c.deleted == false())) \
-                                    .first()
-            if not other:
-                mtype = "error"
-                msg = ("User '%s' does not exist" % escape(email))
-            elif other == trans.get_user():
-                mtype = "error"
-                msg = ("You cannot share a visualization with yourself")
-            elif trans.sa_session.query(model.VisualizationUserShareAssociation) \
-                    .filter_by(user=other, visualization=visualization).count() > 0:
-                mtype = "error"
-                msg = ("Visualization already shared with '%s'" % escape(email))
-            else:
-                share = model.VisualizationUserShareAssociation()
-                share.visualization = visualization
-                share.user = other
-                session = trans.sa_session
-                session.add(share)
-                self.create_item_slug(session, visualization)
-                session.flush()
-                viz_title = escape(visualization.title)
-                other_email = escape(other.email)
-                trans.set_message("Visualization '%s' shared with user '%s'" % (viz_title, other_email))
-                return trans.response.send_redirect(web.url_for("/visualizations/sharing?id=%s" % id))
-        return trans.fill_template("/ind_share_base.mako",
-                                   message=msg,
-                                   messagetype=mtype,
-                                   item=visualization,
-                                   email=email,
-                                   use_panels=use_panels)
+                % (visualization.title, web.url_for("/visualizations/list"), referer_message),
+                use_panels=True,
+            )
 
     @web.expose
-    def display_by_username_and_slug(self, trans, username, slug):
-        """ Display visualization based on a username and slug. """
+    def display_by_username_and_slug(self, trans, username, slug, **kwargs):
+        """Display visualization based on a username and slug."""
 
         # Get visualization.
         session = trans.sa_session
         user = session.query(model.User).filter_by(username=username).first()
-        visualization = trans.sa_session.query(model.Visualization).filter_by(user=user, slug=slug, deleted=False).first()
+        visualization = (
+            trans.sa_session.query(model.Visualization).filter_by(user=user, slug=slug, deleted=False).first()
+        )
         if visualization is None:
             raise web.httpexceptions.HTTPNotFound()
 
         # Security check raises error if user cannot access visualization.
         self.security_check(trans, visualization, check_ownership=False, check_accessible=True)
 
-        # Get rating data.
-        user_item_rating = 0
-        if trans.get_user():
-            user_item_rating = self.get_user_item_rating(trans.sa_session, trans.get_user(), visualization)
-            if user_item_rating:
-                user_item_rating = user_item_rating.rating
-            else:
-                user_item_rating = 0
-        ave_item_rating, num_ratings = self.get_ave_item_rating_data(trans.sa_session, visualization)
+        # Encode page identifier.
+        visualization_id = trans.security.encode_id(visualization.id)
 
-        # Fork to template based on visualization.type (registry or builtin).
-        if((trans.app.visualizations_registry and visualization.type in trans.app.visualizations_registry.plugins) and
-                (visualization.type not in trans.app.visualizations_registry.BUILT_IN_VISUALIZATIONS)):
-            # if a registry visualization, load a version of display.mako that will load the vis into an iframe :(
-            # TODO: simplest path from A to B but not optimal - will be difficult to do reg visualizations any other way
-            # TODO: this will load the visualization twice (once above, once when the iframe src calls 'saved')
-            encoded_visualization_id = trans.security.encode_id(visualization.id)
-            return trans.stream_template_mako('visualization/display_in_frame.mako',
-                                              item=visualization, encoded_visualization_id=encoded_visualization_id,
-                                              user_item_rating=user_item_rating, ave_item_rating=ave_item_rating, num_ratings=num_ratings,
-                                              content_only=True)
-
-        visualization_config = self.get_visualization_config(trans, visualization)
-        return trans.stream_template_mako("visualization/display.mako", item=visualization, item_data=visualization_config,
-                                          user_item_rating=user_item_rating, ave_item_rating=ave_item_rating, num_ratings=num_ratings,
-                                          content_only=True)
+        # Redirect to client.
+        return trans.response.send_redirect(
+            web.url_for(
+                controller="published",
+                action="visualization",
+                id=visualization_id,
+            )
+        )
 
     @web.expose
     @web.json
     @web.require_login("get item name and link")
     def get_name_and_link_async(self, trans, id=None):
-        """ Returns visualization's name and link. """
+        """Returns visualization's name and link."""
         visualization = self.get_visualization(trans, id, check_ownership=False, check_accessible=True)
 
-        if self.create_item_slug(trans.sa_session, visualization):
+        if self.slug_builder.create_item_slug(trans.sa_session, visualization):
             trans.sa_session.flush()
-        return_dict = {"name": visualization.title,
-                       "link": web.url_for(controller='visualization', action="display_by_username_and_slug", username=visualization.user.username, slug=visualization.slug)}
+        return_dict = {
+            "name": visualization.title,
+            "link": web.url_for(
+                controller="visualization",
+                action="display_by_username_and_slug",
+                username=visualization.user.username,
+                slug=visualization.slug,
+            ),
+        }
         return return_dict
 
-    @web.expose
-    def get_item_content_async(self, trans, id):
-        """ Returns item content in HTML format. """
-
-        # Get visualization, making sure it's accessible.
-        visualization = self.get_visualization(trans, id, check_ownership=False, check_accessible=True)
-        if visualization is None:
-            raise web.httpexceptions.HTTPNotFound()
-
-        # Return content.
-        visualization_config = self.get_visualization_config(trans, visualization)
-        return trans.fill_template_mako("visualization/item_content.mako", encoded_id=trans.security.encode_id(visualization.id),
-                                        item=visualization, item_data=visualization_config, content_only=True)
-
     @web.json
     def save(self, trans, vis_json=None, type=None, id=None, title=None, dbkey=None, annotation=None):
         """
         Save a visualization; if visualization does not have an ID, a new
         visualization is created. Returns JSON of visualization.
         """
         # Get visualization attributes from kwargs or from config.
         vis_config = loads(vis_json)
-        vis_type = type or vis_config['type']
-        vis_id = id or vis_config.get('id', None)
-        vis_title = title or vis_config.get('title', None)
-        vis_dbkey = dbkey or vis_config.get('dbkey', None)
-        vis_annotation = annotation or vis_config.get('annotation', None)
+        vis_type = type or vis_config["type"]
+        vis_id = id or vis_config.get("id", None)
+        vis_title = title or vis_config.get("title", None)
+        vis_dbkey = dbkey or vis_config.get("dbkey", None)
+        vis_annotation = annotation or vis_config.get("annotation", None)
         return self.save_visualization(trans, vis_config, vis_type, vis_id, vis_title, vis_dbkey, vis_annotation)
 
     @web.legacy_expose_api
     @web.require_login("edit visualizations")
     def edit(self, trans, payload=None, **kwd):
         """
         Edit a visualization's attributes.
         """
-        id = kwd.get('id')
+        id = kwd.get("id")
         if not id:
-            return self.message_exception(trans, 'No visualization id received for editing.')
+            return self.message_exception(trans, "No visualization id received for editing.")
+        trans_user = trans.get_user()
         v = self.get_visualization(trans, id, check_ownership=True)
-        if trans.request.method == 'GET':
+        if trans.request.method == "GET":
             if v.slug is None:
-                self.create_item_slug(trans.sa_session, v)
+                self.slug_builder.create_item_slug(trans.sa_session, v)
             return {
-                'title'  : 'Edit visualization attributes',
-                'inputs' : [{
-                    'name'      : 'title',
-                    'label'     : 'Name',
-                    'value'     : v.title
-                }, {
-                    'name'      : 'slug',
-                    'label'     : 'Identifier',
-                    'value'     : v.slug,
-                    'help'      : 'A unique identifier that will be used for public links to this visualization. This field can only contain lowercase letters, numbers, and dashes (-).'
-                }, {
-                    'name'      : 'dbkey',
-                    'label'     : 'Build',
-                    'type'      : 'select',
-                    'optional'  : True,
-                    'value'     : v.dbkey,
-                    'options'   : trans.app.genomes.get_dbkeys(trans, chrom_info=True),
-                    'help'      : 'Parameter to associate your visualization with a database key.'
-                }, {
-                    'name'      : 'annotation',
-                    'label'     : 'Annotation',
-                    'value'     : self.get_item_annotation_str(trans.sa_session, trans.user, v),
-                    'help'      : 'A description of the visualization. The annotation is shown alongside published visualizations.'
-                }]
+                "title": "Edit visualization attributes",
+                "inputs": [
+                    {"name": "title", "label": "Name", "value": v.title},
+                    {
+                        "name": "slug",
+                        "label": "Identifier",
+                        "value": v.slug,
+                        "help": "A unique identifier that will be used for public links to this visualization. This field can only contain lowercase letters, numbers, and dashes (-).",
+                    },
+                    {
+                        "name": "dbkey",
+                        "label": "Build",
+                        "type": "select",
+                        "optional": True,
+                        "value": v.dbkey,
+                        "options": trans.app.genomes.get_dbkeys(trans_user, chrom_info=True),
+                        "help": "Parameter to associate your visualization with a database key.",
+                    },
+                    {
+                        "name": "annotation",
+                        "label": "Annotation",
+                        "value": self.get_item_annotation_str(trans.sa_session, trans.user, v),
+                        "help": "A description of the visualization. The annotation is shown alongside published visualizations.",
+                    },
+                ],
             }
         else:
-            v_title = payload.get('title')
-            v_slug = payload.get('slug')
-            v_dbkey = payload.get('dbkey')
-            v_annotation = payload.get('annotation')
+            v_title = payload.get("title")
+            v_slug = payload.get("slug")
+            v_dbkey = payload.get("dbkey")
+            v_annotation = payload.get("annotation")
             if not v_title:
-                return self.message_exception(trans, 'Please provide a visualization name is required.')
+                return self.message_exception(trans, "Please provide a visualization name is required.")
             elif not v_slug:
-                return self.message_exception(trans, 'Please provide a unique identifier.')
+                return self.message_exception(trans, "Please provide a unique identifier.")
             elif not self._is_valid_slug(v_slug):
-                return self.message_exception(trans, 'Visualization identifier can only contain lowercase letters, numbers, and dashes (-).')
-            elif v_slug != v.slug and trans.sa_session.query(model.Visualization).filter_by(user=v.user, slug=v_slug, deleted=False).first():
-                return self.message_exception(trans, 'Visualization id must be unique.')
+                return self.message_exception(
+                    trans, "Visualization identifier can only contain lowercase letters, numbers, and dashes (-)."
+                )
+            elif (
+                v_slug != v.slug
+                and trans.sa_session.query(model.Visualization)
+                .filter_by(user=v.user, slug=v_slug, deleted=False)
+                .first()
+            ):
+                return self.message_exception(trans, "Visualization id must be unique.")
             else:
                 v.title = v_title
                 v.slug = v_slug
                 v.dbkey = v_dbkey
                 if v_annotation:
                     v_annotation = sanitize_html(v_annotation)
-                    self.add_item_annotation(trans.sa_session, trans.get_user(), v, v_annotation)
+                    self.add_item_annotation(trans.sa_session, trans_user, v, v_annotation)
                 trans.sa_session.add(v)
                 trans.sa_session.flush()
-            return {'message': 'Attributes of \'%s\' successfully saved.' % v.title, 'status': 'success'}
+            return {"message": "Attributes of '%s' successfully saved." % v.title, "status": "success"}
 
     # ------------------------- registry.
     @web.expose
     @web.require_login("use Galaxy visualizations", use_panels=True)
     def render(self, trans, visualization_name, embedded=None, **kwargs):
         """
         Render the appropriate visualization template, parsing the `kwargs`
@@ -618,94 +636,97 @@
     def _get_plugin_from_registry(self, trans, visualization_name):
         """
         Get the named plugin from the registry.
         :raises HTTPNotFound: if registry has been turned off in config.
         :raises HTTPNotFound: if visualization_name isn't a registered plugin.
         """
         if not trans.app.visualizations_registry:
-            raise HTTPNotFound('No visualization registry (possibly disabled in galaxy.ini)')
+            raise HTTPNotFound("No visualization registry (possibly disabled in galaxy.ini)")
         return trans.app.visualizations_registry.get_plugin(visualization_name)
 
     def _handle_plugin_error(self, trans, visualization_name, exception):
         """
         Log, raise if debugging; log and show html message if not.
         """
-        log.exception('error rendering visualization (%s)', visualization_name)
+        log.exception("error rendering visualization (%s)", visualization_name)
         if trans.debug:
             raise exception
         return trans.show_error_message(
-            "There was an error rendering the visualization. " +
-            "Contact your Galaxy administrator if the problem persists." +
-            "<br/>Details: " + unicodify(exception), use_panels=False)
+            "There was an error rendering the visualization. "
+            + "Contact your Galaxy administrator if the problem persists."
+            + "<br/>Details: "
+            + unicodify(exception),
+            use_panels=False,
+        )
 
     @web.expose
     @web.require_login("use Galaxy visualizations", use_panels=True)
     def saved(self, trans, id=None, revision=None, type=None, config=None, title=None, **kwargs):
         """
         Save (on POST) or load (on GET) a visualization then render.
         """
         # TODO: consider merging saved and render at this point (could break saved URLs, tho)
-        if trans.request.method == 'POST':
+        if trans.request.method == "POST":
             self._POST_to_saved(trans, id=id, revision=revision, type=type, config=config, title=title, **kwargs)
 
         # check the id and load the saved visualization
         if id is None:
-            return HTTPBadRequest('A valid visualization id is required to load a visualization')
+            return HTTPBadRequest("A valid visualization id is required to load a visualization")
         visualization = self.get_visualization(trans, id, check_ownership=False, check_accessible=True)
 
         # re-add title to kwargs for passing to render
         if title:
-            kwargs['title'] = title
+            kwargs["title"] = title
         plugin = self._get_plugin_from_registry(trans, visualization.type)
         try:
             return plugin.render_saved(visualization, trans=trans, **kwargs)
         except Exception as exception:
             self._handle_plugin_error(trans, visualization.type, exception)
 
     def _POST_to_saved(self, trans, id=None, revision=None, type=None, config=None, title=None, **kwargs):
         """
         Save the visualiztion info (revision, type, config, title, etc.) to
         the Visualization at `id` or to a new Visualization if `id` is None.
 
         Uses POST/redirect/GET after a successful save, redirecting to GET.
         """
-        DEFAULT_VISUALIZATION_NAME = 'Unnamed Visualization'
+        DEFAULT_VISUALIZATION_NAME = "Unnamed Visualization"
 
         # post to saved in order to save a visualization
         if type is None or config is None:
-            return HTTPBadRequest('A visualization type and config are required to save a visualization')
-        if isinstance(config, string_types):
+            return HTTPBadRequest("A visualization type and config are required to save a visualization")
+        if isinstance(config, str):
             config = loads(config)
         title = title or DEFAULT_VISUALIZATION_NAME
 
         # TODO: allow saving to (updating) a specific revision - should be part of UsesVisualization
         # TODO: would be easier if this returned the visualization directly
         # check security if posting to existing visualization
         if id is not None:
             self.get_visualization(trans, id, check_ownership=True, check_accessible=False)
             # ??: on not owner: error raised, but not returned (status = 200)
         # TODO: there's no security check in save visualization (if passed an id)
         returned = self.save_visualization(trans, config, type, id, title)
 
         # redirect to GET to prevent annoying 'Do you want to post again?' dialog on page reload
-        render_url = web.url_for(controller='visualization', action='saved', id=returned.get('vis_id'))
+        render_url = web.url_for(controller="visualization", action="saved", id=returned.get("vis_id"))
         return trans.response.send_redirect(render_url)
 
     #
     # Visualizations.
     #
     @web.expose
     @web.require_login()
     def trackster(self, trans, **kwargs):
         """
         Display browser for the visualization denoted by id and add the datasets listed in `dataset_ids`.
         """
 
         # define app configuration
-        app = {"jscript" : "trackster"}
+        app = {"jscript": "trackster"}
 
         # get dataset to add
         id = kwargs.get("id", None)
 
         # get dataset to add
         new_dataset_id = kwargs.get("dataset_id", None)
 
@@ -713,43 +734,39 @@
         if not id:
             # use dbkey from dataset to be added or from incoming parameter
             dbkey = None
             if new_dataset_id:
                 decoded_id = self.decode_id(new_dataset_id)
                 hda = self.hda_manager.get_owned(decoded_id, trans.user, current_history=trans.user)
                 dbkey = hda.dbkey
-                if dbkey == '?':
+                if dbkey == "?":
                     dbkey = kwargs.get("dbkey", None)
 
             # save database key
-            app['default_dbkey'] = dbkey
+            app["default_dbkey"] = dbkey
         else:
             # load saved visualization
             vis = self.get_visualization(trans, id, check_ownership=False, check_accessible=True)
-            app['viz_config'] = self.get_visualization_config(trans, vis)
+            app["viz_config"] = self.get_visualization_config(trans, vis)
 
         # backup id
-        app['id'] = id
+        app["id"] = id
 
         # add dataset id
-        app['add_dataset'] = new_dataset_id
+        app["add_dataset"] = new_dataset_id
 
         # check for gene region
         gene_region = GenomeRegion.from_str(kwargs.get("gene_region", ""))
 
         # update gene region of saved visualization if user parses a new gene region in the url
         if gene_region.chrom is not None:
-            app['gene_region'] = {
-                'chrom' : gene_region.chrom,
-                'start' : gene_region.start,
-                'end'   : gene_region.end
-            }
+            app["gene_region"] = {"chrom": gene_region.chrom, "start": gene_region.start, "end": gene_region.end}
 
         # fill template
-        return trans.fill_template('galaxy.panels.mako', config={'right_panel': True, 'app': app, 'bundle': 'extended'})
+        return trans.fill_template("visualization/trackster.mako", config={"app": app, "bundle": "extended"})
 
     @web.expose
     def circster(self, trans, id=None, hda_ldda=None, dataset_id=None, dbkey=None):
         """
         Display a circster visualization.
         """
 
@@ -764,86 +781,81 @@
             vis = self.get_visualization(trans, id, check_ownership=False, check_accessible=True)
             dbkey = vis.dbkey
         else:
             # Create new viz.
             if not dbkey:
                 # If dbkey not specified, use dataset's dbkey.
                 dbkey = dataset.dbkey
-                if not dbkey or dbkey == '?':
+                if not dbkey or dbkey == "?":
                     # Circster requires a valid dbkey.
-                    return trans.show_error_message("You must set the dataset's dbkey to view it. You can set "
-                                                    "a dataset's dbkey by clicking on the pencil icon and editing "
-                                                    "its attributes.", use_panels=True)
+                    return trans.show_error_message(
+                        "You must set the dataset's dbkey to view it. You can set "
+                        "a dataset's dbkey by clicking on the pencil icon and editing "
+                        "its attributes.",
+                        use_panels=True,
+                    )
 
             vis = self.create_visualization(trans, type="genome", dbkey=dbkey, save=False)
 
         # Get the vis config and work with it from here on out. Working with the
         # config is only possible because the config structure of trackster/genome
         # visualizations is well known.
         viz_config = self.get_visualization_config(trans, vis)
 
         # Add dataset if specified.
         if dataset:
-            viz_config['tracks'].append(self.get_new_track_config(trans, dataset))
+            viz_config["tracks"].append(self.get_new_track_config(trans, dataset))
 
         # Get genome info.
         chroms_info = self.app.genomes.chroms(trans, dbkey=dbkey)
-        genome = {'dbkey': dbkey, 'chroms_info': chroms_info}
+        genome = {"dbkey": dbkey, "chroms_info": chroms_info}
 
         # Add genome-wide data to each track in viz.
-        tracks = viz_config.get('tracks', [])
+        tracks = viz_config.get("tracks", [])
         for track in tracks:
-            dataset_dict = track['dataset']
-            dataset = self.get_hda_or_ldda(trans, dataset_dict['hda_ldda'], dataset_dict['id'])
+            dataset_dict = track["dataset"]
+            dataset = self.get_hda_or_ldda(trans, dataset_dict["hda_ldda"], dataset_dict["id"])
 
             genome_data = self._get_genome_data(trans, dataset, dbkey)
-            if not isinstance(genome_data, string_types):
-                track['preloaded_data'] = genome_data
+            if not isinstance(genome_data, str):
+                track["preloaded_data"] = genome_data
 
         # define app configuration for generic mako template
-        app = {
-            'jscript'       : "circster",
-            'viz_config'    : viz_config,
-            'genome'        : genome
-        }
+        app = {"jscript": "circster", "viz_config": viz_config, "genome": genome}
 
         # fill template
-        return trans.fill_template('galaxy.panels.mako', config={'app' : app, 'bundle': 'extended'})
+        return trans.fill_template("visualization/trackster.mako", config={"app": app, "bundle": "extended"})
 
     @web.expose
     def sweepster(self, trans, id=None, hda_ldda=None, dataset_id=None, regions=None):
         """
         Displays a sweepster visualization using the incoming parameters. If id is available,
         get the visualization with the given id; otherwise, create a new visualization using
         a given dataset and regions.
         """
-        regions = regions or '{}'
+        regions = regions or "{}"
         # Need to create history if necessary in order to create tool form.
         trans.get_history(most_recent=True, create=True)
 
         if id:
             # Loading a shared visualization.
             viz = self.get_visualization(trans, id)
             viz_config = self.get_visualization_config(trans, viz)
-            decoded_id = self.decode_id(viz_config['dataset_id'])
+            decoded_id = self.decode_id(viz_config["dataset_id"])
             dataset = self.hda_manager.get_owned(decoded_id, trans.user, current_history=trans.history)
         else:
             # Loading new visualization.
             dataset = self.get_hda_or_ldda(trans, hda_ldda, dataset_id)
             job = self.hda_manager.creating_job(dataset)
-            viz_config = {
-                'dataset_id': dataset_id,
-                'tool_id': job.tool_id,
-                'regions': loads(regions)
-            }
+            viz_config = {"dataset_id": dataset_id, "tool_id": job.tool_id, "regions": loads(regions)}
 
         # Add tool, dataset attributes to config based on id.
-        tool = trans.app.toolbox.get_tool(viz_config['tool_id'])
-        viz_config['tool'] = tool.to_dict(trans, io_details=True)
-        viz_config['dataset'] = trans.security.encode_dict_ids(dataset.to_dict())
+        tool = trans.app.toolbox.get_tool(viz_config["tool_id"])
+        viz_config["tool"] = tool.to_dict(trans, io_details=True)
+        viz_config["dataset"] = trans.security.encode_dict_ids(dataset.to_dict())
 
         return trans.fill_template_mako("visualization/sweepster.mako", config=viz_config)
 
     def get_item(self, trans, id):
         return self.get_visualization(trans, id)
 
     @web.expose
@@ -851,15 +863,15 @@
         config = None
         data = None
 
         # if id, then this is a saved visualization; get its config and the dataset_id from there
         if id:
             visualization = self.get_visualization(trans, id)
             config = self.get_visualization_config(trans, visualization)
-            dataset_id = config.get('dataset_id', None)
+            dataset_id = config.get("dataset_id", None)
 
         # get the hda if we can, then its data using the phyloviz parsers
         if dataset_id:
             decoded_id = self.decode_id(dataset_id)
             hda = self.hda_manager.get_accessible(decoded_id, trans.user)
             hda = self.hda_manager.error_if_uploading(hda)
         else:
@@ -867,76 +879,14 @@
 
         pd = PhylovizDataProvider(original_dataset=hda)
         data = pd.get_data(tree_index=tree_index)
 
         # ensure at least a default configuration (gen. an new/unsaved visualization)
         if not config:
             config = {
-                'dataset_id': dataset_id,
-                'title'     : hda.display_name(),
-                'ext'       : hda.datatype.file_ext,
-                'treeIndex' : tree_index,
-                'saved_visualization' : False
+                "dataset_id": dataset_id,
+                "title": hda.display_name(),
+                "ext": hda.datatype.file_ext,
+                "treeIndex": tree_index,
+                "saved_visualization": False,
             }
         return trans.fill_template_mako("visualization/phyloviz.mako", data=data, config=config)
-
-    @web.expose
-    @web.require_login("run Galaxy Interactive Environments")
-    def gie_list(self, trans, **kwargs):
-        if not hasattr(self, 'gie_image_map'):
-            self.gie_image_map = {}
-
-            for gie_dir in self.app.config.gie_dirs:
-                gie_list = os.listdir(gie_dir)
-                for gie in gie_list:
-                    gie_path = os.path.join(gie_dir, gie)
-
-                    if not os.path.isdir(gie_path):
-                        continue
-
-                    if not os.path.exists(self._gie_config_dir(gie_path)):
-                        continue
-
-                    if os.path.exists(self._gie_config_dir(gie_path, 'allowed_images.yml')):
-                        image_file = self._gie_config_dir(gie_path, 'allowed_images.yml')
-                    elif os.path.exists(self._gie_config_dir(gie_path, 'allowed_images.yml.sample')):
-                        image_file = self._gie_config_dir(gie_path, 'allowed_images.yml.sample')
-                    else:
-                        continue
-
-                    with open(image_file, 'r') as handle:
-                        self.gie_image_map[gie] = yaml.safe_load(handle)
-
-        return trans.fill_template_mako(
-            "visualization/gie.mako",
-            gie_image_map=self.gie_image_map,
-            history=trans.get_history(),
-        )
-
-    def _gie_config_dir(self, gie_path, *args):
-        nargs = [gie_path, 'config']
-        if len(args) > 0:
-            nargs += args
-        return os.path.join(*nargs)
-
-    @web.json
-    def bookmarks_from_dataset(self, trans, hda_id=None, ldda_id=None):
-        if hda_id:
-            hda_ldda = "hda"
-            dataset_id = hda_id
-        elif ldda_id:
-            hda_ldda = "ldda"
-            dataset_id = ldda_id
-        dataset = self.get_hda_or_ldda(trans, hda_ldda, dataset_id)
-
-        rows = []
-        if isinstance(dataset.datatype, Bed):
-            data = RawBedDataProvider(original_dataset=dataset).get_iterator()
-            for i, line in enumerate(data):
-                if (i > 500):
-                    break
-                fields = line.split()
-                location = name = "%s:%s-%s" % (fields[0], fields[1], fields[2])
-                if len(fields) > 3:
-                    name = fields[4]
-                rows.append([location, name])
-        return {'data': rows}
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/app.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/app.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,45 +1,50 @@
 import logging
 import sys
 import time
 
 import galaxy.model
 from galaxy.config import configure_logging
+from galaxy.model.base import SharedModelMapping
 from galaxy.security import idencoding
+from galaxy.structured_app import BasicSharedApp
 from galaxy.web_stack import application_stack_instance
 from . import config
 
 log = logging.getLogger(__name__)
 
 
-class UniverseApplication(object):
+class UniverseApplication(BasicSharedApp):
     """Encapsulates the state of a Universe application"""
 
     def __init__(self, **kwargs):
+        super().__init__()
+        self[BasicSharedApp] = self
         log.debug("python path is: %s", ", ".join(sys.path))
         self.name = "reports"
         # Read config file and check for errors
         self.config = config.Configuration(**kwargs)
         self.config.check()
         configure_logging(self.config)
         self.application_stack = application_stack_instance()
         # Determine the database url
         if self.config.database_connection:
             db_url = self.config.database_connection
         else:
-            db_url = "sqlite:///%s?isolation_level=IMMEDIATE" % self.config.database
+            db_url = f"sqlite:///{self.config.database}?isolation_level=IMMEDIATE"
         # Setup the database engine and ORM
-        self.model = galaxy.model.mapping.init(self.config.file_path,
-                                               db_url,
-                                               self.config.database_engine_options,
-                                               create_tables=True)
+        self.model = galaxy.model.mapping.init(self.config.file_path, db_url, self.config.database_engine_options)
         if not self.config.database_connection:
             self.targets_mysql = False
         else:
-            self.targets_mysql = 'mysql' in self.config.database_connection
+            self.targets_mysql = "mysql" in self.config.database_connection
         # Security helper
         self.security = idencoding.IdEncodingHelper(id_secret=self.config.id_secret)
+
+        self._register_singleton(idencoding.IdEncodingHelper, self.security)
+        self._register_singleton(SharedModelMapping, self.model)
+
         # used for cachebusting -- refactor this into a *SINGLE* UniverseApplication base.
         self.server_starttime = int(time.time())
 
     def shutdown(self):
         pass
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/config.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/config.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,55 +6,56 @@
 from galaxy.util import string_as_bool
 
 log = logging.getLogger(__name__)
 
 
 def resolve_path(path, root):
     """If 'path' is relative make absolute by prepending 'root'"""
-    if not(os.path.isabs(path)):
+    if not (os.path.isabs(path)):
         path = os.path.join(root, path)
     return path
 
 
 class ConfigurationError(Exception):
     pass
 
 
-class Configuration(object):
+class Configuration:
     def __init__(self, **kwargs):
         self.config_dict = kwargs
-        self.root = kwargs.get('root_dir', '.')
+        self.root = kwargs.get("root_dir", ".")
         # Database related configuration
         self.database = resolve_path(kwargs.get("database_file", "database/universe.sqlite"), self.root)
         self.database_connection = kwargs.get("database_connection", False)
         self.database_engine_options = get_database_engine_options(kwargs)
         # Where dataset files are stored
-        self.file_path = resolve_path(kwargs.get("file_path", "database/files"), self.root)
+        self.file_path = resolve_path(kwargs.get("file_path", "database/objects"), self.root)
         self.new_file_path = resolve_path(kwargs.get("new_file_path", "database/tmp"), self.root)
         self.id_secret = kwargs.get("id_secret", "USING THE DEFAULT IS NOT SECURE!")
         self.use_remote_user = string_as_bool(kwargs.get("use_remote_user", "False"))
         self.require_login = string_as_bool(kwargs.get("require_login", "False"))
-        self.template_path = resolve_path(kwargs.get("template_path", "templates"), self.root)
-        self.template_cache_path = resolve_path(kwargs.get("template_cache_path", "database/compiled_templates/reports"), self.root)
+        self.template_cache_path = resolve_path(
+            kwargs.get("template_cache_path", "database/compiled_templates/reports"), self.root
+        )
         self.allow_user_creation = string_as_bool(kwargs.get("allow_user_creation", "True"))
         self.allow_user_deletion = string_as_bool(kwargs.get("allow_user_deletion", "False"))
-        self.log_actions = string_as_bool(kwargs.get('log_actions', 'False'))
-        self.brand = kwargs.get('brand', None)
+        self.log_actions = string_as_bool(kwargs.get("log_actions", "False"))
+        self.brand = kwargs.get("brand", None)
         # Configuration for the message box directly below the masthead.
-        self.message_box_visible = string_as_bool(kwargs.get('message_box_visible', False))
-        self.message_box_content = kwargs.get('message_box_content', None)
-        self.message_box_class = kwargs.get('message_box_class', 'info')
-        self.wiki_url = kwargs.get('wiki_url', 'https://galaxyproject.org/')
-        self.blog_url = kwargs.get('blog_url', None)
-        self.screencasts_url = kwargs.get('screencasts_url', None)
+        self.message_box_visible = string_as_bool(kwargs.get("message_box_visible", False))
+        self.message_box_content = kwargs.get("message_box_content", None)
+        self.message_box_class = kwargs.get("message_box_class", "info")
+        self.wiki_url = kwargs.get("wiki_url", "https://galaxyproject.org/")
+        self.blog_url = kwargs.get("blog_url", None)
+        self.screencasts_url = kwargs.get("screencasts_url", None)
         self.log_events = False
         self.cookie_path = kwargs.get("cookie_path", None)
         self.cookie_domain = kwargs.get("cookie_domain", None)
         # Error logging with sentry
-        self.sentry_dsn = kwargs.get('sentry_dsn', None)
+        self.sentry_dsn = kwargs.get("sentry_dsn", None)
 
         # Security/Policy Compliance
         self.redact_username_in_logs = False
         self.redact_email_in_job_name = False
         self.enable_beta_gdpr = string_as_bool(kwargs.get("enable_beta_gdpr", False))
         if self.enable_beta_gdpr:
             self.redact_username_in_logs = True
@@ -62,17 +63,17 @@
             self.allow_user_deletion = True
 
     def get(self, key, default=None):
         return self.config_dict.get(key, default)
 
     def check(self):
         # Check that required directories exist
-        for path in self.root, self.template_path:
+        for path in (self.root,):
             if not os.path.isdir(path):
-                raise ConfigurationError("Directory does not exist: %s" % path)
+                raise ConfigurationError(f"Directory does not exist: {path}")
 
     @property
     def sentry_dsn_public(self):
         """
         Sentry URL with private key removed for use in client side scripts,
         sentry server will need to be configured to accept events
         """
@@ -84,22 +85,22 @@
 
 def get_database_engine_options(kwargs):
     """
     Allow options for the SQLAlchemy database engine to be passed by using
     the prefix "database_engine_option".
     """
     conversions = {
-        'convert_unicode': string_as_bool,
-        'pool_timeout': int,
-        'echo': string_as_bool,
-        'echo_pool': string_as_bool,
-        'pool_recycle': int,
-        'pool_size': int,
-        'max_overflow': int,
-        'pool_threadlocal': string_as_bool
+        "convert_unicode": string_as_bool,
+        "pool_timeout": int,
+        "echo": string_as_bool,
+        "echo_pool": string_as_bool,
+        "pool_recycle": int,
+        "pool_size": int,
+        "max_overflow": int,
+        "pool_threadlocal": string_as_bool,
     }
     prefix = "database_engine_option_"
     prefix_len = len(prefix)
     rval = {}
     for key, value in kwargs.items():
         if key.startswith(prefix):
             key = key[prefix_len:]
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/history.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/history.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,17 +1,19 @@
-import collections
 import logging
 
 import sqlalchemy as sa
 from markupsafe import escape
 from sqlalchemy import and_
 
 import galaxy.model
 from galaxy import util
-from galaxy.webapps.base.controller import BaseUIController, web
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    web,
+)
 
 log = logging.getLogger(__name__)
 
 
 def int_to_octet(size):
     try:
         size = float(size)
@@ -19,176 +21,205 @@
         return "???"
     except TypeError:
         if size is None:
             return "0 o"
         return "???"
     units = ("o", "Ko", "Mo", "Go", "To")
     no_unit = 0
-    while (size >= 1000):
-        size /= 1000.
+    while size >= 1000:
+        size /= 1000.0
         no_unit += 1
     try:
-        return "%.2f %s" % (size, units[no_unit])
+        return f"{size:.2f} {units[no_unit]}"
     except IndexError:
-        return "%.0f %s" % (size * ((no_unit - len(units) + 1) * 1000.), units[-1])
+        return f"{size * ((no_unit - len(units) + 1) * 1000.0):.0f} {units[-1]}"
 
 
 class History(BaseUIController):
     """
     Class defining functions used by reports to make requests to get
     informations and fill templates before being displayed.
     The name of function must be the same as as the field "action" of
     the "href" dict, in .mako templates (templates/webapps/reports).
     """
+
     @web.expose
     def history_and_dataset_per_user(self, trans, **kwd):
         """
         fill history_and_dataset_per_user.mako template with:
             - user email
             - the number of history and their size
             - the number of dataset
         """
-        message = escape(util.restore_text(kwd.get('message', '')))
-        user_cutoff = int(kwd.get('user_cutoff', 60))
+        message = escape(util.restore_text(kwd.get("message", "")))
+        user_cutoff = int(kwd.get("user_cutoff", 60))
 
         # sort by history space, or by user mail or by number of history/dataset
-        sort_by = kwd.get('sorting', 'User')
-        sorting = 0 if sort_by == 'User' else 1 if sort_by == "HSort" else 2 if sort_by == "DSort" else 3
-        descending = 1 if kwd.get('descending', 'desc') == 'desc' else -1
+        sort_by = kwd.get("sorting", "User")
+        sorting = 0 if sort_by == "User" else 1 if sort_by == "HSort" else 2 if sort_by == "DSort" else 3
+        descending = 1 if kwd.get("descending", "desc") == "desc" else -1
         reverse = descending == 1
 
         # select count (h.id) as history, u.email as email
         # from history h, galaxy_user u
         # where h.user_id = u.id and h.deleted='f'
         # group by email order by email desc
         histories = sa.select(
-            (sa.func.count(galaxy.model.History.table.c.id).label('history'),
-             galaxy.model.User.table.c.email.label('email')),
+            (
+                sa.func.count(galaxy.model.History.table.c.id).label("history"),
+                galaxy.model.User.table.c.email.label("email"),
+            ),
             from_obj=[sa.outerjoin(galaxy.model.History.table, galaxy.model.User.table)],
-            whereclause=and_(galaxy.model.History.table.c.user_id == galaxy.model.User.table.c.id,
-                             galaxy.model.History.table.c.deleted == 'f'),
-            group_by=['email'],
-            order_by=[sa.desc('email'), 'history'])
+            whereclause=and_(
+                galaxy.model.History.table.c.user_id == galaxy.model.User.table.c.id,
+                galaxy.model.History.table.c.deleted == "f",
+            ),
+            group_by=["email"],
+            order_by=[sa.desc("email"), "history"],
+        )
 
         # select u.email, count(d.id)
         # from galaxy_user u, dataset d, history_dataset_association hd,history h
         # where d.id=hd.dataset_id and h.id=hd.history_id and u.id = h.user_id and h.deleted='f'
         # group by u.email;
         datasets = sa.select(
-            (sa.func.count(galaxy.model.Dataset.table.c.id).label('dataset'),
-             sa.func.sum(galaxy.model.Dataset.table.c.total_size).label('size'),
-             galaxy.model.User.table.c.email.label('email')),
-            from_obj=[galaxy.model.User.table,
-                      galaxy.model.Dataset.table,
-                      galaxy.model.HistoryDatasetAssociation.table,
-                      galaxy.model.History.table],
-            whereclause=and_(galaxy.model.Dataset.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.dataset_id,
-                             galaxy.model.History.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.history_id,
-                             galaxy.model.History.table.c.user_id == galaxy.model.User.table.c.id,
-                             galaxy.model.History.table.c.deleted == 'f'),
-            group_by=['email'])
+            (
+                sa.func.count(galaxy.model.Dataset.table.c.id).label("dataset"),
+                sa.func.sum(galaxy.model.Dataset.table.c.total_size).label("size"),
+                galaxy.model.User.table.c.email.label("email"),
+            ),
+            from_obj=[
+                galaxy.model.User.table,
+                galaxy.model.Dataset.table,
+                galaxy.model.HistoryDatasetAssociation.table,
+                galaxy.model.History.table,
+            ],
+            whereclause=and_(
+                galaxy.model.Dataset.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.dataset_id,
+                galaxy.model.History.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.history_id,
+                galaxy.model.History.table.c.user_id == galaxy.model.User.table.c.id,
+                galaxy.model.History.table.c.deleted == "f",
+            ),
+            group_by=["email"],
+        )
 
         # execute requests, replace None fields by "Unknown"
         # transform lists to dict with email as key and
         # number of (history/dataset)/size of history as value
-        histories = dict([(_.email if _.email is not None else "Unknown", int(_.history))
-                          for _ in histories.execute()])
-        datasets = dict([(_.email if _.email is not None else "Unknown", (int(_.dataset), int(_.size)))
-                         for _ in datasets.execute()])
+        histories = {
+            _.email if _.email is not None else "Unknown": int(_.history) for _ in trans.sa_session.execute(histories)
+        }
+        datasets = {
+            _.email if _.email is not None else "Unknown": (int(_.dataset), int(_.size))
+            for _ in trans.sa_session.execute(datasets)
+        }
 
         sort_keys = (
             lambda v: v[0].lower(),
             lambda v: histories.get(v, 0),
             lambda v: datasets.get(v, [0])[0],
-            lambda v: datasets.get(v, [0][0])[1]
+            lambda v: datasets.get(v, [0][0])[1],
         )
 
         # fetch all users
         users = list(set(histories.keys()) | set(datasets.keys()))
 
         # sort users depending on sort function, defined by user choices
         users.sort(key=sort_keys[sorting], reverse=reverse)
         if user_cutoff > 0:
             users = users[:user_cutoff]
 
         # to keep ordered
-        data = collections.OrderedDict()
+        data = {}
         for user in users:
             dataset = datasets.get(user, [0, 0])
             history = histories.get(user, 0)
             data[user] = ("%d (%s)" % (history, int_to_octet(dataset[1])), dataset[0])
 
-        return trans.fill_template('/webapps/reports/history_and_dataset_per_user.mako',
-                                   data=data,
-                                   user_cutoff=user_cutoff,
-                                   sorting=sorting,
-                                   descending=descending,
-                                   message=message)
+        return trans.fill_template(
+            "/webapps/reports/history_and_dataset_per_user.mako",
+            data=data,
+            user_cutoff=user_cutoff,
+            sorting=sorting,
+            descending=descending,
+            message=message,
+        )
 
     @web.expose
     def history_and_dataset_type(self, trans, **kwd):
         """
         fill history_and_dataset_type.mako template with:
             - the name of history
             - the number of dataset foreach type
         """
-        message = escape(util.restore_text(kwd.get('message', '')))
-        user_cutoff = int(kwd.get('user_cutoff', 60))
-        descending = 1 if kwd.get('descending', 'desc') == 'desc' else -1
+        message = escape(util.restore_text(kwd.get("message", "")))
+        user_cutoff = int(kwd.get("user_cutoff", 60))
+        descending = 1 if kwd.get("descending", "desc") == "desc" else -1
         reverse = descending == 1
-        user_selection = kwd.get('user_selection', None)
+        user_selection = kwd.get("user_selection", None)
 
         # select d.state, h.name
         # from dataset d, history h , history_dataset_association hda
         # where hda.history_id=h.id and hda.dataset_id=d.id order by h.state;
-        from_obj = [galaxy.model.Dataset.table, galaxy.model.History.table, galaxy.model.HistoryDatasetAssociation.table]
+        from_obj = [
+            galaxy.model.Dataset.table,
+            galaxy.model.History.table,
+            galaxy.model.HistoryDatasetAssociation.table,
+        ]
         if user_selection is not None:
             from_obj.append(galaxy.model.User.table)
-            whereclause = and_(galaxy.model.Dataset.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.dataset_id,
-                               galaxy.model.History.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.history_id,
-                               galaxy.model.User.table.c.id == galaxy.model.History.table.c.user_id,
-                               galaxy.model.User.table.c.email == user_selection)
+            whereclause = and_(
+                galaxy.model.Dataset.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.dataset_id,
+                galaxy.model.History.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.history_id,
+                galaxy.model.User.table.c.id == galaxy.model.History.table.c.user_id,
+                galaxy.model.User.table.c.email == user_selection,
+            )
         else:
-            whereclause = and_(galaxy.model.Dataset.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.dataset_id,
-                               galaxy.model.History.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.history_id)
-        histories = sa.select((galaxy.model.Dataset.table.c.state.label('state'),
-                               galaxy.model.History.table.c.name.label('name')),
-                              from_obj=from_obj,
-                              whereclause=whereclause,
-                              order_by=['name'])
+            whereclause = and_(
+                galaxy.model.Dataset.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.dataset_id,
+                galaxy.model.History.table.c.id == galaxy.model.HistoryDatasetAssociation.table.c.history_id,
+            )
+        histories = sa.select(
+            (galaxy.model.Dataset.table.c.state.label("state"), galaxy.model.History.table.c.name.label("name")),
+            from_obj=from_obj,
+            whereclause=whereclause,
+            order_by=["name"],
+        )
 
         # execute requests, replace None fields by "Unknown"
-        data = [(_.name if _.name is not None else "NoNamedHistory", _.state)
-                for _ in histories.execute()]
+        data = [
+            (_.name if _.name is not None else "NoNamedHistory", _.state) for _ in trans.sa_session.execute(histories)
+        ]
 
         # sort by names descending or ascending
         data.sort(key=lambda v: v[0].lower(), reverse=reverse)
 
         # fetch names in the first list and status in the second
         if data:
             names, status = zip(*tuple(data))
         else:
             names, status = [], []
 
         possible_status = {"ok": 0, "upload": 1, "paused": 2, "queued": 3, "error": 4, "discarded": 5}
         number_of_possible_status = len(possible_status) + 1  # + 1 to handle unknown status!
 
-        # to keep ordered
-        datas = collections.OrderedDict()
+        datas = {}
         for no, name in enumerate(names):
             if name not in datas:
                 if user_cutoff > 0:
                     if len(datas) == user_cutoff:
                         break
                 # creation of a list containing the number of each status
-                datas[name] = ['-'] * number_of_possible_status
+                datas[name] = ["-"] * number_of_possible_status
             # to not execute it several times, we put it in a variable...
             no_status = possible_status.get(status[no], 6)
-            if datas[name][no_status] == '-':
+            if datas[name][no_status] == "-":
                 datas[name][no_status] = 0
             datas[name][no_status] += 1
 
-        return trans.fill_template('/webapps/reports/history_and_dataset_type.mako',
-                                   data=datas,
-                                   user_cutoff=user_cutoff,
-                                   descending=descending,
-                                   message=message)
+        return trans.fill_template(
+            "/webapps/reports/history_and_dataset_type.mako",
+            data=datas,
+            user_cutoff=user_cutoff,
+            descending=descending,
+            message=message,
+        )
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/home.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/home.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,125 +1,125 @@
 import calendar
 import logging
-from datetime import datetime, timedelta
+from datetime import (
+    datetime,
+    timedelta,
+)
 
 import sqlalchemy as sa
 
 from galaxy import model
-from galaxy.webapps.base.controller import BaseUIController, web
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    web,
+)
 from galaxy.webapps.reports.controllers.query import ReportQueryBuilder
 
 log = logging.getLogger(__name__)
 
 
 class HomePage(BaseUIController, ReportQueryBuilder):
     @web.expose
     def run_stats(self, trans, **kwd):
-        message = ''
+        message = ""
         end_date = datetime.utcnow()
         end_date = datetime(end_date.year, end_date.month, end_date.day, end_date.hour)
         end_date_buffer = datetime(end_date.year, end_date.month, end_date.day, end_date.hour + 1)
         start_hours = end_date - timedelta(1)
         start_days = end_date - timedelta(30)
 
         jf_hr_data = [0] * 24
         jf_dy_data = [0] * 30
         jc_hr_data = [0] * 24
         jc_dy_data = [0] * 30
         et_hr_data = []
         et_dy_data = []
 
         recent_jobs = sa.select(
-            (
-                (model.Job.table.c.id),
-                (model.Job.table.c.create_time).label('create_time'),
-                (model.Job.table.c.update_time).label('update_time')
-            )
+            ((model.Job.id), (model.Job.create_time).label("create_time"), (model.Job.update_time).label("update_time"))
         )
 
-        for job in recent_jobs.execute():
-            if(job.create_time >= start_days and
-               job.create_time < end_date_buffer):
-                if(job.create_time >= start_hours and
-                   job.create_time < end_date_buffer):
+        for job in trans.sa_session.execute(recent_jobs):
+            if job.create_time >= start_days and job.create_time < end_date_buffer:
+                if job.create_time >= start_hours and job.create_time < end_date_buffer:
                     # Get the creation time for the jobs in the past day
                     end_day = end_date.day
                     start_day = job.create_time.day
                     end_hour = end_date.hour
                     start_hour = job.create_time.hour
 
-                    if(end_day != start_day):
+                    if end_day != start_day:
                         hours = (end_hour + 24) - start_hour
                     else:
                         hours = end_hour - start_hour
 
-                    if(hours < 24):
+                    if hours < 24:
                         jc_hr_data[int(hours)] += 1
                     else:
                         jc_dy_data[23] += 1
                 # Get the creation time for jobs in the past 30 days
                 end_month = end_date.month
                 start_month = job.create_time.month
                 end_day = end_date.day
                 start_day = job.create_time.day
 
-                if(end_month != start_month):
+                if end_month != start_month:
                     month_weekday, month_range = calendar.monthrange(job.create_time.year, job.create_time.month)
                     day = (end_day + month_range) - start_day
                 else:
                     day = end_day - start_day
 
-                if(day < 30):
+                if day < 30:
                     jc_dy_data[int(day)] += 1
 
-            if(job.update_time >= start_days and
-               job.update_time < end_date_buffer):
-                if(job.update_time >= start_hours and
-                   job.update_time < end_date_buffer):
+            if job.update_time >= start_days and job.update_time < end_date_buffer:
+                if job.update_time >= start_hours and job.update_time < end_date_buffer:
                     # Get the time finishedfor the jobs in the past day
                     end_day = end_date.day
                     start_day = job.update_time.day
                     end_hour = end_date.hour
                     start_hour = job.update_time.hour
 
-                    if(end_day != start_day):
+                    if end_day != start_day:
                         hours = (end_hour + 23) - start_hour
                     else:
                         hours = end_hour - start_hour
 
-                    if(hours < 24):
+                    if hours < 24:
                         jf_hr_data[int(hours)] += 1
 
                         # Get the Elapsed Time for said job
-                        time = (job.update_time - job.create_time)
+                        time = job.update_time - job.create_time
                         seconds = time.seconds
                         minutes = seconds // 60
                         et_hr_data.append(minutes)
                 # Get the time the job finished and run time in the 30 days
                 end_month = end_date.month
                 start_month = job.update_time.month
                 end_day = end_date.day
                 start_day = job.update_time.day
 
-                if(end_month != start_month):
+                if end_month != start_month:
                     month_weekday, month_range = calendar.monthrange(job.update_time.year, job.update_time.month)
                     day = (end_day + (month_range - 1)) - start_day
                 else:
                     day = end_day - start_day
 
-                if(day < 30):
+                if day < 30:
                     jf_dy_data[int(day)] += 1
 
                     # Get the Elapsed Time for said job
-                    time = (job.update_time - job.create_time)
+                    time = job.update_time - job.create_time
                     seconds = time.seconds
                     minutes = seconds // 60
                     et_dy_data.append(minutes)
 
-        return trans.fill_template('/webapps/reports/run_stats.mako',
+        return trans.fill_template(
+            "/webapps/reports/run_stats.mako",
             jf_hr_data=jf_hr_data,
             jf_dy_data=jf_dy_data,
             jc_hr_data=jc_hr_data,
             jc_dy_data=jc_dy_data,
             et_hr_data=et_hr_data,
             et_dy_data=et_dy_data,
-            message=message)
+            message=message,
+        )
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/jobs.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/jobs.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,47 +1,58 @@
-from __future__ import print_function
-
 import calendar
 import logging
 import re
 from collections import namedtuple
 from datetime import (
     date,
     datetime,
-    timedelta
+    timedelta,
+)
+from math import (
+    ceil,
+    floor,
 )
-from math import ceil, floor
 
 import sqlalchemy as sa
 from markupsafe import escape
-from sqlalchemy import and_, not_, or_
+from sqlalchemy import (
+    and_,
+    not_,
+    or_,
+)
 
-from galaxy import model, util
-from galaxy.webapps.base.controller import BaseUIController, web
+from galaxy import (
+    model,
+    util,
+)
+from galaxy.web.legacy_framework import grids
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    web,
+)
 from galaxy.webapps.reports.controllers.query import ReportQueryBuilder
-from galaxy.webapps.reports.framework import grids
 
 log = logging.getLogger(__name__)
 
 
-class Timer(object):
+class Timer:
     def __init__(self):
         self.start()
         self.stop()
         self.ERROR = self.time_elapsed()
 
     def start(self):
         self.start_time = datetime.now()
 
     def stop(self):
         try:
             self.stop_time = datetime.now()
             self.time_delta = self.stop_time - self.start_time
-            del(self.stop_time)
-            del(self.start_time)
+            del self.stop_time
+            del self.start_time
         except NameError:
             print("You need to start before you can stop!")
 
     def time_elapsed(self):
         try:
             return_time = self.time_delta - self.ERROR
         except NameError:
@@ -52,18 +63,18 @@
         return return_time
 
 
 def sorter(default_sort_id, kwd):
     """
     Initialize sorting variables
     """
-    SortSpec = namedtuple('SortSpec', ['sort_id', 'order', 'arrow', 'exc_order'])
+    SortSpec = namedtuple("SortSpec", ["sort_id", "order", "arrow", "exc_order"])
 
-    sort_id = kwd.get('sort_id')
-    order = kwd.get('order')
+    sort_id = kwd.get("sort_id")
+    order = kwd.get("order")
 
     # Parse the default value
     if sort_id == "default":
         sort_id = default_sort_id
 
     # Create the sort
     if order == "asc":
@@ -102,140 +113,159 @@
     else:
         time_period = "days"
         _time_period = 1.0
 
     return time_period, _time_period
 
 
-class SpecifiedDateListGrid(grids.Grid):
+def get_curr_item(check_item, unique_items):
+    """
+    When rendering by item and destination_id,
+    render the item uniquely.
+    """
+    if check_item in unique_items:
+        return ("", unique_items)
+    unique_items.add(check_item)
+    return (check_item, unique_items)
 
-    class JobIdColumn(grids.IntegerColumn):
 
+class SpecifiedDateListGrid(grids.Grid):
+    class JobIdColumn(grids.IntegerColumn):
         def get_value(self, trans, grid, job):
             return job.id
 
     class StateColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, job):
-            return '<div class="count-box state-color-%s">%s</div>' % (job.state, job.state)
+            return f'<div class="count-box state-color-{job.state}">{job.state}</div>'
 
         def filter(self, trans, user, query, column_filter):
-            if column_filter == 'Unfinished':
-                return query.filter(not_(or_(model.Job.table.c.state == model.Job.states.OK,
-                                             model.Job.table.c.state == model.Job.states.ERROR,
-                                             model.Job.table.c.state == model.Job.states.DELETED)))
+            if column_filter == "Unfinished":
+                return query.filter(
+                    not_(
+                        or_(
+                            model.Job.table.c.state == model.Job.states.OK,
+                            model.Job.table.c.state == model.Job.states.ERROR,
+                            model.Job.table.c.state == model.Job.states.DELETED,
+                        )
+                    )
+                )
             return query
 
     class ToolColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, job):
             return job.tool_id
 
         def filter(self, trans, user, query, column_filter):
             if column_filter is not None:
                 query = query.filter(model.Job.table.c.tool_id == column_filter)
 
             return query
 
     class CreateTimeColumn(grids.DateTimeColumn):
-
         def get_value(self, trans, grid, job):
             return job.create_time.strftime("%b %d, %Y, %H:%M:%S")
 
     class UserColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, job):
             if job.user:
-                return escape(job.user.email)
-            return 'anonymous'
+                return escape(job.get_user_email())
+            return "anonymous"
 
     class EmailColumn(grids.GridColumn):
+        def filter(self, trans, user, query, column_filter):
+            if column_filter == "All":
+                return query
+            return query.filter(
+                and_(model.Job.table.c.user_id == model.User.table.c.id, model.User.table.c.email == column_filter)
+            )
 
+    class DestinationIdColumn(grids.GridColumn):
         def filter(self, trans, user, query, column_filter):
-            if column_filter == 'All':
+            if column_filter == "All":
                 return query
-            return query.filter(and_(model.Job.table.c.user_id == model.User.table.c.id,
-                                     model.User.table.c.email == column_filter))
+            return query.filter(model.Job.table.c.destination_id == column_filter)
 
     class SpecifiedDateColumn(grids.GridColumn):
-
         def filter(self, trans, user, query, column_filter):
-            if column_filter == 'All':
+            if column_filter == "All":
                 return query
             # We are either filtering on a date like YYYY-MM-DD or on a month like YYYY-MM,
             # so we need to figure out which type of date we have
-            if column_filter.count('-') == 2:  # We are filtering on a date like YYYY-MM-DD
+            if column_filter.count("-") == 2:  # We are filtering on a date like YYYY-MM-DD
                 year, month, day = map(int, column_filter.split("-"))
                 start_date = date(year, month, day)
                 end_date = start_date + timedelta(days=1)
-            if column_filter.count('-') == 1:  # We are filtering on a month like YYYY-MM
+            if column_filter.count("-") == 1:  # We are filtering on a month like YYYY-MM
                 year, month = map(int, column_filter.split("-"))
                 start_date = date(year, month, 1)
                 end_date = start_date + timedelta(days=calendar.monthrange(year, month)[1])
 
-            return query.filter(and_(self.model_class.table.c.create_time >= start_date,
-                                     self.model_class.table.c.create_time < end_date))
+            return query.filter(
+                and_(
+                    self.model_class.table.c.create_time >= start_date, self.model_class.table.c.create_time < end_date
+                )
+            )
 
     # Grid definition
     use_async = False
     model_class = model.Job
     title = "Jobs"
     default_sort_key = "id"
     columns = [
-        JobIdColumn("Id",
-                    key="id",
-                    link=(lambda item: dict(operation="job_info", id=item.id, webapp="reports")),
-                    attach_popup=False,
-                    filterable="advanced"),
-        StateColumn("State",
-                    key="state",
-                    attach_popup=False),
-        ToolColumn("Tool Id",
-                   key="tool_id",
-                   link=(lambda item: dict(operation="tool_per_month", id=item.id, webapp="reports")),
-                   attach_popup=False),
-        CreateTimeColumn("Creation Time",
-                         key="create_time",
-                         attach_popup=False),
-        UserColumn("User",
-                   key="email",
-                   model_class=model.User,
-                   link=(lambda item: dict(operation="user_per_month", id=item.id, webapp="reports")),
-                   attach_popup=False),
+        JobIdColumn(
+            "Id",
+            key="id",
+            link=(lambda item: dict(operation="job_info", id=item.id, webapp="reports")),
+            attach_popup=False,
+            filterable="advanced",
+        ),
+        StateColumn("State", key="state", attach_popup=False),
+        DestinationIdColumn("Destination Id", key="destination_id", attach_popup=False),
+        ToolColumn(
+            "Tool Id",
+            key="tool_id",
+            link=(lambda item: dict(operation="tool_per_month", id=item.id, webapp="reports")),
+            attach_popup=False,
+        ),
+        CreateTimeColumn("Creation Time", key="create_time", attach_popup=False),
+        UserColumn(
+            "User",
+            key="email",
+            model_class=model.User,
+            link=(lambda item: dict(operation="user_per_month", id=item.id, webapp="reports")),
+            attach_popup=False,
+        ),
         # Columns that are valid for filtering but are not visible.
-        SpecifiedDateColumn("Specified Date",
-                            key="specified_date",
-                            visible=False),
-        EmailColumn("Email",
-                    key="email",
-                    model_class=model.User,
-                    visible=False),
-        grids.StateColumn("State",
-                          key="state",
-                          visible=False,
-                          filterable="advanced")
+        SpecifiedDateColumn("Specified Date", key="specified_date", visible=False),
+        EmailColumn("Email", key="email", model_class=model.User, visible=False),
+        grids.StateColumn("State", key="state", visible=False, filterable="advanced"),
     ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[1], columns[2]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    standard_filters = []
-    default_filter = {'specified_date': 'All'}
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search",
+            cols_to_filter=[columns[1], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
+    default_filter = {"specified_date": "All"}
     num_rows_per_page = 50
     use_paging = True
 
     def build_initial_query(self, trans, **kwd):
         params = util.Params(kwd)
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
         monitor_user_id = get_monitor_id(trans, monitor_email)
-        return trans.sa_session.query(self.model_class) \
-                               .join(model.User) \
-                               .filter(model.Job.table.c.user_id != monitor_user_id)\
-                               .enable_eagerloads(False)
+        return (
+            trans.sa_session.query(self.model_class)
+            .join(model.User)
+            .filter(model.Job.table.c.user_id != monitor_user_id)
+            .enable_eagerloads(False)
+        )
 
 
 class Jobs(BaseUIController, ReportQueryBuilder):
 
     """
     Class contains functions for querying data requested by user via the webapp. It exposes the functions and
     responds to requests with the filled .mako templates.
@@ -244,67 +274,61 @@
     specified_date_list_grid = SpecifiedDateListGrid()
 
     @web.expose
     def specified_date_handler(self, trans, **kwd):
         # We add params to the keyword dict in this method in order to rename the param
         # with an "f-" prefix, simulating filtering by clicking a search link.  We have
         # to take this approach because the "-" character is illegal in HTTP requests.
-        kwd['sort_id'] = 'default'
-        kwd['order'] = 'default'
+        kwd["sort_id"] = "default"
+        kwd["order"] = "default"
 
-        if 'f-specified_date' in kwd and 'specified_date' not in kwd:
+        if "f-specified_date" in kwd and "specified_date" not in kwd:
             # The user clicked a State link in the Advanced Search box, so 'specified_date'
             # will have been eliminated.
             pass
-        elif 'specified_date' not in kwd:
-            kwd['f-specified_date'] = 'All'
+        elif "specified_date" not in kwd:
+            kwd["f-specified_date"] = "All"
         else:
-            kwd['f-specified_date'] = kwd['specified_date']
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+            kwd["f-specified_date"] = kwd["specified_date"]
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "job_info":
-                return trans.response.send_redirect(web.url_for(controller='jobs',
-                                                                action='job_info',
-                                                                **kwd))
+                return trans.response.send_redirect(web.url_for(controller="jobs", action="job_info", **kwd))
             elif operation == "tool_for_month":
-                kwd['f-tool_id'] = kwd['tool_id']
+                kwd["f-tool_id"] = kwd["tool_id"]
             elif operation == "tool_per_month":
                 # The received id is the job id, so we need to get the job's tool_id.
-                job_id = kwd.get('id', None)
+                job_id = kwd.get("id", None)
                 job = get_job(trans, job_id)
-                kwd['tool_id'] = job.tool_id
-                return trans.response.send_redirect(web.url_for(controller='jobs',
-                                                                action='tool_per_month',
-                                                                **kwd))
+                kwd["tool_id"] = job.tool_id
+                return trans.response.send_redirect(web.url_for(controller="jobs", action="tool_per_month", **kwd))
             elif operation == "user_for_month":
-                kwd['f-email'] = util.restore_text(kwd['email'])
+                kwd["f-email"] = util.restore_text(kwd["email"])
+            elif operation == "user_for_month_by_destination":
+                kwd["f-email"] = util.restore_text(kwd["email"])
+                kwd["f-destination_id"] = kwd["destination_id"]
             elif operation == "user_per_month":
                 # The received id is the job id, so we need to get the id of the user
                 # that submitted the job.
-                job_id = kwd.get('id', None)
+                job_id = kwd.get("id", None)
                 job = get_job(trans, job_id)
-                if job.user:
-                    kwd['email'] = job.user.email
-                else:
-                    kwd['email'] = None  # For anonymous users
-                return trans.response.send_redirect(web.url_for(controller='jobs',
-                                                                action='user_per_month',
-                                                                **kwd))
+                kwd["email"] = job.get_user_email()
+                return trans.response.send_redirect(web.url_for(controller="jobs", action="user_per_month", **kwd))
             elif operation == "specified_date_in_error":
-                kwd['f-state'] = 'error'
+                kwd["f-state"] = "error"
             elif operation == "unfinished":
-                kwd['f-state'] = 'Unfinished'
+                kwd["f-state"] = "Unfinished"
             elif operation == "specified_tool_in_error":
-                kwd['f-state'] = 'error'
-                kwd['f-tool_id'] = kwd['tool_id']
+                kwd["f-state"] = "error"
+                kwd["f-tool_id"] = kwd["tool_id"]
         return self.specified_date_list_grid(trans, **kwd)
 
-    def _calculate_trends_for_jobs(self, jobs_query):
+    def _calculate_trends_for_jobs(self, sa_session, jobs_query):
         trends = dict()
-        for job in jobs_query.execute():
+        for job in sa_session.execute(jobs_query):
             job_day = int(job.date.strftime("%-d")) - 1
             job_month = int(job.date.strftime("%-m"))
             job_month_name = job.date.strftime("%B")
             job_year = job.date.strftime("%Y")
             key = str(job_month_name + job_year)
 
             try:
@@ -312,450 +336,551 @@
             except KeyError:
                 job_year = int(job_year)
                 wday, day_range = calendar.monthrange(job_year, job_month)
                 trends[key] = [0] * day_range
                 trends[key][job_day] += 1
         return trends
 
-    def _calculate_job_table(self, jobs_query):
+    def _calculate_job_table(self, sa_session, jobs_query, by_destination=False):
         jobs = []
-        for row in jobs_query.execute():
+        unique_month_year_strs = set()
+        for row in sa_session.execute(jobs_query):
             month_name = row.date.strftime("%B")
             year = int(row.date.strftime("%Y"))
 
-            jobs.append((
-                row.date.strftime("%Y-%m"),
-                row.total_jobs,
-                month_name,
-                year
-            ))
+            if str(by_destination).lower() == "true":
+                month_year_str = f"{month_name} {year}"
+                curr_month_year_str, unique_month_year_strs = get_curr_item(month_year_str, unique_month_year_strs)
+                if curr_month_year_str == "":
+                    curr_month = ""
+                    curr_year = ""
+                else:
+                    curr_month = row.date.strftime("%B")
+                    curr_year = row.date.strftime("%Y")
+                jobs.append(
+                    (
+                        row.date.strftime("%Y-%m"),
+                        row.total_jobs,
+                        curr_month,
+                        curr_year,
+                        row.user_email,
+                        row.destination_id,
+                        row.execute_time,
+                    )
+                )
+            else:
+                jobs.append((row.date.strftime("%Y-%m"), row.total_jobs, month_name, year))
         return jobs
 
     @web.expose
     def specified_month_all(self, trans, **kwd):
         """
         Queries the DB for all jobs in given month, defaults to current month.
         """
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
         params = util.Params(kwd)
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
-        specs = sorter('date', kwd)
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
+        specs = sorter("date", kwd)
         offset = 0
         limit = 10
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         # In case we don't know which is the monitor user we will query for all jobs
         monitor_user_id = get_monitor_id(trans, monitor_email)
 
         # If specified_date is not received, we'll default to the current month
-        specified_date = kwd.get('specified_date', datetime.utcnow().strftime("%Y-%m-%d"))
+        specified_date = kwd.get("specified_date", datetime.utcnow().strftime("%Y-%m-%d"))
         specified_month = specified_date[:7]
 
         year, month = map(int, specified_month.split("-"))
         start_date = date(year, month, 1)
         end_date = start_date + timedelta(days=calendar.monthrange(year, month)[1])
         month_label = start_date.strftime("%B")
         year_label = start_date.strftime("%Y")
 
         # Use to make the page table
-        month_jobs = sa.select((sa.func.date(model.Job.table.c.create_time).label('date'),
-                                sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                               whereclause=sa.and_(model.Job.table.c.user_id != monitor_user_id,
-                                                   model.Job.table.c.create_time >= start_date,
-                                                   model.Job.table.c.create_time < end_date),
-                               from_obj=[model.Job.table],
-                               group_by=['date'],
-                               order_by=[_order],
-                               offset=offset,
-                               limit=limit)
+        month_jobs = sa.select(
+            (
+                sa.func.date(model.Job.table.c.create_time).label("date"),
+                sa.func.count(model.Job.table.c.id).label("total_jobs"),
+            ),
+            whereclause=sa.and_(
+                model.Job.table.c.user_id != monitor_user_id,
+                model.Job.table.c.create_time >= start_date,
+                model.Job.table.c.create_time < end_date,
+            ),
+            from_obj=[model.Job.table],
+            group_by=["date"],
+            order_by=[_order],
+            offset=offset,
+            limit=limit,
+        )
 
         # Use to make trendline
-        all_jobs = sa.select((model.Job.table.c.create_time.label('date'), model.Job.table.c.id.label('id')),
-                             whereclause=sa.and_(model.Job.table.c.user_id != monitor_user_id,
-                                                 model.Job.table.c.create_time >= start_date,
-                                                 model.Job.table.c.create_time < end_date))
+        all_jobs = sa.select(
+            (model.Job.table.c.create_time.label("date"), model.Job.table.c.id.label("id")),
+            whereclause=sa.and_(
+                model.Job.table.c.user_id != monitor_user_id,
+                model.Job.table.c.create_time >= start_date,
+                model.Job.table.c.create_time < end_date,
+            ),
+        )
 
         trends = dict()
-        for job in all_jobs.execute():
+        for job in trans.sa_session.execute(all_jobs):
             job_hour = int(job.date.strftime("%-H"))
             job_day = job.date.strftime("%d")
 
             try:
                 trends[job_day][job_hour] += 1
             except KeyError:
                 trends[job_day] = [0] * 24
                 trends[job_day][job_hour] += 1
 
         jobs = []
-        for row in month_jobs.execute():
+        for row in trans.sa_session.execute(month_jobs):
             row_dayname = row.date.strftime("%A")
             row_day = row.date.strftime("%d")
 
-            jobs.append((row_dayname,
-                         row_day,
-                         row.total_jobs,
-                         row.date))
+            jobs.append((row_dayname, row_day, row.total_jobs, row.date))
 
         pages_found = ceil(len(jobs) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/jobs_specified_month_all.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   month_label=month_label,
-                                   year_label=year_label,
-                                   month=month,
-                                   page_specs=page_specs,
-                                   jobs=jobs,
-                                   trends=trends,
-                                   is_user_jobs_only=monitor_user_id,
-                                   message=message)
+        return trans.fill_template(
+            "/webapps/reports/jobs_specified_month_all.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            month_label=month_label,
+            year_label=year_label,
+            month=month,
+            page_specs=page_specs,
+            jobs=jobs,
+            trends=trends,
+            is_user_jobs_only=monitor_user_id,
+            message=message,
+        )
 
     @web.expose
     def specified_month_in_error(self, trans, **kwd):
         """
         Queries the DB for the user jobs in error.
         """
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
         params = util.Params(kwd)
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
-        specs = sorter('date', kwd)
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
+        specs = sorter("date", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         # In case we don't know which is the monitor user we will query for all jobs instead
         monitor_user_id = get_monitor_id(trans, monitor_email)
 
         # If specified_date is not received, we'll default to the current month
-        specified_date = kwd.get('specified_date', datetime.utcnow().strftime("%Y-%m-%d"))
+        specified_date = kwd.get("specified_date", datetime.utcnow().strftime("%Y-%m-%d"))
         specified_month = specified_date[:7]
         year, month = map(int, specified_month.split("-"))
         start_date = date(year, month, 1)
         end_date = start_date + timedelta(days=calendar.monthrange(year, month)[1])
         month_label = start_date.strftime("%B")
         year_label = start_date.strftime("%Y")
 
-        month_jobs_in_error = sa.select((sa.func.date(model.Job.table.c.create_time).label('date'),
-                                         sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                                        whereclause=sa.and_(model.Job.table.c.user_id != monitor_user_id,
-                                                            model.Job.table.c.state == 'error',
-                                                            model.Job.table.c.create_time >= start_date,
-                                                            model.Job.table.c.create_time < end_date),
-                                        from_obj=[model.Job.table],
-                                        group_by=['date'],
-                                        order_by=[_order],
-                                        offset=offset,
-                                        limit=limit)
+        month_jobs_in_error = sa.select(
+            (
+                sa.func.date(model.Job.table.c.create_time).label("date"),
+                sa.func.count(model.Job.table.c.id).label("total_jobs"),
+            ),
+            whereclause=sa.and_(
+                model.Job.table.c.user_id != monitor_user_id,
+                model.Job.table.c.state == "error",
+                model.Job.table.c.create_time >= start_date,
+                model.Job.table.c.create_time < end_date,
+            ),
+            from_obj=[model.Job.table],
+            group_by=["date"],
+            order_by=[_order],
+            offset=offset,
+            limit=limit,
+        )
 
         # Use to make trendline
-        all_jobs_in_error = sa.select((model.Job.table.c.create_time.label('date'), model.Job.table.c.id.label('id')),
-                                      whereclause=sa.and_(model.Job.table.c.user_id != monitor_user_id,
-                                                          model.Job.table.c.state == 'error',
-                                                          model.Job.table.c.create_time >= start_date,
-                                                          model.Job.table.c.create_time < end_date))
+        all_jobs_in_error = sa.select(
+            (model.Job.table.c.create_time.label("date"), model.Job.table.c.id.label("id")),
+            whereclause=sa.and_(
+                model.Job.table.c.user_id != monitor_user_id,
+                model.Job.table.c.state == "error",
+                model.Job.table.c.create_time >= start_date,
+                model.Job.table.c.create_time < end_date,
+            ),
+        )
 
         trends = dict()
-        for job in all_jobs_in_error.execute():
+        for job in trans.sa_session.execute(all_jobs_in_error):
             job_hour = int(job.date.strftime("%-H"))
             job_day = job.date.strftime("%d")
 
             try:
                 trends[job_day][job_hour] += 1
             except KeyError:
                 trends[job_day] = [0] * 24
                 trends[job_day][job_hour] += 1
 
         jobs = []
-        for row in month_jobs_in_error.execute():
+        for row in trans.sa_session.execute(month_jobs_in_error):
             row_dayname = row.date.strftime("%A")
             row_day = row.date.strftime("%d")
 
-            jobs.append((row_dayname,
-                         row_day,
-                         row.total_jobs,
-                         row.date))
+            jobs.append((row_dayname, row_day, row.total_jobs, row.date))
 
         pages_found = ceil(len(jobs) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/jobs_specified_month_in_error.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   month_label=month_label,
-                                   year_label=year_label,
-                                   month=month,
-                                   jobs=jobs,
-                                   trends=trends,
-                                   message=message,
-                                   is_user_jobs_only=monitor_user_id,
-                                   page_specs=page_specs)
+        return trans.fill_template(
+            "/webapps/reports/jobs_specified_month_in_error.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            month_label=month_label,
+            year_label=year_label,
+            month=month,
+            jobs=jobs,
+            trends=trends,
+            message=message,
+            is_user_jobs_only=monitor_user_id,
+            page_specs=page_specs,
+        )
 
     @web.expose
     def per_month_all(self, trans, **kwd):
         """
-        Queries the DB for all jobs. Avoids monitor jobs.
+        Queries the DB for all jobs. Avoids monitor jobs.  The
+        by_destination param will group by User.email and
+        Job.destination_id.
         """
-
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        by_destination = str(kwd.get("by_destination", False)).lower()
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
         params = util.Params(kwd)
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
-        specs = sorter('date', kwd)
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
+        specs = sorter("date", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         # In case we don't know which is the monitor user we will query for all jobs
         monitor_user_id = get_monitor_id(trans, monitor_email)
 
         # Use to make the page table
-        jobs_by_month = sa.select((self.select_month(model.Job.table.c.create_time).label('date'),
-                                   sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                                  whereclause=model.Job.table.c.user_id != monitor_user_id,
-                                  from_obj=[model.Job.table],
-                                  group_by=self.group_by_month(model.Job.table.c.create_time),
-                                  order_by=[_order],
-                                  offset=offset,
-                                  limit=limit)
+        if by_destination == "true":
+            jobs_by_month = sa.select(
+                (
+                    self.select_month(model.Job.table.c.create_time).label("date"),
+                    model.Job.table.c.destination_id.label("destination_id"),
+                    sa.func.sum(model.Job.table.c.update_time - model.Job.table.c.create_time).label("execute_time"),
+                    sa.func.count(model.Job.table.c.id).label("total_jobs"),
+                    model.User.table.c.email.label("user_email"),
+                ),
+                whereclause=model.Job.table.c.user_id != monitor_user_id,
+                from_obj=[sa.join(model.Job.table, model.User.table)],
+                group_by=["user_email", "date", "destination_id"],
+                order_by=[_order],
+                offset=offset,
+                limit=limit,
+            )
+        else:
+            jobs_by_month = sa.select(
+                (
+                    self.select_month(model.Job.table.c.create_time).label("date"),
+                    sa.func.count(model.Job.table.c.id).label("total_jobs"),
+                ),
+                whereclause=model.Job.table.c.user_id != monitor_user_id,
+                from_obj=[model.Job.table],
+                group_by=self.group_by_month(model.Job.table.c.create_time),
+                order_by=[_order],
+                offset=offset,
+                limit=limit,
+            )
 
         # Use to make sparkline
-        all_jobs = sa.select((self.select_day(model.Job.table.c.create_time).label('date'),
-                              model.Job.table.c.id.label('id')))
+        all_jobs = sa.select(
+            (self.select_day(model.Job.table.c.create_time).label("date"), model.Job.table.c.id.label("id"))
+        )
 
-        trends = self._calculate_trends_for_jobs(all_jobs)
-        jobs = self._calculate_job_table(jobs_by_month)
+        trends = self._calculate_trends_for_jobs(trans.sa_session, all_jobs)
+        jobs = self._calculate_job_table(trans.sa_session, jobs_by_month, by_destination=by_destination)
 
         pages_found = ceil(len(jobs) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/jobs_per_month_all.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   trends=trends,
-                                   jobs=jobs,
-                                   is_user_jobs_only=monitor_user_id,
-                                   message=message,
-                                   page_specs=page_specs)
+        if by_destination == "true":
+            page = "/webapps/reports/jobs_per_month_by_user_and_destination.mako"
+        else:
+            page = "/webapps/reports/jobs_per_month_all.mako"
+        return trans.fill_template(
+            page,
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            trends=trends,
+            jobs=jobs,
+            is_user_jobs_only=monitor_user_id,
+            message=message,
+            page_specs=page_specs,
+        )
 
     @web.expose
     def per_month_in_error(self, trans, **kwd):
         """
         Queries the DB for user jobs in error. Filters out monitor jobs.
         """
 
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
         params = util.Params(kwd)
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
-        specs = sorter('date', kwd)
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
+        specs = sorter("date", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         # In case we don't know which is the monitor user we will query for all jobs
         monitor_user_id = get_monitor_id(trans, monitor_email)
 
         # Use to make the page table
-        jobs_in_error_by_month = sa.select((self.select_month(model.Job.table.c.create_time).label('date'),
-                                            sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                                           whereclause=sa.and_(model.Job.table.c.state == 'error',
-                                                               model.Job.table.c.user_id != monitor_user_id),
-                                           from_obj=[model.Job.table],
-                                           group_by=self.group_by_month(model.Job.table.c.create_time),
-                                           order_by=[_order],
-                                           offset=offset,
-                                           limit=limit)
+        jobs_in_error_by_month = sa.select(
+            (
+                self.select_month(model.Job.table.c.create_time).label("date"),
+                sa.func.count(model.Job.table.c.id).label("total_jobs"),
+            ),
+            whereclause=sa.and_(model.Job.table.c.state == "error", model.Job.table.c.user_id != monitor_user_id),
+            from_obj=[model.Job.table],
+            group_by=self.group_by_month(model.Job.table.c.create_time),
+            order_by=[_order],
+            offset=offset,
+            limit=limit,
+        )
 
         # Use to make trendline
-        all_jobs = sa.select((self.select_day(model.Job.table.c.create_time).label('date'),
-                              model.Job.table.c.id.label('id')),
-                             whereclause=sa.and_(model.Job.table.c.state == 'error',
-                                                 model.Job.table.c.user_id != monitor_user_id))
+        all_jobs = sa.select(
+            (self.select_day(model.Job.table.c.create_time).label("date"), model.Job.table.c.id.label("id")),
+            whereclause=sa.and_(model.Job.table.c.state == "error", model.Job.table.c.user_id != monitor_user_id),
+        )
 
-        trends = self._calculate_trends_for_jobs(all_jobs)
-        jobs = self._calculate_job_table(jobs_in_error_by_month)
+        trends = self._calculate_trends_for_jobs(trans.sa_session, all_jobs)
+        jobs = self._calculate_job_table(trans.sa_session, jobs_in_error_by_month)
 
         pages_found = ceil(len(jobs) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/jobs_per_month_in_error.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   trends=trends,
-                                   jobs=jobs,
-                                   message=message,
-                                   is_user_jobs_only=monitor_user_id,
-                                   page_specs=page_specs,
-                                   offset=offset,
-                                   limit=limit)
+        return trans.fill_template(
+            "/webapps/reports/jobs_per_month_in_error.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            trends=trends,
+            jobs=jobs,
+            message=message,
+            is_user_jobs_only=monitor_user_id,
+            page_specs=page_specs,
+            offset=offset,
+            limit=limit,
+        )
 
     @web.expose
     def per_user(self, trans, **kwd):
+        """
+        Queries the DB for jobs per user.  The by_destination
+        param will group by Job.destination_id.
+        """
+        by_destination = str(kwd.get("by_destination", False)).lower()
         total_time = Timer()
         q_time = Timer()
 
         total_time.start()
         params = util.Params(kwd)
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
-        specs = sorter('user_email', kwd)
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
+        specs = sorter("user_email", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
-        time_period = kwd.get('spark_time')
+        time_period = kwd.get("spark_time")
         time_period, _time_period = get_spark_time(time_period)
         spark_limit = 30
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         jobs = []
-        jobs_per_user = sa.select((model.User.table.c.email.label('user_email'),
-                                   sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                                  from_obj=[sa.outerjoin(model.Job.table, model.User.table)],
-                                  group_by=['user_email'],
-                                  order_by=[_order],
-                                  offset=offset,
-                                  limit=limit)
+        if by_destination == "true":
+            jobs_per_user = sa.select(
+                (
+                    model.User.table.c.email.label("user_email"),
+                    sa.func.count(model.Job.table.c.id).label("total_jobs"),
+                    model.Job.table.c.destination_id.label("destination_id"),
+                ),
+                from_obj=[sa.outerjoin(model.Job.table, model.User.table)],
+                group_by=["user_email", "destination_id"],
+                order_by=[_order],
+                offset=offset,
+                limit=limit,
+            )
+
+        else:
+            jobs_per_user = sa.select(
+                (model.User.table.c.email.label("user_email"), sa.func.count(model.Job.table.c.id).label("total_jobs")),
+                from_obj=[sa.outerjoin(model.Job.table, model.User.table)],
+                group_by=["user_email"],
+                order_by=[_order],
+                offset=offset,
+                limit=limit,
+            )
 
         q_time.start()
-        for row in jobs_per_user.execute():
-            if (row.user_email is None):
-                jobs.append(('Anonymous',
-                             row.total_jobs))
-            elif (row.user_email == monitor_email):
+        unique_users = set()
+        for row in trans.sa_session.execute(jobs_per_user):
+            if row.user_email is None:
+                curr_user, unique_users = get_curr_item("Anonymous", unique_users)
+                if by_destination == "true":
+                    jobs.append((curr_user, row.destination_id, row.total_jobs))
+                else:
+                    jobs.append((curr_user, row.total_jobs))
+            elif row.user_email == monitor_email:
                 continue
             else:
-                jobs.append((row.user_email,
-                             row.total_jobs))
+                curr_user, unique_users = get_curr_item(row.user_email, unique_users)
+                if by_destination == "true":
+                    jobs.append((curr_user, row.destination_id, row.total_jobs))
+                else:
+                    jobs.append((curr_user, row.total_jobs))
         q_time.stop()
         query1time = q_time.time_elapsed()
 
-        users = sa.select([model.User.table.c.email],
-                          from_obj=[model.User.table])
+        users = sa.select([model.User.table.c.email], from_obj=[model.User.table])
 
-        all_jobs_per_user = sa.select((model.Job.table.c.id.label('id'),
-                                       model.Job.table.c.create_time.label('date'),
-                                       model.User.table.c.email.label('user_email')),
-                                      from_obj=[sa.outerjoin(model.Job.table, model.User.table)],
-                                      whereclause=model.User.table.c.email.in_(users))
+        all_jobs_per_user = sa.select(
+            (
+                model.Job.table.c.id.label("id"),
+                model.Job.table.c.create_time.label("date"),
+                model.User.table.c.email.label("user_email"),
+            ),
+            from_obj=[sa.outerjoin(model.Job.table, model.User.table)],
+            whereclause=model.User.table.c.email.in_(users),
+        )
 
         currday = datetime.today()
         trends = dict()
         q_time.start()
-        for job in all_jobs_per_user.execute():
+        for job in trans.sa_session.execute(all_jobs_per_user):
             if job.user_email is None:
-                curr_user = 'Anonymous'
+                curr_user = "Anonymous"
             else:
-                curr_user = re.sub(r'\W+', '', job.user_email)
+                curr_user = re.sub(r"\W+", "", job.user_email)
 
             try:
                 day = currday - job.date
             except TypeError:
                 day = currday - datetime.date(job.date)
 
             day = day.days
@@ -772,54 +897,84 @@
         query2time = q_time.time_elapsed()
 
         pages_found = ceil(len(jobs) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
         total_time.stop()
         ttime = total_time.time_elapsed()
-        return trans.fill_template('/webapps/reports/jobs_per_user.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   spark_limit=spark_limit,
-                                   time_period=time_period,
-                                   q1time=query1time,
-                                   q2time=query2time,
-                                   ttime=ttime,
-                                   trends=trends,
-                                   jobs=jobs,
-                                   message=message,
-                                   page_specs=page_specs)
+        if by_destination == "true":
+            page = "/webapps/reports/jobs_per_user_by_destination.mako"
+        else:
+            page = "/webapps/reports/jobs_per_user.mako"
+        return trans.fill_template(
+            page,
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            spark_limit=spark_limit,
+            time_period=time_period,
+            q1time=query1time,
+            q2time=query2time,
+            ttime=ttime,
+            trends=trends,
+            jobs=jobs,
+            message=message,
+            page_specs=page_specs,
+        )
 
     @web.expose
     def user_per_month(self, trans, **kwd):
+        """
+        Queries the DB for jobs per user per month.  The
+        by_destination param will group by Job.destination_id.
+        """
+        by_destination = str(kwd.get("by_destination", False)).lower()
         params = util.Params(kwd)
-        message = ''
+        message = ""
 
-        email = util.restore_text(params.get('email', ''))
-        specs = sorter('date', kwd)
+        email = util.restore_text(params.get("email", ""))
+        specs = sorter("date", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
 
-        q = sa.select((self.select_month(model.Job.table.c.create_time).label('date'),
-                       sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                      whereclause=model.User.table.c.email == email,
-                      from_obj=[sa.join(model.Job.table, model.User.table)],
-                      group_by=self.group_by_month(model.Job.table.c.create_time),
-                      order_by=[_order])
-
-        all_jobs_per_user = sa.select((model.Job.table.c.create_time.label('date'),
-                                       model.Job.table.c.id.label('job_id')),
-                                      whereclause=sa.and_(model.User.table.c.email == email),
-                                      from_obj=[sa.join(model.Job.table, model.User.table)])
+        if by_destination == "true":
+            q = sa.select(
+                (
+                    self.select_month(model.Job.table.c.create_time).label("date"),
+                    model.Job.table.c.destination_id.label("destination_id"),
+                    sa.func.sum(model.Job.table.c.update_time - model.Job.table.c.create_time).label("execute_time"),
+                    sa.func.count(model.Job.table.c.id).label("total_jobs"),
+                ),
+                whereclause=model.User.table.c.email == email,
+                from_obj=[sa.join(model.Job.table, model.User.table)],
+                group_by=["date", "destination_id"],
+                order_by=[_order],
+            )
+        else:
+            q = sa.select(
+                (
+                    self.select_month(model.Job.table.c.create_time).label("date"),
+                    sa.func.count(model.Job.table.c.id).label("total_jobs"),
+                ),
+                whereclause=model.User.table.c.email == email,
+                from_obj=[sa.join(model.Job.table, model.User.table)],
+                group_by=self.group_by_month(model.Job.table.c.create_time),
+                order_by=[_order],
+            )
+
+        all_jobs_per_user = sa.select(
+            (model.Job.table.c.create_time.label("date"), model.Job.table.c.id.label("job_id")),
+            whereclause=sa.and_(model.User.table.c.email == email),
+            from_obj=[sa.join(model.Job.table, model.User.table)],
+        )
 
         trends = dict()
-        for job in all_jobs_per_user.execute():
+        for job in trans.sa_session.execute(all_jobs_per_user):
             job_day = int(job.date.strftime("%-d")) - 1
             job_month = int(job.date.strftime("%-m"))
             job_month_name = job.date.strftime("%B")
             job_year = job.date.strftime("%Y")
             key = str(job_month_name + job_year)
 
             try:
@@ -827,85 +982,117 @@
             except KeyError:
                 job_year = int(job_year)
                 wday, day_range = calendar.monthrange(job_year, job_month)
                 trends[key] = [0] * day_range
                 trends[key][job_day] += 1
 
         jobs = []
-        for row in q.execute():
-            jobs.append((row.date.strftime("%Y-%m"),
-                         row.total_jobs,
-                         row.date.strftime("%B"),
-                         row.date.strftime("%Y")))
-        return trans.fill_template('/webapps/reports/jobs_user_per_month.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   id=kwd.get('id'),
-                                   trends=trends,
-                                   email=util.sanitize_text(email),
-                                   jobs=jobs, message=message)
+        unique_month_year_strs = set()
+        for row in trans.sa_session.execute(q):
+            if by_destination == "true":
+                month_year_str = f"{row.date.strftime('%B')} {row.date.strftime('%Y')}"
+                curr_month_year_str, unique_month_year_strs = get_curr_item(month_year_str, unique_month_year_strs)
+                if curr_month_year_str == "":
+                    curr_month = ""
+                    curr_year = ""
+                else:
+                    curr_month = row.date.strftime("%B")
+                    curr_year = row.date.strftime("%Y")
+                jobs.append(
+                    (
+                        row.date.strftime("%Y-%m"),
+                        row.execute_time,
+                        row.total_jobs,
+                        curr_month,
+                        curr_year,
+                        row.destination_id,
+                    )
+                )
+            else:
+                jobs.append(
+                    (row.date.strftime("%Y-%m"), row.total_jobs, row.date.strftime("%B"), row.date.strftime("%Y"))
+                )
+        if by_destination == "true":
+            page = "/webapps/reports/jobs_user_per_month_by_destination.mako"
+        else:
+            page = "/webapps/reports/jobs_user_per_month.mako"
+        return trans.fill_template(
+            page,
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            id=kwd.get("id"),
+            trends=trends,
+            email=util.sanitize_text(email),
+            jobs=jobs,
+            message=message,
+        )
 
     @web.expose
     def per_tool(self, trans, **kwd):
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
         params = util.Params(kwd)
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
-        specs = sorter('tool_id', kwd)
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
+        specs = sorter("tool_id", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
-        time_period = kwd.get('spark_time')
+        time_period = kwd.get("spark_time")
         time_period, _time_period = get_spark_time(time_period)
         spark_limit = 30
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         # In case we don't know which is the monitor user we will query for all jobs
         monitor_user_id = get_monitor_id(trans, monitor_email)
 
         jobs = []
-        q = sa.select((model.Job.table.c.tool_id.label('tool_id'),
-                       sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                      whereclause=model.Job.table.c.user_id != monitor_user_id,
-                      from_obj=[model.Job.table],
-                      group_by=['tool_id'],
-                      order_by=[_order],
-                      offset=offset,
-                      limit=limit)
-
-        all_jobs_per_tool = sa.select((model.Job.table.c.tool_id.label('tool_id'),
-                                       model.Job.table.c.id.label('id'),
-                                       self.select_day(model.Job.table.c.create_time).label('date')),
-                                      whereclause=model.Job.table.c.user_id != monitor_user_id,
-                                      from_obj=[model.Job.table])
+        q = sa.select(
+            (model.Job.table.c.tool_id.label("tool_id"), sa.func.count(model.Job.table.c.id).label("total_jobs")),
+            whereclause=model.Job.table.c.user_id != monitor_user_id,
+            from_obj=[model.Job.table],
+            group_by=["tool_id"],
+            order_by=[_order],
+            offset=offset,
+            limit=limit,
+        )
+
+        all_jobs_per_tool = sa.select(
+            (
+                model.Job.table.c.tool_id.label("tool_id"),
+                model.Job.table.c.id.label("id"),
+                self.select_day(model.Job.table.c.create_time).label("date"),
+            ),
+            whereclause=model.Job.table.c.user_id != monitor_user_id,
+            from_obj=[model.Job.table],
+        )
 
         currday = date.today()
         trends = dict()
-        for job in all_jobs_per_tool.execute():
-            curr_tool = re.sub(r'\W+', '', str(job.tool_id))
+        for job in trans.sa_session.execute(all_jobs_per_tool):
+            curr_tool = re.sub(r"\W+", "", str(job.tool_id))
             try:
                 day = currday - job.date
             except TypeError:
                 day = currday - datetime.date(job.date)
 
             day = day.days
             container = floor(day / _time_period)
@@ -914,95 +1101,99 @@
                 if container < spark_limit:
                     trends[curr_tool][container] += 1
             except KeyError:
                 trends[curr_tool] = [0] * spark_limit
                 if container < spark_limit:
                     trends[curr_tool][container] += 1
 
-        for row in q.execute():
-            jobs.append((row.tool_id,
-                         row.total_jobs))
+        for row in trans.sa_session.execute(q):
+            jobs.append((row.tool_id, row.total_jobs))
 
         pages_found = ceil(len(jobs) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/jobs_per_tool.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   spark_limit=spark_limit,
-                                   time_period=time_period,
-                                   trends=trends,
-                                   jobs=jobs,
-                                   message=message,
-                                   is_user_jobs_only=monitor_user_id,
-                                   page_specs=page_specs)
+        return trans.fill_template(
+            "/webapps/reports/jobs_per_tool.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            spark_limit=spark_limit,
+            time_period=time_period,
+            trends=trends,
+            jobs=jobs,
+            message=message,
+            is_user_jobs_only=monitor_user_id,
+            page_specs=page_specs,
+        )
 
     @web.expose
     def errors_per_tool(self, trans, **kwd):
         """
         Queries the DB for user jobs in error. Filters out monitor jobs.
         """
 
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
         params = util.Params(kwd)
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
-        specs = sorter('tool_id', kwd)
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
+        specs = sorter("tool_id", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
-        time_period = kwd.get('spark_time')
+        time_period = kwd.get("spark_time")
         time_period, _time_period = get_spark_time(time_period)
         spark_limit = 30
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         # In case we don't know which is the monitor user we will query for all jobs
         monitor_user_id = get_monitor_id(trans, monitor_email)
 
-        jobs_in_error_per_tool = sa.select((model.Job.table.c.tool_id.label('tool_id'),
-                                            sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                                           whereclause=sa.and_(model.Job.table.c.state == 'error',
-                                                               model.Job.table.c.user_id != monitor_user_id),
-                                           from_obj=[model.Job.table],
-                                           group_by=['tool_id'],
-                                           order_by=[_order],
-                                           offset=offset,
-                                           limit=limit)
-
-        all_jobs_per_tool_errors = sa.select((self.select_day(model.Job.table.c.create_time).label('date'),
-                                              model.Job.table.c.id.label('id'),
-                                              model.Job.table.c.tool_id.label('tool_id')),
-                                             whereclause=sa.and_(model.Job.table.c.state == 'error',
-                                                                 model.Job.table.c.user_id != monitor_user_id),
-                                             from_obj=[model.Job.table])
+        jobs_in_error_per_tool = sa.select(
+            (model.Job.table.c.tool_id.label("tool_id"), sa.func.count(model.Job.table.c.id).label("total_jobs")),
+            whereclause=sa.and_(model.Job.table.c.state == "error", model.Job.table.c.user_id != monitor_user_id),
+            from_obj=[model.Job.table],
+            group_by=["tool_id"],
+            order_by=[_order],
+            offset=offset,
+            limit=limit,
+        )
+
+        all_jobs_per_tool_errors = sa.select(
+            (
+                self.select_day(model.Job.table.c.create_time).label("date"),
+                model.Job.table.c.id.label("id"),
+                model.Job.table.c.tool_id.label("tool_id"),
+            ),
+            whereclause=sa.and_(model.Job.table.c.state == "error", model.Job.table.c.user_id != monitor_user_id),
+            from_obj=[model.Job.table],
+        )
 
         currday = date.today()
         trends = dict()
-        for job in all_jobs_per_tool_errors.execute():
-            curr_tool = re.sub(r'\W+', '', str(job.tool_id))
+        for job in trans.sa_session.execute(all_jobs_per_tool_errors):
+            curr_tool = re.sub(r"\W+", "", str(job.tool_id))
             try:
                 day = currday - job.date
             except TypeError:
                 day = currday - datetime.date(job.date)
 
             # convert day into days/weeks/months/years
             day = day.days
@@ -1012,65 +1203,73 @@
                 if container < spark_limit:
                     trends[curr_tool][container] += 1
             except KeyError:
                 trends[curr_tool] = [0] * spark_limit
                 if day < spark_limit:
                     trends[curr_tool][container] += 1
         jobs = []
-        for row in jobs_in_error_per_tool.execute():
+        for row in trans.sa_session.execute(jobs_in_error_per_tool):
             jobs.append((row.total_jobs, row.tool_id))
 
         pages_found = ceil(len(jobs) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/jobs_errors_per_tool.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   spark_limit=spark_limit,
-                                   time_period=time_period,
-                                   trends=trends,
-                                   jobs=jobs,
-                                   message=message,
-                                   is_user_jobs_only=monitor_user_id,
-                                   page_specs=page_specs)
+        return trans.fill_template(
+            "/webapps/reports/jobs_errors_per_tool.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            spark_limit=spark_limit,
+            time_period=time_period,
+            trends=trends,
+            jobs=jobs,
+            message=message,
+            is_user_jobs_only=monitor_user_id,
+            page_specs=page_specs,
+        )
 
     @web.expose
     def tool_per_month(self, trans, **kwd):
-        message = ''
+        message = ""
 
         params = util.Params(kwd)
-        monitor_email = params.get('monitor_email', 'monitor@bx.psu.edu')
-        specs = sorter('date', kwd)
+        monitor_email = params.get("monitor_email", "monitor@bx.psu.edu")
+        specs = sorter("date", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
         # In case we don't know which is the monitor user we will query for all jobs
         monitor_user_id = get_monitor_id(trans, monitor_email)
 
-        tool_id = params.get('tool_id', 'Add a column1')
-        specified_date = params.get('specified_date', datetime.utcnow().strftime("%Y-%m-%d"))
-        q = sa.select((self.select_month(model.Job.table.c.create_time).label('date'),
-                       sa.func.count(model.Job.table.c.id).label('total_jobs')),
-                      whereclause=sa.and_(model.Job.table.c.tool_id == tool_id,
-                                          model.Job.table.c.user_id != monitor_user_id),
-                      from_obj=[model.Job.table],
-                      group_by=self.group_by_month(model.Job.table.c.create_time),
-                      order_by=[_order])
+        tool_id = params.get("tool_id", "Add a column1")
+        specified_date = params.get("specified_date", datetime.utcnow().strftime("%Y-%m-%d"))
+        q = sa.select(
+            (
+                self.select_month(model.Job.table.c.create_time).label("date"),
+                sa.func.count(model.Job.table.c.id).label("total_jobs"),
+            ),
+            whereclause=sa.and_(model.Job.table.c.tool_id == tool_id, model.Job.table.c.user_id != monitor_user_id),
+            from_obj=[model.Job.table],
+            group_by=self.group_by_month(model.Job.table.c.create_time),
+            order_by=[_order],
+        )
 
         # Use to make sparkline
-        all_jobs_for_tool = sa.select((self.select_month(model.Job.table.c.create_time).label('month'),
-                                       self.select_day(model.Job.table.c.create_time).label('day'),
-                                       model.Job.table.c.id.label('id')),
-                                      whereclause=sa.and_(model.Job.table.c.tool_id == tool_id,
-                                                          model.Job.table.c.user_id != monitor_user_id),
-                             from_obj=[model.Job.table])
+        all_jobs_for_tool = sa.select(
+            (
+                self.select_month(model.Job.table.c.create_time).label("month"),
+                self.select_day(model.Job.table.c.create_time).label("day"),
+                model.Job.table.c.id.label("id"),
+            ),
+            whereclause=sa.and_(model.Job.table.c.tool_id == tool_id, model.Job.table.c.user_id != monitor_user_id),
+            from_obj=[model.Job.table],
+        )
         trends = dict()
-        for job in all_jobs_for_tool.execute():
+        for job in trans.sa_session.execute(all_jobs_for_tool):
             job_day = int(job.day.strftime("%-d")) - 1
             job_month = int(job.month.strftime("%-m"))
             job_month_name = job.month.strftime("%B")
             job_year = job.month.strftime("%Y")
             key = str(job_month_name + job_year)
 
             try:
@@ -1078,50 +1277,47 @@
             except KeyError:
                 job_year = int(job_year)
                 wday, day_range = calendar.monthrange(job_year, job_month)
                 trends[key] = [0] * day_range
                 trends[key][job_day] += 1
 
         jobs = []
-        for row in q.execute():
-            jobs.append((row.date.strftime("%Y-%m"),
-                         row.total_jobs,
-                         row.date.strftime("%B"),
-                         row.date.strftime("%Y")))
-        return trans.fill_template('/webapps/reports/jobs_tool_per_month.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   specified_date=specified_date,
-                                   tool_id=tool_id,
-                                   trends=trends,
-                                   jobs=jobs,
-                                   message=message,
-                                   is_user_jobs_only=monitor_user_id)
+        for row in trans.sa_session.execute(q):
+            jobs.append((row.date.strftime("%Y-%m"), row.total_jobs, row.date.strftime("%B"), row.date.strftime("%Y")))
+        return trans.fill_template(
+            "/webapps/reports/jobs_tool_per_month.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            specified_date=specified_date,
+            tool_id=tool_id,
+            trends=trends,
+            jobs=jobs,
+            message=message,
+            is_user_jobs_only=monitor_user_id,
+        )
 
     @web.expose
     def job_info(self, trans, **kwd):
-        message = ''
-        job = trans.sa_session.query(model.Job) \
-                              .get(trans.security.decode_id(kwd.get('id', '')))
-        return trans.fill_template('/webapps/reports/job_info.mako',
-                                   job=job,
-                                   message=message)
+        message = ""
+        job = trans.sa_session.query(model.Job).get(trans.security.decode_id(kwd.get("id", "")))
+        return trans.fill_template("/webapps/reports/job_info.mako", job=job, message=message)
+
 
 # ---- Utility methods -------------------------------------------------------
 
 
 def get_job(trans, id):
     return trans.sa_session.query(trans.model.Job).get(trans.security.decode_id(id))
 
 
 def get_monitor_id(trans, monitor_email):
     """
     A convenience method to obtain the monitor job id.
     """
     monitor_user_id = None
-    monitor_row = trans.sa_session.query(trans.model.User.table.c.id) \
-        .filter(trans.model.User.table.c.email == monitor_email) \
-        .first()
+    monitor_row = (
+        trans.sa_session.query(trans.model.User.id).filter(trans.model.User.table.c.email == monitor_email).first()
+    )
     if monitor_row is not None:
         monitor_user_id = monitor_row[0]
     return monitor_user_id
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/system.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/system.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,214 +1,238 @@
 import logging
-import os
-import subprocess
-from datetime import datetime, timedelta
+import shutil
+from datetime import (
+    datetime,
+    timedelta,
+)
 from decimal import Decimal
 
-from sqlalchemy import and_, desc, false, null, true
-from sqlalchemy.orm import eagerload
-
-from galaxy import model, util
-from galaxy.util import unicodify
-from galaxy.webapps.base.controller import BaseUIController, web
+from sqlalchemy import (
+    and_,
+    desc,
+    false,
+    null,
+    true,
+)
+from sqlalchemy.orm import joinedload
+
+from galaxy import (
+    model,
+    util,
+)
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    web,
+)
 
 log = logging.getLogger(__name__)
 
 
 class System(BaseUIController):
     @web.expose
     def index(self, trans, **kwd):
         params = util.Params(kwd)
-        message = ''
+        message = ""
         if params.userless_histories_days:
             userless_histories_days = params.userless_histories_days
         else:
-            userless_histories_days = '60'
+            userless_histories_days = "60"
         if params.deleted_histories_days:
             deleted_histories_days = params.deleted_histories_days
         else:
-            deleted_histories_days = '60'
+            deleted_histories_days = "60"
         if params.deleted_datasets_days:
             deleted_datasets_days = params.deleted_datasets_days
         else:
-            deleted_datasets_days = '60'
+            deleted_datasets_days = "60"
         file_path, disk_usage, datasets, file_size_str = self.disk_usage(trans, **kwd)
-        if 'action' in kwd:
-            if kwd['action'] == "userless_histories":
+        if "action" in kwd:
+            if kwd["action"] == "userless_histories":
                 userless_histories_days, message = self.userless_histories(trans, **kwd)
-            elif kwd['action'] == "deleted_histories":
+            elif kwd["action"] == "deleted_histories":
                 deleted_histories_days, message = self.deleted_histories(trans, **kwd)
-            elif kwd['action'] == "deleted_datasets":
+            elif kwd["action"] == "deleted_datasets":
                 deleted_datasets_days, message = self.deleted_datasets(trans, **kwd)
-        return trans.fill_template('/webapps/reports/system.mako',
-                                   file_path=file_path,
-                                   disk_usage=disk_usage,
-                                   datasets=datasets,
-                                   file_size_str=file_size_str,
-                                   userless_histories_days=userless_histories_days,
-                                   deleted_histories_days=deleted_histories_days,
-                                   deleted_datasets_days=deleted_datasets_days,
-                                   message=message,
-                                   nice_size=nice_size)
+        return trans.fill_template(
+            "/webapps/reports/system.mako",
+            file_path=file_path,
+            disk_usage=disk_usage,
+            datasets=datasets,
+            file_size_str=file_size_str,
+            userless_histories_days=userless_histories_days,
+            deleted_histories_days=deleted_histories_days,
+            deleted_datasets_days=deleted_datasets_days,
+            message=message,
+            nice_size=nice_size,
+        )
 
     def userless_histories(self, trans, **kwd):
         """The number of userless histories and associated datasets that have not been updated for the specified number of days."""
         params = util.Params(kwd)
-        message = ''
+        message = ""
         if params.userless_histories_days:
             userless_histories_days = int(params.userless_histories_days)
             cutoff_time = datetime.utcnow() - timedelta(days=userless_histories_days)
             history_count = 0
             dataset_count = 0
-            for history in trans.sa_session.query(model.History) \
-                    .filter(and_(model.History.table.c.user_id == null(),
+            for history in trans.sa_session.query(model.History).filter(
+                and_(
+                    model.History.table.c.user_id == null(),
                     model.History.table.c.deleted == true(),
-                    model.History.table.c.update_time < cutoff_time)):
+                    model.History.update_time < cutoff_time,
+                )
+            ):
                 for dataset in history.datasets:
                     if not dataset.deleted:
                         dataset_count += 1
                 history_count += 1
-            message = "%d userless histories ( including a total of %d datasets ) have not been updated for at least %d days." % (history_count, dataset_count, userless_histories_days)
+            message = (
+                "%d userless histories ( including a total of %d datasets ) have not been updated for at least %d days."
+                % (history_count, dataset_count, userless_histories_days)
+            )
         else:
             message = "Enter the number of days."
         return str(userless_histories_days), message
 
     def deleted_histories(self, trans, **kwd):
         """
         The number of histories that were deleted more than the specified number of days ago, but have not yet been purged.
         Also included is the number of datasets associated with the histories.
         """
         params = util.Params(kwd)
-        message = ''
+        message = ""
         if params.deleted_histories_days:
             deleted_histories_days = int(params.deleted_histories_days)
             cutoff_time = datetime.utcnow() - timedelta(days=deleted_histories_days)
             history_count = 0
             dataset_count = 0
             disk_space = 0
-            histories = trans.sa_session.query(model.History) \
-                .filter(and_(model.History.table.c.deleted == true(),
-                    model.History.table.c.purged == false(),
-                    model.History.table.c.update_time < cutoff_time)) \
-                .options(eagerload('datasets'))
+            histories = (
+                trans.sa_session.query(model.History)
+                .filter(
+                    and_(
+                        model.History.table.c.deleted == true(),
+                        model.History.table.c.purged == false(),
+                        model.History.update_time < cutoff_time,
+                    )
+                )
+                .options(joinedload(model.History.datasets))
+            )
 
             for history in histories:
                 for hda in history.datasets:
                     if not hda.dataset.purged:
                         dataset_count += 1
                         try:
                             disk_space += hda.dataset.file_size
                         except Exception:
                             pass
                 history_count += 1
-            message = "%d histories ( including a total of %d datasets ) were deleted more than %d days ago, but have not yet been purged, " \
+            message = (
+                "%d histories ( including a total of %d datasets ) were deleted more than %d days ago, but have not yet been purged, "
                 "disk space: %s." % (history_count, dataset_count, deleted_histories_days, nice_size(disk_space, True))
+            )
         else:
             message = "Enter the number of days."
         return str(deleted_histories_days), message
 
     def deleted_datasets(self, trans, **kwd):
         """The number of datasets that were deleted more than the specified number of days ago, but have not yet been purged."""
         params = util.Params(kwd)
-        message = ''
+        message = ""
         if params.deleted_datasets_days:
             deleted_datasets_days = int(params.deleted_datasets_days)
             cutoff_time = datetime.utcnow() - timedelta(days=deleted_datasets_days)
             dataset_count = 0
             disk_space = 0
-            for dataset in trans.sa_session.query(model.Dataset) \
-                .filter(and_(model.Dataset.table.c.deleted == true(),
+            for dataset in trans.sa_session.query(model.Dataset).filter(
+                and_(
+                    model.Dataset.table.c.deleted == true(),
                     model.Dataset.table.c.purged == false(),
-                    model.Dataset.table.c.update_time < cutoff_time)):
+                    model.Dataset.table.c.update_time < cutoff_time,
+                )
+            ):
                 dataset_count += 1
                 try:
                     disk_space += dataset.file_size
                 except Exception:
                     pass
-            message = "%d datasets were deleted more than %d days ago, but have not yet been purged," \
+            message = (
+                "%d datasets were deleted more than %d days ago, but have not yet been purged,"
                 " disk space: %s." % (dataset_count, deleted_datasets_days, nice_size(disk_space, True))
+            )
         else:
             message = "Enter the number of days."
         return str(deleted_datasets_days), message
 
     @web.expose
     def dataset_info(self, trans, **kwd):
-        message = ''
-        dataset = trans.sa_session.query(model.Dataset).get(trans.security.decode_id(kwd.get('id', '')))
+        message = ""
+        dataset = trans.sa_session.query(model.Dataset).get(trans.security.decode_id(kwd.get("id", "")))
         # Get all associated hdas and lddas that use the same disk file.
-        associated_hdas = trans.sa_session.query(trans.model.HistoryDatasetAssociation) \
-            .filter(and_(trans.model.HistoryDatasetAssociation.deleted == false(),
-            trans.model.HistoryDatasetAssociation.dataset_id == dataset.id)) \
+        associated_hdas = (
+            trans.sa_session.query(trans.model.HistoryDatasetAssociation)
+            .filter(
+                and_(
+                    trans.model.HistoryDatasetAssociation.deleted == false(),
+                    trans.model.HistoryDatasetAssociation.dataset_id == dataset.id,
+                )
+            )
             .all()
-        associated_lddas = trans.sa_session.query(trans.model.LibraryDatasetDatasetAssociation) \
-            .filter(and_(trans.model.LibraryDatasetDatasetAssociation.deleted == false(),
-            trans.model.LibraryDatasetDatasetAssociation.dataset_id == dataset.id)) \
+        )
+        associated_lddas = (
+            trans.sa_session.query(trans.model.LibraryDatasetDatasetAssociation)
+            .filter(
+                and_(
+                    trans.model.LibraryDatasetDatasetAssociation.deleted == false(),
+                    trans.model.LibraryDatasetDatasetAssociation.dataset_id == dataset.id,
+                )
+            )
             .all()
-        return trans.fill_template('/webapps/reports/dataset_info.mako',
-                                   dataset=dataset,
-                                   associated_hdas=associated_hdas,
-                                   associated_lddas=associated_lddas,
-                                   message=message)
+        )
+        return trans.fill_template(
+            "/webapps/reports/dataset_info.mako",
+            dataset=dataset,
+            associated_hdas=associated_hdas,
+            associated_lddas=associated_lddas,
+            message=message,
+        )
 
     def get_disk_usage(self, file_path):
-        is_sym_link = os.path.islink(file_path)
-        file_system = disk_size = disk_used = disk_avail = disk_cap_pct = mount = None
-        df_output = unicodify(subprocess.check_output(['df', '-h', file_path]))
-
-        for df_line in df_output.splitlines():
-            df_line = df_line.strip()
-            if df_line:
-                df_line = df_line.lower()
-                if 'filesystem' in df_line or 'proc' in df_line:
-                    continue
-                elif is_sym_link:
-                    if ':' in df_line and '/' in df_line:
-                        mount = df_line
-                    else:
-                        try:
-                            disk_size, disk_used, disk_avail, disk_cap_pct, file_system = df_line.split()
-                            break
-                        except Exception:
-                            pass
-                else:
-                    try:
-                        file_system, disk_size, disk_used, disk_avail, disk_cap_pct, mount = df_line.split()
-                        break
-                    except Exception:
-                        pass
-            else:
-                break  # EOF
-        return (file_system, disk_size, disk_used, disk_avail, disk_cap_pct, mount)
+        disk_usage = shutil.disk_usage(file_path)
+        pct_used = round(disk_usage.used / disk_usage.total * 100, 2)
+        return (nice_size(disk_usage.total), nice_size(disk_usage.used), nice_size(disk_usage.free), pct_used)
 
     @web.expose
     def disk_usage(self, trans, **kwd):
         file_path = trans.app.config.file_path
         disk_usage = self.get_disk_usage(file_path)
-        min_file_size = 2 ** 32  # 4 Gb
+        min_file_size = 2**32  # 4 Gb
         file_size_str = nice_size(min_file_size)
-        datasets = trans.sa_session.query(model.Dataset) \
-                                   .filter(and_(model.Dataset.table.c.purged == false(),
-                                                model.Dataset.table.c.file_size > min_file_size)) \
-                                   .order_by(desc(model.Dataset.table.c.file_size))
+        datasets = (
+            trans.sa_session.query(model.Dataset)
+            .filter(and_(model.Dataset.table.c.purged == false(), model.Dataset.table.c.file_size > min_file_size))
+            .order_by(desc(model.Dataset.table.c.file_size))
+        )
         return file_path, disk_usage, datasets, file_size_str
 
 
 def nice_size(size, include_bytes=False):
     """Returns a readably formatted string with the size"""
     niced = False
-    nice_string = "%s bytes" % size
+    nice_string = f"{size} bytes"
     try:
         nsize = Decimal(size)
-        for x in ['bytes', 'KB', 'MB', 'GB']:
+        for x in ["bytes", "KB", "MB", "GB"]:
             if nsize.compare(Decimal("1024.0")) == Decimal("-1"):
-                nice_string = "%3.1f %s" % (nsize, x)
+                nice_string = f"{nsize:3.1f} {x}"
                 niced = True
                 break
             nsize /= Decimal("1024.0")
         if not niced:
-            nice_string = "%3.1f %s" % (nsize, 'TB')
+            nice_string = f"{nsize:3.1f} TB"
             niced = True
-        if include_bytes and x != 'bytes':
-            nice_string = "%s (%s bytes)" % (nice_string, size)
+        if include_bytes and x != "bytes":
+            nice_string = f"{nice_string} ({size} bytes)"
     except Exception:
         pass
     return nice_string
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/tools.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/tools.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,22 +1,23 @@
-import collections
 import logging
 from datetime import timedelta
 
 import sqlalchemy as sa
 from markupsafe import escape
 from sqlalchemy import and_
 
 import galaxy.model
 from galaxy.util import (
     restore_text,
-    unicodify
+    unicodify,
+)
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    web,
 )
-from galaxy.webapps.base.controller import BaseUIController, web
-
 
 log = logging.getLogger(__name__)
 
 
 def int_to_octet(size):
     try:
         size = float(size)
@@ -24,195 +25,212 @@
         return "???"
     except TypeError:
         if size is None:
             return "0 o"
         return "???"
     units = ("o", "Ko", "Mo", "Go", "To")
     no_unit = 0
-    while (size >= 1000):
-        size /= 1000.
+    while size >= 1000:
+        size /= 1000.0
         no_unit += 1
     try:
-        return "%.2f %s" % (size, units[no_unit])
+        return f"{size:.2f} {units[no_unit]}"
     except IndexError:
-        return "%.0f %s" % (size * ((no_unit - len(units) + 1) * 1000.), units[-1])
+        return f"{size * ((no_unit - len(units) + 1) * 1000.0):.0f} {units[-1]}"
 
 
 class Tools(BaseUIController):
     """
     Class defining functions used by reports to make requests to get
     informations and fill templates before being displayed.
     The name of function must be the same as as the field "action" of
     the "href" dict, in .mako templates (templates/webapps/reports).
     """
 
     def formatted(self, date, colored=False):
-        splited = str(date).split(',')
+        splited = str(date).split(",")
         if len(splited) == 2:
-            returned = "%s %dH" % (splited[0], int(splited[1].split(':')[0]))
+            returned = "%s %dH" % (splited[0], int(splited[1].split(":")[0]))
             if colored:
-                return '<font color="red">' + returned + '</font>'
+                return f'<font color="red">{returned}</font>'
             return returned
         else:
-            splited = tuple([float(_) for _ in str(date).split(':')])
+            splited = tuple(float(_) for _ in str(date).split(":"))
             if splited[0]:
-                returned = '%d h. %d min.' % splited[:2]
+                returned = "%d h. %d min." % splited[:2]
                 if colored:
-                    return '<font color="orange">' + returned + '</font>'
+                    return f'<font color="orange">{returned}</font>'
                 return returned
             if splited[1]:
                 return "%d min. %d sec." % splited[1:3]
-            return "%.1f sec." % splited[2]
+            return f"{splited[2]:.1f} sec."
 
     @web.expose
     def tools_and_job_state(self, trans, **kwd):
         """
         fill tools_and_job_state.mako template with
             - the name of the tool
             - the number of jobs using this tool in state 'ok'
             - the number of jobs using this tool in error
         """
 
-        message = escape(restore_text(kwd.get('message', '')))
-        user_cutoff = int(kwd.get('user_cutoff', 60))
+        message = escape(restore_text(kwd.get("message", "")))
+        user_cutoff = int(kwd.get("user_cutoff", 60))
 
         # sort by history space, or by user mail or by number of history/dataset
-        sort_by = kwd.get('sorting', 'Tool')
-        sorting = 0 if sort_by == 'Tool' else 1 if sort_by == 'ok' else 2
-        descending = 1 if kwd.get('descending', 'desc') == 'desc' else -1
+        sort_by = kwd.get("sorting", "Tool")
+        sorting = 0 if sort_by == "Tool" else 1 if sort_by == "ok" else 2
+        descending = 1 if kwd.get("descending", "desc") == "desc" else -1
         reverse = descending == 1
         sort_keys = (
             lambda v: v.lower(),
             lambda v: tools_and_jobs_ok.get(v, 0),
-            lambda v: tools_and_jobs_error.get(v, 0))
+            lambda v: tools_and_jobs_error.get(v, 0),
+        )
 
-        data = collections.OrderedDict()
+        data = {}
 
         # select count(id), tool_id from job where state='ok' group by tool_id;
-        tools_and_jobs_ok = sa.select((galaxy.model.Job.table.c.tool_id .label('tool'),
-                                       sa.func.count(galaxy.model.Job.table.c.id).label('job')),
-                                      from_obj=[galaxy.model.Job.table],
-                                      whereclause=(galaxy.model.Job.table.c.state == 'ok'),
-                                      group_by=['tool'])
+        tools_and_jobs_ok = sa.select(
+            (galaxy.model.Job.table.c.tool_id.label("tool"), sa.func.count(galaxy.model.Job.table.c.id).label("job")),
+            from_obj=[galaxy.model.Job.table],
+            whereclause=(galaxy.model.Job.table.c.state == "ok"),
+            group_by=["tool"],
+        )
 
         # select count(id), tool_id from job where state='error' group by tool_id;
-        tools_and_jobs_error = sa.select((galaxy.model.Job.table.c.tool_id .label('tool'),
-                                          sa.func.count(galaxy.model.Job.table.c.id).label('job')),
-                                         from_obj=[galaxy.model.Job.table],
-                                         whereclause=(galaxy.model.Job.table.c.state == 'error'),
-                                         group_by=['tool'])
+        tools_and_jobs_error = sa.select(
+            (galaxy.model.Job.table.c.tool_id.label("tool"), sa.func.count(galaxy.model.Job.table.c.id).label("job")),
+            from_obj=[galaxy.model.Job.table],
+            whereclause=(galaxy.model.Job.table.c.state == "error"),
+            group_by=["tool"],
+        )
 
-        tools_and_jobs_ok = dict(list(tools_and_jobs_ok.execute()))
-        tools_and_jobs_error = dict(list(tools_and_jobs_error.execute()))
+        tools_and_jobs_ok = dict(list(trans.sa_session.execute(tools_and_jobs_ok)))
+        tools_and_jobs_error = dict(list(trans.sa_session.execute(tools_and_jobs_error)))
 
         # select each job name one time
         tools = list(set(tools_and_jobs_ok.keys()) | set(tools_and_jobs_error.keys()))
         tools.sort(key=sort_keys[sorting], reverse=reverse)
 
         for tool in tools:
-            data[tool] = (str(tools_and_jobs_ok.get(tool, '-')), str(tools_and_jobs_error.get(tool, '-')))
+            data[tool] = (str(tools_and_jobs_ok.get(tool, "-")), str(tools_and_jobs_error.get(tool, "-")))
 
-        return trans.fill_template('/webapps/reports/tools_and_job_state.mako',
-                                   data=data,
-                                   user_cutoff=user_cutoff,
-                                   sorting=sorting,
-                                   descending=descending,
-                                   message=message)
+        return trans.fill_template(
+            "/webapps/reports/tools_and_job_state.mako",
+            data=data,
+            user_cutoff=user_cutoff,
+            sorting=sorting,
+            descending=descending,
+            message=message,
+        )
 
     @web.expose
     def tools_and_job_state_per_month(self, trans, **kwd):
         """
         fill tools_and_job_state_per_month.mako template with
             - the name of the tool
             - the number of jobs using this tool in state 'ok'
             - the number of jobs using this tool in error
         """
 
-        message = escape(restore_text(kwd.get('message', '')))
-        user_cutoff = int(kwd.get('user_cutoff', 60))
+        message = escape(restore_text(kwd.get("message", "")))
+        user_cutoff = int(kwd.get("user_cutoff", 60))
 
         # sort by history space, or by user mail or by number of history/dataset
         # sort_by = kwd.get( 'sorting', 'Tool' )
         # sorting = 0 if sort_by == 'Tool' else 1 if sort_by == 'ok' else 2
         # descending = 1 if kwd.get( 'descending', 'desc' ) == 'desc' else -1
-        tool = kwd.get('tool', None)
+        tool = kwd.get("tool", None)
 
         if tool is None:
             raise TypeError("Tool can't be None")
 
-        data = collections.OrderedDict()
+        data = {}
 
         # select count(id), create_time from job where state='ok' and tool_id=$tool group by date;
-        date_and_jobs_ok = sa.select((sa.func.date(galaxy.model.Job.table.c.create_time).label('date'),
-                                      sa.func.count(galaxy.model.Job.table.c.id).label('job')),
-                                     from_obj=[galaxy.model.Job.table],
-                                     whereclause=and_(galaxy.model.Job.table.c.state == 'ok', galaxy.model.Job.table.c.tool_id == tool),
-                                     group_by=['date'])
+        date_and_jobs_ok = sa.select(
+            (
+                sa.func.date(galaxy.model.Job.table.c.create_time).label("date"),
+                sa.func.count(galaxy.model.Job.table.c.id).label("job"),
+            ),
+            from_obj=[galaxy.model.Job.table],
+            whereclause=and_(galaxy.model.Job.table.c.state == "ok", galaxy.model.Job.table.c.tool_id == tool),
+            group_by=["date"],
+        )
 
         # select count(id), create_time from job where state='error' and tool_id=$tool group by date;
-        date_and_jobs_error = sa.select((sa.func.date(galaxy.model.Job.table.c.create_time).label('date'),
-                                         sa.func.count(galaxy.model.Job.table.c.id).label('job')),
-                                        from_obj=[galaxy.model.Job.table],
-                                        whereclause=and_(galaxy.model.Job.table.c.state == 'error', galaxy.model.Job.table.c.tool_id == tool),
-                                        group_by=['date'])
+        date_and_jobs_error = sa.select(
+            (
+                sa.func.date(galaxy.model.Job.table.c.create_time).label("date"),
+                sa.func.count(galaxy.model.Job.table.c.id).label("job"),
+            ),
+            from_obj=[galaxy.model.Job.table],
+            whereclause=and_(galaxy.model.Job.table.c.state == "error", galaxy.model.Job.table.c.tool_id == tool),
+            group_by=["date"],
+        )
 
-        date_and_jobs_ok = dict(list(date_and_jobs_ok.execute()))
-        date_and_jobs_error = dict(list(date_and_jobs_error.execute()))
+        date_and_jobs_ok = dict(list(trans.sa_session.execute(date_and_jobs_ok)))
+        date_and_jobs_error = dict(list(trans.sa_session.execute(date_and_jobs_error)))
 
         # select each date
         dates = list(set(date_and_jobs_ok.keys()) | set(date_and_jobs_error.keys()))
         dates.sort(reverse=True)
         for date in dates:
             date_key = date.strftime("%B %Y")
             if date_key not in data:
                 data[date_key] = [int(date_and_jobs_ok.get(date, 0)), int(date_and_jobs_error.get(date, 0))]
             else:
                 data[date_key][0] += int(date_and_jobs_ok.get(date, 0))
                 data[date_key][1] += int(date_and_jobs_error.get(date, 0))
 
-        return trans.fill_template('/webapps/reports/tools_and_job_state_per_month.mako',
-                                   data=data,
-                                   tool=tool,
-                                   user_cutoff=user_cutoff,
-                                   message=message)
+        return trans.fill_template(
+            "/webapps/reports/tools_and_job_state_per_month.mako",
+            data=data,
+            tool=tool,
+            user_cutoff=user_cutoff,
+            message=message,
+        )
 
     @web.expose
     def tool_execution_time(self, trans, **kwd):
         """
         Fill the template tool_execution_time.mako with informations:
             - Tool name
             - Tool average execution time
             - last job execution time
             - min and max execution time
         """
 
         # liste des tools + temps moyen d'exec du job + temps d'execution du dernier job + tps min et max / mois (?)
         user_cutoff = int(kwd.get("user_cutoff", 60))
         sort_by = kwd.get("sort_by", "tool")
-        descending = 1 if kwd.get('descending', 'desc') == 'desc' else -1
+        descending = 1 if kwd.get("descending", "desc") == "desc" else -1
         reverse = descending == 1
         sort_by = 0 if sort_by == "tool" else 1 if sort_by == "avg" else 2 if sort_by == "min" else 3
-        color = True if kwd.get("color", '') == "True" else False
+        color = True if kwd.get("color", "") == "True" else False
 
         data = {}
-        ordered_data = collections.OrderedDict()
+        ordered_data = {}
 
-        sort_keys = (
-            lambda v: v.lower(),
-            lambda v: data[v]['avg'],
-            lambda v: data[v]['min'],
-            lambda v: data[v]['max'])
-
-        jobs_times = sa.select((galaxy.model.Job.table.c.tool_id.label("name"),
-                                galaxy.model.Job.table.c.create_time.label("create_time"),
-                                galaxy.model.Job.table.c.update_time.label("update_time"),
-                                galaxy.model.Job.table.c.update_time - galaxy.model.Job.table.c.create_time),
-                               from_obj=[galaxy.model.Job.table])
+        sort_keys = (lambda v: v.lower(), lambda v: data[v]["avg"], lambda v: data[v]["min"], lambda v: data[v]["max"])
 
-        jobs_times = [(name, (create, update, time)) for name, create, update, time in jobs_times.execute()]
+        jobs_times = sa.select(
+            (
+                galaxy.model.Job.table.c.tool_id.label("name"),
+                galaxy.model.Job.table.c.create_time.label("create_time"),
+                galaxy.model.Job.table.c.update_time.label("update_time"),
+                galaxy.model.Job.table.c.update_time - galaxy.model.Job.table.c.create_time,
+            ),
+            from_obj=[galaxy.model.Job.table],
+        )
+
+        jobs_times = [
+            (name, (create, update, time)) for name, create, update, time in trans.sa_session.execute(jobs_times)
+        ]
         for tool, attr in jobs_times:
             if tool not in data:
                 data[tool] = {"last": [(attr[1], attr[0])], "avg": [attr[2]]}
             else:
                 data[tool]["last"].append((attr[1], attr[0]))
                 data[tool]["avg"].append(attr[2])
 
@@ -224,126 +242,149 @@
             data[tool]["avg"] = sum(data[tool]["avg"], timedelta()) / len(data[tool]["avg"])
 
         tools = list(data.keys())
         if user_cutoff:
             tools = tools[:user_cutoff]
         tools.sort(key=sort_keys[sort_by], reverse=reverse)
         for tool in tools:
-            ordered_data[tool] = {"min": self.formatted(data[tool]["min"], color),
-                                  "max": self.formatted(data[tool]["max"], color),
-                                  "avg": self.formatted(data[tool]["avg"], color),
-                                  "last": self.formatted(data[tool]["last"], color)}
-
-        return trans.fill_template('/webapps/reports/tool_execution_time.mako',
-                                   data=ordered_data,
-                                   descending=descending,
-                                   user_cutoff=user_cutoff,
-                                   sort_by=sort_by)
+            ordered_data[tool] = {
+                "min": self.formatted(data[tool]["min"], color),
+                "max": self.formatted(data[tool]["max"], color),
+                "avg": self.formatted(data[tool]["avg"], color),
+                "last": self.formatted(data[tool]["last"], color),
+            }
+
+        return trans.fill_template(
+            "/webapps/reports/tool_execution_time.mako",
+            data=ordered_data,
+            descending=descending,
+            user_cutoff=user_cutoff,
+            sort_by=sort_by,
+        )
 
     @web.expose
     def tool_execution_time_per_month(self, trans, **kwd):
         """
         Fill the template tool_execution_time_per_month.mako with informations:
             - Tool average execution time
             - last job execution time
             - min and max execution time
         """
 
         # liste des tools + temps moyen d'exec du job + temps d'execution du dernier job + tps min et max / mois(?)
         user_cutoff = int(kwd.get("user_cutoff", 60))
         sort_by = kwd.get("sort_by", "month")
-        descending = 1 if kwd.get('descending', 'desc') == 'desc' else -1
+        descending = 1 if kwd.get("descending", "desc") == "desc" else -1
         reverse = descending == 1
         sort_by = 0 if sort_by == "month" else 1 if sort_by == "min" else 2 if sort_by == "max" else 3
         tool = kwd.get("tool", None)
-        color = True if kwd.get("color", '') == "True" else False
+        color = True if kwd.get("color", "") == "True" else False
 
         if tool is None:
             raise ValueError("Tool can't be None")
 
-        ordered_data = collections.OrderedDict()
+        ordered_data = {}
         sort_keys = [(lambda v, i=i: v[i]) for i in range(4)]
 
-        jobs_times = sa.select((sa.func.date_trunc('month', galaxy.model.Job.table.c.create_time).label('date'),
-                                sa.func.max(galaxy.model.Job.table.c.update_time - galaxy.model.Job.table.c.create_time),
-                                sa.func.avg(galaxy.model.Job.table.c.update_time - galaxy.model.Job.table.c.create_time),
-                                sa.func.min(galaxy.model.Job.table.c.update_time - galaxy.model.Job.table.c.create_time)),
-                               from_obj=[galaxy.model.Job.table],
-                               whereclause=galaxy.model.Job.table.c.tool_id == tool,
-                               group_by=['date'])
+        jobs_times = sa.select(
+            (
+                sa.func.date_trunc("month", galaxy.model.Job.table.c.create_time).label("date"),
+                sa.func.max(galaxy.model.Job.table.c.update_time - galaxy.model.Job.table.c.create_time),
+                sa.func.avg(galaxy.model.Job.table.c.update_time - galaxy.model.Job.table.c.create_time),
+                sa.func.min(galaxy.model.Job.table.c.update_time - galaxy.model.Job.table.c.create_time),
+            ),
+            from_obj=[galaxy.model.Job.table],
+            whereclause=galaxy.model.Job.table.c.tool_id == tool,
+            group_by=["date"],
+        )
 
-        months = list(jobs_times.execute())
+        months = list(trans.sa_session.execute(jobs_times.execute))
         months.sort(key=sort_keys[sort_by], reverse=reverse)
         if user_cutoff:
             months = months[:user_cutoff]
 
         for month in months:
-            ordered_data[str(month[0]).split(' ')[0][:-3]] = (self.formatted(month[1], color),
-                                                              self.formatted(month[2], color),
-                                                              self.formatted(month[3], color))
-
-        return trans.fill_template('/webapps/reports/tool_execution_time_per_month.mako',
-                                   data=ordered_data,
-                                   tool=tool,
-                                   descending=descending,
-                                   user_cutoff=user_cutoff,
-                                   sort_by=sort_by)
+            ordered_data[str(month[0]).split(" ")[0][:-3]] = (
+                self.formatted(month[1], color),
+                self.formatted(month[2], color),
+                self.formatted(month[3], color),
+            )
+
+        return trans.fill_template(
+            "/webapps/reports/tool_execution_time_per_month.mako",
+            data=ordered_data,
+            tool=tool,
+            descending=descending,
+            user_cutoff=user_cutoff,
+            sort_by=sort_by,
+        )
 
     @web.expose
     def tool_error_messages(self, trans, **kwd):
         tool_name = kwd.get("tool", None)
-        descending = 1 if kwd.get("descending", 'desc') == "desc" else -1
+        descending = 1 if kwd.get("descending", "desc") == "desc" else -1
         reverse = descending == 1
         sort_by = 0 if kwd.get("sort_by", "time") == "time" else 1
         cutoff = int(kwd.get("user_cutoff", 60))
-        sort_keys = (
-            lambda v: counter[v][1],
-            lambda v: counter[v][0])
+        sort_keys = (lambda v: counter[v][1], lambda v: counter[v][0])
 
         if tool_name is None:
             raise ValueError("Tool can't be none")
-        tool_errors = [[unicodify(a), b] for a, b in
-                       sa.select((galaxy.model.Job.table.c.stderr, galaxy.model.Job.table.c.create_time),
-                        from_obj=[galaxy.model.Job.table],
-                        whereclause=and_(galaxy.model.Job.table.c.tool_id == tool_name,
-                                         galaxy.model.Job.table.c.state == 'error')).execute()]
+        tool_errors = [
+            [unicodify(a), b]
+            for a, b in trans.sa_session.execute(
+                sa.select(
+                    (
+                        galaxy.model.Job.table.c.tool_stderr,
+                        galaxy.model.Job.table.c.create_time,
+                    ),
+                    from_obj=[galaxy.model.Job.table],
+                    whereclause=and_(
+                        galaxy.model.Job.table.c.tool_id == tool_name,
+                        galaxy.model.Job.table.c.state == "error",
+                    ),
+                )
+            )
+        ]
 
         counter = {}
         for error in tool_errors:
             if error[0] in counter:
                 counter[error[0]][0] += 1
             else:
                 counter[error[0]] = [1, error[1]]
 
-        data = collections.OrderedDict()
+        data = {}
         keys = list(counter.keys())
         if cutoff:
             keys = keys[:cutoff]
         keys.sort(key=sort_keys[sort_by], reverse=reverse)
 
-        spaces = [' ', '\t', '    ']
+        spaces = [" ", "\t", "    "]
         for key in keys:
-            new_key = '</br>'.join(_ for _ in key.split('\n') if _ and _ not in spaces)
+            new_key = "</br>".join(_ for _ in key.split("\n") if _ and _ not in spaces)
             if len(new_key) >= 100:
                 to_replace = []
-                words = key.split('\n')
+                words = key.split("\n")
                 for word in words:
                     if word in to_replace:
                         continue
                     if words.count(word) > 1:
                         to_replace.append(word)
                 for word in to_replace:
-                    sentence = ("</br>" + word) * 2
+                    sentence = f"</br>{word}" * 2
                     count = 2
-                    while sentence + "</br>" + word in new_key:
-                        sentence += "</br>" + word
+                    while f"{sentence}</br>{word}" in new_key:
+                        sentence += f"</br>{word}"
                         count += 1
                     if sentence in new_key:
-                        new_key = new_key.replace(sentence, '</br>' + word + " [this line in %d times]" % (count))
+                        new_key = new_key.replace(sentence, f"</br>{word}{' [this line in %d times]' % count}")
             data[new_key] = counter[key]
 
-        return trans.fill_template("/webapps/reports/tool_error_messages.mako",
-                                   data=data,
-                                   descending=descending,
-                                   tool_name=tool_name,
-                                   sort_by=sort_by,
-                                   user_cutoff=cutoff)
+        return trans.fill_template(
+            "/webapps/reports/tool_error_messages.mako",
+            data=data,
+            descending=descending,
+            tool_name=tool_name,
+            sort_by=sort_by,
+            user_cutoff=cutoff,
+        )
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/users.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/users.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,224 +1,249 @@
 import calendar
 import logging
 import operator
 from datetime import (
     date,
     datetime,
-    timedelta
+    timedelta,
 )
 
 import sqlalchemy as sa
 from markupsafe import escape
 from sqlalchemy import false
 
 import galaxy.model
 from galaxy import util
-from galaxy.webapps.base.controller import BaseUIController, web
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    web,
+)
 from galaxy.webapps.reports.controllers.jobs import sorter
 from galaxy.webapps.reports.controllers.query import ReportQueryBuilder
 
 log = logging.getLogger(__name__)
 
 
 class Users(BaseUIController, ReportQueryBuilder):
-
     @web.expose
     def registered_users(self, trans, **kwd):
-        message = escape(util.restore_text(kwd.get('message', '')))
+        message = escape(util.restore_text(kwd.get("message", "")))
         num_users = trans.sa_session.query(galaxy.model.User).count()
-        return trans.fill_template('/webapps/reports/registered_users.mako', num_users=num_users, message=message)
+        return trans.fill_template("/webapps/reports/registered_users.mako", num_users=num_users, message=message)
 
     @web.expose
     def registered_users_per_month(self, trans, **kwd):
-        message = escape(util.restore_text(kwd.get('message', '')))
-        specs = sorter('date', kwd)
+        message = escape(util.restore_text(kwd.get("message", "")))
+        specs = sorter("date", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
 
-        q = sa.select((self.select_month(galaxy.model.User.table.c.create_time).label('date'),
-                       sa.func.count(galaxy.model.User.table.c.id).label('num_users')),
-                      from_obj=[galaxy.model.User.table],
-                      group_by=self.group_by_month(galaxy.model.User.table.c.create_time),
-                      order_by=[_order])
+        q = sa.select(
+            (
+                self.select_month(galaxy.model.User.table.c.create_time).label("date"),
+                sa.func.count(galaxy.model.User.table.c.id).label("num_users"),
+            ),
+            from_obj=[galaxy.model.User.table],
+            group_by=self.group_by_month(galaxy.model.User.table.c.create_time),
+            order_by=[_order],
+        )
         users = []
-        for row in q.execute():
-            users.append((row.date.strftime("%Y-%m"),
-                          row.num_users,
-                          row.date.strftime("%B"),
-                          row.date.strftime("%Y")))
-        return trans.fill_template('/webapps/reports/registered_users_per_month.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   users=users,
-                                   message=message)
+        for row in trans.sa_session.execute(q):
+            users.append((row.date.strftime("%Y-%m"), row.num_users, row.date.strftime("%B"), row.date.strftime("%Y")))
+        return trans.fill_template(
+            "/webapps/reports/registered_users_per_month.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            users=users,
+            message=message,
+        )
 
     @web.expose
     def specified_month(self, trans, **kwd):
-        message = escape(util.restore_text(kwd.get('message', '')))
+        message = escape(util.restore_text(kwd.get("message", "")))
         # If specified_date is not received, we'll default to the current month
-        specified_date = kwd.get('specified_date', datetime.utcnow().strftime("%Y-%m-%d"))
+        specified_date = kwd.get("specified_date", datetime.utcnow().strftime("%Y-%m-%d"))
         specified_month = specified_date[:7]
         year, month = map(int, specified_month.split("-"))
         start_date = date(year, month, 1)
         end_date = start_date + timedelta(days=calendar.monthrange(year, month)[1])
         month_label = start_date.strftime("%B")
         year_label = start_date.strftime("%Y")
-        q = sa.select((self.select_day(galaxy.model.User.table.c.create_time).label('date'),
-                       sa.func.count(galaxy.model.User.table.c.id).label('num_users')),
-                      whereclause=sa.and_(galaxy.model.User.table.c.create_time >= start_date,
-                                          galaxy.model.User.table.c.create_time < end_date),
-                      from_obj=[galaxy.model.User.table],
-                      group_by=self.group_by_day(galaxy.model.User.table.c.create_time),
-                      order_by=[sa.desc('date')])
+        q = sa.select(
+            (
+                self.select_day(galaxy.model.User.table.c.create_time).label("date"),
+                sa.func.count(galaxy.model.User.table.c.id).label("num_users"),
+            ),
+            whereclause=sa.and_(
+                galaxy.model.User.table.c.create_time >= start_date, galaxy.model.User.table.c.create_time < end_date
+            ),
+            from_obj=[galaxy.model.User.table],
+            group_by=self.group_by_day(galaxy.model.User.table.c.create_time),
+            order_by=[sa.desc("date")],
+        )
         users = []
-        for row in q.execute():
-            users.append((row.date.strftime("%Y-%m-%d"),
-                          row.date.strftime("%d"),
-                          row.num_users,
-                          row.date.strftime("%A")))
-        return trans.fill_template('/webapps/reports/registered_users_specified_month.mako',
-                                   month_label=month_label,
-                                   year_label=year_label,
-                                   month=month,
-                                   users=users,
-                                   message=message)
+        for row in trans.sa_session.execute(q):
+            users.append(
+                (row.date.strftime("%Y-%m-%d"), row.date.strftime("%d"), row.num_users, row.date.strftime("%A"))
+            )
+        return trans.fill_template(
+            "/webapps/reports/registered_users_specified_month.mako",
+            month_label=month_label,
+            year_label=year_label,
+            month=month,
+            users=users,
+            message=message,
+        )
 
     @web.expose
     def specified_date(self, trans, **kwd):
-        message = escape(util.restore_text(kwd.get('message', '')))
+        message = escape(util.restore_text(kwd.get("message", "")))
         # If specified_date is not received, we'll default to the current month
-        specified_date = kwd.get('specified_date', datetime.utcnow().strftime("%Y-%m-%d"))
+        specified_date = kwd.get("specified_date", datetime.utcnow().strftime("%Y-%m-%d"))
         year, month, day = map(int, specified_date.split("-"))
         start_date = date(year, month, day)
         end_date = start_date + timedelta(days=1)
         day_of_month = start_date.strftime("%d")
         day_label = start_date.strftime("%A")
         month_label = start_date.strftime("%B")
         year_label = start_date.strftime("%Y")
-        q = sa.select((self.select_day(galaxy.model.User.table.c.create_time).label('date'),
-                       galaxy.model.User.table.c.email),
-                      whereclause=sa.and_(galaxy.model.User.table.c.create_time >= start_date,
-                                          galaxy.model.User.table.c.create_time < end_date),
-                      from_obj=[galaxy.model.User.table],
-                      order_by=[galaxy.model.User.table.c.email])
+        q = sa.select(
+            (self.select_day(galaxy.model.User.table.c.create_time).label("date"), galaxy.model.User.table.c.email),
+            whereclause=sa.and_(
+                galaxy.model.User.table.c.create_time >= start_date, galaxy.model.User.table.c.create_time < end_date
+            ),
+            from_obj=[galaxy.model.User.table],
+            order_by=[galaxy.model.User.table.c.email],
+        )
         users = []
-        for row in q.execute():
-            users.append((row.email))
-        return trans.fill_template('/webapps/reports/registered_users_specified_date.mako',
-                                   specified_date=start_date,
-                                   day_label=day_label,
-                                   month_label=month_label,
-                                   year_label=year_label,
-                                   day_of_month=day_of_month,
-                                   users=users,
-                                   message=message)
+        for row in trans.sa_session.execute(q):
+            users.append(row.email)
+        return trans.fill_template(
+            "/webapps/reports/registered_users_specified_date.mako",
+            specified_date=start_date,
+            day_label=day_label,
+            month_label=month_label,
+            year_label=year_label,
+            day_of_month=day_of_month,
+            users=users,
+            message=message,
+        )
 
     @web.expose
     def last_access_date(self, trans, **kwd):
-        message = escape(util.restore_text(kwd.get('message', '')))
-        specs = sorter('one', kwd)
+        message = escape(util.restore_text(kwd.get("message", "")))
+        specs = sorter("one", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
 
         def name_to_num(name):
             num = None
 
-            if name is not None and name.lower() == 'zero':
+            if name is not None and name.lower() == "zero":
                 num = 0
             else:
                 num = 1
 
             return num
 
         if order == "desc":
             _order = True
         else:
             _order = False
 
-        days_not_logged_in = kwd.get('days_not_logged_in', 90)
+        days_not_logged_in = kwd.get("days_not_logged_in", 90)
         if not days_not_logged_in:
             days_not_logged_in = 0
         cutoff_time = datetime.utcnow() - timedelta(days=int(days_not_logged_in))
         users = []
-        for user in trans.sa_session.query(galaxy.model.User) \
-                                    .filter(galaxy.model.User.table.c.deleted == false()) \
-                                    .order_by(galaxy.model.User.table.c.email):
+        for user in (
+            trans.sa_session.query(galaxy.model.User)
+            .filter(galaxy.model.User.table.c.deleted == false())
+            .order_by(galaxy.model.User.table.c.email)
+        ):
             if user.galaxy_sessions:
                 last_galaxy_session = user.galaxy_sessions[0]
                 if last_galaxy_session.update_time < cutoff_time:
                     users.append((user.email, last_galaxy_session.update_time.strftime("%Y-%m-%d")))
             else:
                 # The user has never logged in
                 users.append((user.email, "never logged in"))
         users = sorted(users, key=operator.itemgetter(name_to_num(sort_id)), reverse=_order)
-        return trans.fill_template('/webapps/reports/users_last_access_date.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   days_not_logged_in=days_not_logged_in,
-                                   users=users,
-                                   message=message)
+        return trans.fill_template(
+            "/webapps/reports/users_last_access_date.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            days_not_logged_in=days_not_logged_in,
+            users=users,
+            message=message,
+        )
 
     @web.expose
     def user_disk_usage(self, trans, **kwd):
-        message = escape(util.restore_text(kwd.get('message', '')))
-        specs = sorter('disk_usage', kwd)
+        message = escape(util.restore_text(kwd.get("message", "")))
+        specs = sorter("disk_usage", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
 
         if order == "desc":
             _order = True
         else:
             _order = False
 
-        user_cutoff = int(kwd.get('user_cutoff', 60))
+        user_cutoff = int(kwd.get("user_cutoff", 60))
         # disk_usage isn't indexed
         all_users = trans.sa_session.query(galaxy.model.User).all()
         sort_attrgetter = operator.attrgetter(str(sort_id))
         users = sorted(all_users, key=lambda x: sort_attrgetter(x) or 0, reverse=_order)
         if user_cutoff:
             users = users[:user_cutoff]
-        return trans.fill_template('/webapps/reports/users_user_disk_usage.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   users=users,
-                                   user_cutoff=user_cutoff,
-                                   message=message)
+        return trans.fill_template(
+            "/webapps/reports/users_user_disk_usage.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            users=users,
+            user_cutoff=user_cutoff,
+            message=message,
+        )
 
     @web.expose
     def history_per_user(self, trans, **kwd):
-        message = escape(util.restore_text(kwd.get('message', '')))
-        user_cutoff = int(kwd.get('user_cutoff', 60))
-        sorting = 0 if kwd.get('sorting', 'User') == 'User' else 1
-        descending = 1 if kwd.get('descending', 'desc') == 'desc' else -1
+        message = escape(util.restore_text(kwd.get("message", "")))
+        user_cutoff = int(kwd.get("user_cutoff", 60))
+        sorting = 0 if kwd.get("sorting", "User") == "User" else 1
+        descending = 1 if kwd.get("descending", "desc") == "desc" else -1
         reverse = descending == 1
-        sort_keys = (
-            lambda v: v[0].lower(),
-            lambda v: v[1]
-        )
+        sort_keys = (lambda v: v[0].lower(), lambda v: v[1])
 
         req = sa.select(
-            (sa.func.count(galaxy.model.History.table.c.id).label('history'),
-             galaxy.model.User.table.c.username.label('username')),
+            (
+                sa.func.count(galaxy.model.History.table.c.id).label("history"),
+                galaxy.model.User.table.c.username.label("username"),
+            ),
             from_obj=[sa.outerjoin(galaxy.model.History.table, galaxy.model.User.table)],
             whereclause=galaxy.model.History.table.c.user_id == galaxy.model.User.table.c.id,
-            group_by=['username'],
-            order_by=[sa.desc('username'), 'history'])
+            group_by=["username"],
+            order_by=[sa.desc("username"), "history"],
+        )
 
-        histories = [(_.username if _.username is not None else "Unknown", _.history) for _ in req.execute()]
+        histories = [
+            (_.username if _.username is not None else "Unknown", _.history) for _ in trans.sa_session.execute(req)
+        ]
         histories.sort(key=sort_keys[sorting], reverse=reverse)
         if user_cutoff != 0:
             histories = histories[:user_cutoff]
 
-        return trans.fill_template('/webapps/reports/history_per_user.mako',
-                                   histories=histories,
-                                   user_cutoff=user_cutoff,
-                                   sorting=sorting,
-                                   descending=descending,
-                                   message=message)
+        return trans.fill_template(
+            "/webapps/reports/history_per_user.mako",
+            histories=histories,
+            user_cutoff=user_cutoff,
+            sorting=sorting,
+            descending=descending,
+            message=message,
+        )
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/reports/controllers/workflows.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/reports/controllers/workflows.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,207 +1,214 @@
 import calendar
 import logging
 import re
 from collections import namedtuple
 from datetime import (
     date,
     datetime,
-    timedelta
+    timedelta,
 )
 from math import (
     ceil,
-    floor
+    floor,
 )
 
 import sqlalchemy as sa
 from markupsafe import escape
 from sqlalchemy import and_
 
-from galaxy import model, util
-from galaxy.webapps.base.controller import BaseUIController, web
+from galaxy import (
+    model,
+    util,
+)
+from galaxy.web.legacy_framework import grids
+from galaxy.webapps.base.controller import (
+    BaseUIController,
+    web,
+)
 from galaxy.webapps.reports.controllers.jobs import (
     get_spark_time,
-    sorter
+    sorter,
 )
 from galaxy.webapps.reports.controllers.query import ReportQueryBuilder
-from galaxy.webapps.reports.framework import grids
 
 log = logging.getLogger(__name__)
 
 
 class SpecifiedDateListGrid(grids.Grid):
-
     class WorkflowNameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, stored_workflow):
             return escape(stored_workflow.name)
 
     class CreateTimeColumn(grids.DateTimeColumn):
-
         def get_value(self, trans, grid, stored_workflow):
             return stored_workflow.create_time
 
     class UserColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, stored_workflow):
             if stored_workflow.user:
                 return escape(stored_workflow.user.email)
-            return 'unknown'
+            return "unknown"
 
     class EmailColumn(grids.GridColumn):
-
         def filter(self, trans, user, query, column_filter):
-            if column_filter == 'All':
+            if column_filter == "All":
                 return query
-            return query.filter(and_(model.StoredWorkflow.table.c.user_id == model.User.table.c.id,
-                                     model.User.table.c.email == column_filter))
+            return query.filter(
+                and_(
+                    model.StoredWorkflow.table.c.user_id == model.User.table.c.id,
+                    model.User.table.c.email == column_filter,
+                )
+            )
 
     class SpecifiedDateColumn(grids.GridColumn):
-
         def filter(self, trans, user, query, column_filter):
-            if column_filter == 'All':
+            if column_filter == "All":
                 return query
             # We are either filtering on a date like YYYY-MM-DD or on a month like YYYY-MM,
             # so we need to figure out which type of date we have
-            if column_filter.count('-') == 2:
+            if column_filter.count("-") == 2:
                 # We are filtering on a date like YYYY-MM-DD
                 year, month, day = map(int, column_filter.split("-"))
                 start_date = date(year, month, day)
                 end_date = start_date + timedelta(days=1)
-                return query.filter(and_(self.model_class.table.c.create_time >= start_date,
-                                         self.model_class.table.c.create_time < end_date))
-            if column_filter.count('-') == 1:
+                return query.filter(
+                    and_(
+                        self.model_class.table.c.create_time >= start_date,
+                        self.model_class.table.c.create_time < end_date,
+                    )
+                )
+            if column_filter.count("-") == 1:
                 # We are filtering on a month like YYYY-MM
                 year, month = map(int, column_filter.split("-"))
                 start_date = date(year, month, 1)
                 end_date = start_date + timedelta(days=calendar.monthrange(year, month)[1])
-                return query.filter(and_(self.model_class.table.c.create_time >= start_date,
-                                         self.model_class.table.c.create_time < end_date))
+                return query.filter(
+                    and_(
+                        self.model_class.table.c.create_time >= start_date,
+                        self.model_class.table.c.create_time < end_date,
+                    )
+                )
 
     # Grid definition
     use_async = False
     model_class = model.StoredWorkflow
     title = "Workflows"
     default_sort_key = "name"
     columns = [
-        WorkflowNameColumn("Name",
-                           key="name",
-                           attach_popup=False,
-                           filterable="advanced"),
-        CreateTimeColumn("Creation Time",
-                         key="create_time",
-                         attach_popup=False),
-        UserColumn("User",
-                   key="email",
-                   model_class=model.User,
-                   link=(lambda item: dict(operation="user_per_month", id=item.id, webapp="reports")),
-                   attach_popup=False),
+        WorkflowNameColumn("Name", key="name", attach_popup=False, filterable="advanced"),
+        CreateTimeColumn("Creation Time", key="create_time", attach_popup=False),
+        UserColumn(
+            "User",
+            key="email",
+            model_class=model.User,
+            link=(lambda item: dict(operation="user_per_month", id=item.id, webapp="reports")),
+            attach_popup=False,
+        ),
         # Columns that are valid for filtering but are not visible.
-        SpecifiedDateColumn("Specified Date",
-                            key="specified_date",
-                            visible=False),
-        EmailColumn("Email",
-                    key="email",
-                    model_class=model.User,
-                    visible=False),
+        SpecifiedDateColumn("Specified Date", key="specified_date", visible=False),
+        EmailColumn("Email", key="email", model_class=model.User, visible=False),
     ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0], columns[2]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    standard_filters = []
-    default_filter = {'specified_date': 'All'}
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search",
+            cols_to_filter=[columns[0], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
+    default_filter = {"specified_date": "All"}
     num_rows_per_page = 50
     use_paging = True
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(self.model_class) \
-                               .join(model.User) \
-                               .enable_eagerloads(False)
+        return trans.sa_session.query(self.model_class).join(model.User).enable_eagerloads(False)
 
 
 class Workflows(BaseUIController, ReportQueryBuilder):
-
     specified_date_list_grid = SpecifiedDateListGrid()
 
     @web.expose
     def specified_date_handler(self, trans, **kwd):
         # We add params to the keyword dict in this method in order to rename the param
         # with an "f-" prefix, simulating filtering by clicking a search link.  We have
         # to take this approach because the "-" character is illegal in HTTP requests.
-        if 'f-specified_date' in kwd and 'specified_date' not in kwd:
+        if "f-specified_date" in kwd and "specified_date" not in kwd:
             # The user clicked a State link in the Advanced Search box, so 'specified_date'
             # will have been eliminated.
             pass
-        elif 'specified_date' not in kwd:
-            kwd['f-specified_date'] = 'All'
+        elif "specified_date" not in kwd:
+            kwd["f-specified_date"] = "All"
         else:
-            kwd['f-specified_date'] = kwd['specified_date']
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+            kwd["f-specified_date"] = kwd["specified_date"]
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "workflow_per_month":
                 # The received id is the stored_workflow id.
-                return trans.response.send_redirect(web.url_for(controller='workflows',
-                                                                action='workflow_per_month',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="workflows", action="workflow_per_month", **kwd)
+                )
             elif operation == "user_per_month":
-                stored_workflow_id = kwd.get('id', None)
+                stored_workflow_id = kwd.get("id", None)
                 workflow = get_workflow(trans, stored_workflow_id)
                 if workflow.user:
-                    kwd['email'] = workflow.user.email
+                    kwd["email"] = workflow.user.email
                 else:
-                    kwd['email'] = None  # For anonymous users ( shouldn't happen with workflows )
-                return trans.response.send_redirect(web.url_for(controller='workflows',
-                                                                action='user_per_month',
-                                                                **kwd))
+                    kwd["email"] = None  # For anonymous users ( shouldn't happen with workflows )
+                return trans.response.send_redirect(web.url_for(controller="workflows", action="user_per_month", **kwd))
         return self.specified_date_list_grid(trans, **kwd)
 
     @web.expose
     def per_month_all(self, trans, **kwd):
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
-        specs = sorter('date', kwd)
+        specs = sorter("date", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
-        q = sa.select((self.select_month(model.StoredWorkflow.table.c.create_time).label('date'),
-                       sa.func.count(model.StoredWorkflow.table.c.id).label('total_workflows')),
-                      from_obj=[sa.outerjoin(model.StoredWorkflow.table, model.User.table)],
-                      group_by=self.group_by_month(model.StoredWorkflow.table.c.create_time),
-                      order_by=[_order],
-                      offset=offset,
-                      limit=limit)
-
-        all_workflows = sa.select((self.select_day(model.StoredWorkflow.table.c.create_time).label('date'),
-                     model.StoredWorkflow.table.c.id))
+        q = sa.select(
+            (
+                self.select_month(model.StoredWorkflow.table.c.create_time).label("date"),
+                sa.func.count(model.StoredWorkflow.table.c.id).label("total_workflows"),
+            ),
+            from_obj=[sa.outerjoin(model.StoredWorkflow.table, model.User.table)],
+            group_by=self.group_by_month(model.StoredWorkflow.table.c.create_time),
+            order_by=[_order],
+            offset=offset,
+            limit=limit,
+        )
+
+        all_workflows = sa.select(
+            (self.select_day(model.StoredWorkflow.table.c.create_time).label("date"), model.StoredWorkflow.table.c.id)
+        )
 
         trends = dict()
-        for workflow in all_workflows.execute():
+        for workflow in trans.sa_session.execute(all_workflows):
             workflow_day = int(workflow.date.strftime("%-d")) - 1
             workflow_month = int(workflow.date.strftime("%-m"))
             workflow_month_name = workflow.date.strftime("%B")
             workflow_year = workflow.date.strftime("%Y")
             key = str(workflow_month_name + workflow_year)
 
             try:
@@ -209,85 +216,91 @@
             except KeyError:
                 workflow_year = int(workflow_year)
                 wday, day_range = calendar.monthrange(workflow_year, workflow_month)
                 trends[key] = [0] * day_range
                 trends[key][workflow_day] += 1
 
         workflows = []
-        for row in q.execute():
+        for row in trans.sa_session.execute(q):
             month_name = row.date.strftime("%B")
             year = int(row.date.strftime("%Y"))
 
-            workflows.append((row.date.strftime("%Y-%m"),
-                              row.total_workflows,
-                              month_name,
-                              year))
+            workflows.append((row.date.strftime("%Y-%m"), row.total_workflows, month_name, year))
 
         pages_found = ceil(len(workflows) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/workflows_per_month_all.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   trends=trends,
-                                   workflows=workflows,
-                                   message=message,
-                                   page_specs=page_specs)
+        return trans.fill_template(
+            "/webapps/reports/workflows_per_month_all.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            trends=trends,
+            workflows=workflows,
+            message=message,
+            page_specs=page_specs,
+        )
 
     @web.expose
     def per_user(self, trans, **kwd):
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
-        specs = sorter('user_email', kwd)
+        specs = sorter("user_email", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
-        time_period = kwd.get('spark_time')
+        time_period = kwd.get("spark_time")
         time_period, _time_period = get_spark_time(time_period)
         spark_limit = 30
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         workflows = []
-        q = sa.select((model.User.table.c.email.label('user_email'),
-                       sa.func.count(model.StoredWorkflow.table.c.id).label('total_workflows')),
-                      from_obj=[sa.outerjoin(model.StoredWorkflow.table, model.User.table)],
-                      group_by=['user_email'],
-                      order_by=[_order],
-                      offset=offset,
-                      limit=limit)
-
-        all_workflows_per_user = sa.select((model.User.table.c.email.label('user_email'),
-                                            self.select_day(model.StoredWorkflow.table.c.create_time).label('date'),
-                                            model.StoredWorkflow.table.c.id),
-                                           from_obj=[sa.outerjoin(model.StoredWorkflow.table,
-                                                                  model.User.table)])
+        q = sa.select(
+            (
+                model.User.table.c.email.label("user_email"),
+                sa.func.count(model.StoredWorkflow.table.c.id).label("total_workflows"),
+            ),
+            from_obj=[sa.outerjoin(model.StoredWorkflow.table, model.User.table)],
+            group_by=["user_email"],
+            order_by=[_order],
+            offset=offset,
+            limit=limit,
+        )
+
+        all_workflows_per_user = sa.select(
+            (
+                model.User.table.c.email.label("user_email"),
+                self.select_day(model.StoredWorkflow.table.c.create_time).label("date"),
+                model.StoredWorkflow.table.c.id,
+            ),
+            from_obj=[sa.outerjoin(model.StoredWorkflow.table, model.User.table)],
+        )
         currday = datetime.today()
         trends = dict()
-        for workflow in all_workflows_per_user.execute():
-            curr_user = re.sub(r'\W+', '', workflow.user_email)
+        for workflow in trans.sa_session.execute(all_workflows_per_user):
+            curr_user = re.sub(r"\W+", "", workflow.user_email)
             try:
                 day = currday - workflow.date
             except TypeError:
                 day = datetime.date(currday) - datetime.date(workflow.date)
 
             day = day.days
             container = floor(day / _time_period)
@@ -296,58 +309,64 @@
                 if container < spark_limit:
                     trends[curr_user][container] += 1
             except KeyError:
                 trends[curr_user] = [0] * spark_limit
                 if container < spark_limit:
                     trends[curr_user][container] += 1
 
-        for row in q.execute():
-            workflows.append((row.user_email,
-                              row.total_workflows))
+        for row in trans.sa_session.execute(q):
+            workflows.append((row.user_email, row.total_workflows))
 
         pages_found = ceil(len(workflows) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/workflows_per_user.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   spark_limit=spark_limit,
-                                   trends=trends,
-                                   time_period=time_period,
-                                   workflows=workflows,
-                                   message=message,
-                                   page_specs=page_specs)
+        return trans.fill_template(
+            "/webapps/reports/workflows_per_user.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            spark_limit=spark_limit,
+            trends=trends,
+            time_period=time_period,
+            workflows=workflows,
+            message=message,
+            page_specs=page_specs,
+        )
 
     @web.expose
     def user_per_month(self, trans, **kwd):
         params = util.Params(kwd)
-        message = ''
-        specs = sorter('date', kwd)
+        message = ""
+        specs = sorter("date", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
-        email = util.restore_text(params.get('email', ''))
-        user_id = trans.security.decode_id(params.get('id', ''))
+        email = util.restore_text(params.get("email", ""))
+        user_id = trans.security.decode_id(params.get("id", ""))
 
-        q = sa.select((self.select_month(model.StoredWorkflow.table.c.create_time).label('date'),
-                       sa.func.count(model.StoredWorkflow.table.c.id).label('total_workflows')),
-                      whereclause=model.StoredWorkflow.table.c.user_id == user_id,
-                      from_obj=[model.StoredWorkflow.table],
-                      group_by=self.group_by_month(model.StoredWorkflow.table.c.create_time),
-                      order_by=[_order])
-
-        all_workflows_user_month = sa.select((self.select_day(model.StoredWorkflow.table.c.create_time).label('date'),
-                                              model.StoredWorkflow.table.c.id),
-                                             whereclause=model.StoredWorkflow.table.c.user_id == user_id,
-                                             from_obj=[model.StoredWorkflow.table])
+        q = sa.select(
+            (
+                self.select_month(model.StoredWorkflow.table.c.create_time).label("date"),
+                sa.func.count(model.StoredWorkflow.table.c.id).label("total_workflows"),
+            ),
+            whereclause=model.StoredWorkflow.table.c.user_id == user_id,
+            from_obj=[model.StoredWorkflow.table],
+            group_by=self.group_by_month(model.StoredWorkflow.table.c.create_time),
+            order_by=[_order],
+        )
+
+        all_workflows_user_month = sa.select(
+            (self.select_day(model.StoredWorkflow.table.c.create_time).label("date"), model.StoredWorkflow.table.c.id),
+            whereclause=model.StoredWorkflow.table.c.user_id == user_id,
+            from_obj=[model.StoredWorkflow.table],
+        )
 
         trends = dict()
-        for workflow in all_workflows_user_month.execute():
+        for workflow in trans.sa_session.execute(all_workflows_user_month):
             workflow_day = int(workflow.date.strftime("%-d")) - 1
             workflow_month = int(workflow.date.strftime("%-m"))
             workflow_month_name = workflow.date.strftime("%B")
             workflow_year = workflow.date.strftime("%Y")
             key = str(workflow_month_name + workflow_year)
 
             try:
@@ -355,84 +374,91 @@
             except KeyError:
                 workflow_year = int(workflow_year)
                 wday, day_range = calendar.monthrange(workflow_year, workflow_month)
                 trends[key] = [0] * day_range
                 trends[key][workflow_day] += 1
 
         workflows = []
-        for row in q.execute():
-            workflows.append((row.date.strftime("%Y-%m"),
-                              row.total_workflows,
-                              row.date.strftime("%B"),
-                              row.date.strftime("%Y")))
-        return trans.fill_template('/webapps/reports/workflows_user_per_month.mako',
-                                   email=util.sanitize_text(email),
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   trends=trends,
-                                   workflows=workflows,
-                                   message=message)
+        for row in trans.sa_session.execute(q):
+            workflows.append(
+                (row.date.strftime("%Y-%m"), row.total_workflows, row.date.strftime("%B"), row.date.strftime("%Y"))
+            )
+        return trans.fill_template(
+            "/webapps/reports/workflows_user_per_month.mako",
+            email=util.sanitize_text(email),
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            trends=trends,
+            workflows=workflows,
+            message=message,
+        )
 
     @web.expose
     def per_workflow(self, trans, **kwd):
-        message = ''
-        PageSpec = namedtuple('PageSpec', ['entries', 'offset', 'page', 'pages_found'])
+        message = ""
+        PageSpec = namedtuple("PageSpec", ["entries", "offset", "page", "pages_found"])
 
-        specs = sorter('workflow_name', kwd)
+        specs = sorter("workflow_name", kwd)
         sort_id = specs.sort_id
         order = specs.order
         arrow = specs.arrow
         _order = specs.exc_order
-        time_period = kwd.get('spark_time')
+        time_period = kwd.get("spark_time")
         time_period, _time_period = get_spark_time(time_period)
         spark_limit = 30
         offset = 0
         limit = 10
 
         if "entries" in kwd:
-            entries = int(kwd.get('entries'))
+            entries = int(kwd.get("entries"))
         else:
             entries = 10
         limit = entries * 4
 
         if "offset" in kwd:
-            offset = int(kwd.get('offset'))
+            offset = int(kwd.get("offset"))
         else:
             offset = 0
 
         if "page" in kwd:
-            page = int(kwd.get('page'))
+            page = int(kwd.get("page"))
         else:
             page = 1
 
         # In case we don't know which is the monitor user we will query for all jobs
 
-        q = sa.select((model.Workflow.table.c.id.label('workflow_id'),
-                       sa.func.min(model.Workflow.table.c.name).label('workflow_name'),
-                       sa.func.count(model.WorkflowInvocation.table.c.id).label('total_runs')),
-                      from_obj=[model.Workflow.table,
-                                model.WorkflowInvocation.table],
-                      whereclause=sa.and_(model.WorkflowInvocation.table.c.workflow_id == model.Workflow.table.c.id),
-                      group_by=[model.Workflow.table.c.id],
-                      order_by=[_order],
-                      offset=offset,
-                      limit=limit)
-
-        all_runs_per_workflow = sa.select((model.Workflow.table.c.id.label('workflow_id'),
-                                           model.Workflow.table.c.name.label('workflow_name'),
-                                           self.select_day(model.WorkflowInvocation.table.c.create_time).label('date')),
-                                          from_obj=[model.Workflow.table,
-                                                    model.WorkflowInvocation.table],
-                                          whereclause=sa.and_(model.WorkflowInvocation.table.c.workflow_id == model.Workflow.table.c.id))
+        q = sa.select(
+            (
+                model.Workflow.table.c.id.label("workflow_id"),
+                sa.func.min(model.Workflow.table.c.name).label("workflow_name"),
+                sa.func.count(model.WorkflowInvocation.table.c.id).label("total_runs"),
+            ),
+            from_obj=[model.Workflow.table, model.WorkflowInvocation.table],
+            whereclause=sa.and_(model.WorkflowInvocation.table.c.workflow_id == model.Workflow.table.c.id),
+            group_by=[model.Workflow.table.c.id],
+            order_by=[_order],
+            offset=offset,
+            limit=limit,
+        )
+
+        all_runs_per_workflow = sa.select(
+            (
+                model.Workflow.table.c.id.label("workflow_id"),
+                model.Workflow.table.c.name.label("workflow_name"),
+                self.select_day(model.WorkflowInvocation.table.c.create_time).label("date"),
+            ),
+            from_obj=[model.Workflow.table, model.WorkflowInvocation.table],
+            whereclause=sa.and_(model.WorkflowInvocation.table.c.workflow_id == model.Workflow.table.c.id),
+        )
 
         currday = date.today()
         trends = dict()
-        for run in all_runs_per_workflow.execute():
-            curr_tool = re.sub(r'\W+', '', str(run.workflow_id))
+        for run in trans.sa_session.execute(all_runs_per_workflow):
+            curr_tool = re.sub(r"\W+", "", str(run.workflow_id))
             try:
                 day = currday - run.date
             except TypeError:
                 day = currday - datetime.date(run.date)
 
             day = day.days
             container = floor(day / _time_period)
@@ -442,31 +468,32 @@
                     trends[curr_tool][container] += 1
             except KeyError:
                 trends[curr_tool] = [0] * spark_limit
                 if container < spark_limit:
                     trends[curr_tool][container] += 1
 
         runs = []
-        for row in q.execute():
-            runs.append((row.workflow_name,
-                         row.total_runs,
-                         row.workflow_id))
+        for row in trans.sa_session.execute(q):
+            runs.append((row.workflow_name, row.total_runs, row.workflow_id))
 
         pages_found = ceil(len(runs) / float(entries))
         page_specs = PageSpec(entries, offset, page, pages_found)
 
-        return trans.fill_template('/webapps/reports/workflows_per_workflow.mako',
-                                   order=order,
-                                   arrow=arrow,
-                                   sort_id=sort_id,
-                                   spark_limit=spark_limit,
-                                   time_period=time_period,
-                                   trends=trends,
-                                   runs=runs,
-                                   message=message,
-                                   page_specs=page_specs)
+        return trans.fill_template(
+            "/webapps/reports/workflows_per_workflow.mako",
+            order=order,
+            arrow=arrow,
+            sort_id=sort_id,
+            spark_limit=spark_limit,
+            time_period=time_period,
+            trends=trends,
+            runs=runs,
+            message=message,
+            page_specs=page_specs,
+        )
+
 
 # ---- Utility methods -------------------------------------------------------
 
 
 def get_workflow(trans, id):
     return trans.sa_session.query(trans.model.Workflow).get(trans.security.decode_id(id))
```

### Comparing `galaxy-web-apps-20.5.0/galaxy/webapps/util.py` & `galaxy-web-apps-23.0.2/galaxy/webapps/util.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,13 @@
-from __future__ import absolute_import
-
 import logging
 
 import mako.exceptions
 
 from galaxy.util import unicodify
 
-
 log = logging.getLogger(__name__)
 
 
 class MiddlewareWrapUnsupported(Exception):
     pass
 
 
@@ -22,44 +19,48 @@
     """
     formatters = []
     # Formatter for mako
 
     def mako_html_data(exc_value):
         if isinstance(exc_value, (mako.exceptions.CompileException, mako.exceptions.SyntaxException)):
             return mako.exceptions.html_error_template().render(full=False, css=False)
-        if isinstance(exc_value, AttributeError) and exc_value.args[0].startswith("'Undefined' object has no attribute"):
+        if isinstance(exc_value, AttributeError) and exc_value.args[0].startswith(
+            "'Undefined' object has no attribute"
+        ):
             return mako.exceptions.html_error_template().render(full=False, css=False)
+
     formatters.append(mako_html_data)
     return formatters
 
 
 def wrap_if_allowed_or_fail(app, stack, wrap, name=None, args=None, kwargs=None):
     """
     Wrap the application with the given method if the application stack allows for it.
 
     Arguments are the same as for :func:`wrap_if_allowed`.
 
-    Raises :exception:`MiddlewareWrapUnsupported` if the stack does not allow the middleware.
+    Raises py:class:`MiddlewareWrapUnsupported` if the stack does not allow the middleware.
     """
     name = name or wrap.__name__
     if not stack.allowed_middleware(wrap):
         raise MiddlewareWrapUnsupported(
             "'%s' is enabled in your configuration but the %s application stack does not support it, this "
-            "middleware has been disabled" % (name, stack.name))
+            "middleware has been disabled" % (name, stack.name)
+        )
     args = args or []
     kwargs = kwargs or {}
     log.debug("Enabling '%s' middleware", name)
     return wrap(app, *args, **kwargs)
 
 
 def wrap_if_allowed(app, stack, wrap, name=None, args=None, kwargs=None):
     """
     Wrap the application with the given method if the application stack allows for it.
 
-    :type   app:    :class:`galaxy.web.framework.webapp.WebApplication` subclass
+    :type   app:    :class:`galaxy.webapps.base.webapp.WebApplication` subclass
     :param  app:    application to wrap
     :type   stack:  :class:`galaxy.web_stack.ApplicationStack` subclass
     :param  stack:  instance of application stack implementing `allowed_middleware()` method
     :type   wrap:   types.FunctionType or types.LambdaType
     :param  wrap:   function to wrap application with
     :type   name:   str
     :param  name:   alternative wrap function name for logging purposes (`wrap.__name__` if None)
```

### Comparing `galaxy-web-apps-20.5.0/LICENSE` & `galaxy-web-apps-23.0.2/LICENSE`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,61 @@
-Copyright (c) 2005-2016 Galaxy Contributors (see CONTRIBUTORS.md)
+Copyright (c) 2005-2022 Galaxy Contributors (see CONTRIBUTORS.md)
+
+Work contributed from 2021-04-07 onwards is licensed under the MIT License.
+Work contributed before this date is licensed under the Academic Free License
+version 3.0.
+See https://github.com/galaxyproject/galaxy/ for the contribution history.
+See below for the full text of both licenses.
+
+
+Some icons found in Galaxy are from the Silk Icons set, available under
+the Creative Commons Attribution 2.5 License, from:
+
+http://www.famfamfam.com/lab/icons/silk/
+
+
+Other images and documentation are licensed under the Creative Commons
+Attribution 3.0 (CC BY 3.0) License. See:
+
+http://creativecommons.org/licenses/by/3.0/
+
+
+--------------------------------------------------------------------------------
+
+
+MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+
+--------------------------------------------------------------------------------
+
+
+Academic Free License ("AFL") v. 3.0
+
+This Academic Free License (the "License") applies to any original work of
+authorship (the "Original Work") whose owner (the "Licensor") has placed the
+following licensing notice adjacent to the copyright notice for the Original
+Work:
 
 Licensed under the Academic Free License version 3.0
 
  1) Grant of Copyright License. Licensor grants You a worldwide, royalty-free, 
     non-exclusive, sublicensable license, for the duration of the copyright, to 
     do the following:
 
@@ -169,18 +222,7 @@
     replace the notice specified in the first paragraph above with the 
     notice "Licensed under <insert your license name here>" or with a notice 
     of your own that is not confusingly similar to the notice in this 
     License; and (iii) You may not claim that your original works are open 
     source software unless your Modified License has been approved by Open 
     Source Initiative (OSI) and You comply with its license review and 
     certification process.
-
-
-Some icons found in Galaxy are from the Silk Icons set, available under
-the Creative Commons Attribution 2.5 License, from:
-
-http://www.famfamfam.com/lab/icons/silk/
-
-
-Other images and documentation are licensed under the Creative Commons Attribution 3.0 (CC BY 3.0) License.   See 
-
-http://creativecommons.org/licenses/by/3.0/
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/dependencies/attribute_handlers.py` & `galaxy-web-apps-23.0.2/tool_shed/dependencies/attribute_handlers.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,148 +1,160 @@
 import copy
 import logging
-from collections import OrderedDict
-
-from galaxy.util import asbool
+from typing import (
+    Dict,
+    List,
+    Optional,
+    Tuple,
+)
+
+from galaxy.util import (
+    asbool,
+    etree,
+)
 from galaxy.web import url_for
 from tool_shed.dependencies.tool import tag_attribute_handler
-from tool_shed.repository_types.util import REPOSITORY_DEPENDENCY_DEFINITION_FILENAME
-from tool_shed.repository_types.util import TOOL_DEPENDENCY_DEFINITION_FILENAME
-from tool_shed.util import hg_util
-from tool_shed.util import metadata_util
-from tool_shed.util import repository_util
-from tool_shed.util import xml_util
+from tool_shed.repository_types.util import (
+    REPOSITORY_DEPENDENCY_DEFINITION_FILENAME,
+    TOOL_DEPENDENCY_DEFINITION_FILENAME,
+)
+from tool_shed.util import (
+    hg_util,
+    metadata_util,
+    repository_util,
+    xml_util,
+)
 
 log = logging.getLogger(__name__)
 
 
-class RepositoryDependencyAttributeHandler(object):
-
+class RepositoryDependencyAttributeHandler:
     def __init__(self, app, unpopulate):
         self.app = app
         self.file_name = REPOSITORY_DEPENDENCY_DEFINITION_FILENAME
         self.unpopulate = unpopulate
 
     def check_tag_attributes(self, elem):
         # <repository name="molecule_datatypes" owner="test" />
-        error_message = ''
-        name = elem.get('name')
+        error_message = ""
+        name = elem.get("name")
         if not name:
-            error_message += 'The tag is missing the required name attribute.  '
-        owner = elem.get('owner')
+            error_message += "The tag is missing the required name attribute.  "
+        owner = elem.get("owner")
         if not owner:
-            error_message += 'The tag is missing the required owner attribute.  '
+            error_message += "The tag is missing the required owner attribute.  "
         log.debug(error_message)
         return error_message
 
     def handle_complex_dependency_elem(self, parent_elem, elem_index, elem):
         """
         Populate or unpopulate the toolshed and changeset_revision attributes of a
         <repository> tag that defines a complex repository dependency.
         """
         # <repository name="package_eigen_2_0" owner="test" prior_installation_required="True" />
         altered, new_elem, error_message = self.handle_elem(elem)
         if error_message:
-            error_message += '  The %s file contains an invalid <repository> tag.' % TOOL_DEPENDENCY_DEFINITION_FILENAME
+            error_message += f"  The {TOOL_DEPENDENCY_DEFINITION_FILENAME} file contains an invalid <repository> tag."
         return altered, new_elem, error_message
 
     def handle_elem(self, elem):
         """Populate or unpopulate the changeset_revision and toolshed attributes of repository tags."""
         # <repository name="molecule_datatypes" owner="test" changeset_revision="1a070566e9c6" />
         # <repository changeset_revision="xxx" name="package_xorg_macros_1_17_1" owner="test" toolshed="yyy">
         #    <package name="xorg_macros" version="1.17.1" />
         # </repository>
-        error_message = ''
-        name = elem.get('name')
-        owner = elem.get('owner')
+        error_message = ""
+        name = elem.get("name")
+        owner = elem.get("owner")
         # The name and owner attributes are always required, so if either are missing, return the error message.
         if not name or not owner:
             error_message = self.check_tag_attributes(elem)
             return False, elem, error_message
         altered = False
-        toolshed = elem.get('toolshed')
-        changeset_revision = elem.get('changeset_revision')
+        toolshed = elem.get("toolshed")
+        changeset_revision = elem.get("changeset_revision")
         # Over a short period of time a bug existed which caused the prior_installation_required attribute
         # to be set to False and included in the <repository> tag when a repository was exported along with
         # its dependencies.  The following will eliminate this problematic attribute upon import.
-        prior_installation_required = elem.get('prior_installation_required')
+        prior_installation_required = elem.get("prior_installation_required")
         if prior_installation_required is not None and not asbool(prior_installation_required):
-            del elem.attrib['prior_installation_required']
+            del elem.attrib["prior_installation_required"]
         sub_elems = [child_elem for child_elem in list(elem)]
         if len(sub_elems) > 0:
             # At this point, a <repository> tag will point only to a package.
             # <package name="xorg_macros" version="1.17.1" />
-            # Coerce the list to an OrderedDict().
-            sub_elements = OrderedDict()
+            # Coerce the list to dict.
+            sub_elements = {}
             packages = []
             for sub_elem in sub_elems:
                 sub_elem_type = sub_elem.tag
-                sub_elem_name = sub_elem.get('name')
-                sub_elem_version = sub_elem.get('version')
+                sub_elem_name = sub_elem.get("name")
+                sub_elem_version = sub_elem.get("version")
                 if sub_elem_type and sub_elem_name and sub_elem_version:
                     packages.append((sub_elem_name, sub_elem_version))
-            sub_elements['packages'] = packages
+            sub_elements["packages"] = packages
         else:
             # Set to None.
             sub_elements = None
         if self.unpopulate:
             # We're exporting the repository, so eliminate all toolshed and changeset_revision attributes
             # from the <repository> tag.
             if toolshed or changeset_revision:
-                attributes = OrderedDict()
-                attributes['name'] = name
-                attributes['owner'] = owner
-                prior_installation_required = elem.get('prior_installation_required')
+                attributes = {}
+                attributes["name"] = name
+                attributes["owner"] = owner
+                prior_installation_required = elem.get("prior_installation_required")
                 if asbool(prior_installation_required):
-                    attributes['prior_installation_required'] = 'True'
-                new_elem = xml_util.create_element('repository', attributes=attributes, sub_elements=sub_elements)
+                    attributes["prior_installation_required"] = "True"
+                new_elem = _create_element("repository", attributes=attributes, sub_elements=sub_elements)
                 altered = True
             return altered, new_elem, error_message
         # From here on we're populating the toolshed and changeset_revision attributes if necessary.
         if not toolshed:
             # Default the setting to the current tool shed.
-            toolshed = str(url_for('/', qualified=True)).rstrip('/')
-            elem.attrib['toolshed'] = toolshed
+            toolshed = str(url_for("/", qualified=True)).rstrip("/")
+            elem.attrib["toolshed"] = toolshed
             altered = True
         if not changeset_revision:
             # Populate the changeset_revision attribute with the latest installable metadata revision for
             # the defined repository.  We use the latest installable revision instead of the latest metadata
             # revision to ensure that the contents of the revision are valid.
             repository = repository_util.get_repository_by_name_and_owner(self.app, name, owner)
             if repository:
-                lastest_installable_changeset_revision = \
-                    metadata_util.get_latest_downloadable_changeset_revision(self.app, repository)
+                lastest_installable_changeset_revision = metadata_util.get_latest_downloadable_changeset_revision(
+                    self.app, repository
+                )
                 if lastest_installable_changeset_revision != hg_util.INITIAL_CHANGELOG_HASH:
-                    elem.attrib['changeset_revision'] = lastest_installable_changeset_revision
+                    elem.attrib["changeset_revision"] = lastest_installable_changeset_revision
                     altered = True
                 else:
-                    error_message = 'Invalid latest installable changeset_revision %s ' % \
-                        str(lastest_installable_changeset_revision)
-                    error_message += 'retrieved for repository %s owned by %s.  ' % (str(name), str(owner))
+                    error_message = "Invalid latest installable changeset_revision %s " % str(
+                        lastest_installable_changeset_revision
+                    )
+                    error_message += f"retrieved for repository {name} owned by {owner}.  "
             else:
-                error_message = 'Unable to locate repository with name %s and owner %s.  ' % (str(name), str(owner))
+                error_message = f"Unable to locate repository with name {name} and owner {owner}.  "
         return altered, elem, error_message
 
     def handle_sub_elem(self, parent_elem, elem_index, elem):
         """
         Populate or unpopulate the toolshed and changeset_revision attributes for each of
         the following tag sets.
         <action type="set_environment_for_install">
         <action type="setup_r_environment">
         <action type="setup_ruby_environment">
         """
         sub_elem_altered = False
-        error_message = ''
+        error_message = ""
         for sub_index, sub_elem in enumerate(elem):
             # Make sure to skip comments and tags that are not <repository>.
-            if sub_elem.tag == 'repository':
+            if sub_elem.tag == "repository":
                 altered, new_sub_elem, message = self.handle_elem(sub_elem)
                 if message:
-                    error_message += 'The %s file contains an invalid <repository> tag.  %s' % \
-                        (TOOL_DEPENDENCY_DEFINITION_FILENAME, message)
+                    error_message += f"The {TOOL_DEPENDENCY_DEFINITION_FILENAME} file contains an invalid <repository> tag.  {message}"
                 if altered:
                     if not sub_elem_altered:
                         sub_elem_altered = True
                     elem[sub_index] = new_sub_elem
         if sub_elem_altered:
             parent_elem[elem_index] = elem
         return sub_elem_altered, parent_elem, error_message
@@ -158,43 +170,90 @@
         tree, error_message = xml_util.parse_xml(config)
         if tree is None:
             return False, None, error_message
         root = tree.getroot()
         root_altered = False
         new_root = copy.deepcopy(root)
         for index, elem in enumerate(root):
-            if elem.tag == 'repository':
+            if elem.tag == "repository":
                 # <repository name="molecule_datatypes" owner="test" changeset_revision="1a070566e9c6" />
                 altered, new_elem, error_message = self.handle_elem(elem)
                 if error_message:
-                    error_message = 'The %s file contains an invalid <repository> tag.  %s' % (self.file_name, error_message)
+                    error_message = f"The {self.file_name} file contains an invalid <repository> tag.  {error_message}"
                     return False, None, error_message
                 if altered:
                     if not root_altered:
                         root_altered = True
                     new_root[index] = new_elem
         return root_altered, new_root, error_message
 
 
-class ToolDependencyAttributeHandler(object):
-
+class ToolDependencyAttributeHandler:
     def __init__(self, app, unpopulate):
         self.app = app
         self.file_name = TOOL_DEPENDENCY_DEFINITION_FILENAME
         self.unpopulate = unpopulate
 
     def handle_tag_attributes(self, tool_dependencies_config):
         """
         Populate or unpopulate the tooshed and changeset_revision attributes of each <repository>
         tag defined within a tool_dependencies.xml file.
         """
         rdah = RepositoryDependencyAttributeHandler(self.app, self.unpopulate)
         tah = tag_attribute_handler.TagAttributeHandler(self.app, rdah, self.unpopulate)
         altered = False
-        error_message = ''
+        error_message = ""
         # Make sure we're looking at a valid tool_dependencies.xml file.
         tree, error_message = xml_util.parse_xml(tool_dependencies_config)
         if tree is None:
             return False, None, error_message
         root = tree.getroot()
         altered, new_root, error_message = tah.process_config(root, skip_actions_tags=False)
         return altered, new_root, error_message
+
+
+def _create_element(
+    tag: str,
+    attributes: Optional[Dict[str, str]] = None,
+    sub_elements: Optional[Dict[str, List[Tuple[str, str]]]] = None,
+) -> Optional[etree.Element]:
+    """
+    Create a new element whose tag is the value of the received tag, and whose attributes are all
+    key / value pairs in the received attributes and sub_elements.
+    """
+    if tag:
+        elem = etree.Element(tag)
+        if attributes:
+            # The received attributes is an odict to preserve ordering.
+            for k, attribute_value in attributes.items():
+                elem.set(k, attribute_value)
+        if sub_elements:
+            # The received attributes is an odict.  These handle information that tends to be
+            # long text including paragraphs (e.g., description and long_description.
+            for k, v in sub_elements.items():
+                # Don't include fields that are blank.
+                if v:
+                    if k == "packages":
+                        # The received sub_elements is an odict whose key is 'packages' and whose
+                        # value is a list of ( name, version ) tuples.
+                        for v_tuple in v:
+                            sub_elem = etree.SubElement(elem, "package")
+                            sub_elem_name, sub_elem_version = v_tuple
+                            sub_elem.set("name", sub_elem_name)
+                            sub_elem.set("version", sub_elem_version)
+                    elif isinstance(v, list):
+                        sub_elem = etree.SubElement(elem, k)
+                        # If v is a list, then it must be a list of tuples where the first
+                        # item is the tag and the second item is the text value.
+                        for v_tuple in v:
+                            if len(v_tuple) == 2:
+                                v_tag = v_tuple[0]
+                                v_text = v_tuple[1]
+                                # Don't include fields that are blank.
+                                if v_text:
+                                    v_elem = etree.SubElement(sub_elem, v_tag)
+                                    v_elem.text = v_text
+                    else:
+                        sub_elem = etree.SubElement(elem, k)
+                        sub_elem.text = v
+        return elem
+    return None
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/dependencies/repository/relation_builder.py` & `galaxy-web-apps-23.0.2/tool_shed/dependencies/repository/relation_builder.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 import logging
 
 import tool_shed.util.repository_util
-from galaxy.util import asbool, listify
+from galaxy.util import (
+    asbool,
+    listify,
+)
 from tool_shed.util import (
     common_util,
     container_util,
     metadata_util,
-    shed_util_common as suc
+    shed_util_common as suc,
 )
 
 log = logging.getLogger(__name__)
 
 
-class RelationBuilder(object):
-
+class RelationBuilder:
     def __init__(self, app, repository, repository_metadata, tool_shed_url):
         self.all_repository_dependencies = {}
         self.app = app
         self.circular_repository_dependencies = []
         self.repository = repository
         self.repository_metadata = repository_metadata
         self.handled_key_rd_dicts = []
@@ -46,85 +48,109 @@
         """
         Return a copy of the received key_rd_dict with repository dependencies that are needed
         only_if_compiling_contained_td filtered out of the list of repository dependencies for
         each rd_key.
         """
         filtered_key_rd_dict = {}
         for rd_key, required_rd_tup in key_rd_dict.items():
-            tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-                common_util.parse_repository_dependency_tuple(required_rd_tup)
+            (
+                tool_shed,
+                name,
+                owner,
+                changeset_revision,
+                prior_installation_required,
+                only_if_compiling_contained_td,
+            ) = common_util.parse_repository_dependency_tuple(required_rd_tup)
             if not asbool(only_if_compiling_contained_td):
                 filtered_key_rd_dict[rd_key] = required_rd_tup
         return filtered_key_rd_dict
 
     def get_prior_installation_required_and_only_if_compiling_contained_td(self):
         """
         This method is called from the tool shed and never Galaxy.  If self.all_repository_dependencies
         contains a repository dependency tuple that is associated with self.repository, return the
         value of the tuple's prior_installation_required component.
         """
         cleaned_toolshed_base_url = common_util.remove_protocol_from_tool_shed_url(self.tool_shed_url)
         if self.all_repository_dependencies:
             for rd_key, rd_tups in self.all_repository_dependencies.items():
-                if rd_key in ['root_key', 'description']:
+                if rd_key in ["root_key", "description"]:
                     continue
                 for rd_tup in rd_tups:
-                    rd_toolshed, rd_name, rd_owner, rd_changeset_revision, \
-                        rd_prior_installation_required, \
-                        rd_only_if_compiling_contained_td = \
-                        common_util.parse_repository_dependency_tuple(rd_tup)
+                    (
+                        rd_toolshed,
+                        rd_name,
+                        rd_owner,
+                        rd_changeset_revision,
+                        rd_prior_installation_required,
+                        rd_only_if_compiling_contained_td,
+                    ) = common_util.parse_repository_dependency_tuple(rd_tup)
                     cleaned_rd_toolshed = common_util.remove_protocol_from_tool_shed_url(rd_toolshed)
-                    if cleaned_rd_toolshed == cleaned_toolshed_base_url and \
-                            rd_name == self.repository.name and \
-                            rd_owner == self.repository.user.username and \
-                            rd_changeset_revision == self.repository_metadata.changeset_revision:
+                    if (
+                        cleaned_rd_toolshed == cleaned_toolshed_base_url
+                        and rd_name == self.repository.name
+                        and rd_owner == self.repository.user.username
+                        and rd_changeset_revision == self.repository_metadata.changeset_revision
+                    ):
                         return rd_prior_installation_required, rd_only_if_compiling_contained_td
         elif self.repository_metadata:
             # Get the list of changeset revisions from the tool shed to which self.repository may be updated.
             metadata = self.repository_metadata.metadata
             current_changeset_revision = str(self.repository_metadata.changeset_revision)
             # Get the changeset revision to which the current value of required_repository_changeset_revision
             # should be updated if it's not current.
-            text = metadata_util.get_updated_changeset_revisions(self.app,
-                                                                 name=str(self.repository.name),
-                                                                 owner=str(self.repository.user.username),
-                                                                 changeset_revision=current_changeset_revision)
+            text = metadata_util.get_updated_changeset_revisions(
+                self.app,
+                name=str(self.repository.name),
+                owner=str(self.repository.user.username),
+                changeset_revision=current_changeset_revision,
+            )
             if text:
                 valid_changeset_revisions = listify(text)
                 if current_changeset_revision not in valid_changeset_revisions:
                     valid_changeset_revisions.append(current_changeset_revision)
             else:
                 valid_changeset_revisions = [current_changeset_revision]
-            repository_dependencies_dict = metadata['repository_dependencies']
-            rd_tups = repository_dependencies_dict.get('repository_dependencies', [])
+            repository_dependencies_dict = metadata["repository_dependencies"]
+            rd_tups = repository_dependencies_dict.get("repository_dependencies", [])
             for rd_tup in rd_tups:
-                rd_toolshed, rd_name, rd_owner, rd_changeset_revision, \
-                    rd_prior_installation_required, \
-                    rd_only_if_compiling_contained_td = \
-                    common_util.parse_repository_dependency_tuple(rd_tup)
+                (
+                    rd_toolshed,
+                    rd_name,
+                    rd_owner,
+                    rd_changeset_revision,
+                    rd_prior_installation_required,
+                    rd_only_if_compiling_contained_td,
+                ) = common_util.parse_repository_dependency_tuple(rd_tup)
                 cleaned_rd_toolshed = common_util.remove_protocol_from_tool_shed_url(rd_toolshed)
-                if cleaned_rd_toolshed == cleaned_toolshed_base_url and \
-                        rd_name == self.repository.name and \
-                        rd_owner == self.repository.user.username and \
-                        rd_changeset_revision in valid_changeset_revisions:
+                if (
+                    cleaned_rd_toolshed == cleaned_toolshed_base_url
+                    and rd_name == self.repository.name
+                    and rd_owner == self.repository.user.username
+                    and rd_changeset_revision in valid_changeset_revisions
+                ):
                     return rd_prior_installation_required, rd_only_if_compiling_contained_td
         # Default both prior_installation_required and only_if_compiling_contained_td to False.
-        return 'False', 'False'
+        return "False", "False"
 
     def get_key_for_repository_changeset_revision(self):
         # The received toolshed_base_url must include the port, but doesn't have to include the protocol.
-        prior_installation_required, only_if_compiling_contained_td = \
-            self.get_prior_installation_required_and_only_if_compiling_contained_td()
+        (
+            prior_installation_required,
+            only_if_compiling_contained_td,
+        ) = self.get_prior_installation_required_and_only_if_compiling_contained_td()
         # Create a key with the value of prior_installation_required defaulted to False.
-        key = container_util.generate_repository_dependencies_key_for_repository(self.tool_shed_url,
-                                                                                 self.repository.name,
-                                                                                 self.repository.user.username,
-                                                                                 self.repository_metadata.changeset_revision,
-                                                                                 prior_installation_required,
-                                                                                 only_if_compiling_contained_td)
+        key = container_util.generate_repository_dependencies_key_for_repository(
+            self.tool_shed_url,
+            self.repository.name,
+            self.repository.user.username,
+            self.repository_metadata.changeset_revision,
+            prior_installation_required,
+            only_if_compiling_contained_td,
+        )
         return key
 
     def get_repository_dependencies_for_changeset_revision(self):
         """
         Return a dictionary of all repositories upon which the contents of self.repository_metadata
         record depend.  The dictionary keys are name-spaced values consisting of:
         self.tool_shed_url/repository_name/repository_owner/changeset_revision
@@ -134,24 +160,24 @@
         """
         # Assume the current repository does not have repository dependencies defined for it.
         current_repository_key = None
         metadata = self.repository_metadata.metadata
         if metadata:
             # The value of self.tool_shed_url must include the port, but doesn't have to include
             # the protocol.
-            if 'repository_dependencies' in metadata:
+            if "repository_dependencies" in metadata:
                 current_repository_key = self.get_key_for_repository_changeset_revision()
-                repository_dependencies_dict = metadata['repository_dependencies']
+                repository_dependencies_dict = metadata["repository_dependencies"]
                 if not self.all_repository_dependencies:
                     self.initialize_all_repository_dependencies(current_repository_key, repository_dependencies_dict)
                 # Handle the repository dependencies defined in the current repository, if any, and populate
                 # the various repository dependency objects for this round of processing.
-                current_repository_key_rd_dicts = \
-                    self.populate_repository_dependency_objects_for_processing(current_repository_key,
-                                                                               repository_dependencies_dict)
+                current_repository_key_rd_dicts = self.populate_repository_dependency_objects_for_processing(
+                    current_repository_key, repository_dependencies_dict
+                )
         if current_repository_key:
             if current_repository_key_rd_dicts:
                 # There should be only a single current_repository_key_rd_dict in this list.
                 current_repository_key_rd_dict = current_repository_key_rd_dicts[0]
                 # Handle circular repository dependencies.
                 if not self.in_circular_repository_dependencies(current_repository_key_rd_dict):
                     if current_repository_key in self.all_repository_dependencies:
@@ -162,91 +188,113 @@
                 self.handle_next_repository_dependency()
         elif self.key_rd_dicts_to_be_processed:
             self.handle_next_repository_dependency()
         self.all_repository_dependencies = self.prune_invalid_repository_dependencies(self.all_repository_dependencies)
         return self.all_repository_dependencies
 
     def get_repository_dependency_as_key(self, repository_dependency):
-        tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-            common_util.parse_repository_dependency_tuple(repository_dependency)
-        return container_util.generate_repository_dependencies_key_for_repository(tool_shed,
-                                                                                  name,
-                                                                                  owner,
-                                                                                  changeset_revision,
-                                                                                  prior_installation_required,
-                                                                                  only_if_compiling_contained_td)
+        (
+            tool_shed,
+            name,
+            owner,
+            changeset_revision,
+            prior_installation_required,
+            only_if_compiling_contained_td,
+        ) = common_util.parse_repository_dependency_tuple(repository_dependency)
+        return container_util.generate_repository_dependencies_key_for_repository(
+            tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td
+        )
 
     def get_updated_changeset_revisions_for_repository_dependencies(self, key_rd_dicts):
         updated_key_rd_dicts = []
         for key_rd_dict in key_rd_dicts:
             key = next(iter(key_rd_dict))
             repository_dependency = key_rd_dict[key]
-            rd_toolshed, rd_name, rd_owner, rd_changeset_revision, \
-                rd_prior_installation_required, \
-                rd_only_if_compiling_contained_td = \
-                common_util.parse_repository_dependency_tuple(repository_dependency)
+            (
+                rd_toolshed,
+                rd_name,
+                rd_owner,
+                rd_changeset_revision,
+                rd_prior_installation_required,
+                rd_only_if_compiling_contained_td,
+            ) = common_util.parse_repository_dependency_tuple(repository_dependency)
             if suc.tool_shed_is_this_tool_shed(rd_toolshed):
-                repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
+                repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(
+                    self.app, rd_name, rd_owner
+                )
                 if repository:
                     repository_id = self.app.security.encode_id(repository.id)
-                    repository_metadata = \
-                        metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app,
-                                                                                                  repository_id,
-                                                                                                  rd_changeset_revision)
+                    repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+                        self.app, repository_id, rd_changeset_revision
+                    )
                     if repository_metadata:
                         # The repository changeset_revision is installable, so no updates are available.
                         new_key_rd_dict = {}
                         new_key_rd_dict[key] = repository_dependency
                         updated_key_rd_dicts.append(key_rd_dict)
                     else:
                         # The repository changeset_revision is no longer installable, so see if there's been an update.
-                        changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
+                        changeset_revision = metadata_util.get_next_downloadable_changeset_revision(
+                            self.app, repository, rd_changeset_revision
+                        )
                         if changeset_revision != rd_changeset_revision:
-                            repository_metadata = \
-                                metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app,
-                                                                                                          repository_id,
-                                                                                                          changeset_revision)
+                            repository_metadata = (
+                                metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+                                    self.app, repository_id, changeset_revision
+                                )
+                            )
                         if repository_metadata:
                             new_key_rd_dict = {}
-                            new_key_rd_dict[key] = \
-                                [rd_toolshed,
-                                 rd_name,
-                                 rd_owner,
-                                 repository_metadata.changeset_revision,
-                                 rd_prior_installation_required,
-                                 rd_only_if_compiling_contained_td]
+                            new_key_rd_dict[key] = [
+                                rd_toolshed,
+                                rd_name,
+                                rd_owner,
+                                repository_metadata.changeset_revision,
+                                rd_prior_installation_required,
+                                rd_only_if_compiling_contained_td,
+                            ]
                             # We have the updated changeset revision.
                             updated_key_rd_dicts.append(new_key_rd_dict)
                         else:
                             repository_components_tuple = container_util.get_components_from_key(key)
-                            components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
-                            toolshed, repository_name, repository_owner, repository_changeset_revision = components_list[0:4]
+                            components_list = tool_shed.util.repository_util.extract_components_from_tuple(
+                                repository_components_tuple
+                            )
+                            (
+                                toolshed,
+                                repository_name,
+                                repository_owner,
+                                repository_changeset_revision,
+                            ) = components_list[0:4]
                             # For backward compatibility to the 12/20/12 Galaxy release.
                             if len(components_list) in (4, 5):
-                                rd_only_if_compiling_contained_td = 'False'
-                            message = "The revision %s defined for repository %s owned by %s is invalid, so repository " % \
-                                (str(rd_changeset_revision), str(rd_name), str(rd_owner))
-                            message += "dependencies defined for repository %s will be ignored." % str(repository_name)
+                                rd_only_if_compiling_contained_td = "False"
+                            message = (
+                                "The revision %s defined for repository %s owned by %s is invalid, so repository "
+                                % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
+                            )
+                            message += f"dependencies defined for repository {repository_name} will be ignored."
                             log.debug(message)
                 else:
                     repository_components_tuple = container_util.get_components_from_key(key)
-                    components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
+                    components_list = tool_shed.util.repository_util.extract_components_from_tuple(
+                        repository_components_tuple
+                    )
                     toolshed, repository_name, repository_owner, repository_changeset_revision = components_list[0:4]
-                    message = "The revision %s defined for repository %s owned by %s is invalid, so repository " % \
-                        (str(rd_changeset_revision), str(rd_name), str(rd_owner))
-                    message += "dependencies defined for repository %s will be ignored." % str(repository_name)
+                    message = f"The revision {rd_changeset_revision} defined for repository {rd_name} owned by {rd_owner} is invalid, "
+                    message += f"so repository dependencies defined for repository {repository_name} will be ignored."
                     log.debug(message)
         return updated_key_rd_dicts
 
     def handle_circular_repository_dependency(self, repository_key, repository_dependency):
-        all_repository_dependencies_root_key = self.all_repository_dependencies['root_key']
+        all_repository_dependencies_root_key = self.all_repository_dependencies["root_key"]
         repository_dependency_as_key = self.get_repository_dependency_as_key(repository_dependency)
-        self.update_circular_repository_dependencies(repository_key,
-                                                     repository_dependency,
-                                                     self.all_repository_dependencies[repository_dependency_as_key])
+        self.update_circular_repository_dependencies(
+            repository_key, repository_dependency, self.all_repository_dependencies[repository_dependency_as_key]
+        )
         if all_repository_dependencies_root_key != repository_dependency_as_key:
             self.all_repository_dependencies[repository_key] = [repository_dependency]
 
     def handle_current_repository_dependency(self, current_repository_key):
         current_repository_key_rd_dicts = []
         for rd in self.all_repository_dependencies[current_repository_key]:
             rd_copy = [str(item) for item in rd]
@@ -256,44 +304,51 @@
         if current_repository_key_rd_dicts:
             self.handle_key_rd_dicts_for_repository(current_repository_key, current_repository_key_rd_dicts)
             return self.get_repository_dependencies_for_changeset_revision()
 
     def handle_key_rd_dicts_for_repository(self, current_repository_key, repository_key_rd_dicts):
         key_rd_dict = repository_key_rd_dicts.pop(0)
         repository_dependency = key_rd_dict[current_repository_key]
-        toolshed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-            common_util.parse_repository_dependency_tuple(repository_dependency)
+        (
+            toolshed,
+            name,
+            owner,
+            changeset_revision,
+            prior_installation_required,
+            only_if_compiling_contained_td,
+        ) = common_util.parse_repository_dependency_tuple(repository_dependency)
         if suc.tool_shed_is_this_tool_shed(toolshed):
             required_repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, name, owner)
             self.repository = required_repository
             repository_id = self.app.security.encode_id(required_repository.id)
-            required_repository_metadata = \
-                metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app,
-                                                                                          repository_id,
-                                                                                          changeset_revision)
+            required_repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+                self.app, repository_id, changeset_revision
+            )
             self.repository_metadata = required_repository_metadata
             if required_repository_metadata:
                 # The required_repository_metadata changeset_revision is installable.
                 required_metadata = required_repository_metadata.metadata
                 if required_metadata:
                     for current_repository_key_rd_dict in repository_key_rd_dicts:
                         if not self.in_key_rd_dicts(current_repository_key_rd_dict, self.key_rd_dicts_to_be_processed):
                             # Add the current repository_dependency into self.key_rd_dicts_to_be_processed.
                             self.key_rd_dicts_to_be_processed.append(current_repository_key_rd_dict)
             if not self.in_key_rd_dicts(key_rd_dict, self.handled_key_rd_dicts):
                 # Add the current repository_dependency into self.handled_key_rd_dicts.
                 self.handled_key_rd_dicts.append(key_rd_dict)
             if self.in_key_rd_dicts(key_rd_dict, self.key_rd_dicts_to_be_processed):
                 # Remove the current repository from self.key_rd_dicts_to_be_processed.
-                self.key_rd_dicts_to_be_processed = self.remove_from_key_rd_dicts(key_rd_dict, self.key_rd_dicts_to_be_processed)
+                self.key_rd_dicts_to_be_processed = self.remove_from_key_rd_dicts(
+                    key_rd_dict, self.key_rd_dicts_to_be_processed
+                )
         else:
             # The repository is in a different tool shed, so build an url and send a request.
             error_message = "Repository dependencies are currently supported only within the same Tool Shed.  "
             error_message += "Ignoring repository dependency definition for tool shed "
-            error_message += "%s, name %s, owner %s, changeset revision %s" % (toolshed, name, owner, changeset_revision)
+            error_message += f"{toolshed}, name {name}, owner {owner}, changeset revision {changeset_revision}"
             log.debug(error_message)
 
     def handle_next_repository_dependency(self):
         next_repository_key_rd_dict = self.key_rd_dicts_to_be_processed.pop(0)
         next_repository_key_rd_dicts = [next_repository_key_rd_dict]
         next_repository_key = next(iter(next_repository_key_rd_dict))
         self.handle_key_rd_dicts_for_repository(next_repository_key, next_repository_key_rd_dicts)
@@ -335,19 +390,19 @@
                 if key == k and val == v:
                     return True
         return False
 
     def initialize_all_repository_dependencies(self, current_repository_key, repository_dependencies_dict):
         """Initialize the self.all_repository_dependencies dictionary."""
         # It's safe to assume that current_repository_key in this case will have a value.
-        self.all_repository_dependencies['root_key'] = current_repository_key
+        self.all_repository_dependencies["root_key"] = current_repository_key
         self.all_repository_dependencies[current_repository_key] = []
         # Store the value of the 'description' key only once, the first time through this recursive method.
-        description = repository_dependencies_dict.get('description', None)
-        self.all_repository_dependencies['description'] = description
+        description = repository_dependencies_dict.get("description", None)
+        self.all_repository_dependencies["description"] = description
 
     def is_circular_repository_dependency(self, repository_key, repository_dependency):
         """
         Return True if the received repository_dependency is a key in self.all_repository_dependencies
         whose list of repository dependencies includes the received repository_key.
         """
         repository_dependency_as_key = self.get_repository_dependency_as_key(repository_dependency)
@@ -355,35 +410,39 @@
         for key, val in self.all_repository_dependencies.items():
             if key != repository_dependency_as_key:
                 continue
             if repository_key_as_repository_dependency in val:
                 return True
         return False
 
-    def populate_repository_dependency_objects_for_processing(self, current_repository_key, repository_dependencies_dict):
+    def populate_repository_dependency_objects_for_processing(
+        self, current_repository_key, repository_dependencies_dict
+    ):
         """
         The process that discovers all repository dependencies for a specified repository's changeset
         revision uses this method to populate the following items for the current processing loop:
         filtered_current_repository_key_rd_dicts, self.key_rd_dicts_to_be_processed,
         self.handled_key_rd_dicts, self.all_repository_dependencies.  Each processing loop may discover
         more repository dependencies, so this method is repeatedly called until all repository
         dependencies have been discovered.
         """
         current_repository_key_rd_dicts = []
         filtered_current_repository_key_rd_dicts = []
-        for rd_tup in repository_dependencies_dict['repository_dependencies']:
+        for rd_tup in repository_dependencies_dict["repository_dependencies"]:
             new_key_rd_dict = {}
             new_key_rd_dict[current_repository_key] = rd_tup
             current_repository_key_rd_dicts.append(new_key_rd_dict)
         if current_repository_key_rd_dicts and current_repository_key:
             # Remove all repository dependencies that point to a revision within its own repository.
-            current_repository_key_rd_dicts = \
-                self.remove_repository_dependency_reference_to_self(current_repository_key_rd_dicts)
-        current_repository_key_rd_dicts = \
-            self.get_updated_changeset_revisions_for_repository_dependencies(current_repository_key_rd_dicts)
+            current_repository_key_rd_dicts = self.remove_repository_dependency_reference_to_self(
+                current_repository_key_rd_dicts
+            )
+        current_repository_key_rd_dicts = self.get_updated_changeset_revisions_for_repository_dependencies(
+            current_repository_key_rd_dicts
+        )
         for key_rd_dict in current_repository_key_rd_dicts:
             if self.filter_dependencies_needed_for_compiling:
                 # Filter out repository dependencies that are required only if compiling the dependent
                 # repository's tool dependency.
                 # TODO: this temporary work-around should be removed when the underlying framework
                 # support for handling only_if_compiling_contained_td-flagged repositories is completed.
                 key_rd_dict = self.filter_only_if_compiling_contained_td(key_rd_dict)
@@ -418,26 +477,26 @@
         """
         Eliminate all invalid entries in the received repository_dependencies dictionary.  An entry
         is invalid if the value_list of the key/value pair is empty.  This occurs when an invalid
         combination of tool shed, name , owner, changeset_revision is used and a repository_metadata
         record is not found.
         """
         valid_repository_dependencies = {}
-        description = repository_dependencies.get('description', None)
-        root_key = repository_dependencies.get('root_key', None)
+        description = repository_dependencies.get("description", None)
+        root_key = repository_dependencies.get("root_key", None)
         if root_key is None:
             return valid_repository_dependencies
         for key, value in repository_dependencies.items():
-            if key in ['description', 'root_key']:
+            if key in ["description", "root_key"]:
                 continue
             if value:
                 valid_repository_dependencies[key] = value
         if valid_repository_dependencies:
-            valid_repository_dependencies['description'] = description
-            valid_repository_dependencies['root_key'] = root_key
+            valid_repository_dependencies["description"] = description
+            valid_repository_dependencies["root_key"] = root_key
         return valid_repository_dependencies
 
     def remove_from_key_rd_dicts(self, key_rd_dict, key_rd_dicts):
         """Eliminate the key_rd_dict from the list of key_rd_dicts if it is contained in the list."""
         k = next(iter(key_rd_dict))
         v = key_rd_dict[k]
         clean_key_rd_dicts = []
@@ -450,28 +509,38 @@
         return clean_key_rd_dicts
 
     def remove_repository_dependency_reference_to_self(self, key_rd_dicts):
         """Remove all repository dependencies that point to a revision within its own repository."""
         clean_key_rd_dicts = []
         key = next(iter(key_rd_dicts[0]))
         repository_tup = key.split(container_util.STRSEP)
-        rd_toolshed, rd_name, rd_owner, rd_changeset_revision, \
-            rd_prior_installation_required, \
-            rd_only_if_compiling_contained_td = \
-            common_util.parse_repository_dependency_tuple(repository_tup)
+        (
+            rd_toolshed,
+            rd_name,
+            rd_owner,
+            rd_changeset_revision,
+            rd_prior_installation_required,
+            rd_only_if_compiling_contained_td,
+        ) = common_util.parse_repository_dependency_tuple(repository_tup)
         cleaned_rd_toolshed = common_util.remove_protocol_from_tool_shed_url(rd_toolshed)
         for key_rd_dict in key_rd_dicts:
             k = next(iter(key_rd_dict))
             repository_dependency = key_rd_dict[k]
-            toolshed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-                common_util.parse_repository_dependency_tuple(repository_dependency)
+            (
+                toolshed,
+                name,
+                owner,
+                changeset_revision,
+                prior_installation_required,
+                only_if_compiling_contained_td,
+            ) = common_util.parse_repository_dependency_tuple(repository_dependency)
             cleaned_toolshed = common_util.remove_protocol_from_tool_shed_url(toolshed)
             if cleaned_rd_toolshed == cleaned_toolshed and rd_name == name and rd_owner == owner:
-                debug_msg = "Removing repository dependency for repository %s owned by %s " % (name, owner)
-                debug_msg += 'since it refers to a revision within itself.'
+                debug_msg = f"Removing repository dependency for repository {name} owned by {owner} "
+                debug_msg += "since it refers to a revision within itself."
                 log.debug(debug_msg)
             else:
                 new_key_rd_dict = {}
                 new_key_rd_dict[key] = repository_dependency
                 clean_key_rd_dicts.append(new_key_rd_dict)
         return clean_key_rd_dicts
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/dependencies/tool/tag_attribute_handler.py` & `galaxy-web-apps-23.0.2/tool_shed/dependencies/tool/tag_attribute_handler.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import copy
 import logging
 
 log = logging.getLogger(__name__)
 
 
-class TagAttributeHandler(object):
-
+class TagAttributeHandler:
     def __init__(self, app, rdd, unpopulate):
         self.app = app
         self.altered = False
         self.rdd = rdd
         self.unpopulate = unpopulate
 
     def process_action_tag_set(self, elem, message):
@@ -19,21 +18,19 @@
         #        <package name="readline" version="6.2" />
         #    </repository>
         # </action>
         elem_altered = False
         new_elem = copy.deepcopy(elem)
         for sub_index, sub_elem in enumerate(elem):
             altered = False
-            error_message = ''
-            if sub_elem.tag == 'repository':
-                altered, new_sub_elem, error_message = \
-                    self.process_repository_tag_set(parent_elem=elem,
-                                                    elem_index=sub_index,
-                                                    elem=sub_elem,
-                                                    message=message)
+            error_message = ""
+            if sub_elem.tag == "repository":
+                altered, new_sub_elem, error_message = self.process_repository_tag_set(
+                    parent_elem=elem, elem_index=sub_index, elem=sub_elem, message=message
+                )
             if error_message and error_message not in message:
                 message += error_message
             if altered:
                 if not self.altered:
                     self.altered = True
                 if not elem_altered:
                     elem_altered = True
@@ -45,33 +42,32 @@
         #     <package name="libgtextutils" version="0.6">
         #         <repository name="package_libgtextutils_0_6" owner="test" prior_installation_required="True" />
         #     </package>
         elem_altered = False
         new_elem = copy.deepcopy(elem)
         for sub_index, sub_elem in enumerate(elem):
             altered = False
-            error_message = ''
-            if sub_elem.tag == 'package':
-                altered, new_sub_elem, error_message = self.process_package_tag_set(elem=sub_elem,
-                                                                                    message=message,
-                                                                                    skip_actions_tags=skip_actions_tags)
-            elif sub_elem.tag == 'action':
+            error_message = ""
+            if sub_elem.tag == "package":
+                altered, new_sub_elem, error_message = self.process_package_tag_set(
+                    elem=sub_elem, message=message, skip_actions_tags=skip_actions_tags
+                )
+            elif sub_elem.tag == "action":
                 # <action type="set_environment_for_install">
                 #    <repository name="package_readline_6_2" owner="devteam"">
                 #        <package name="readline" version="6.2" />
                 #    </repository>
                 # </action>
-                altered, new_sub_elem, error_message = self.process_action_tag_set(elem=sub_elem,
-                                                                                   message=message)
+                altered, new_sub_elem, error_message = self.process_action_tag_set(elem=sub_elem, message=message)
             else:
                 # Inspect the sub elements of elem to locate all <repository> tags and
                 # populate them with toolshed and changeset_revision attributes if necessary.
-                altered, new_sub_elem, error_message = self.rdd.handle_sub_elem(parent_elem=elem,
-                                                                                elem_index=sub_index,
-                                                                                elem=sub_elem)
+                altered, new_sub_elem, error_message = self.rdd.handle_sub_elem(
+                    parent_elem=elem, elem_index=sub_index, elem=sub_elem
+                )
             if error_message and error_message not in message:
                 message += error_message
             if altered:
                 if not self.altered:
                     self.altered = True
                 if not elem_altered:
                     elem_altered = True
@@ -83,80 +79,75 @@
         # tag sets that define os and architecture attributes.  We want to inspect
         # only the last <actions> tag set contained within the <actions_group> tag
         # set to see if a complex repository dependency is defined.
         elem_altered = False
         new_elem = copy.deepcopy(elem)
         for sub_index, sub_elem in enumerate(elem):
             altered = False
-            error_message = ''
-            if sub_elem.tag == 'actions':
+            error_message = ""
+            if sub_elem.tag == "actions":
                 if skip_actions_tags:
                     # Skip all actions tags that include os or architecture attributes.
-                    system = sub_elem.get('os')
-                    architecture = sub_elem.get('architecture')
+                    system = sub_elem.get("os")
+                    architecture = sub_elem.get("architecture")
                     if system or architecture:
                         continue
-                altered, new_sub_elem, error_message = \
-                    self.process_actions_tag_set(elem=sub_elem,
-                                                 message=message,
-                                                 skip_actions_tags=skip_actions_tags)
+                altered, new_sub_elem, error_message = self.process_actions_tag_set(
+                    elem=sub_elem, message=message, skip_actions_tags=skip_actions_tags
+                )
             if error_message and error_message not in message:
                 message += error_message
             if altered:
                 if not self.altered:
                     self.altered = True
                 if not elem_altered:
                     elem_altered = True
                 new_elem[sub_index] = new_sub_elem
         return elem_altered, new_elem, message
 
     def process_config(self, root, skip_actions_tags=True):
-        error_message = ''
+        error_message = ""
         new_root = copy.deepcopy(root)
-        if root.tag == 'tool_dependency':
+        if root.tag == "tool_dependency":
             for elem_index, elem in enumerate(root):
                 altered = False
-                if elem.tag == 'package':
+                if elem.tag == "package":
                     # <package name="eigen" version="2.0.17">
-                    altered, new_elem, error_message = \
-                        self.process_package_tag_set(elem=elem,
-                                                     message=error_message,
-                                                     skip_actions_tags=skip_actions_tags)
+                    altered, new_elem, error_message = self.process_package_tag_set(
+                        elem=elem, message=error_message, skip_actions_tags=skip_actions_tags
+                    )
                 if altered:
                     if not self.altered:
                         self.altered = True
                     new_root[elem_index] = new_elem
         else:
             error_message = "Invalid tool_dependencies.xml file."
         return self.altered, new_root, error_message
 
     def process_install_tag_set(self, elem, message, skip_actions_tags=True):
         # <install version="1.0">
         elem_altered = False
         new_elem = copy.deepcopy(elem)
         for sub_index, sub_elem in enumerate(elem):
             altered = False
-            error_message = ''
-            if sub_elem.tag == 'actions_group':
-                altered, new_sub_elem, error_message = \
-                    self.process_actions_group_tag_set(elem=sub_elem,
-                                                       message=message,
-                                                       skip_actions_tags=skip_actions_tags)
-            elif sub_elem.tag == 'actions':
-                altered, new_sub_elem, error_message = \
-                    self.process_actions_tag_set(elem=sub_elem,
-                                                 message=message,
-                                                 skip_actions_tags=skip_actions_tags)
+            error_message = ""
+            if sub_elem.tag == "actions_group":
+                altered, new_sub_elem, error_message = self.process_actions_group_tag_set(
+                    elem=sub_elem, message=message, skip_actions_tags=skip_actions_tags
+                )
+            elif sub_elem.tag == "actions":
+                altered, new_sub_elem, error_message = self.process_actions_tag_set(
+                    elem=sub_elem, message=message, skip_actions_tags=skip_actions_tags
+                )
             else:
-                package_name = elem.get('name', '')
-                package_version = elem.get('version', '')
-                error_message += 'Version %s of the %s package cannot be installed because ' % \
-                    (str(package_version), str(package_name))
-                error_message += 'the recipe for installing the package is missing either an '
-                error_message += '&lt;actions&gt; tag set or an &lt;actions_group&gt; tag set.'
+                package_name = elem.get("name", "")
+                package_version = elem.get("version", "")
+                error_message += f"Version {package_version} of the {package_name} package cannot be "
+                error_message += "installed because the recipe for installing the package is missing "
+                error_message += "either an &lt;actions&gt; tag set or an &lt;actions_group&gt; tag set."
             if error_message and error_message not in message:
                 message += error_message
             if altered:
                 if not self.altered:
                     self.altered = True
                 if not elem_altered:
                     elem_altered = True
@@ -164,40 +155,37 @@
         return elem_altered, new_elem, message
 
     def process_package_tag_set(self, elem, message, skip_actions_tags=True):
         elem_altered = False
         new_elem = copy.deepcopy(elem)
         for sub_index, sub_elem in enumerate(elem):
             altered = False
-            error_message = ''
-            if sub_elem.tag == 'install':
-                altered, new_sub_elem, error_message = \
-                    self.process_install_tag_set(elem=sub_elem,
-                                                 message=message,
-                                                 skip_actions_tags=skip_actions_tags)
-            elif sub_elem.tag == 'repository':
-                altered, new_sub_elem, error_message = \
-                    self.process_repository_tag_set(parent_elem=elem,
-                                                    elem_index=sub_index,
-                                                    elem=sub_elem,
-                                                    message=message)
+            error_message = ""
+            if sub_elem.tag == "install":
+                altered, new_sub_elem, error_message = self.process_install_tag_set(
+                    elem=sub_elem, message=message, skip_actions_tags=skip_actions_tags
+                )
+            elif sub_elem.tag == "repository":
+                altered, new_sub_elem, error_message = self.process_repository_tag_set(
+                    parent_elem=elem, elem_index=sub_index, elem=sub_elem, message=message
+                )
             if error_message and error_message not in message:
                 message += error_message
             if altered:
                 if not self.altered:
                     self.altered = True
                 if not elem_altered:
                     elem_altered = True
                 new_elem[sub_index] = new_sub_elem
         return elem_altered, new_elem, message
 
     def process_repository_tag_set(self, parent_elem, elem_index, elem, message):
         # We have a complex repository dependency.
-        altered, new_elem, error_message = self.rdd.handle_complex_dependency_elem(parent_elem=parent_elem,
-                                                                                   elem_index=elem_index,
-                                                                                   elem=elem)
+        altered, new_elem, error_message = self.rdd.handle_complex_dependency_elem(
+            parent_elem=parent_elem, elem_index=elem_index, elem=elem
+        )
         if error_message and error_message not in message:
             message += error_message
         if altered:
             if not self.altered:
                 self.altered = True
         return altered, new_elem, message
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/grids/admin_grids.py` & `galaxy-web-apps-23.0.2/tool_shed/grids/admin_grids.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,509 +1,499 @@
 import logging
 
 from markupsafe import escape
 from sqlalchemy import and_
 
 from galaxy.web.framework.helpers import time_ago
-from galaxy.webapps.reports.framework import grids
-from tool_shed.grids.repository_grids import CategoryGrid, RepositoryGrid
+from galaxy.web.legacy_framework import grids
+from tool_shed.grids.repository_grids import (
+    CategoryGrid,
+    RepositoryGrid,
+)
 from tool_shed.util import hg_util
 from tool_shed.webapp import model
 
 log = logging.getLogger(__name__)
 
 
 class UserGrid(grids.Grid):
-
     class UserLoginColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, user):
             return escape(user.email)
 
     class UserNameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, user):
             if user.username:
                 return escape(user.username)
-            return 'not set'
+            return "not set"
 
     class GroupsColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, user):
             if user.groups:
                 return len(user.groups)
             return 0
 
     class RolesColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, user):
             if user.roles:
                 return len(user.roles)
             return 0
 
     class ExternalColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, user):
             if user.external:
-                return 'yes'
-            return 'no'
+                return "yes"
+            return "no"
 
     class LastLoginColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, user):
             if user.galaxy_sessions:
                 return self.format(user.galaxy_sessions[0].update_time)
-            return 'never'
+            return "never"
 
     class StatusColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, user):
             if user.purged:
                 return "purged"
             elif user.deleted:
                 return "deleted"
             return ""
 
     class EmailColumn(grids.GridColumn):
-
         def filter(self, trans, user, query, column_filter):
-            if column_filter == 'All':
+            if column_filter == "All":
                 return query
-            return query.filter(and_(model.Tool.table.c.user_id == model.User.table.c.id,
-                                     model.User.table.c.email == column_filter))
+            return query.filter(
+                and_(model.Tool.table.c.user_id == model.User.table.c.id, model.User.table.c.email == column_filter)
+            )
 
     title = "Users"
     model_class = model.User
     default_sort_key = "email"
     columns = [
-        UserLoginColumn("Email",
-                        key="email",
-                        link=(lambda item: dict(operation="information", id=item.id)),
-                        attach_popup=True,
-                        filterable="advanced"),
-        UserNameColumn("User Name",
-                       key="username",
-                       attach_popup=False,
-                       filterable="advanced"),
+        UserLoginColumn(
+            "Email",
+            key="email",
+            link=(lambda item: dict(operation="information", id=item.id)),
+            attach_popup=True,
+            filterable="advanced",
+        ),
+        UserNameColumn("User Name", key="username", attach_popup=False, filterable="advanced"),
         GroupsColumn("Groups", attach_popup=False),
         RolesColumn("Roles", attach_popup=False),
         ExternalColumn("External", attach_popup=False),
         LastLoginColumn("Last Login", format=time_ago),
         StatusColumn("Status", attach_popup=False),
         # Columns that are valid for filtering but are not visible.
-        EmailColumn("Email",
-                    key="email",
-                    visible=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0], columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    global_actions = [
-        grids.GridAction("Create new user",
-                         dict(controller='admin', action='users', operation='create'))
+        EmailColumn("Email", key="email", visible=False),
     ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
+    global_actions = [grids.GridAction("Create new user", dict(controller="admin", action="users", operation="create"))]
     operations = [
-        grids.GridOperation("Manage Roles and Groups",
-                            condition=(lambda item: not item.deleted),
-                            allow_multiple=False,
-                            url_args=dict(action="manage_roles_and_groups_for_user")),
-        grids.GridOperation("Reset Password",
-                            condition=(lambda item: not item.deleted),
-                            allow_multiple=True,
-                            allow_popup=False,
-                            url_args=dict(action="reset_user_password"))
+        grids.GridOperation(
+            "Manage Roles and Groups",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="manage_roles_and_groups_for_user"),
+        ),
+        grids.GridOperation(
+            "Reset Password",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=True,
+            allow_popup=False,
+            url_args=dict(action="reset_user_password"),
+        ),
     ]
     standard_filters = [
         grids.GridColumnFilter("Active", args=dict(deleted=False)),
         grids.GridColumnFilter("Deleted", args=dict(deleted=True, purged=False)),
         grids.GridColumnFilter("Purged", args=dict(purged=True)),
-        grids.GridColumnFilter("All", args=dict(deleted='All'))
+        grids.GridColumnFilter("All", args=dict(deleted="All")),
     ]
 
     use_paging = False
 
     def get_current_item(self, trans, **kwargs):
         return trans.user
 
 
 class RoleGrid(grids.Grid):
-
     class NameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, role):
             return escape(str(role.name))
 
     class DescriptionColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, role):
             if role.description:
                 return str(role.description)
-            return ''
+            return ""
 
     class TypeColumn(grids.TextColumn):
         def get_value(self, trans, grid, role):
             return str(role.type)
 
     class StatusColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, role):
             if role.deleted:
                 return "deleted"
             return ""
 
     class GroupsColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, role):
             if role.groups:
                 return len(role.groups)
             return 0
 
     class RepositoriesColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, role):
             if role.repositories:
                 return len(role.repositories)
             return 0
 
     class UsersColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, role):
             if role.users:
                 return len(role.users)
             return 0
 
     title = "Roles"
     model_class = model.Role
     default_sort_key = "name"
     columns = [
-        NameColumn("Name",
-                   key="name",
-                   link=(lambda item: dict(operation="Manage role associations", id=item.id)),
-                   attach_popup=True,
-                   filterable="advanced"),
-        DescriptionColumn("Description",
-                          key='description',
-                          attach_popup=False,
-                          filterable="advanced"),
+        NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="Manage role associations", id=item.id)),
+            attach_popup=True,
+            filterable="advanced",
+        ),
+        DescriptionColumn("Description", key="description", attach_popup=False, filterable="advanced"),
         GroupsColumn("Groups", attach_popup=False),
         RepositoriesColumn("Repositories", attach_popup=False),
         UsersColumn("Users", attach_popup=False),
         # Columns that are valid for filtering but are not visible.
-        grids.DeletedColumn("Deleted",
-                            key="deleted",
-                            visible=False,
-                            filterable="advanced")
-    ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    global_actions = [
-        grids.GridAction("Add new role",
-                         dict(controller='admin', action='roles', operation='create'))
+        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
     ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search", cols_to_filter=[columns[0]], key="free-text-search", visible=False, filterable="standard"
+        )
+    )
+    global_actions = [grids.GridAction("Add new role", dict(controller="admin", action="roles", operation="create"))]
     # Repository admin roles currently do not have any operations since they are managed automatically based
     # on other events.  For example, if a repository is renamed, its associated admin role is automatically
     # renamed accordingly and if a repository is deleted its associated admin role is automatically deleted.
-    operations = [grids.GridOperation("Rename",
-                                      condition=(lambda item: not item.deleted and not item.is_repository_admin_role),
-                                      allow_multiple=False,
-                                      url_args=dict(action="rename_role")),
-                  grids.GridOperation("Delete",
-                                      condition=(lambda item: not item.deleted and not item.is_repository_admin_role),
-                                      allow_multiple=True,
-                                      url_args=dict(action="mark_role_deleted")),
-                  grids.GridOperation("Undelete",
-                                      condition=(lambda item: item.deleted and not item.is_repository_admin_role),
-                                      allow_multiple=True,
-                                      url_args=dict(action="undelete_role")),
-                  grids.GridOperation("Purge",
-                                      condition=(lambda item: item.deleted and not item.is_repository_admin_role),
-                                      allow_multiple=True,
-                                      url_args=dict(action="purge_role"))]
+    operations = [
+        grids.GridOperation(
+            "Rename",
+            condition=(lambda item: not item.deleted and not item.is_repository_admin_role),
+            allow_multiple=False,
+            url_args=dict(action="rename_role"),
+        ),
+        grids.GridOperation(
+            "Delete",
+            condition=(lambda item: not item.deleted and not item.is_repository_admin_role),
+            allow_multiple=True,
+            url_args=dict(action="mark_role_deleted"),
+        ),
+        grids.GridOperation(
+            "Undelete",
+            condition=(lambda item: item.deleted and not item.is_repository_admin_role),
+            allow_multiple=True,
+            url_args=dict(action="undelete_role"),
+        ),
+        grids.GridOperation(
+            "Purge",
+            condition=(lambda item: item.deleted and not item.is_repository_admin_role),
+            allow_multiple=True,
+            url_args=dict(action="purge_role"),
+        ),
+    ]
     standard_filters = [
         grids.GridColumnFilter("Active", args=dict(deleted=False)),
         grids.GridColumnFilter("Deleted", args=dict(deleted=True)),
-        grids.GridColumnFilter("All", args=dict(deleted='All'))
+        grids.GridColumnFilter("All", args=dict(deleted="All")),
     ]
 
     use_paging = False
 
     def apply_query_filter(self, trans, query, **kwd):
         return query.filter(model.Role.type != model.Role.types.PRIVATE)
 
 
 class GroupGrid(grids.Grid):
-
     class NameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, group):
             return str(group.name)
 
     class StatusColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, group):
             if group.deleted:
                 return "deleted"
             return ""
 
     class RolesColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, group):
             if group.roles:
                 return len(group.roles)
             return 0
 
     class UsersColumn(grids.GridColumn):
-
         def get_value(self, trans, grid, group):
-            if group.members:
-                return len(group.members)
+            if group.users:
+                return len(group.users)
             return 0
 
     title = "Groups"
     model_class = model.Group
     default_sort_key = "name"
     columns = [
-        NameColumn("Name",
-                   key="name",
-                   link=(lambda item: dict(operation="Manage users and roles", id=item.id)),
-                   attach_popup=True),
+        NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="Manage users and roles", id=item.id)),
+            attach_popup=True,
+        ),
         UsersColumn("Users", attach_popup=False),
         RolesColumn("Roles", attach_popup=False),
         StatusColumn("Status", attach_popup=False),
         # Columns that are valid for filtering but are not visible.
-        grids.DeletedColumn("Deleted",
-                            key="deleted",
-                            visible=False,
-                            filterable="advanced")
-    ]
-    columns.append(grids.MulticolFilterColumn("Search",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    global_actions = [
-        grids.GridAction("Add new group",
-                         dict(controller='admin', action='groups', operation='create'))
+        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search", cols_to_filter=[columns[0]], key="free-text-search", visible=False, filterable="standard"
+        )
+    )
+    global_actions = [grids.GridAction("Add new group", dict(controller="admin", action="groups", operation="create"))]
+    operations = [
+        grids.GridOperation(
+            "Rename",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=False,
+            url_args=dict(action="rename_group"),
+        ),
+        grids.GridOperation(
+            "Delete",
+            condition=(lambda item: not item.deleted),
+            allow_multiple=True,
+            url_args=dict(action="mark_group_deleted"),
+        ),
+        grids.GridOperation(
+            "Undelete",
+            condition=(lambda item: item.deleted),
+            allow_multiple=True,
+            url_args=dict(action="undelete_group"),
+        ),
+        grids.GridOperation(
+            "Purge", condition=(lambda item: item.deleted), allow_multiple=True, url_args=dict(action="purge_group")
+        ),
     ]
-    operations = [grids.GridOperation("Rename",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=False,
-                                      url_args=dict(action="rename_group")),
-                  grids.GridOperation("Delete",
-                                      condition=(lambda item: not item.deleted),
-                                      allow_multiple=True,
-                                      url_args=dict(action="mark_group_deleted")),
-                  grids.GridOperation("Undelete",
-                                      condition=(lambda item: item.deleted),
-                                      allow_multiple=True,
-                                      url_args=dict(action="undelete_group")),
-                  grids.GridOperation("Purge",
-                                      condition=(lambda item: item.deleted),
-                                      allow_multiple=True,
-                                      url_args=dict(action="purge_group"))]
     standard_filters = [
         grids.GridColumnFilter("Active", args=dict(deleted=False)),
         grids.GridColumnFilter("Deleted", args=dict(deleted=True)),
-        grids.GridColumnFilter("All", args=dict(deleted='All'))
+        grids.GridColumnFilter("All", args=dict(deleted="All")),
     ]
 
     use_paging = False
 
 
 class ManageCategoryGrid(CategoryGrid):
     columns = [col for col in CategoryGrid.columns]
     # Override the NameColumn to include an Edit link
-    columns[0] = CategoryGrid.NameColumn("Name",
-                                         key="Category.name",
-                                         link=(lambda item: dict(operation="Edit", id=item.id)),
-                                         model_class=model.Category,
-                                         attach_popup=False)
+    columns[0] = CategoryGrid.NameColumn(
+        "Name",
+        key="Category.name",
+        link=(lambda item: dict(operation="Edit", id=item.id)),
+        model_class=model.Category,
+        attach_popup=False,
+    )
     global_actions = [
-        grids.GridAction("Add new category",
-                         dict(controller='admin', action='manage_categories', operation='create'))
+        grids.GridAction("Add new category", dict(controller="admin", action="manage_categories", operation="create"))
     ]
 
 
 class AdminRepositoryGrid(RepositoryGrid):
-
     class DeletedColumn(grids.BooleanColumn):
-
         def get_value(self, trans, grid, repository):
             if repository.deleted:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
-    columns = [RepositoryGrid.NameColumn("Name",
-                                         key="name",
-                                         link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                         attach_popup=True),
-               RepositoryGrid.HeadsColumn("Heads"),
-               RepositoryGrid.UserColumn("Owner",
-                                         model_class=model.User,
-                                         link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
-                                         attach_popup=False,
-                                         key="User.username"),
-               RepositoryGrid.DeprecatedColumn("Deprecated", key="deprecated", attach_popup=False),
-               # Columns that are valid for filtering but are not visible.
-               DeletedColumn("Deleted", key="deleted", attach_popup=False)]
-    columns.append(grids.MulticolFilterColumn("Search repository name",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
+    columns = [
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=True,
+        ),
+        RepositoryGrid.HeadsColumn("Heads"),
+        RepositoryGrid.UserColumn(
+            "Owner",
+            model_class=model.User,
+            link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
+            attach_popup=False,
+            key="User.username",
+        ),
+        RepositoryGrid.DeprecatedColumn("Deprecated", key="deprecated", attach_popup=False),
+        # Columns that are valid for filtering but are not visible.
+        DeletedColumn("Deleted", key="deleted", attach_popup=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name",
+            cols_to_filter=[columns[0]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
     operations = [operation for operation in RepositoryGrid.operations]
-    operations.append(grids.GridOperation("Delete",
-                                          allow_multiple=False,
-                                          condition=(lambda item: not item.deleted),
-                                          async_compatible=False))
-    operations.append(grids.GridOperation("Undelete",
-                                          allow_multiple=False,
-                                          condition=(lambda item: item.deleted),
-                                          async_compatible=False))
-    standard_filters = []
-    default_filter = {}
+    operations.append(
+        grids.GridOperation(
+            "Delete", allow_multiple=False, condition=(lambda item: not item.deleted), async_compatible=False
+        )
+    )
+    operations.append(
+        grids.GridOperation(
+            "Undelete", allow_multiple=False, condition=(lambda item: item.deleted), async_compatible=False
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.Repository) \
-                               .join(model.User.table)
+        return trans.sa_session.query(model.Repository).join(model.User.table)
 
 
 class RepositoryMetadataGrid(grids.Grid):
-
     class IdColumn(grids.IntegerColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             return repository_metadata.id
 
     class NameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             return escape(repository_metadata.repository.name)
 
     class OwnerColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             return escape(repository_metadata.repository.user.username)
 
     class RevisionColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             repository = repository_metadata.repository
-            return hg_util.get_revision_label(trans.app,
-                                              repository,
-                                              repository_metadata.changeset_revision,
-                                              include_date=True,
-                                              include_hash=True)
+            return hg_util.get_revision_label(
+                trans.app, repository, repository_metadata.changeset_revision, include_date=True, include_hash=True
+            )
 
     class ToolsColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
-            tools_str = '0'
+            tools_str = "0"
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    if 'tools' in metadata:
+                    if "tools" in metadata:
                         # We used to display the following, but grid was too cluttered.
                         # for tool_metadata_dict in metadata[ 'tools' ]:
                         #    tools_str += '%s <b>%s</b><br/>' % ( tool_metadata_dict[ 'id' ], tool_metadata_dict[ 'version' ] )
-                        return '%d' % len(metadata['tools'])
+                        return "%d" % len(metadata["tools"])
             return tools_str
 
     class DatatypesColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
-            datatypes_str = '0'
+            datatypes_str = "0"
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    if 'datatypes' in metadata:
+                    if "datatypes" in metadata:
                         # We used to display the following, but grid was too cluttered.
                         # for datatype_metadata_dict in metadata[ 'datatypes' ]:
                         #    datatypes_str += '%s<br/>' % datatype_metadata_dict[ 'extension' ]
-                        return '%d' % len(metadata['datatypes'])
+                        return "%d" % len(metadata["datatypes"])
             return datatypes_str
 
     class WorkflowsColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
-            workflows_str = '0'
+            workflows_str = "0"
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    if 'workflows' in metadata:
+                    if "workflows" in metadata:
                         # We used to display the following, but grid was too cluttered.
                         # workflows_str += '<b>Workflows:</b><br/>'
                         # metadata[ 'workflows' ] is a list of tuples where each contained tuple is
                         # [ <relative path to the .ga file in the repository>, <exported workflow dict> ]
                         # workflow_tups = metadata[ 'workflows' ]
                         # workflow_metadata_dicts = [ workflow_tup[1] for workflow_tup in workflow_tups ]
                         # for workflow_metadata_dict in workflow_metadata_dicts:
                         #    workflows_str += '%s<br/>' % workflow_metadata_dict[ 'name' ]
-                        return '%d' % len(metadata['workflows'])
+                        return "%d" % len(metadata["workflows"])
             return workflows_str
 
     class DeletedColumn(grids.BooleanColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.repository.deleted:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class DeprecatedColumn(grids.BooleanColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.repository.deprecated:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class MaliciousColumn(grids.BooleanColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.malicious:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     # Grid definition
     title = "Repository Metadata"
     model_class = model.RepositoryMetadata
     default_sort_key = "name"
     use_hide_message = False
     columns = [
-        IdColumn("Id",
-                 visible=False,
-                 attach_popup=False),
-        NameColumn("Name",
-                   key="name",
-                   model_class=model.Repository,
-                   link=(lambda item: dict(operation="view_or_manage_repository_revision", id=item.id)),
-                   attach_popup=True),
+        IdColumn("Id", visible=False, attach_popup=False),
+        NameColumn(
+            "Name",
+            key="name",
+            model_class=model.Repository,
+            link=(lambda item: dict(operation="view_or_manage_repository_revision", id=item.id)),
+            attach_popup=True,
+        ),
         OwnerColumn("Owner", attach_popup=False),
         RevisionColumn("Revision", attach_popup=False),
         ToolsColumn("Tools", attach_popup=False),
         DatatypesColumn("Datatypes", attach_popup=False),
         WorkflowsColumn("Workflows", attach_popup=False),
         DeletedColumn("Deleted", attach_popup=False),
         DeprecatedColumn("Deprecated", attach_popup=False),
-        MaliciousColumn("Malicious", attach_popup=False)
+        MaliciousColumn("Malicious", attach_popup=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name",
+            cols_to_filter=[columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
+    operations = [
+        grids.GridOperation(
+            "Delete",
+            allow_multiple=False,
+            allow_popup=True,
+            async_compatible=False,
+            confirm="Repository metadata records cannot be recovered after they are deleted. Click OK to delete the selected items.",
+        )
     ]
-    columns.append(grids.MulticolFilterColumn("Search repository name",
-                                              cols_to_filter=[columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = [grids.GridOperation("Delete",
-                                      allow_multiple=False,
-                                      allow_popup=True,
-                                      async_compatible=False,
-                                      confirm="Repository metadata records cannot be recovered after they are deleted. Click OK to delete the selected items.")]
-    standard_filters = []
-    default_filter = {}
-    use_paging = False
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.RepositoryMetadata) \
-                               .join(model.Repository.table)
+        return trans.sa_session.query(model.RepositoryMetadata).join(model.Repository.table)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/grids/repository_grid_filter_manager.py` & `galaxy-web-apps-23.0.2/tool_shed/grids/repository_grid_filter_manager.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,49 +1,51 @@
 import logging
 
 from galaxy.util.bunch import Bunch
 
 log = logging.getLogger(__name__)
 
 
-class RepositoryGridFilterManager(object):
+class RepositoryGridFilterManager:
     """Provides filtered views of the many Tool SHed repository grids."""
 
-    filters = Bunch(CERTIFIED_LEVEL_ONE='certified_level_one',
-                    CERTIFIED_LEVEL_TWO='certified_level_two',
-                    CERTIFIED_LEVEL_ONE_SUITES='certified_level_one_suites',
-                    CERTIFIED_LEVEL_TWO_SUITES='certified_level_two_suites',
-                    SUITES='suites')
+    filters = Bunch(
+        CERTIFIED_LEVEL_ONE="certified_level_one",
+        CERTIFIED_LEVEL_TWO="certified_level_two",
+        CERTIFIED_LEVEL_ONE_SUITES="certified_level_one_suites",
+        CERTIFIED_LEVEL_TWO_SUITES="certified_level_two_suites",
+        SUITES="suites",
+    )
 
-    def get_grid_title(self, trans, trailing_string='', default=''):
+    def get_grid_title(self, trans, trailing_string="", default=""):
         filter = self.get_filter(trans)
         if filter == self.filters.CERTIFIED_LEVEL_ONE:
-            return "Certified 1 Repositories %s" % trailing_string
+            return f"Certified 1 Repositories {trailing_string}"
         if filter == self.filters.CERTIFIED_LEVEL_TWO:
-            return "Certified 2 Repositories %s" % trailing_string
+            return f"Certified 2 Repositories {trailing_string}"
         if filter == self.filters.CERTIFIED_LEVEL_ONE_SUITES:
-            return "Certified 1 Repository Suites %s" % trailing_string
+            return f"Certified 1 Repository Suites {trailing_string}"
         if filter == self.filters.CERTIFIED_LEVEL_TWO_SUITES:
-            return "Certified 2 Repository Suites %s" % trailing_string
+            return f"Certified 2 Repository Suites {trailing_string}"
         if filter == self.filters.SUITES:
-            return "Repository Suites %s" % trailing_string
-        return "%s %s" % (default, trailing_string)
+            return f"Repository Suites {trailing_string}"
+        return f"{default} {trailing_string}"
 
     def get_filter(self, trans):
-        filter = trans.get_cookie(name='toolshedrepogridfilter')
+        filter = trans.get_cookie(name="toolshedrepogridfilter")
         return filter or None
 
     def is_valid_filter(self, filter):
         if filter is None:
             return True
-        for valid_key, valid_filter in self.filters.items():
+        for valid_filter in self.filters.values():
             if filter == valid_filter:
                 return True
         return False
 
     def set_filter(self, trans, **kwd):
         # Set a session cookie value with the selected filter.
-        filter = kwd.get('filter', None)
+        filter = kwd.get("filter", None)
         if filter is not None and self.is_valid_filter(filter):
-            trans.set_cookie(value=filter, name='toolshedrepogridfilter')
+            trans.set_cookie(value=filter, name="toolshedrepogridfilter")
         # if the filter is not valid, expire the cookie.
-        trans.set_cookie(value=filter, name='toolshedrepogridfilter', age=-1)
+        trans.set_cookie(value=filter, name="toolshedrepogridfilter", age=-1)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/grids/repository_grids.py` & `galaxy-web-apps-23.0.2/tool_shed/grids/repository_grids.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,632 +1,669 @@
 import json
 import logging
 
 from markupsafe import escape as escape_html
-from sqlalchemy import and_, false, or_, true
+from sqlalchemy import (
+    and_,
+    false,
+    or_,
+    true,
+)
 
 import tool_shed.grids.util as grids_util
 import tool_shed.repository_types.util as rt_util
 import tool_shed.util.shed_util_common as suc
-from galaxy.webapps.reports.framework import grids
-from tool_shed.util import hg_util, metadata_util, repository_util
+from galaxy.web.legacy_framework import grids
+from tool_shed.util import (
+    hg_util,
+    metadata_util,
+    repository_util,
+)
 from tool_shed.webapp import model
 
 log = logging.getLogger(__name__)
 
 
 class CategoryGrid(grids.Grid):
-
     class NameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, category):
             return category.name
 
     class DescriptionColumn(grids.TextColumn):
         def get_value(self, trans, grid, category):
             return category.description
 
     class RepositoriesColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, category):
             category_name = str(category.name)
             filter = trans.app.repository_grid_filter_manager.get_filter(trans)
             if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE:
-                return trans.app.repository_registry.certified_level_one_viewable_repositories_and_suites_by_category.get(category_name, 0)
+                return (
+                    trans.app.repository_registry.certified_level_one_viewable_repositories_and_suites_by_category.get(
+                        category_name, 0
+                    )
+                )
             elif filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE_SUITES:
-                return trans.app.repository_registry.certified_level_one_viewable_suites_by_category.get(category_name, 0)
+                return trans.app.repository_registry.certified_level_one_viewable_suites_by_category.get(
+                    category_name, 0
+                )
             elif filter == trans.app.repository_grid_filter_manager.filters.SUITES:
                 return trans.app.repository_registry.viewable_suites_by_category.get(category_name, 0)
             else:
                 # The value filter is None.
                 return trans.app.repository_registry.viewable_repositories_and_suites_by_category.get(category_name, 0)
 
     title = "Categories"
     model_class = model.Category
-    template = '/webapps/tool_shed/category/grid.mako'
+    template = "/webapps/tool_shed/category/grid.mako"
     default_sort_key = "name"
     columns = [
-        NameColumn("Name",
-                   key="Category.name",
-                   link=(lambda item: dict(operation="repositories_by_category", id=item.id)),
-                   attach_popup=False),
-        DescriptionColumn("Description",
-                          key="Category.description",
-                          attach_popup=False),
-        RepositoriesColumn("Repositories",
-                           model_class=model.Repository,
-                           attach_popup=False)
+        NameColumn(
+            "Name",
+            key="Category.name",
+            link=(lambda item: dict(operation="repositories_by_category", id=item.id)),
+            attach_popup=False,
+        ),
+        DescriptionColumn("Description", key="Category.description", attach_popup=False),
+        RepositoriesColumn("Repositories", model_class=model.Repository, attach_popup=False),
     ]
     # Override these
-    default_filter = {}
-    global_actions = []
-    operations = []
-    standard_filters = []
     num_rows_per_page = 50
-    use_paging = False
 
 
 class RepositoryGrid(grids.Grid):
-
     class NameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository):
             return escape_html(repository.name)
 
     class TypeColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository):
             type_class = repository.get_type_class(trans.app)
             return escape_html(type_class.label)
 
     class HeadsColumn(grids.GridColumn):
-
         def __init__(self, col_name):
             grids.GridColumn.__init__(self, col_name)
 
         def get_value(self, trans, grid, repository):
             """Display the current repository heads."""
             repo = repository.hg_repo
             heads = hg_util.get_repository_heads(repo)
             multiple_heads = len(heads) > 1
             if multiple_heads:
                 heads_str = '<font color="red">'
             else:
-                heads_str = ''
+                heads_str = ""
             for ctx in heads:
-                heads_str += '%s<br/>' % hg_util.get_revision_label_from_ctx(ctx, include_date=True)
-            heads_str.rstrip('<br/>')
+                heads_str += f"{hg_util.get_revision_label_from_ctx(ctx, include_date=True)}<br/>"
+            heads_str.rstrip("<br/>")
             if multiple_heads:
-                heads_str += '</font>'
+                heads_str += "</font>"
             return heads_str
 
     class MetadataRevisionColumn(grids.GridColumn):
-
         def __init__(self, col_name):
             grids.GridColumn.__init__(self, col_name)
 
         def get_value(self, trans, grid, repository):
             """Display a SelectField whose options are the changeset_revision strings of all metadata revisions of this repository."""
             # A repository's metadata revisions may not all be installable, as some may contain only invalid tools.
             select_field = grids_util.build_changeset_revision_select_field(trans, repository, downloadable=False)
             if len(select_field.options) > 1:
-                tmpl = "<select name='%s'>" % select_field.name
+                tmpl = f"<select name='{select_field.name}'>"
                 for o in select_field.options:
-                    tmpl += "<option value='%s'>%s</option>" % (o[1], o[0])
+                    tmpl += f"<option value='{o[1]}'>{o[0]}</option>"
                 tmpl += "</select>"
                 return tmpl
             elif len(select_field.options) == 1:
                 option_items = select_field.options[0][0]
-                rev_label, rev_date = option_items.split(' ')
-                rev_date = '<i><font color="#666666">%s</font></i>' % rev_date
-                return '%s %s' % (rev_label, rev_date)
-            return ''
+                rev_label, rev_date = option_items.split(" ")
+                rev_date = f'<i><font color="#666666">{rev_date}</font></i>'
+                return f"{rev_label} {rev_date}"
+            return ""
 
     class LatestInstallableRevisionColumn(grids.GridColumn):
-
         def __init__(self, col_name):
             grids.GridColumn.__init__(self, col_name)
 
         def get_value(self, trans, grid, repository):
             """Display the latest installable revision label (may not be the repository tip)."""
             select_field = grids_util.build_changeset_revision_select_field(trans, repository, downloadable=False)
             if select_field.options:
                 return select_field.options[0][0]
-            return ''
+            return ""
 
     class TipRevisionColumn(grids.GridColumn):
-
         def __init__(self, col_name):
             grids.GridColumn.__init__(self, col_name)
 
         def get_value(self, trans, grid, repository):
             """Display the repository tip revision label."""
             return escape_html(repository.revision())
 
     class DescriptionColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository):
             return escape_html(repository.description)
 
     class CategoryColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository):
-            rval = '<ul>'
+            rval = "<ul>"
             if repository.categories:
                 for rca in repository.categories:
-                    rval += '<li><a href="browse_repositories?operation=repositories_by_category&id=%s">%s</a></li>' \
-                        % (trans.security.encode_id(rca.category.id), rca.category.name)
+                    rval += f'<li><a href="browse_repositories?operation=repositories_by_category&id={trans.security.encode_id(rca.category.id)}">{rca.category.name}</a></li>'
             else:
-                rval += '<li>not set</li>'
-            rval += '</ul>'
+                rval += "<li>not set</li>"
+            rval += "</ul>"
             return rval
 
     class RepositoryCategoryColumn(grids.GridColumn):
-
         def filter(self, trans, user, query, column_filter):
             """Modify query to filter by category."""
             if column_filter == "All":
                 return query
             return query.filter(model.Category.name == column_filter)
 
     class UserColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository):
             if repository.user:
                 return escape_html(repository.user.username)
-            return 'no user'
+            return "no user"
 
     class EmailColumn(grids.TextColumn):
-
         def filter(self, trans, user, query, column_filter):
-            if column_filter == 'All':
+            if column_filter == "All":
                 return query
-            return query.filter(and_(model.Repository.table.c.user_id == model.User.table.c.id,
-                                     model.User.table.c.email == column_filter))
+            return query.filter(
+                and_(
+                    model.Repository.table.c.user_id == model.User.table.c.id, model.User.table.c.email == column_filter
+                )
+            )
 
     class EmailAlertsColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository):
             if trans.user and repository.email_alerts and trans.user.email in json.loads(repository.email_alerts):
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class DeprecatedColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository):
             if repository.deprecated:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     title = "Repositories"
     model_class = model.Repository
     default_sort_key = "name"
     use_hide_message = False
     columns = [
-        NameColumn("Name",
-                   key="name",
-                   link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                   attach_popup=False),
-        DescriptionColumn("Synopsis",
-                          key="description",
-                          attach_popup=False),
+        NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
+        DescriptionColumn("Synopsis", key="description", attach_popup=False),
         TypeColumn("Type"),
         MetadataRevisionColumn("Metadata<br/>Revisions"),
-        UserColumn("Owner",
-                   model_class=model.User,
-                   link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
-                   attach_popup=False,
-                   key="User.username"),
+        UserColumn(
+            "Owner",
+            model_class=model.User,
+            link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
+            attach_popup=False,
+            key="User.username",
+        ),
         # Columns that are valid for filtering but are not visible.
-        EmailColumn("Email",
-                    model_class=model.User,
-                    key="email",
-                    visible=False),
-        RepositoryCategoryColumn("Category",
-                                 model_class=model.Category,
-                                 key="Category.name",
-                                 visible=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name, description",
-                                              cols_to_filter=[columns[0], columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = []
-    standard_filters = []
+        EmailColumn("Email", model_class=model.User, key="email", visible=False),
+        RepositoryCategoryColumn("Category", model_class=model.Category, key="Category.name", visible=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name, description",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
     default_filter = dict(deleted="False")
     num_rows_per_page = 50
-    use_paging = False
+    use_paging = True
+    allow_fetching_all_results = False
 
     def build_initial_query(self, trans, **kwd):
         filter = trans.app.repository_grid_filter_manager.get_filter(trans)
         if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE:
-            return trans.sa_session.query(model.Repository) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+            return (
+                trans.sa_session.query(model.Repository)
+                .join(model.RepositoryMetadata.table)
+                .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
         if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE_SUITES:
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION)
+                .join(model.RepositoryMetadata.table)
+                .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
         else:
             # The filter is None.
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(and_(model.Repository.table.c.deleted == false(),
-                                                model.Repository.table.c.deprecated == false())) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(model.Repository.table.c.deleted == false(), model.Repository.table.c.deprecated == false())
+                )
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
 
 
 class DockerImageGrid(RepositoryGrid):
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
-        RepositoryGrid.DescriptionColumn("Synopsis",
-                                         key="description",
-                                         attach_popup=False),
-        RepositoryGrid.UserColumn("Owner",
-                                  model_class=model.User,
-                                  link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
-                                  attach_popup=False,
-                                  key="User.username"),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
+        RepositoryGrid.DescriptionColumn("Synopsis", key="description", attach_popup=False),
+        RepositoryGrid.UserColumn(
+            "Owner",
+            model_class=model.User,
+            link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
+            attach_popup=False,
+            key="User.username",
+        ),
         RepositoryGrid.EmailAlertsColumn("Alert", attach_popup=False),
     ]
     operations = [grids.GridOperation("Include in Docker image", allow_multiple=True)]
     show_item_checkboxes = True
 
 
 class EmailAlertsRepositoryGrid(RepositoryGrid):
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
-        RepositoryGrid.DescriptionColumn("Synopsis",
-                                         key="description",
-                                         attach_popup=False),
-        RepositoryGrid.UserColumn("Owner",
-                                  model_class=model.User,
-                                  link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
-                                  attach_popup=False,
-                                  key="User.username"),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
+        RepositoryGrid.DescriptionColumn("Synopsis", key="description", attach_popup=False),
+        RepositoryGrid.UserColumn(
+            "Owner",
+            model_class=model.User,
+            link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
+            attach_popup=False,
+            key="User.username",
+        ),
         RepositoryGrid.EmailAlertsColumn("Alert", attach_popup=False),
         # Columns that are valid for filtering but are not visible.
-        grids.DeletedColumn("Deleted",
-                            key="deleted",
-                            visible=False,
-                            filterable="advanced")
+        grids.DeletedColumn("Deleted", key="deleted", visible=False, filterable="advanced"),
     ]
     operations = [grids.GridOperation("Receive email alerts", allow_multiple=True)]
     global_actions = [
-        grids.GridAction("User preferences", dict(controller='user', action='index', cntrller='repository'))
+        grids.GridAction("User preferences", dict(controller="user", action="index", cntrller="repository"))
     ]
 
 
 class MatchedRepositoryGrid(grids.Grid):
     # This grid filters out repositories that have been marked as deleted or deprecated.
 
     class NameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             return escape_html(repository_metadata.repository.name)
 
     class DescriptionColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             return escape_html(repository_metadata.repository.description)
 
     class RevisionColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             return repository_metadata.changeset_revision
 
     class UserColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.repository.user:
                 return escape_html(repository_metadata.repository.user.username)
-            return 'no user'
+            return "no user"
 
     # Grid definition
     title = "Matching repositories"
     model_class = model.RepositoryMetadata
     default_sort_key = "Repository.name"
     use_hide_message = False
     columns = [
-        NameColumn("Repository name",
-                   link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                   attach_popup=True),
-        DescriptionColumn("Synopsis",
-                          attach_popup=False),
+        NameColumn(
+            "Repository name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=True,
+        ),
+        DescriptionColumn("Synopsis", attach_popup=False),
         RevisionColumn("Revision"),
-        UserColumn("Owner",
-                   model_class=model.User,
-                   attach_popup=False)
+        UserColumn("Owner", model_class=model.User, attach_popup=False),
     ]
     operations = [grids.GridOperation("Install to Galaxy", allow_multiple=True)]
-    standard_filters = []
-    default_filter = {}
     num_rows_per_page = 50
     use_paging = False
 
     def build_initial_query(self, trans, **kwd):
-        match_tuples = kwd.get('match_tuples', [])
+        match_tuples = kwd.get("match_tuples", [])
         clause_list = []
         if match_tuples:
             for match_tuple in match_tuples:
                 repository_id, changeset_revision = match_tuple
-                clause_list.append(and_(
-                    model.RepositoryMetadata.repository_id == int(repository_id),
-                    model.RepositoryMetadata.changeset_revision == changeset_revision))
-            return trans.sa_session.query(model.RepositoryMetadata) \
-                                   .join(model.Repository) \
-                                   .filter(and_(model.Repository.table.c.deleted == false(),
-                                                model.Repository.table.c.deprecated == false())) \
-                                   .join(model.User.table) \
-                                   .filter(or_(*clause_list)) \
-                                   .order_by(model.Repository.name)
+                clause_list.append(
+                    and_(
+                        model.RepositoryMetadata.repository_id == int(repository_id),
+                        model.RepositoryMetadata.changeset_revision == changeset_revision,
+                    )
+                )
+            return (
+                trans.sa_session.query(model.RepositoryMetadata)
+                .join(model.Repository)
+                .filter(
+                    and_(model.Repository.table.c.deleted == false(), model.Repository.table.c.deprecated == false())
+                )
+                .join(model.User.table)
+                .filter(or_(*clause_list))
+                .order_by(model.Repository.name)
+            )
         # Return an empty query
-        return trans.sa_session.query(model.RepositoryMetadata) \
-                               .filter(model.RepositoryMetadata.id < 0)
+        return trans.sa_session.query(model.RepositoryMetadata).filter(model.RepositoryMetadata.id < 0)
 
 
 class InstallMatchedRepositoryGrid(MatchedRepositoryGrid):
     columns = [col for col in MatchedRepositoryGrid.columns]
     # Override the NameColumn
-    columns[0] = MatchedRepositoryGrid.NameColumn("Name",
-                                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                                  attach_popup=False)
+    columns[0] = MatchedRepositoryGrid.NameColumn(
+        "Name", link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)), attach_popup=False
+    )
 
 
 class MyWritableRepositoriesGrid(RepositoryGrid):
     # This grid filters out repositories that have been marked as either deprecated or deleted.
-    title = 'Repositories I can change'
+    title = "Repositories I can change"
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
         RepositoryGrid.TypeColumn("Type"),
         RepositoryGrid.MetadataRevisionColumn("Metadata<br/>Revisions"),
-        RepositoryGrid.UserColumn("Owner",
-                                  model_class=model.User,
-                                  link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
-                                  attach_popup=False,
-                                  key="User.username")
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = []
-    use_paging = False
+        RepositoryGrid.UserColumn(
+            "Owner",
+            model_class=model.User,
+            link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
+            attach_popup=False,
+            key="User.username",
+        ),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name",
+            cols_to_filter=[columns[0]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
         # TODO: improve performance by adding a db table associating users with repositories for which they have write access.
         username = trans.user.username
         clause_list = []
-        for repository in trans.sa_session.query(model.Repository) \
-                                          .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                       model.Repository.table.c.deleted == false())):
+        for repository in trans.sa_session.query(model.Repository).filter(
+            and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+        ):
             allow_push = repository.allow_push()
             if allow_push:
-                allow_push_usernames = allow_push.split(',')
+                allow_push_usernames = allow_push.split(",")
                 if username in allow_push_usernames:
                     clause_list.append(model.Repository.table.c.id == repository.id)
         if clause_list:
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(or_(*clause_list)) \
-                                   .join(model.User.table)
+            return trans.sa_session.query(model.Repository).filter(or_(*clause_list)).join(model.User.table)
         # Return an empty query.
-        return trans.sa_session.query(model.Repository) \
-                               .filter(model.Repository.table.c.id < 0)
+        return trans.sa_session.query(model.Repository).filter(model.Repository.table.c.id < 0)
 
 
 class RepositoriesByUserGrid(RepositoryGrid):
     title = "Repositories by user"
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
-        RepositoryGrid.DescriptionColumn("Synopsis",
-                                         key="description",
-                                         attach_popup=False),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
+        RepositoryGrid.DescriptionColumn("Synopsis", key="description", attach_popup=False),
         RepositoryGrid.TypeColumn("Type"),
         RepositoryGrid.MetadataRevisionColumn("Metadata<br/>Revisions"),
-        RepositoryGrid.CategoryColumn("Category",
-                                      model_class=model.Category,
-                                      key="Category.name",
-                                      attach_popup=False)
+        RepositoryGrid.CategoryColumn("Category", model_class=model.Category, key="Category.name", attach_popup=False),
     ]
-    operations = []
-    standard_filters = []
     default_filter = dict(deleted="False")
-    num_rows_per_page = 50
-    use_paging = False
 
     def build_initial_query(self, trans, **kwd):
-        decoded_user_id = trans.security.decode_id(kwd['user_id'])
+        decoded_user_id = trans.security.decode_id(kwd["user_id"])
         filter = trans.app.repository_grid_filter_manager.get_filter(trans)
         if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE:
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(model.Repository.table.c.user_id == decoded_user_id) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(model.Repository.table.c.user_id == decoded_user_id)
+                .join(model.RepositoryMetadata.table)
+                .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
         if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE_SUITES:
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(and_(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION,
-                                                model.Repository.table.c.user_id == decoded_user_id)) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(
+                        model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION,
+                        model.Repository.table.c.user_id == decoded_user_id,
+                    )
+                )
+                .join(model.RepositoryMetadata.table)
+                .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
         else:
             # The value of filter is None.
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(and_(model.Repository.table.c.deleted == false(),
-                                                model.Repository.table.c.deprecated == false(),
-                                                model.Repository.table.c.user_id == decoded_user_id)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(
+                        model.Repository.table.c.deleted == false(),
+                        model.Repository.table.c.deprecated == false(),
+                        model.Repository.table.c.user_id == decoded_user_id,
+                    )
+                )
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
 
 
 class RepositoriesInCategoryGrid(RepositoryGrid):
     title = "Category"
 
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(controller="repository", operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
-        RepositoryGrid.DescriptionColumn("Synopsis",
-                                         key="description",
-                                         attach_popup=False),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(controller="repository", operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
+        RepositoryGrid.DescriptionColumn("Synopsis", key="description", attach_popup=False),
         RepositoryGrid.TypeColumn("Type"),
         RepositoryGrid.MetadataRevisionColumn("Metadata<br/>Revisions"),
-        RepositoryGrid.UserColumn("Owner",
-                                  model_class=model.User,
-                                  link=(lambda item: dict(controller="repository", operation="repositories_by_user", id=item.id)),
-                                  attach_popup=False,
-                                  key="User.username"),
+        RepositoryGrid.UserColumn(
+            "Owner",
+            model_class=model.User,
+            link=(lambda item: dict(controller="repository", operation="repositories_by_user", id=item.id)),
+            attach_popup=False,
+            key="User.username",
+        ),
         # Columns that are valid for filtering but are not visible.
-        RepositoryGrid.EmailColumn("Email",
-                                   model_class=model.User,
-                                   key="email",
-                                   visible=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name, description",
-                                              cols_to_filter=[columns[0], columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = []
-    use_paging = False
+        RepositoryGrid.EmailColumn("Email", model_class=model.User, key="email", visible=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name, description",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
-        category_id = kwd.get('id', None)
+        category_id = kwd.get("id", None)
         filter = trans.app.repository_grid_filter_manager.get_filter(trans)
         if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE:
             if category_id:
                 category = suc.get_category(trans.app, category_id)
                 if category:
-                    return trans.sa_session.query(model.Repository) \
-                                           .join(model.RepositoryMetadata.table) \
-                                           .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                           .join(model.User.table) \
-                                           .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                           .outerjoin(model.Category.table) \
-                                           .filter(model.Category.table.c.name == category.name)
-            return trans.sa_session.query(model.Repository) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+                    return (
+                        trans.sa_session.query(model.Repository)
+                        .join(model.RepositoryMetadata.table)
+                        .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                        .join(model.User.table)
+                        .outerjoin(model.RepositoryCategoryAssociation.table)
+                        .outerjoin(model.Category.table)
+                        .filter(model.Category.table.c.name == category.name)
+                    )
+            return (
+                trans.sa_session.query(model.Repository)
+                .join(model.RepositoryMetadata.table)
+                .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
         if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE_SUITES:
             if category_id:
                 category = suc.get_category(trans.app, category_id)
                 if category:
-                    return trans.sa_session.query(model.Repository) \
-                                           .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION) \
-                                           .join(model.RepositoryMetadata.table) \
-                                           .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                           .join(model.User.table) \
-                                           .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                           .outerjoin(model.Category.table) \
-                                           .filter(model.Category.table.c.name == category.name)
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+                    return (
+                        trans.sa_session.query(model.Repository)
+                        .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION)
+                        .join(model.RepositoryMetadata.table)
+                        .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                        .join(model.User.table)
+                        .outerjoin(model.RepositoryCategoryAssociation.table)
+                        .outerjoin(model.Category.table)
+                        .filter(model.Category.table.c.name == category.name)
+                    )
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION)
+                .join(model.RepositoryMetadata.table)
+                .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
         else:
             # The value of filter is None.
             if category_id:
                 category = suc.get_category(trans.app, category_id)
                 if category:
-                    return trans.sa_session.query(model.Repository) \
-                                           .filter(and_(model.Repository.table.c.deleted == false(),
-                                                        model.Repository.table.c.deprecated == false())) \
-                                           .join(model.User.table) \
-                                           .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                           .outerjoin(model.Category.table) \
-                                           .filter(model.Category.table.c.name == category.name)
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(and_(model.Repository.table.c.deleted == false(),
-                                                model.Repository.table.c.deprecated == false())) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table)
+                    return (
+                        trans.sa_session.query(model.Repository)
+                        .filter(
+                            and_(
+                                model.Repository.table.c.deleted == false(),
+                                model.Repository.table.c.deprecated == false(),
+                            )
+                        )
+                        .join(model.User.table)
+                        .outerjoin(model.RepositoryCategoryAssociation.table)
+                        .outerjoin(model.Category.table)
+                        .filter(model.Category.table.c.name == category.name)
+                    )
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(model.Repository.table.c.deleted == false(), model.Repository.table.c.deprecated == false())
+                )
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            )
 
 
 class RepositoriesIOwnGrid(RepositoryGrid):
     title = "Repositories I own"
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
         RepositoryGrid.TypeColumn("Type"),
         RepositoryGrid.MetadataRevisionColumn("Metadata<br/>Revisions"),
-        RepositoryGrid.DeprecatedColumn("Deprecated")
+        RepositoryGrid.DeprecatedColumn("Deprecated"),
     ]
-    columns.append(grids.MulticolFilterColumn("Search repository name",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = []
-    use_paging = False
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name",
+            cols_to_filter=[columns[0]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.Repository) \
-                               .filter(and_(model.Repository.table.c.deleted == false(),
-                                            model.Repository.table.c.user_id == trans.user.id)) \
-                               .join(model.User.table) \
-                               .outerjoin(model.RepositoryCategoryAssociation.table) \
-                               .outerjoin(model.Category.table)
+        return (
+            trans.sa_session.query(model.Repository)
+            .filter(
+                and_(model.Repository.table.c.deleted == false(), model.Repository.table.c.user_id == trans.user.id)
+            )
+            .join(model.User.table)
+            .outerjoin(model.RepositoryCategoryAssociation.table)
+            .outerjoin(model.Category.table)
+        )
 
 
 class RepositoriesICanAdministerGrid(RepositoryGrid):
     title = "Repositories I can administer"
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
         RepositoryGrid.UserColumn("Owner"),
         RepositoryGrid.MetadataRevisionColumn("Metadata<br/>Revisions"),
-        RepositoryGrid.DeprecatedColumn("Deprecated")
+        RepositoryGrid.DeprecatedColumn("Deprecated"),
     ]
-    columns.append(grids.MulticolFilterColumn("Search repository name",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = []
-    use_paging = False
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name",
+            cols_to_filter=[columns[0]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
         """
         Retrieve all repositories for which the current user has been granted administrative privileges.
         """
         current_user = trans.user
         # Build up an or-based clause list containing role table records.
@@ -637,787 +674,894 @@
         # Include each role associated with each group of which the user is a member.
         for uga in current_user.groups:
             group = uga.group
             for gra in group.roles:
                 clause_list.append(model.Role.table.c.id == gra.role_id)
         # Filter out repositories for which the user does not have the administrative role either directly
         # via a role association or indirectly via a group -> role association.
-        return trans.sa_session.query(model.Repository) \
-                               .filter(model.Repository.table.c.deleted == false()) \
-                               .outerjoin(model.RepositoryRoleAssociation.table) \
-                               .outerjoin(model.Role.table) \
-                               .filter(or_(*clause_list)) \
-                               .join(model.User.table) \
-                               .outerjoin(model.RepositoryCategoryAssociation.table) \
-                               .outerjoin(model.Category.table)
+        return (
+            trans.sa_session.query(model.Repository)
+            .filter(model.Repository.table.c.deleted == false())
+            .outerjoin(model.RepositoryRoleAssociation.table)
+            .outerjoin(model.Role.table)
+            .filter(or_(*clause_list))
+            .join(model.User.table)
+            .outerjoin(model.RepositoryCategoryAssociation.table)
+            .outerjoin(model.Category.table)
+        )
 
 
 class RepositoriesMissingToolTestComponentsGrid(RepositoryGrid):
     # This grid displays only the latest installable revision of each repository.
     title = "Repositories with missing tool test components"
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
         RepositoryGrid.LatestInstallableRevisionColumn("Latest Installable Revision"),
-        RepositoryGrid.UserColumn("Owner",
-                                  key="User.username",
-                                  model_class=model.User,
-                                  link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
-                                  attach_popup=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = []
-    use_paging = False
+        RepositoryGrid.UserColumn(
+            "Owner",
+            key="User.username",
+            model_class=model.User,
+            link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
+            attach_popup=False,
+        ),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name",
+            cols_to_filter=[columns[0]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
         # Filter by latest installable revisions that contain tools with missing tool test components.
         revision_clause_list = []
-        for repository in trans.sa_session.query(model.Repository) \
-                                          .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                       model.Repository.table.c.deleted == false())):
-            changeset_revision = \
-                grids_util.filter_by_latest_downloadable_changeset_revision_that_has_missing_tool_test_components(trans, repository)
+        for repository in trans.sa_session.query(model.Repository).filter(
+            and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+        ):
+            changeset_revision = (
+                grids_util.filter_by_latest_downloadable_changeset_revision_that_has_missing_tool_test_components(
+                    trans, repository
+                )
+            )
             if changeset_revision:
                 revision_clause_list.append(model.RepositoryMetadata.table.c.changeset_revision == changeset_revision)
         if revision_clause_list:
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                model.Repository.table.c.deleted == false())) \
-                                   .join(model.RepositoryMetadata) \
-                                   .filter(or_(*revision_clause_list)) \
-                                   .join(model.User.table)
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+                )
+                .join(model.RepositoryMetadata)
+                .filter(or_(*revision_clause_list))
+                .join(model.User.table)
+            )
         # Return an empty query.
-        return trans.sa_session.query(model.Repository) \
-                               .filter(model.Repository.table.c.id < 0)
+        return trans.sa_session.query(model.Repository).filter(model.Repository.table.c.id < 0)
 
 
 class MyWritableRepositoriesMissingToolTestComponentsGrid(RepositoriesMissingToolTestComponentsGrid):
     # This grid displays only the latest installable revision of each repository.
     title = "Repositories I can change with missing tool test components"
     columns = [col for col in RepositoriesMissingToolTestComponentsGrid.columns]
-    operations = []
-    use_paging = False
 
     def build_initial_query(self, trans, **kwd):
         # First get all repositories that the current user is authorized to update.
         username = trans.user.username
         user_clause_list = []
-        for repository in trans.sa_session.query(model.Repository) \
-                                          .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                       model.Repository.table.c.deleted == false())):
+        for repository in trans.sa_session.query(model.Repository).filter(
+            and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+        ):
             allow_push = repository.allow_push()
             if allow_push:
-                allow_push_usernames = allow_push.split(',')
+                allow_push_usernames = allow_push.split(",")
                 if username in allow_push_usernames:
                     user_clause_list.append(model.Repository.table.c.id == repository.id)
         if user_clause_list:
             # We have the list of repositories that the current user is authorized to update, so filter
             # further by latest installable revisions that contain tools with missing tool test components.
             revision_clause_list = []
-            for repository in trans.sa_session.query(model.Repository) \
-                                              .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                           model.Repository.table.c.deleted == false())) \
-                                              .filter(or_(*user_clause_list)):
-                changeset_revision = \
-                    grids_util.filter_by_latest_downloadable_changeset_revision_that_has_missing_tool_test_components(trans, repository)
+            for repository in (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+                )
+                .filter(or_(*user_clause_list))
+            ):
+                changeset_revision = (
+                    grids_util.filter_by_latest_downloadable_changeset_revision_that_has_missing_tool_test_components(
+                        trans, repository
+                    )
+                )
                 if changeset_revision:
-                    revision_clause_list.append(model.RepositoryMetadata.table.c.changeset_revision == changeset_revision)
+                    revision_clause_list.append(
+                        model.RepositoryMetadata.table.c.changeset_revision == changeset_revision
+                    )
             if revision_clause_list:
-                return trans.sa_session.query(model.Repository) \
-                                       .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                    model.Repository.table.c.deleted == false())) \
-                                       .join(model.User.table) \
-                                       .filter(or_(*user_clause_list)) \
-                                       .join(model.RepositoryMetadata) \
-                                       .filter(or_(*revision_clause_list))
+                return (
+                    trans.sa_session.query(model.Repository)
+                    .filter(
+                        and_(
+                            model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false()
+                        )
+                    )
+                    .join(model.User.table)
+                    .filter(or_(*user_clause_list))
+                    .join(model.RepositoryMetadata)
+                    .filter(or_(*revision_clause_list))
+                )
         # Return an empty query.
-        return trans.sa_session.query(model.Repository) \
-                               .filter(model.Repository.table.c.id < 0)
+        return trans.sa_session.query(model.Repository).filter(model.Repository.table.c.id < 0)
 
 
 class DeprecatedRepositoriesIOwnGrid(RepositoriesIOwnGrid):
     title = "Deprecated repositories I own"
     columns = [
-        RepositoriesIOwnGrid.NameColumn("Name",
-                                        key="name",
-                                        link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                        attach_popup=False),
+        RepositoriesIOwnGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
         RepositoryGrid.TypeColumn("Type"),
         RepositoriesIOwnGrid.MetadataRevisionColumn("Metadata<br/>Revisions"),
-        RepositoriesIOwnGrid.CategoryColumn("Category",
-                                            model_class=model.Category,
-                                            key="Category.name",
-                                            attach_popup=False),
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name",
-                                              cols_to_filter=[columns[0]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    use_paging = False
+        RepositoriesIOwnGrid.CategoryColumn(
+            "Category", model_class=model.Category, key="Category.name", attach_popup=False
+        ),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name",
+            cols_to_filter=[columns[0]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.Repository) \
-                               .filter(and_(model.Repository.table.c.deleted == false(),
-                                            model.Repository.table.c.user_id == trans.user.id,
-                                            model.Repository.table.c.deprecated == true())) \
-                               .join(model.User.table) \
-                               .outerjoin(model.RepositoryCategoryAssociation.table) \
-                               .outerjoin(model.Category.table)
+        return (
+            trans.sa_session.query(model.Repository)
+            .filter(
+                and_(
+                    model.Repository.table.c.deleted == false(),
+                    model.Repository.table.c.user_id == trans.user.id,
+                    model.Repository.table.c.deprecated == true(),
+                )
+            )
+            .join(model.User.table)
+            .outerjoin(model.RepositoryCategoryAssociation.table)
+            .outerjoin(model.Category.table)
+        )
 
 
 class RepositoriesWithInvalidToolsGrid(RepositoryGrid):
     # This grid displays only the latest installable revision of each repository.
 
     class InvalidToolConfigColumn(grids.GridColumn):
-
         def __init__(self, col_name):
             grids.GridColumn.__init__(self, col_name)
 
         def get_value(self, trans, grid, repository):
             # At the time this grid is displayed we know that the received repository will have invalid tools in its latest changeset revision
             # that has associated metadata.
-            val = ''
-            repository_metadata = \
-                grids_util.get_latest_repository_metadata_if_it_includes_invalid_tools(trans, repository)
+            val = ""
+            repository_metadata = grids_util.get_latest_repository_metadata_if_it_includes_invalid_tools(
+                trans, repository
+            )
             metadata = repository_metadata.metadata
-            invalid_tools = metadata.get('invalid_tools', [])
+            invalid_tools = metadata.get("invalid_tools", [])
             if invalid_tools:
                 for invalid_tool_config in invalid_tools:
-                    href_str = '<a href="load_invalid_tool?repository_id=%s&tool_config=%s&changeset_revision=%s">%s</a>' % \
-                        (trans.security.encode_id(repository.id), invalid_tool_config, repository_metadata.changeset_revision, invalid_tool_config)
+                    href_str = (
+                        '<a href="load_invalid_tool?repository_id=%s&tool_config=%s&changeset_revision=%s">%s</a>'
+                        % (
+                            trans.security.encode_id(repository.id),
+                            invalid_tool_config,
+                            repository_metadata.changeset_revision,
+                            invalid_tool_config,
+                        )
+                    )
                     val += href_str
-                    val += '<br/>'
-                val = val.rstrip('<br/>')
+                    val += "<br/>"
+                val = val.rstrip("<br/>")
             return val
 
     title = "Repositories with invalid tools"
     columns = [
         InvalidToolConfigColumn("Tool config"),
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                  attach_popup=False),
+        RepositoryGrid.NameColumn(
+            "Name",
+            key="name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
         RepositoryGrid.LatestInstallableRevisionColumn("Latest Metadata Revision"),
-        RepositoryGrid.UserColumn("Owner",
-                                  key="User.username",
-                                  model_class=model.User,
-                                  link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
-                                  attach_popup=False)
+        RepositoryGrid.UserColumn(
+            "Owner",
+            key="User.username",
+            model_class=model.User,
+            link=(lambda item: dict(operation="repositories_by_user", id=item.id)),
+            attach_popup=False,
+        ),
     ]
-    operations = []
-    use_paging = False
 
     def build_initial_query(self, trans, **kwd):
         # Filter by latest metadata revisions that contain invalid tools.
         revision_clause_list = []
-        for repository in trans.sa_session.query(model.Repository) \
-                                          .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                       model.Repository.table.c.deleted == false())):
-            changeset_revision = \
-                grids_util.filter_by_latest_metadata_changeset_revision_that_has_invalid_tools(trans, repository)
+        for repository in trans.sa_session.query(model.Repository).filter(
+            and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+        ):
+            changeset_revision = grids_util.filter_by_latest_metadata_changeset_revision_that_has_invalid_tools(
+                trans, repository
+            )
             if changeset_revision:
                 revision_clause_list.append(model.RepositoryMetadata.table.c.changeset_revision == changeset_revision)
         if revision_clause_list:
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                model.Repository.table.c.deleted == false())) \
-                                   .join(model.RepositoryMetadata) \
-                                   .filter(or_(*revision_clause_list)) \
-                                   .join(model.User.table)
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+                )
+                .join(model.RepositoryMetadata)
+                .filter(or_(*revision_clause_list))
+                .join(model.User.table)
+            )
         # Return an empty query.
-        return trans.sa_session.query(model.Repository) \
-                               .filter(model.Repository.table.c.id < 0)
+        return trans.sa_session.query(model.Repository).filter(model.Repository.table.c.id < 0)
 
 
 class MyWritableRepositoriesWithInvalidToolsGrid(RepositoriesWithInvalidToolsGrid):
     # This grid displays only the latest installable revision of each repository.
     title = "Repositories I can change with invalid tools"
     columns = [col for col in RepositoriesWithInvalidToolsGrid.columns]
-    operations = []
-    use_paging = False
 
     def build_initial_query(self, trans, **kwd):
         # First get all repositories that the current user is authorized to update.
         username = trans.user.username
         user_clause_list = []
-        for repository in trans.sa_session.query(model.Repository) \
-                                          .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                       model.Repository.table.c.deleted == false())):
+        for repository in trans.sa_session.query(model.Repository).filter(
+            and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+        ):
             allow_push = repository.allow_push()
             if allow_push:
-                allow_push_usernames = allow_push.split(',')
+                allow_push_usernames = allow_push.split(",")
                 if username in allow_push_usernames:
                     user_clause_list.append(model.Repository.table.c.id == repository.id)
         if user_clause_list:
             # We have the list of repositories that the current user is authorized to update, so filter
             # further by latest metadata revisions that contain invalid tools.
             revision_clause_list = []
-            for repository in trans.sa_session.query(model.Repository) \
-                                              .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                           model.Repository.table.c.deleted == false())) \
-                                              .filter(or_(*user_clause_list)):
-                changeset_revision = \
-                    grids_util.filter_by_latest_metadata_changeset_revision_that_has_invalid_tools(trans, repository)
+            for repository in (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false())
+                )
+                .filter(or_(*user_clause_list))
+            ):
+                changeset_revision = grids_util.filter_by_latest_metadata_changeset_revision_that_has_invalid_tools(
+                    trans, repository
+                )
                 if changeset_revision:
-                    revision_clause_list.append(model.RepositoryMetadata.table.c.changeset_revision == changeset_revision)
+                    revision_clause_list.append(
+                        model.RepositoryMetadata.table.c.changeset_revision == changeset_revision
+                    )
             if revision_clause_list:
-                return trans.sa_session.query(model.Repository) \
-                                       .filter(and_(model.Repository.table.c.deprecated == false(),
-                                                    model.Repository.table.c.deleted == false())) \
-                                       .join(model.User.table) \
-                                       .filter(or_(*user_clause_list)) \
-                                       .join(model.RepositoryMetadata) \
-                                       .filter(or_(*revision_clause_list))
+                return (
+                    trans.sa_session.query(model.Repository)
+                    .filter(
+                        and_(
+                            model.Repository.table.c.deprecated == false(), model.Repository.table.c.deleted == false()
+                        )
+                    )
+                    .join(model.User.table)
+                    .filter(or_(*user_clause_list))
+                    .join(model.RepositoryMetadata)
+                    .filter(or_(*revision_clause_list))
+                )
         # Return an empty query.
-        return trans.sa_session.query(model.Repository) \
-                               .filter(model.Repository.table.c.id < 0)
+        return trans.sa_session.query(model.Repository).filter(model.Repository.table.c.id < 0)
 
 
 class RepositoryMetadataGrid(grids.Grid):
-
     class RepositoryNameColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             repository = repository_metadata.repository
             return escape_html(repository.name)
 
     class RepositoryTypeColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             repository = repository_metadata.repository
             type_class = repository.get_type_class(trans.app)
             return escape_html(type_class.label)
 
     class RepositoryOwnerColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             repository = repository_metadata.repository
             return escape_html(repository.user.username)
 
     class ChangesetRevisionColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             repository = repository_metadata.repository
             changeset_revision = repository_metadata.changeset_revision
-            changeset_revision_label = hg_util.get_revision_label(trans.app, repository, changeset_revision, include_date=True)
+            changeset_revision_label = hg_util.get_revision_label(
+                trans.app, repository, changeset_revision, include_date=True
+            )
             return changeset_revision_label
 
     class MaliciousColumn(grids.BooleanColumn):
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.malicious:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class DownloadableColumn(grids.BooleanColumn):
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.downloadable:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class HasRepositoryDependenciesColumn(grids.BooleanColumn):
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.has_repository_dependencies:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class IncludesDatatypesColumn(grids.BooleanColumn):
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.includes_datatypes:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class IncludesToolsColumn(grids.BooleanColumn):
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.includes_tools:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class IncludesToolDependenciesColumn(grids.BooleanColumn):
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.includes_tool_dependencies:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     class IncludesWorkflowsColumn(grids.BooleanColumn):
         def get_value(self, trans, grid, repository_metadata):
             if repository_metadata.includes_workflows:
-                return 'yes'
-            return ''
+                return "yes"
+            return ""
 
     title = "Repository metadata"
     model_class = model.RepositoryMetadata
     default_sort_key = "Repository.name"
     columns = [
-        RepositoryNameColumn("Repository name",
-                             key="Repository.name",
-                             link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                             attach_popup=False),
+        RepositoryNameColumn(
+            "Repository name",
+            key="Repository.name",
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+        ),
         RepositoryNameColumn("Type"),
-        RepositoryOwnerColumn("Owner",
-                              model_class=model.User,
-                              attach_popup=False,
-                              key="User.username")
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name, description",
-                                              cols_to_filter=[columns[0], columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = []
-    standard_filters = []
+        RepositoryOwnerColumn("Owner", model_class=model.User, attach_popup=False, key="User.username"),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name, description",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
     default_filter = dict(malicious="False")
     num_rows_per_page = 50
-    use_paging = False
+    use_paging = True
+    allow_fetching_all_results = False
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.RepositoryMetadata) \
-                               .join(model.Repository) \
-                               .filter(and_(model.Repository.table.c.deleted == false(),
-                                            model.Repository.table.c.deprecated == false())) \
-                               .join(model.User.table)
+        return (
+            trans.sa_session.query(model.RepositoryMetadata)
+            .join(model.Repository)
+            .filter(and_(model.Repository.table.c.deleted == false(), model.Repository.table.c.deprecated == false()))
+            .join(model.User.table)
+        )
 
 
 class RepositoryDependenciesGrid(RepositoryMetadataGrid):
-
     class RequiredRepositoryColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             rd_str = []
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    rd_dict = metadata.get('repository_dependencies', {})
+                    rd_dict = metadata.get("repository_dependencies", {})
                     if rd_dict:
-                        rd_tups = rd_dict['repository_dependencies']
+                        rd_tups = rd_dict["repository_dependencies"]
                         # "repository_dependencies": [["http://localhost:9009", "bwa059", "test", "a07baa797d53"]]
                         # Sort rd_tups by by required repository name.
                         sorted_rd_tups = sorted(rd_tups, key=lambda rd_tup: rd_tup[1])
                         for rd_tup in sorted_rd_tups:
                             name, owner, changeset_revision = rd_tup[1:4]
-                            rd_line = ''
-                            required_repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
+                            rd_line = ""
+                            required_repository = repository_util.get_repository_by_name_and_owner(
+                                trans.app, name, owner
+                            )
                             if required_repository and not required_repository.deleted:
                                 required_repository_id = trans.security.encode_id(required_repository.id)
-                                required_repository_metadata = \
-                                    metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                                              required_repository_id,
-                                                                                                              changeset_revision)
+                                required_repository_metadata = (
+                                    metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+                                        trans.app, required_repository_id, changeset_revision
+                                    )
+                                )
                                 if not required_repository_metadata:
-                                    updated_changeset_revision = \
-                                        metadata_util.get_next_downloadable_changeset_revision(trans.app, required_repository, changeset_revision)
-                                    required_repository_metadata = \
-                                        metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                                                  required_repository_id,
-                                                                                                                  updated_changeset_revision)
-                                required_repository_metadata_id = trans.security.encode_id(required_repository_metadata.id)
-                                rd_line += '<a href="browse_repository_dependencies?operation=view_or_manage_repository&id=%s">' % (required_repository_metadata_id)
-                            rd_line += 'Repository <b>%s</b> revision <b>%s</b> owned by <b>%s</b>' % (escape_html(name), escape_html(owner), escape_html(changeset_revision))
+                                    updated_changeset_revision = metadata_util.get_next_downloadable_changeset_revision(
+                                        trans.app, required_repository, changeset_revision
+                                    )
+                                    required_repository_metadata = (
+                                        metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+                                            trans.app, required_repository_id, updated_changeset_revision
+                                        )
+                                    )
+                                required_repository_metadata_id = trans.security.encode_id(
+                                    required_repository_metadata.id
+                                )
+                                rd_line += f'<a href="browse_repository_dependencies?operation=view_or_manage_repository&id={required_repository_metadata_id}">'
+                            rd_line += f"Repository <b>{escape_html(name)}</b> revision <b>{escape_html(owner)}</b> owned by <b>{escape_html(changeset_revision)}</b>"
                             if required_repository:
-                                rd_line += '</a>'
+                                rd_line += "</a>"
                             rd_str.append(rd_line)
-            return '<br />'.join(rd_str)
+            return "<br />".join(rd_str)
 
     title = "Valid repository dependency definitions in this tool shed"
     default_sort_key = "Repository.name"
     columns = [
-        RequiredRepositoryColumn("Repository dependency",
-                                 attach_popup=False),
-        RepositoryMetadataGrid.RepositoryNameColumn("Repository name",
-                                                    model_class=model.Repository,
-                                                    link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                                    attach_popup=False,
-                                                    key="Repository.name"),
-        RepositoryMetadataGrid.RepositoryOwnerColumn("Owner",
-                                                     model_class=model.User,
-                                                     attach_popup=False,
-                                                     key="User.username"),
-        RepositoryMetadataGrid.ChangesetRevisionColumn("Revision",
-                                                       attach_popup=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name, owner",
-                                              cols_to_filter=[columns[1], columns[2]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
+        RequiredRepositoryColumn("Repository dependency", attach_popup=False),
+        RepositoryMetadataGrid.RepositoryNameColumn(
+            "Repository name",
+            model_class=model.Repository,
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+            key="Repository.name",
+        ),
+        RepositoryMetadataGrid.RepositoryOwnerColumn(
+            "Owner", model_class=model.User, attach_popup=False, key="User.username"
+        ),
+        RepositoryMetadataGrid.ChangesetRevisionColumn("Revision", attach_popup=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name, owner",
+            cols_to_filter=[columns[1], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.RepositoryMetadata) \
-                               .join(model.Repository) \
-                               .filter(and_(model.RepositoryMetadata.table.c.has_repository_dependencies == true(),
-                                            model.Repository.table.c.deleted == false(),
-                                            model.Repository.table.c.deprecated == false())) \
-                               .join(model.User.table)
+        return (
+            trans.sa_session.query(model.RepositoryMetadata)
+            .join(model.Repository)
+            .filter(
+                and_(
+                    model.RepositoryMetadata.table.c.has_repository_dependencies == true(),
+                    model.Repository.table.c.deleted == false(),
+                    model.Repository.table.c.deprecated == false(),
+                )
+            )
+            .join(model.User.table)
+        )
 
 
 class DatatypesGrid(RepositoryMetadataGrid):
-
     class DatatypesColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             datatype_list = []
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    datatype_dicts = metadata.get('datatypes', [])
+                    datatype_dicts = metadata.get("datatypes", [])
                     if datatype_dicts:
                         # Create tuples of the attributes we want so we can sort them by extension.
                         datatype_tups = []
                         for datatype_dict in datatype_dicts:
                             # Example: {"display_in_upload": "true", "dtype": "galaxy.datatypes.blast:BlastXml", "extension": "blastxml", "mimetype": "application/xml"}
-                            extension = datatype_dict.get('extension', '')
-                            dtype = datatype_dict.get('dtype', '')
+                            extension = datatype_dict.get("extension", "")
+                            dtype = datatype_dict.get("dtype", "")
                             # For now we'll just display extension and dtype.
                             if extension and dtype:
                                 datatype_tups.append((extension, dtype))
                         sorted_datatype_tups = sorted(datatype_tups, key=lambda datatype_tup: datatype_tup[0])
                         for datatype_tup in sorted_datatype_tups:
                             extension, datatype = datatype_tup[:2]
-                            datatype_str = '<a href="browse_datatypes?operation=view_or_manage_repository&id=%s">' % trans.security.encode_id(repository_metadata.id)
-                            datatype_str += '<b>%s:</b> %s' % (escape_html(extension), escape_html(datatype))
-                            datatype_str += '</a>'
+                            datatype_str = f'<a href="browse_datatypes?operation=view_or_manage_repository&id={trans.security.encode_id(repository_metadata.id)}">'
+                            datatype_str += f"<b>{escape_html(extension)}:</b> {escape_html(datatype)}"
+                            datatype_str += "</a>"
                             datatype_list.append(datatype_str)
-            return '<br />'.join(datatype_list)
+            return "<br />".join(datatype_list)
 
     title = "Custom datatypes in this tool shed"
     default_sort_key = "Repository.name"
     columns = [
-        DatatypesColumn("Datatype extension and class",
-                        attach_popup=False),
-        RepositoryMetadataGrid.RepositoryNameColumn("Repository name",
-                                                    model_class=model.Repository,
-                                                    link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                                    attach_popup=False,
-                                                    key="Repository.name"),
-        RepositoryMetadataGrid.RepositoryOwnerColumn("Owner",
-                                                     model_class=model.User,
-                                                     attach_popup=False,
-                                                     key="User.username"),
-        RepositoryMetadataGrid.ChangesetRevisionColumn("Revision",
-                                                       attach_popup=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name, owner",
-                                              cols_to_filter=[columns[1], columns[2]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
+        DatatypesColumn("Datatype extension and class", attach_popup=False),
+        RepositoryMetadataGrid.RepositoryNameColumn(
+            "Repository name",
+            model_class=model.Repository,
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+            key="Repository.name",
+        ),
+        RepositoryMetadataGrid.RepositoryOwnerColumn(
+            "Owner", model_class=model.User, attach_popup=False, key="User.username"
+        ),
+        RepositoryMetadataGrid.ChangesetRevisionColumn("Revision", attach_popup=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name, owner",
+            cols_to_filter=[columns[1], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.RepositoryMetadata) \
-                               .join(model.Repository) \
-                               .filter(and_(model.RepositoryMetadata.table.c.includes_datatypes == true(),
-                                            model.Repository.table.c.deleted == false(),
-                                            model.Repository.table.c.deprecated == false())) \
-                               .join(model.User.table)
+        return (
+            trans.sa_session.query(model.RepositoryMetadata)
+            .join(model.Repository)
+            .filter(
+                and_(
+                    model.RepositoryMetadata.table.c.includes_datatypes == true(),
+                    model.Repository.table.c.deleted == false(),
+                    model.Repository.table.c.deprecated == false(),
+                )
+            )
+            .join(model.User.table)
+        )
 
 
 class ToolDependenciesGrid(RepositoryMetadataGrid):
-
     class ToolDependencyColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
-            td_str = ''
+            td_str = ""
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    tds_dict = metadata.get('tool_dependencies', {})
+                    tds_dict = metadata.get("tool_dependencies", {})
                     if tds_dict:
                         # Example: {"bwa/0.5.9": {"name": "bwa", "type": "package", "version": "0.5.9"}}
                         sorted_keys = sorted(tds_dict.keys())
                         num_keys = len(sorted_keys)
                         # Handle environment settings first.
-                        if 'set_environment' in sorted_keys:
+                        if "set_environment" in sorted_keys:
                             # Example: "set_environment": [{"name": "JAVA_JAR_FILE", "type": "set_environment"}]
-                            env_dicts = tds_dict['set_environment']
+                            env_dicts = tds_dict["set_environment"]
                             num_env_dicts = len(env_dicts)
                             if num_env_dicts > 0:
-                                td_str += '<a href="browse_datatypes?operation=view_or_manage_repository&id=%s">' % trans.security.encode_id(repository_metadata.id)
-                                td_str += '<b>environment:</b> '
-                                td_str += ', '.join(escape_html(env_dict['name']) for env_dict in env_dicts)
-                                td_str += '</a><br/>'
+                                td_str += f'<a href="browse_datatypes?operation=view_or_manage_repository&id={trans.security.encode_id(repository_metadata.id)}">'
+                                td_str += "<b>environment:</b> "
+                                td_str += ", ".join(escape_html(env_dict["name"]) for env_dict in env_dicts)
+                                td_str += "</a><br/>"
                         for index, key in enumerate(sorted_keys):
-                            if key == 'set_environment':
+                            if key == "set_environment":
                                 continue
                             td_dict = tds_dict[key]
                             # Example: {"name": "bwa", "type": "package", "version": "0.5.9"}
-                            name = td_dict['name']
-                            version = td_dict['version']
-                            td_str += '<a href="browse_datatypes?operation=view_or_manage_repository&id=%s">' % trans.security.encode_id(repository_metadata.id)
-                            td_str += '<b>%s</b> version <b>%s</b>' % (escape_html(name), escape_html(version))
-                            td_str += '</a>'
+                            name = td_dict["name"]
+                            version = td_dict["version"]
+                            td_str += f'<a href="browse_datatypes?operation=view_or_manage_repository&id={trans.security.encode_id(repository_metadata.id)}">'
+                            td_str += f"<b>{escape_html(name)}</b> version <b>{escape_html(version)}</b>"
+                            td_str += "</a>"
                             if index < num_keys - 1:
-                                td_str += '<br/>'
+                                td_str += "<br/>"
             return td_str
 
     title = "Tool dependency definitions in this tool shed"
     default_sort_key = "Repository.name"
     columns = [
-        ToolDependencyColumn("Tool dependency",
-                             attach_popup=False),
-        RepositoryMetadataGrid.RepositoryNameColumn("Repository name",
-                                                    model_class=model.Repository,
-                                                    link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                                    attach_popup=False,
-                                                    key="Repository.name"),
-        RepositoryMetadataGrid.RepositoryOwnerColumn("Owner",
-                                                     model_class=model.User,
-                                                     attach_popup=False,
-                                                     key="User.username"),
-        RepositoryMetadataGrid.ChangesetRevisionColumn("Revision",
-                                                       attach_popup=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name, owner",
-                                              cols_to_filter=[columns[1], columns[2]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
+        ToolDependencyColumn("Tool dependency", attach_popup=False),
+        RepositoryMetadataGrid.RepositoryNameColumn(
+            "Repository name",
+            model_class=model.Repository,
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+            key="Repository.name",
+        ),
+        RepositoryMetadataGrid.RepositoryOwnerColumn(
+            "Owner", model_class=model.User, attach_popup=False, key="User.username"
+        ),
+        RepositoryMetadataGrid.ChangesetRevisionColumn("Revision", attach_popup=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name, owner",
+            cols_to_filter=[columns[1], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.RepositoryMetadata) \
-                               .join(model.Repository) \
-                               .filter(and_(model.RepositoryMetadata.table.c.includes_tool_dependencies == true(),
-                                            model.Repository.table.c.deleted == false(),
-                                            model.Repository.table.c.deprecated == false())) \
-                               .join(model.User.table)
+        return (
+            trans.sa_session.query(model.RepositoryMetadata)
+            .join(model.Repository)
+            .filter(
+                and_(
+                    model.RepositoryMetadata.table.c.includes_tool_dependencies == true(),
+                    model.Repository.table.c.deleted == false(),
+                    model.Repository.table.c.deprecated == false(),
+                )
+            )
+            .join(model.User.table)
+        )
 
 
 class ToolsGrid(RepositoryMetadataGrid):
-
     class ToolsColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository_metadata):
             tool_line = []
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    tool_dicts = metadata.get('tools', [])
+                    tool_dicts = metadata.get("tools", [])
                     if tool_dicts:
                         # Create tuples of the attributes we want so we can sort them by extension.
                         tool_tups = []
                         for tool_dict in tool_dicts:
-                            tool_id = tool_dict.get('id', '')
-                            version = tool_dict.get('version', '')
+                            tool_id = tool_dict.get("id", "")
+                            version = tool_dict.get("version", "")
                             # For now we'll just display tool id and version.
                             if tool_id and version:
                                 tool_tups.append((tool_id, version))
                         sorted_tool_tups = sorted(tool_tups, key=lambda tool_tup: tool_tup[0])
                         for tool_tup in sorted_tool_tups:
                             tool_id, version = tool_tup[:2]
-                            tool_str = '<a href="browse_datatypes?operation=view_or_manage_repository&id=%s">' % trans.security.encode_id(repository_metadata.id)
-                            tool_str += '<b>%s:</b> %s' % (escape_html(tool_id), escape_html(version))
-                            tool_str += '</a>'
+                            tool_str = f'<a href="browse_datatypes?operation=view_or_manage_repository&id={trans.security.encode_id(repository_metadata.id)}">'
+                            tool_str += f"<b>{escape_html(tool_id)}:</b> {escape_html(version)}"
+                            tool_str += "</a>"
                             tool_line.append(tool_str)
-            return '<br />'.join(tool_line)
+            return "<br />".join(tool_line)
 
     title = "Valid tools in this tool shed"
     default_sort_key = "Repository.name"
     columns = [
-        ToolsColumn("Tool id and version",
-                    attach_popup=False),
-        RepositoryMetadataGrid.RepositoryNameColumn("Repository name",
-                                                    model_class=model.Repository,
-                                                    link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
-                                                    attach_popup=False,
-                                                    key="Repository.name"),
-        RepositoryMetadataGrid.RepositoryOwnerColumn("Owner",
-                                                     model_class=model.User,
-                                                     attach_popup=False,
-                                                     key="User.username"),
-        RepositoryMetadataGrid.ChangesetRevisionColumn("Revision",
-                                                       attach_popup=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name, owner",
-                                              cols_to_filter=[columns[1], columns[2]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
+        ToolsColumn("Tool id and version", attach_popup=False),
+        RepositoryMetadataGrid.RepositoryNameColumn(
+            "Repository name",
+            model_class=model.Repository,
+            link=(lambda item: dict(operation="view_or_manage_repository", id=item.id)),
+            attach_popup=False,
+            key="Repository.name",
+        ),
+        RepositoryMetadataGrid.RepositoryOwnerColumn(
+            "Owner", model_class=model.User, attach_popup=False, key="User.username"
+        ),
+        RepositoryMetadataGrid.ChangesetRevisionColumn("Revision", attach_popup=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name, owner",
+            cols_to_filter=[columns[1], columns[2]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
-        return trans.sa_session.query(model.RepositoryMetadata) \
-                               .join(model.Repository) \
-                               .filter(and_(model.RepositoryMetadata.table.c.includes_tools == true(),
-                                            model.Repository.table.c.deleted == false(),
-                                            model.Repository.table.c.deprecated == false())) \
-                               .join(model.User.table)
+        return (
+            trans.sa_session.query(model.RepositoryMetadata)
+            .join(model.Repository)
+            .filter(
+                and_(
+                    model.RepositoryMetadata.table.c.includes_tools == true(),
+                    model.Repository.table.c.deleted == false(),
+                    model.Repository.table.c.deprecated == false(),
+                )
+            )
+            .join(model.User.table)
+        )
 
 
 class ValidCategoryGrid(CategoryGrid):
-
     class RepositoriesColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, category):
             category_name = str(category.name)
             filter = trans.app.repository_grid_filter_manager.get_filter(trans)
             if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE:
-                return trans.app.repository_registry.certified_level_one_viewable_repositories_and_suites_by_category.get(category_name, 0)
+                return (
+                    trans.app.repository_registry.certified_level_one_viewable_repositories_and_suites_by_category.get(
+                        category_name, 0
+                    )
+                )
             elif filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE_SUITES:
-                return trans.app.repository_registry.certified_level_one_viewable_suites_by_category.get(category_name, 0)
+                return trans.app.repository_registry.certified_level_one_viewable_suites_by_category.get(
+                    category_name, 0
+                )
             elif filter == trans.app.repository_grid_filter_manager.filters.SUITES:
                 return trans.app.repository_registry.viewable_valid_suites_by_category.get(category_name, 0)
             else:
                 # The value filter is None.
-                return trans.app.repository_registry.viewable_valid_repositories_and_suites_by_category.get(category_name, 0)
+                return trans.app.repository_registry.viewable_valid_repositories_and_suites_by_category.get(
+                    category_name, 0
+                )
 
     title = "Categories of Valid Repositories"
     model_class = model.Category
-    template = '/webapps/tool_shed/category/valid_grid.mako'
+    template = "/webapps/tool_shed/category/valid_grid.mako"
     default_sort_key = "name"
     columns = [
-        CategoryGrid.NameColumn("Name",
-                                key="Category.name",
-                                link=(lambda item: dict(operation="valid_repositories_by_category", id=item.id)),
-                                attach_popup=False),
-        CategoryGrid.DescriptionColumn("Description",
-                                       key="Category.description",
-                                       attach_popup=False),
+        CategoryGrid.NameColumn(
+            "Name",
+            key="Category.name",
+            link=(lambda item: dict(operation="valid_repositories_by_category", id=item.id)),
+            attach_popup=False,
+        ),
+        CategoryGrid.DescriptionColumn("Description", key="Category.description", attach_popup=False),
         # Columns that are valid for filtering but are not visible.
-        RepositoriesColumn("Valid repositories",
-                           model_class=model.Repository,
-                           attach_popup=False)
+        RepositoriesColumn("Valid repositories", model_class=model.Repository, attach_popup=False),
     ]
     # Override these
-    default_filter = {}
-    global_actions = []
-    operations = []
-    standard_filters = []
     num_rows_per_page = 50
-    use_paging = False
 
 
 class ValidRepositoryGrid(RepositoryGrid):
     # This grid filters out repositories that have been marked as either deleted or deprecated.
 
     class CategoryColumn(grids.TextColumn):
-
         def get_value(self, trans, grid, repository):
-            rval = '<ul>'
+            rval = "<ul>"
             if repository.categories:
                 for rca in repository.categories:
-                    rval += '<li><a href="browse_repositories?operation=valid_repositories_by_category&id=%s">%s</a></li>' \
+                    rval += (
+                        '<li><a href="browse_repositories?operation=valid_repositories_by_category&id=%s">%s</a></li>'
                         % (trans.security.encode_id(rca.category.id), rca.category.name)
+                    )
             else:
-                rval += '<li>not set</li>'
-            rval += '</ul>'
+                rval += "<li>not set</li>"
+            rval += "</ul>"
             return rval
 
     class RepositoryCategoryColumn(grids.GridColumn):
-
         def filter(self, trans, user, query, column_filter):
             """Modify query to filter by category."""
             if column_filter == "All":
                 return query
             return query.filter(model.Category.name == column_filter)
 
     class InstallableRevisionColumn(grids.GridColumn):
-
         def __init__(self, col_name):
             grids.GridColumn.__init__(self, col_name)
 
         def get_value(self, trans, grid, repository):
             """Display a SelectField whose options are the changeset_revision strings of all download-able revisions of this repository."""
             select_field = grids_util.build_changeset_revision_select_field(trans, repository, downloadable=True)
             if len(select_field.options) > 1:
-                tmpl = "<select name='%s'>" % select_field.name
+                tmpl = f"<select name='{select_field.name}'>"
                 for o in select_field.options:
-                    tmpl += "<option value='%s'>%s</option>" % (o[1], o[0])
+                    tmpl += f"<option value='{o[1]}'>{o[0]}</option>"
                 tmpl += "</select>"
                 return tmpl
             elif len(select_field.options) == 1:
                 return select_field.options[0][0]
-            return ''
+            return ""
 
     title = "Valid Repositories"
     columns = [
-        RepositoryGrid.NameColumn("Name",
-                                  key="name",
-                                  attach_popup=True),
-        RepositoryGrid.DescriptionColumn("Synopsis",
-                                         key="description",
-                                         attach_popup=False),
+        RepositoryGrid.NameColumn("Name", key="name", attach_popup=True),
+        RepositoryGrid.DescriptionColumn("Synopsis", key="description", attach_popup=False),
         RepositoryGrid.TypeColumn("Type"),
         InstallableRevisionColumn("Installable Revisions"),
-        RepositoryGrid.UserColumn("Owner",
-                                  model_class=model.User,
-                                  attach_popup=False),
+        RepositoryGrid.UserColumn("Owner", model_class=model.User, attach_popup=False),
         # Columns that are valid for filtering but are not visible.
-        RepositoryCategoryColumn("Category",
-                                 model_class=model.Category,
-                                 key="Category.name",
-                                 visible=False)
-    ]
-    columns.append(grids.MulticolFilterColumn("Search repository name, description",
-                                              cols_to_filter=[columns[0], columns[1]],
-                                              key="free-text-search",
-                                              visible=False,
-                                              filterable="standard"))
-    operations = []
-    use_paging = False
+        RepositoryCategoryColumn("Category", model_class=model.Category, key="Category.name", visible=False),
+    ]
+    columns.append(
+        grids.MulticolFilterColumn(
+            "Search repository name, description",
+            cols_to_filter=[columns[0], columns[1]],
+            key="free-text-search",
+            visible=False,
+            filterable="standard",
+        )
+    )
 
     def build_initial_query(self, trans, **kwd):
         filter = trans.app.repository_grid_filter_manager.get_filter(trans)
-        if 'id' in kwd:
+        if "id" in kwd:
             # The user is browsing categories of valid repositories, so filter the request by the received id,
             # which is a category id.
             if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE:
-                return trans.sa_session.query(model.Repository) \
-                                       .join(model.RepositoryMetadata.table) \
-                                       .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                       .join(model.User.table) \
-                                       .join(model.RepositoryCategoryAssociation.table) \
-                                       .join(model.Category.table) \
-                                       .filter(and_(model.Category.table.c.id == trans.security.decode_id(kwd['id']),
-                                                    model.RepositoryMetadata.table.c.downloadable == true()))
+                return (
+                    trans.sa_session.query(model.Repository)
+                    .join(model.RepositoryMetadata.table)
+                    .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                    .join(model.User.table)
+                    .join(model.RepositoryCategoryAssociation.table)
+                    .join(model.Category.table)
+                    .filter(
+                        and_(
+                            model.Category.table.c.id == trans.security.decode_id(kwd["id"]),
+                            model.RepositoryMetadata.table.c.downloadable == true(),
+                        )
+                    )
+                )
             if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE_SUITES:
-                return trans.sa_session.query(model.Repository) \
-                                       .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION) \
-                                       .join(model.RepositoryMetadata.table) \
-                                       .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                       .join(model.User.table) \
-                                       .join(model.RepositoryCategoryAssociation.table) \
-                                       .join(model.Category.table) \
-                                       .filter(and_(model.Category.table.c.id == trans.security.decode_id(kwd['id']),
-                                                    model.RepositoryMetadata.table.c.downloadable == true()))
+                return (
+                    trans.sa_session.query(model.Repository)
+                    .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION)
+                    .join(model.RepositoryMetadata.table)
+                    .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                    .join(model.User.table)
+                    .join(model.RepositoryCategoryAssociation.table)
+                    .join(model.Category.table)
+                    .filter(
+                        and_(
+                            model.Category.table.c.id == trans.security.decode_id(kwd["id"]),
+                            model.RepositoryMetadata.table.c.downloadable == true(),
+                        )
+                    )
+                )
             else:
                 # The value of filter is None.
-                return trans.sa_session.query(model.Repository) \
-                                       .filter(and_(model.Repository.table.c.deleted == false(),
-                                                    model.Repository.table.c.deprecated == false())) \
-                                       .join(model.RepositoryMetadata.table) \
-                                       .join(model.User.table) \
-                                       .join(model.RepositoryCategoryAssociation.table) \
-                                       .join(model.Category.table) \
-                                       .filter(and_(model.Category.table.c.id == trans.security.decode_id(kwd['id']),
-                                                    model.RepositoryMetadata.table.c.downloadable == true()))
+                return (
+                    trans.sa_session.query(model.Repository)
+                    .filter(
+                        and_(
+                            model.Repository.table.c.deleted == false(), model.Repository.table.c.deprecated == false()
+                        )
+                    )
+                    .join(model.RepositoryMetadata.table)
+                    .join(model.User.table)
+                    .join(model.RepositoryCategoryAssociation.table)
+                    .join(model.Category.table)
+                    .filter(
+                        and_(
+                            model.Category.table.c.id == trans.security.decode_id(kwd["id"]),
+                            model.RepositoryMetadata.table.c.downloadable == true(),
+                        )
+                    )
+                )
         # The user performed a free text search on the ValidCategoryGrid.
         if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE:
-            return trans.sa_session.query(model.Repository) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table) \
-                                   .filter(model.RepositoryMetadata.table.c.downloadable == true())
+            return (
+                trans.sa_session.query(model.Repository)
+                .join(model.RepositoryMetadata.table)
+                .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+                .filter(model.RepositoryMetadata.table.c.downloadable == true())
+            )
         if filter == trans.app.repository_grid_filter_manager.filters.CERTIFIED_LEVEL_ONE_SUITES:
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list)) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table) \
-                                   .filter(model.RepositoryMetadata.table.c.downloadable == true())
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(model.Repository.type == rt_util.REPOSITORY_SUITE_DEFINITION)
+                .join(model.RepositoryMetadata.table)
+                .filter(or_(*trans.app.repository_registry.certified_level_one_clause_list))
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+                .filter(model.RepositoryMetadata.table.c.downloadable == true())
+            )
         else:
             # The value of filter is None.
-            return trans.sa_session.query(model.Repository) \
-                                   .filter(and_(model.Repository.table.c.deleted == false(),
-                                                model.Repository.table.c.deprecated == false())) \
-                                   .join(model.RepositoryMetadata.table) \
-                                   .join(model.User.table) \
-                                   .outerjoin(model.RepositoryCategoryAssociation.table) \
-                                   .outerjoin(model.Category.table) \
-                                   .filter(model.RepositoryMetadata.table.c.downloadable == true())
+            return (
+                trans.sa_session.query(model.Repository)
+                .filter(
+                    and_(model.Repository.table.c.deleted == false(), model.Repository.table.c.deprecated == false())
+                )
+                .join(model.RepositoryMetadata.table)
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+                .filter(model.RepositoryMetadata.table.c.downloadable == true())
+            )
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/grids/util.py` & `galaxy-web-apps-23.0.2/tool_shed/grids/util.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,80 +1,53 @@
 import logging
 
 from galaxy.web.form_builder import SelectField
-from tool_shed.util import hg_util, metadata_util
+from tool_shed.util import (
+    hg_util,
+    metadata_util,
+)
 
 log = logging.getLogger(__name__)
 
 
-def build_approved_select_field(trans, name, selected_value=None, for_component=True):
-    options = [('No', trans.model.ComponentReview.approved_states.NO),
-               ('Yes', trans.model.ComponentReview.approved_states.YES)]
-    if for_component:
-        options.append(('Not applicable', trans.model.ComponentReview.approved_states.NA))
-        if selected_value is None:
-            selected_value = trans.model.ComponentReview.approved_states.NA
-    select_field = SelectField(name=name)
-    for option_tup in options:
-        selected = selected_value and option_tup[1] == selected_value
-        select_field.add_option(option_tup[0], option_tup[1], selected=selected)
-    return select_field
-
-
-def build_changeset_revision_select_field(trans, repository, selected_value=None, add_id_to_name=True,
-                                          downloadable=False, reviewed=False, not_reviewed=False):
+def build_changeset_revision_select_field(
+    trans,
+    repository,
+    selected_value=None,
+    add_id_to_name=True,
+    downloadable=False,
+):
     """
     Build a SelectField whose options are the changeset_rev strings of certain revisions of the
     received repository.
     """
     options = []
     changeset_tups = []
     refresh_on_change_values = []
     if downloadable:
         # Restrict the options to downloadable revisions.
         repository_metadata_revisions = repository.downloadable_revisions
-    elif reviewed:
-        # Restrict the options to revisions that have been reviewed.
-        repository_metadata_revisions = []
-        metadata_changeset_revision_hashes = []
-        for metadata_revision in repository.metadata_revisions:
-            metadata_changeset_revision_hashes.append(metadata_revision.changeset_revision)
-        for review in repository.reviews:
-            if review.changeset_revision in metadata_changeset_revision_hashes:
-                repository_metadata_revisions.append(review.repository_metadata)
-    elif not_reviewed:
-        # Restrict the options to revisions that have not yet been reviewed.
-        repository_metadata_revisions = []
-        reviewed_metadata_changeset_revision_hashes = []
-        for review in repository.reviews:
-            reviewed_metadata_changeset_revision_hashes.append(review.changeset_revision)
-        for metadata_revision in repository.metadata_revisions:
-            if metadata_revision.changeset_revision not in reviewed_metadata_changeset_revision_hashes:
-                repository_metadata_revisions.append(metadata_revision)
     else:
         # Restrict the options to all revisions that have associated metadata.
         repository_metadata_revisions = repository.metadata_revisions
     for repository_metadata in repository_metadata_revisions:
-        rev, label, changeset_revision = \
-            hg_util.get_rev_label_changeset_revision_from_repository_metadata(trans.app,
-                                                                              repository_metadata,
-                                                                              repository=repository,
-                                                                              include_date=True,
-                                                                              include_hash=False)
+        rev, label, changeset_revision = hg_util.get_rev_label_changeset_revision_from_repository_metadata(
+            trans.app, repository_metadata, repository=repository, include_date=True, include_hash=False
+        )
         changeset_tups.append((rev, label, changeset_revision))
         refresh_on_change_values.append(changeset_revision)
     # Sort options by the revision label.  Even though the downloadable_revisions query sorts by update_time,
     # the changeset revisions may not be sorted correctly because setting metadata over time will reset update_time.
     for changeset_tup in sorted(changeset_tups):
         # Display the latest revision first.
         options.insert(0, (changeset_tup[1], changeset_tup[2]))
     if add_id_to_name:
-        name = 'changeset_revision_%d' % repository.id
+        name = "changeset_revision_%d" % repository.id
     else:
-        name = 'changeset_revision'
+        name = "changeset_revision"
     select_field = SelectField(name=name, refresh_on_change=True)
     for option_tup in options:
         selected = selected_value and option_tup[1] == selected_value
         select_field.add_option(option_tup[0], option_tup[1], selected=selected)
     return select_field
 
 
@@ -83,16 +56,15 @@
     Inspect the latest downloadable changeset revision for the received repository to see if it
     includes tools that are either missing functional tests or functional test data.  If the
     changset revision includes tools but is missing tool test components, return the changeset
     revision hash.  This will filter out repositories of type repository_suite_definition and
     tool_dependency_definition.
     """
     repository_metadata = get_latest_downloadable_repository_metadata_if_it_includes_tools(trans, repository)
-    if repository_metadata is not None \
-            and repository_metadata.missing_test_components:
+    if repository_metadata is not None and repository_metadata.missing_test_components:
         return repository_metadata.changeset_revision
     return None
 
 
 def filter_by_latest_metadata_changeset_revision_that_has_invalid_tools(trans, repository):
     """
     Inspect the latest changeset revision with associated metadata for the received repository
@@ -105,32 +77,36 @@
     return None
 
 
 def get_latest_downloadable_repository_metadata(trans, repository):
     """
     Return the latest downloadable repository_metadata record for the received repository.  This will
     return repositories of type unrestricted as well as types repository_suite_definition and
-     tool_dependency_definition.
+    tool_dependency_definition.
     """
     encoded_repository_id = trans.security.encode_id(repository.id)
     repo = repository.hg_repo
     tip_ctx = str(repo[repo.changelog.tip()])
     repository_metadata = None
     try:
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, encoded_repository_id, tip_ctx)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, encoded_repository_id, tip_ctx
+        )
         if repository_metadata is not None and repository_metadata.downloadable:
             return repository_metadata
         return None
     except Exception:
-        latest_downloadable_revision = metadata_util.get_previous_metadata_changeset_revision(trans.app, repository, tip_ctx, downloadable=True)
+        latest_downloadable_revision = metadata_util.get_previous_metadata_changeset_revision(
+            trans.app, repository, tip_ctx, downloadable=True
+        )
         if latest_downloadable_revision == hg_util.INITIAL_CHANGELOG_HASH:
             return None
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                          encoded_repository_id,
-                                                                                          latest_downloadable_revision)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, encoded_repository_id, latest_downloadable_revision
+        )
         if repository_metadata is not None and repository_metadata.downloadable:
             return repository_metadata
         return None
 
 
 def get_latest_downloadable_repository_metadata_if_it_includes_tools(trans, repository):
     """
@@ -144,37 +120,41 @@
     return None
 
 
 def get_latest_repository_metadata(trans, repository):
     """
     Return the latest repository_metadata record for the received repository if it exists.  This will
     return repositories of type unrestricted as well as types repository_suite_definition and
-     tool_dependency_definition.
+    tool_dependency_definition.
     """
     encoded_repository_id = trans.security.encode_id(repository.id)
     repo = repository.hg_repo
     tip_ctx = str(repo[repo.changelog.tip()])
     try:
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, encoded_repository_id, tip_ctx)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, encoded_repository_id, tip_ctx
+        )
         return repository_metadata
     except Exception:
-        latest_downloadable_revision = metadata_util.get_previous_metadata_changeset_revision(trans.app, repository, tip_ctx, downloadable=False)
+        latest_downloadable_revision = metadata_util.get_previous_metadata_changeset_revision(
+            trans.app, repository, tip_ctx, downloadable=False
+        )
         if latest_downloadable_revision == hg_util.INITIAL_CHANGELOG_HASH:
             return None
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                          encoded_repository_id,
-                                                                                          latest_downloadable_revision)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, encoded_repository_id, latest_downloadable_revision
+        )
         return repository_metadata
 
 
 def get_latest_repository_metadata_if_it_includes_invalid_tools(trans, repository):
     """
     Return the latest repository_metadata record for the received repository that contains invalid
     tools if one exists.  This will filter out repositories of type repository_suite_definition and
     tool_dependency_definition.
     """
     repository_metadata = get_latest_repository_metadata(trans, repository)
     if repository_metadata is not None:
         metadata = repository_metadata.metadata
-        if metadata is not None and 'invalid_tools' in metadata:
+        if metadata is not None and "invalid_tools" in metadata:
             return repository_metadata
     return None
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/managers/groups.py` & `galaxy-web-apps-23.0.2/tool_shed/managers/groups.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,85 +1,94 @@
 """
 Manager and Serializer for TS groups.
 """
 import logging
 
-from sqlalchemy import false, true
-from sqlalchemy.orm.exc import MultipleResultsFound
-from sqlalchemy.orm.exc import NoResultFound
-
-from galaxy.exceptions import Conflict
-from galaxy.exceptions import InconsistentDatabase
-from galaxy.exceptions import InternalServerError
-from galaxy.exceptions import ItemAccessibilityException
-from galaxy.exceptions import ObjectNotFound
-from galaxy.exceptions import RequestParameterInvalidException
+from sqlalchemy import (
+    false,
+    true,
+)
+from sqlalchemy.orm.exc import (
+    MultipleResultsFound,
+    NoResultFound,
+)
+
+from galaxy.exceptions import (
+    Conflict,
+    InconsistentDatabase,
+    InternalServerError,
+    ItemAccessibilityException,
+    ObjectNotFound,
+    RequestParameterInvalidException,
+)
 
 log = logging.getLogger(__name__)
 
 
 # =============================================================================
-class GroupManager(object):
+class GroupManager:
     """
     Interface/service object for interacting with TS groups.
     """
 
     def __init__(self, *args, **kwargs):
-        super(GroupManager, self).__init__(*args, **kwargs)
+        super().__init__(*args, **kwargs)
 
     def get(self, trans, decoded_group_id=None, name=None):
         """
         Get the group from the DB based on its ID or name.
 
         :param  decoded_group_id:       decoded group id
         :type   decoded_group_id:       int
 
         :returns:   the requested group
-        :rtype:     Group
+        :rtype:     tool_shed.model.Group
         """
         if decoded_group_id is None and name is None:
-            raise RequestParameterInvalidException('You must supply either ID or a name of the group.')
+            raise RequestParameterInvalidException("You must supply either ID or a name of the group.")
 
         name_query = trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.name == name)
-        id_query = trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.id == decoded_group_id)
+        id_query = trans.sa_session.query(trans.app.model.Group).filter(
+            trans.app.model.Group.table.c.id == decoded_group_id
+        )
 
         try:
             group = id_query.one() if decoded_group_id else name_query.one()
         except MultipleResultsFound:
-            raise InconsistentDatabase('Multiple groups found with the same identifier.')
+            raise InconsistentDatabase("Multiple groups found with the same identifier.")
         except NoResultFound:
-            raise ObjectNotFound('No group found with the identifier provided.')
+            raise ObjectNotFound("No group found with the identifier provided.")
         except Exception:
-            raise InternalServerError('Error loading from the database.')
+            raise InternalServerError("Error loading from the database.")
         return group
 
-    def create(self, trans, name, description=''):
+    def create(self, trans, name, description=""):
         """
         Create a new group.
         """
         if not trans.user_is_admin:
-            raise ItemAccessibilityException('Only administrators can create groups.')
+            raise ItemAccessibilityException("Only administrators can create groups.")
         else:
             if self.get(trans, name=name):
-                raise Conflict('Group with the given name already exists. Name: ' + str(name))
+                raise Conflict(f"Group with the given name already exists. Name: {str(name)}")
             # TODO add description field to the model
             group = trans.app.model.Group(name=name)
             trans.sa_session.add(group)
             trans.sa_session.flush()
             return group
 
     def update(self, trans, group, name=None, description=None):
         """
         Update the given group
         """
         changed = False
         if not trans.user_is_admin:
-            raise ItemAccessibilityException('Only administrators can update groups.')
+            raise ItemAccessibilityException("Only administrators can update groups.")
         if group.deleted:
-            raise RequestParameterInvalidException('You cannot modify a deleted group. Undelete it first.')
+            raise RequestParameterInvalidException("You cannot modify a deleted group. Undelete it first.")
         if name is not None:
             group.name = name
             changed = True
         if description is not None:
             group.description = description
             changed = True
         if changed:
@@ -88,15 +97,15 @@
         return group
 
     def delete(self, trans, group, undelete=False):
         """
         Mark given group deleted/undeleted based on the flag.
         """
         if not trans.user_is_admin:
-            raise ItemAccessibilityException('Only administrators can delete and undelete groups.')
+            raise ItemAccessibilityException("Only administrators can delete and undelete groups.")
         if undelete:
             group.deleted = False
         else:
             group.deleted = True
         trans.sa_session.add(group)
         trans.sa_session.flush()
         return group
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/managers/repositories.py` & `galaxy-web-apps-23.0.2/tool_shed/managers/repositories.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,60 +1,72 @@
 """
 Manager and Serializer for TS repositories.
 """
 import logging
 
-from sqlalchemy.orm.exc import MultipleResultsFound, NoResultFound
-
-from galaxy.exceptions import (InconsistentDatabase, InternalServerError,
-    RequestParameterInvalidException)
+from sqlalchemy.orm.exc import (
+    MultipleResultsFound,
+    NoResultFound,
+)
+
+from galaxy.exceptions import (
+    InconsistentDatabase,
+    InternalServerError,
+    RequestParameterInvalidException,
+)
 
 log = logging.getLogger(__name__)
 
 
 # =============================================================================
-class RepoManager(object):
+class RepoManager:
     """
     Interface/service object for interacting with TS repositories.
     """
 
     def __init__(self, *args, **kwargs):
-        super(RepoManager, self).__init__(*args, **kwargs)
+        super().__init__(*args, **kwargs)
 
     def get(self, trans, decoded_repo_id):
         """
         Get the repo from the DB.
 
         :param  decoded_repo_id:       decoded repo id
         :type   decoded_repo_id:       int
 
         :returns:   the requested repo
-        :rtype:     Repository
+        :rtype:     tool_shed.webapp.model.Repository
         """
         try:
-            repo = trans.sa_session.query(trans.app.model.Repository).filter(trans.app.model.Repository.table.c.id == decoded_repo_id).one()
+            repo = (
+                trans.sa_session.query(trans.app.model.Repository)
+                .filter(trans.app.model.Repository.table.c.id == decoded_repo_id)
+                .one()
+            )
         except MultipleResultsFound:
-            raise InconsistentDatabase('Multiple repositories found with the same id.')
+            raise InconsistentDatabase("Multiple repositories found with the same id.")
         except NoResultFound:
-            raise RequestParameterInvalidException('No repository found with the id provided.')
+            raise RequestParameterInvalidException("No repository found with the id provided.")
         except Exception:
-            raise InternalServerError('Error loading from the database.')
+            raise InternalServerError("Error loading from the database.")
         return repo
 
     def list_by_owner(self, trans, user_id):
         """
         Return a list of of repositories owned by a given TS user from the DB.
 
         :returns: query that will emit repositories owned by given user
         :rtype:   sqlalchemy query
         """
-        query = trans.sa_session.query(trans.app.model.Repository).filter(trans.app.model.Repository.table.c.user_id == user_id)
+        query = trans.sa_session.query(trans.app.model.Repository).filter(
+            trans.app.model.Repository.table.c.user_id == user_id
+        )
         return query
 
-    def create(self, trans, name, description=''):
+    def create(self, trans, name, description=""):
         """
         Create a new group.
         """
 
     def update(self, trans, group, name=None, description=None):
         """
         Update the given group
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/metadata/repository_metadata_manager.py` & `galaxy-web-apps-23.0.2/tool_shed/metadata/repository_metadata_manager.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,204 +1,260 @@
 import logging
 import tempfile
 
-from sqlalchemy import false, or_
+from sqlalchemy import (
+    false,
+    or_,
+)
 
 from galaxy import util
 from galaxy.util import inflector
 from galaxy.web.form_builder import SelectField
 from tool_shed.metadata import metadata_generator
 from tool_shed.repository_types import util as rt_util
 from tool_shed.repository_types.metadata import TipOnly
-from tool_shed.util import (basic_util, common_util, hg_util, metadata_util,
-    repository_util, shed_util_common as suc, tool_util)
+from tool_shed.structured_app import ToolShedApp
+from tool_shed.util import (
+    basic_util,
+    common_util,
+    hg_util,
+    metadata_util,
+    repository_util,
+    shed_util_common as suc,
+    tool_util,
+)
 
 log = logging.getLogger(__name__)
 
 
 class RepositoryMetadataManager(metadata_generator.MetadataGenerator):
-
-    def __init__(self, app, user, repository=None, changeset_revision=None, repository_clone_url=None,
-                 shed_config_dict=None, relative_install_dir=None, repository_files_dir=None,
-                 resetting_all_metadata_on_repository=False, updating_installed_repository=False,
-                 persist=False, metadata_dict=None):
-        super(RepositoryMetadataManager, self).__init__(app, repository, changeset_revision,
-                                                        repository_clone_url, shed_config_dict,
-                                                        relative_install_dir, repository_files_dir,
-                                                        resetting_all_metadata_on_repository,
-                                                        updating_installed_repository, persist,
-                                                        metadata_dict=metadata_dict, user=user)
+    def __init__(
+        self,
+        app: ToolShedApp,
+        user,
+        repository=None,
+        changeset_revision=None,
+        repository_clone_url=None,
+        shed_config_dict=None,
+        relative_install_dir=None,
+        repository_files_dir=None,
+        resetting_all_metadata_on_repository=False,
+        updating_installed_repository=False,
+        persist=False,
+        metadata_dict=None,
+    ):
+        super().__init__(
+            app,
+            repository,
+            changeset_revision,
+            repository_clone_url,
+            shed_config_dict,
+            relative_install_dir,
+            repository_files_dir,
+            resetting_all_metadata_on_repository,
+            updating_installed_repository,
+            persist,
+            metadata_dict=metadata_dict,
+            user=user,
+        )
         self.app = app
         self.user = user
         # Repository metadata comparisons for changeset revisions.
-        self.EQUAL = 'equal'
-        self.NO_METADATA = 'no metadata'
-        self.NOT_EQUAL_AND_NOT_SUBSET = 'not equal and not subset'
-        self.SUBSET = 'subset'
+        self.EQUAL = "equal"
+        self.NO_METADATA = "no metadata"
+        self.NOT_EQUAL_AND_NOT_SUBSET = "not equal and not subset"
+        self.SUBSET = "subset"
         self.SUBSET_VALUES = [self.EQUAL, self.SUBSET]
 
     def add_tool_versions(self, id, repository_metadata, changeset_revisions):
         # Build a dictionary of { 'tool id' : 'parent tool id' } pairs for each tool in repository_metadata.
         metadata = repository_metadata.metadata
         tool_versions_dict = {}
-        for tool_dict in metadata.get('tools', []):
+        for tool_dict in metadata.get("tools", []):
             # We have at least 2 changeset revisions to compare tool guids and tool ids.
-            parent_id = self.get_parent_id(id,
-                                           tool_dict['id'],
-                                           tool_dict['version'],
-                                           tool_dict['guid'],
-                                           changeset_revisions)
-            tool_versions_dict[tool_dict['guid']] = parent_id
+            parent_id = self.get_parent_id(
+                id, tool_dict["id"], tool_dict["version"], tool_dict["guid"], changeset_revisions
+            )
+            tool_versions_dict[tool_dict["guid"]] = parent_id
         if tool_versions_dict:
             repository_metadata.tool_versions = tool_versions_dict
             self.sa_session.add(repository_metadata)
             self.sa_session.flush()
 
-    def build_repository_ids_select_field(self, name='repository_ids', multiple=True, display='checkboxes',
-                                          my_writable=False):
+    def build_repository_ids_select_field(
+        self, name="repository_ids", multiple=True, display="checkboxes", my_writable=False
+    ):
         """Generate the current list of repositories for resetting metadata."""
         repositories_select_field = SelectField(name=name, multiple=multiple, display=display)
         query = self.get_query_for_setting_metadata_on_repositories(my_writable=my_writable, order=True)
         for repository in query:
             owner = str(repository.user.username)
-            option_label = '%s (%s)' % (str(repository.name), owner)
-            option_value = '%s' % self.app.security.encode_id(repository.id)
+            option_label = f"{str(repository.name)} ({owner})"
+            option_value = f"{self.app.security.encode_id(repository.id)}"
             repositories_select_field.add_option(option_label, option_value)
         return repositories_select_field
 
     def clean_repository_metadata(self, changeset_revisions):
         # Delete all repository_metadata records associated with the repository that have
         # a changeset_revision that is not in changeset_revisions.  We sometimes see multiple
         # records with the same changeset revision value - no idea how this happens. We'll
         # assume we can delete the older records, so we'll order by update_time descending and
         # delete records that have the same changeset_revision we come across later.
         changeset_revisions_checked = []
-        for repository_metadata in \
-            self.sa_session.query(self.app.model.RepositoryMetadata) \
-                           .filter(self.app.model.RepositoryMetadata.table.c.repository_id == self.repository.id) \
-                           .order_by(self.app.model.RepositoryMetadata.table.c.changeset_revision,
-                                     self.app.model.RepositoryMetadata.table.c.update_time.desc()):
+        for repository_metadata in (
+            self.sa_session.query(self.app.model.RepositoryMetadata)
+            .filter(self.app.model.RepositoryMetadata.table.c.repository_id == self.repository.id)
+            .order_by(
+                self.app.model.RepositoryMetadata.table.c.changeset_revision,
+                self.app.model.RepositoryMetadata.table.c.update_time.desc(),
+            )
+        ):
             changeset_revision = repository_metadata.changeset_revision
             if changeset_revision in changeset_revisions_checked or changeset_revision not in changeset_revisions:
                 self.sa_session.delete(repository_metadata)
                 self.sa_session.flush()
 
     def compare_changeset_revisions(self, ancestor_changeset_revision, ancestor_metadata_dict):
         """
         Compare the contents of two changeset revisions to determine if a new repository
         metadata revision should be created.
         """
         # The metadata associated with ancestor_changeset_revision is ancestor_metadata_dict.
         # This changeset_revision is an ancestor of self.changeset_revision which is associated
         # with self.metadata_dict.  A new repository_metadata record will be created only
         # when this method returns the constant value self.NOT_EQUAL_AND_NOT_SUBSET.
-        ancestor_datatypes = ancestor_metadata_dict.get('datatypes', [])
-        ancestor_tools = ancestor_metadata_dict.get('tools', [])
-        ancestor_guids = [tool_dict['guid'] for tool_dict in ancestor_tools]
+        ancestor_datatypes = ancestor_metadata_dict.get("datatypes", [])
+        ancestor_tools = ancestor_metadata_dict.get("tools", [])
+        ancestor_guids = [tool_dict["guid"] for tool_dict in ancestor_tools]
         ancestor_guids.sort()
-        ancestor_readme_files = ancestor_metadata_dict.get('readme_files', [])
-        ancestor_repository_dependencies_dict = ancestor_metadata_dict.get('repository_dependencies', {})
-        ancestor_repository_dependencies = ancestor_repository_dependencies_dict.get('repository_dependencies', [])
-        ancestor_tool_dependencies = ancestor_metadata_dict.get('tool_dependencies', {})
-        ancestor_workflows = ancestor_metadata_dict.get('workflows', [])
-        ancestor_data_manager = ancestor_metadata_dict.get('data_manager', {})
-        current_datatypes = self.metadata_dict.get('datatypes', [])
-        current_tools = self.metadata_dict.get('tools', [])
-        current_guids = [tool_dict['guid'] for tool_dict in current_tools]
+        ancestor_readme_files = ancestor_metadata_dict.get("readme_files", [])
+        ancestor_repository_dependencies_dict = ancestor_metadata_dict.get("repository_dependencies", {})
+        ancestor_repository_dependencies = ancestor_repository_dependencies_dict.get("repository_dependencies", [])
+        ancestor_tool_dependencies = ancestor_metadata_dict.get("tool_dependencies", {})
+        ancestor_workflows = ancestor_metadata_dict.get("workflows", [])
+        ancestor_data_manager = ancestor_metadata_dict.get("data_manager", {})
+        current_datatypes = self.metadata_dict.get("datatypes", [])
+        current_tools = self.metadata_dict.get("tools", [])
+        current_guids = [tool_dict["guid"] for tool_dict in current_tools]
         current_guids.sort()
-        current_readme_files = self.metadata_dict.get('readme_files', [])
-        current_repository_dependencies_dict = self.metadata_dict.get('repository_dependencies', {})
-        current_repository_dependencies = current_repository_dependencies_dict.get('repository_dependencies', [])
-        current_tool_dependencies = self.metadata_dict.get('tool_dependencies', {})
-        current_workflows = self.metadata_dict.get('workflows', [])
-        current_data_manager = self.metadata_dict.get('data_manager', {})
+        current_readme_files = self.metadata_dict.get("readme_files", [])
+        current_repository_dependencies_dict = self.metadata_dict.get("repository_dependencies", {})
+        current_repository_dependencies = current_repository_dependencies_dict.get("repository_dependencies", [])
+        current_tool_dependencies = self.metadata_dict.get("tool_dependencies", {})
+        current_workflows = self.metadata_dict.get("workflows", [])
+        current_data_manager = self.metadata_dict.get("data_manager", {})
         # Handle case where no metadata exists for either changeset.
         no_datatypes = not ancestor_datatypes and not current_datatypes
         no_readme_files = not ancestor_readme_files and not current_readme_files
         no_repository_dependencies = not ancestor_repository_dependencies and not current_repository_dependencies
         no_tool_dependencies = not ancestor_tool_dependencies and not current_tool_dependencies
         no_tools = not ancestor_guids and not current_guids
         no_workflows = not ancestor_workflows and not current_workflows
         no_data_manager = not ancestor_data_manager and not current_data_manager
-        if no_datatypes and no_readme_files and no_repository_dependencies and \
-                no_tool_dependencies and no_tools and no_workflows and \
-                no_data_manager:
+        if (
+            no_datatypes
+            and no_readme_files
+            and no_repository_dependencies
+            and no_tool_dependencies
+            and no_tools
+            and no_workflows
+            and no_data_manager
+        ):
             return self.NO_METADATA
         # Uncomment the following if we decide that README files should affect how installable
         # repository revisions are defined.  See the NOTE in self.compare_readme_files().
         # readme_file_comparision = self.compare_readme_files( ancestor_readme_files, current_readme_files )
-        repository_dependency_comparison = self.compare_repository_dependencies(ancestor_repository_dependencies,
-                                                                                current_repository_dependencies)
-        tool_dependency_comparison = self.compare_tool_dependencies(ancestor_tool_dependencies,
-                                                                    current_tool_dependencies)
+        repository_dependency_comparison = self.compare_repository_dependencies(
+            ancestor_repository_dependencies, current_repository_dependencies
+        )
+        tool_dependency_comparison = self.compare_tool_dependencies(
+            ancestor_tool_dependencies, current_tool_dependencies
+        )
         workflow_comparison = self.compare_workflows(ancestor_workflows, current_workflows)
         datatype_comparison = self.compare_datatypes(ancestor_datatypes, current_datatypes)
         data_manager_comparison = self.compare_data_manager(ancestor_data_manager, current_data_manager)
         # Handle case where all metadata is the same.
-        if ancestor_guids == current_guids and \
-                repository_dependency_comparison == self.EQUAL and \
-                tool_dependency_comparison == self.EQUAL and \
-                workflow_comparison == self.EQUAL and \
-                datatype_comparison == self.EQUAL and \
-                data_manager_comparison == self.EQUAL:
+        if (
+            ancestor_guids == current_guids
+            and repository_dependency_comparison == self.EQUAL
+            and tool_dependency_comparison == self.EQUAL
+            and workflow_comparison == self.EQUAL
+            and datatype_comparison == self.EQUAL
+            and data_manager_comparison == self.EQUAL
+        ):
             return self.EQUAL
         # Handle case where ancestor metadata is a subset of current metadata.
         # readme_file_is_subset = readme_file_comparision in [ self.EQUAL, self.SUBSET ]
         repository_dependency_is_subset = repository_dependency_comparison in self.SUBSET_VALUES
         tool_dependency_is_subset = tool_dependency_comparison in self.SUBSET_VALUES
         workflow_dependency_is_subset = workflow_comparison in self.SUBSET_VALUES
         datatype_is_subset = datatype_comparison in self.SUBSET_VALUES
         datamanager_is_subset = data_manager_comparison in self.SUBSET_VALUES
-        if repository_dependency_is_subset and tool_dependency_is_subset and \
-                workflow_dependency_is_subset and datatype_is_subset and \
-                datamanager_is_subset:
+        if (
+            repository_dependency_is_subset
+            and tool_dependency_is_subset
+            and workflow_dependency_is_subset
+            and datatype_is_subset
+            and datamanager_is_subset
+        ):
             is_subset = True
             for guid in ancestor_guids:
                 if guid not in current_guids:
                     is_subset = False
                     break
             if is_subset:
                 return self.SUBSET
         return self.NOT_EQUAL_AND_NOT_SUBSET
 
     def compare_data_manager(self, ancestor_metadata, current_metadata):
         """Determine if ancestor_metadata is the same as or a subset of current_metadata for data_managers."""
+
         def __data_manager_dict_to_tuple_list(metadata_dict):
             # we do not check tool_guid or tool conf file name
-            return set(sorted([(name,
-                                tuple(sorted(value.get('data_tables', []))),
-                                value.get('guid'),
-                                value.get('version'),
-                                value.get('name'),
-                                value.get('id')) for name, value in metadata_dict.items()]))
+            return set(
+                sorted(
+                    (
+                        name,
+                        tuple(sorted(value.get("data_tables", []))),
+                        value.get("guid"),
+                        value.get("version"),
+                        value.get("name"),
+                        value.get("id"),
+                    )
+                    for name, value in metadata_dict.items()
+                )
+            )
+
         # only compare valid entries, any invalid entries are ignored
-        ancestor_metadata = __data_manager_dict_to_tuple_list(ancestor_metadata.get('data_managers', {}))
-        current_metadata = __data_manager_dict_to_tuple_list(current_metadata.get('data_managers', {}))
+        ancestor_metadata = __data_manager_dict_to_tuple_list(ancestor_metadata.get("data_managers", {}))
+        current_metadata = __data_manager_dict_to_tuple_list(current_metadata.get("data_managers", {}))
         # use set comparisons
         if ancestor_metadata.issubset(current_metadata):
             if ancestor_metadata == current_metadata:
                 return self.EQUAL
             return self.SUBSET
         return self.NOT_EQUAL_AND_NOT_SUBSET
 
     def compare_datatypes(self, ancestor_datatypes, current_datatypes):
         """Determine if ancestor_datatypes is the same as or a subset of current_datatypes."""
         # Each datatype dict looks something like:
         # {"dtype": "galaxy.datatypes.images:Image", "extension": "pdf", "mimetype": "application/pdf"}
         if len(ancestor_datatypes) <= len(current_datatypes):
             for ancestor_datatype in ancestor_datatypes:
                 # Currently the only way to differentiate datatypes is by name.
-                ancestor_datatype_dtype = ancestor_datatype['dtype']
-                ancestor_datatype_extension = ancestor_datatype['extension']
-                ancestor_datatype_mimetype = ancestor_datatype.get('mimetype', None)
+                ancestor_datatype_dtype = ancestor_datatype["dtype"]
+                ancestor_datatype_extension = ancestor_datatype["extension"]
+                ancestor_datatype_mimetype = ancestor_datatype.get("mimetype", None)
                 found_in_current = False
                 for current_datatype in current_datatypes:
-                    if current_datatype['dtype'] == ancestor_datatype_dtype and \
-                            current_datatype['extension'] == ancestor_datatype_extension and \
-                            current_datatype.get('mimetype', None) == ancestor_datatype_mimetype:
+                    if (
+                        current_datatype["dtype"] == ancestor_datatype_dtype
+                        and current_datatype["extension"] == ancestor_datatype_extension
+                        and current_datatype.get("mimetype", None) == ancestor_datatype_mimetype
+                    ):
                         found_in_current = True
                         break
                 if not found_in_current:
                     return self.NOT_EQUAL_AND_NOT_SUBSET
             if len(ancestor_datatypes) == len(current_datatypes):
                 return self.EQUAL
             else:
@@ -234,38 +290,53 @@
         current_repository_dependencies.
         """
         # The list of repository_dependencies looks something like:
         # [["http://localhost:9009", "emboss_datatypes", "test", "ab03a2a5f407", "False", "False"]].
         # Create a string from each tuple in the list for easier comparison.
         if len(ancestor_repository_dependencies) <= len(current_repository_dependencies):
             for ancestor_tup in ancestor_repository_dependencies:
-                a_tool_shed, a_repo_name, a_repo_owner, a_changeset_revision, \
-                    a_prior_installation_required, \
-                    a_only_if_compiling_contained_td = ancestor_tup
+                (
+                    a_tool_shed,
+                    a_repo_name,
+                    a_repo_owner,
+                    a_changeset_revision,
+                    a_prior_installation_required,
+                    a_only_if_compiling_contained_td,
+                ) = ancestor_tup
                 cleaned_a_tool_shed = common_util.remove_protocol_from_tool_shed_url(a_tool_shed)
                 found_in_current = False
                 for current_tup in current_repository_dependencies:
-                    c_tool_shed, c_repo_name, c_repo_owner, \
-                        c_changeset_revision, c_prior_installation_required, \
-                        c_only_if_compiling_contained_td = current_tup
+                    (
+                        c_tool_shed,
+                        c_repo_name,
+                        c_repo_owner,
+                        c_changeset_revision,
+                        c_prior_installation_required,
+                        c_only_if_compiling_contained_td,
+                    ) = current_tup
                     cleaned_c_tool_shed = common_util.remove_protocol_from_tool_shed_url(c_tool_shed)
-                    if cleaned_c_tool_shed == cleaned_a_tool_shed and \
-                            c_repo_name == a_repo_name and \
-                            c_repo_owner == a_repo_owner and \
-                            c_changeset_revision == a_changeset_revision and \
-                            util.string_as_bool(c_prior_installation_required) == util.string_as_bool(a_prior_installation_required) and \
-                            util.string_as_bool(c_only_if_compiling_contained_td) == util.string_as_bool(a_only_if_compiling_contained_td):
+                    if (
+                        cleaned_c_tool_shed == cleaned_a_tool_shed
+                        and c_repo_name == a_repo_name
+                        and c_repo_owner == a_repo_owner
+                        and c_changeset_revision == a_changeset_revision
+                        and util.string_as_bool(c_prior_installation_required)
+                        == util.string_as_bool(a_prior_installation_required)
+                        and util.string_as_bool(c_only_if_compiling_contained_td)
+                        == util.string_as_bool(a_only_if_compiling_contained_td)
+                    ):
                         found_in_current = True
                         break
                 if not found_in_current:
                     # In some cases, the only difference between a dependency definition in the lists
                     # is the changeset_revision value.  We'll check to see if this is the case, and if
                     # the defined dependency is a repository that has metadata set only on its tip.
-                    if not self.different_revision_defines_tip_only_repository_dependency(ancestor_tup,
-                                                                                          current_repository_dependencies):
+                    if not self.different_revision_defines_tip_only_repository_dependency(
+                        ancestor_tup, current_repository_dependencies
+                    ):
                         return self.NOT_EQUAL_AND_NOT_SUBSET
                     return self.SUBSET
             if len(ancestor_repository_dependencies) == len(current_repository_dependencies):
                 return self.EQUAL
             else:
                 return self.SUBSET
         return self.NOT_EQUAL_AND_NOT_SUBSET
@@ -273,15 +344,15 @@
     def compare_tool_dependencies(self, ancestor_tool_dependencies, current_tool_dependencies):
         """
         Determine if ancestor_tool_dependencies is the same as or a subset of current_tool_dependencies.
         """
         # The tool_dependencies dictionary looks something like:
         # {'bwa/0.5.9': {'readme': 'some string', 'version': '0.5.9', 'type': 'package', 'name': 'bwa'}}
         if len(ancestor_tool_dependencies) <= len(current_tool_dependencies):
-            for ancestor_td_key, ancestor_requirements_dict in ancestor_tool_dependencies.items():
+            for ancestor_td_key in ancestor_tool_dependencies.keys():
                 if ancestor_td_key in current_tool_dependencies:
                     # The only values that could have changed between the 2 dictionaries are the
                     # "readme" or "type" values.  Changing the readme value makes no difference.
                     # Changing the type will change the installation process, but for now we'll
                     # assume it was a typo, so new metadata shouldn't be generated.
                     continue
                 else:
@@ -300,125 +371,140 @@
         """
         if len(ancestor_workflows) <= len(current_workflows):
             for ancestor_workflow_tup in ancestor_workflows:
                 # ancestor_workflows is a list of tuples where each contained tuple is
                 # [ <relative path to the .ga file in the repository>, <exported workflow dict> ]
                 ancestor_workflow_dict = ancestor_workflow_tup[1]
                 # Currently the only way to differentiate workflows is by name.
-                ancestor_workflow_name = ancestor_workflow_dict['name']
-                num_ancestor_workflow_steps = len(ancestor_workflow_dict['steps'])
+                ancestor_workflow_name = ancestor_workflow_dict["name"]
+                num_ancestor_workflow_steps = len(ancestor_workflow_dict["steps"])
                 found_in_current = False
                 for current_workflow_tup in current_workflows:
                     current_workflow_dict = current_workflow_tup[1]
                     # Assume that if the name and number of steps are euqal, then the workflows
                     # are the same.  Of course, this may not be true...
-                    if current_workflow_dict['name'] == ancestor_workflow_name and \
-                            len(current_workflow_dict['steps']) == num_ancestor_workflow_steps:
+                    if (
+                        current_workflow_dict["name"] == ancestor_workflow_name
+                        and len(current_workflow_dict["steps"]) == num_ancestor_workflow_steps
+                    ):
                         found_in_current = True
                         break
                 if not found_in_current:
                     return self.NOT_EQUAL_AND_NOT_SUBSET
             if len(ancestor_workflows) == len(current_workflows):
                 return self.EQUAL
             else:
                 return self.SUBSET
         return self.NOT_EQUAL_AND_NOT_SUBSET
 
     def create_or_update_repository_metadata(self, changeset_revision, metadata_dict):
         """Create or update a repository_metadata record in the tool shed."""
         has_repository_dependencies = False
         has_repository_dependencies_only_if_compiling_contained_td = False
-        includes_datatypes = False
         includes_tools = False
         includes_tool_dependencies = False
-        includes_workflows = False
         if metadata_dict:
-            repository_dependencies_dict = metadata_dict.get('repository_dependencies', {})
-            repository_dependencies = repository_dependencies_dict.get('repository_dependencies', [])
-            has_repository_dependencies, has_repository_dependencies_only_if_compiling_contained_td = \
-                repository_util.get_repository_dependency_types(repository_dependencies)
-            if 'datatypes' in metadata_dict:
-                includes_datatypes = True
-            if 'tools' in metadata_dict:
+            repository_dependencies_dict = metadata_dict.get("repository_dependencies", {})
+            repository_dependencies = repository_dependencies_dict.get("repository_dependencies", [])
+            (
+                has_repository_dependencies,
+                has_repository_dependencies_only_if_compiling_contained_td,
+            ) = repository_util.get_repository_dependency_types(repository_dependencies)
+            if "tools" in metadata_dict:
                 includes_tools = True
-            if 'tool_dependencies' in metadata_dict:
+            if "tool_dependencies" in metadata_dict:
                 includes_tool_dependencies = True
-            if 'workflows' in metadata_dict:
-                includes_workflows = True
-        if has_repository_dependencies or \
-                has_repository_dependencies_only_if_compiling_contained_td or \
-                includes_datatypes or includes_tools or \
-                includes_tool_dependencies or includes_workflows:
+        if (
+            has_repository_dependencies
+            or has_repository_dependencies_only_if_compiling_contained_td
+            or includes_tools
+            or includes_tool_dependencies
+        ):
             downloadable = True
         else:
             downloadable = False
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app,
-                                                                                          self.app.security.encode_id(self.repository.id),
-                                                                                          changeset_revision)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            self.app, self.app.security.encode_id(self.repository.id), changeset_revision
+        )
         if repository_metadata:
             repository_metadata.metadata = metadata_dict
             repository_metadata.downloadable = downloadable
             repository_metadata.has_repository_dependencies = has_repository_dependencies
-            repository_metadata.includes_datatypes = includes_datatypes
+            repository_metadata.includes_datatypes = False
             repository_metadata.includes_tools = includes_tools
             repository_metadata.includes_tool_dependencies = includes_tool_dependencies
-            repository_metadata.includes_workflows = includes_workflows
+            repository_metadata.includes_workflows = False
         else:
-            repository_metadata = \
-                self.app.model.RepositoryMetadata(repository_id=self.repository.id,
-                                                  changeset_revision=changeset_revision,
-                                                  metadata=metadata_dict,
-                                                  downloadable=downloadable,
-                                                  has_repository_dependencies=has_repository_dependencies,
-                                                  includes_datatypes=includes_datatypes,
-                                                  includes_tools=includes_tools,
-                                                  includes_tool_dependencies=includes_tool_dependencies,
-                                                  includes_workflows=includes_workflows)
+            repository_metadata = self.app.model.RepositoryMetadata(
+                repository_id=self.repository.id,
+                changeset_revision=changeset_revision,
+                metadata=metadata_dict,
+                downloadable=downloadable,
+                has_repository_dependencies=has_repository_dependencies,
+                includes_datatypes=False,
+                includes_tools=includes_tools,
+                includes_tool_dependencies=includes_tool_dependencies,
+                includes_workflows=False,
+            )
         # Always set the default values for the following columns.  When resetting all metadata
         # on a repository this will reset the values.
         repository_metadata.missing_test_components = False
         self.sa_session.add(repository_metadata)
         self.sa_session.flush()
 
         return repository_metadata
 
     def different_revision_defines_tip_only_repository_dependency(self, rd_tup, repository_dependencies):
         """
         Determine if the only difference between rd_tup and a dependency definition in the list of
         repository_dependencies is the changeset_revision value.
         """
-        rd_tool_shed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td = \
-            common_util.parse_repository_dependency_tuple(rd_tup)
+        (
+            rd_tool_shed,
+            rd_name,
+            rd_owner,
+            rd_changeset_revision,
+            rd_prior_installation_required,
+            rd_only_if_compiling_contained_td,
+        ) = common_util.parse_repository_dependency_tuple(rd_tup)
         cleaned_rd_tool_shed = common_util.remove_protocol_from_tool_shed_url(rd_tool_shed)
         for repository_dependency in repository_dependencies:
-            tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-                common_util.parse_repository_dependency_tuple(repository_dependency)
+            (
+                tool_shed,
+                name,
+                owner,
+                changeset_revision,
+                prior_installation_required,
+                only_if_compiling_contained_td,
+            ) = common_util.parse_repository_dependency_tuple(repository_dependency)
             cleaned_tool_shed = common_util.remove_protocol_from_tool_shed_url(tool_shed)
             if cleaned_rd_tool_shed == cleaned_tool_shed and rd_name == name and rd_owner == owner:
                 # Determine if the repository represented by the dependency tuple is an instance of the repository type TipOnly.
                 required_repository = repository_util.get_repository_by_name_and_owner(self.app, name, owner)
                 repository_type_class = self.app.repository_types_registry.get_class_by_label(required_repository.type)
                 return isinstance(repository_type_class, TipOnly)
         return False
 
     def get_parent_id(self, id, old_id, version, guid, changeset_revisions):
         parent_id = None
         # Compare from most recent to oldest.
         changeset_revisions.reverse()
         for changeset_revision in changeset_revisions:
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app, id, changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                self.app, id, changeset_revision
+            )
             metadata = repository_metadata.metadata
-            tools_dicts = metadata.get('tools', [])
+            tools_dicts = metadata.get("tools", [])
             for tool_dict in tools_dicts:
-                if tool_dict['guid'] == guid:
+                if tool_dict["guid"] == guid:
                     # The tool has not changed between the compared changeset revisions.
                     continue
-                if tool_dict['id'] == old_id and tool_dict['version'] != version:
+                if tool_dict["id"] == old_id and tool_dict["version"] != version:
                     # The tool version is different, so we've found the parent.
-                    return tool_dict['guid']
+                    return tool_dict["guid"]
         if parent_id is None:
             # The tool did not change through all of the changeset revisions.
             return old_id
 
     def get_query_for_setting_metadata_on_repositories(self, my_writable=False, order=True):
         """
         Return a query containing repositories for resetting metadata.  The order parameter
@@ -427,65 +513,69 @@
         """
         # When called from the Tool Shed API, the metadata is reset on all repositories of types
         # repository_suite_definition and tool_dependency_definition in addition to other selected
         # repositories.
         if my_writable:
             username = self.user.username
             clause_list = []
-            for repository in self.sa_session.query(self.app.model.Repository) \
-                                             .filter(self.app.model.Repository.table.c.deleted == false()):
+            for repository in self.sa_session.query(self.app.model.Repository).filter(
+                self.app.model.Repository.table.c.deleted == false()
+            ):
                 # Always reset metadata on all repositories of types repository_suite_definition and
                 # tool_dependency_definition.
                 if repository.type in [rt_util.REPOSITORY_SUITE_DEFINITION, rt_util.TOOL_DEPENDENCY_DEFINITION]:
                     clause_list.append(self.app.model.Repository.table.c.id == repository.id)
                 else:
                     allow_push = repository.allow_push()
                     if allow_push:
                         # Include all repositories that are writable by the current user.
-                        allow_push_usernames = allow_push.split(',')
+                        allow_push_usernames = allow_push.split(",")
                         if username in allow_push_usernames:
                             clause_list.append(self.app.model.Repository.table.c.id == repository.id)
             if clause_list:
                 if order:
-                    return self.sa_session.query(self.app.model.Repository) \
-                                          .filter(or_(*clause_list)) \
-                                          .order_by(self.app.model.Repository.table.c.name,
-                                                    self.app.model.Repository.table.c.user_id)
+                    return (
+                        self.sa_session.query(self.app.model.Repository)
+                        .filter(or_(*clause_list))
+                        .order_by(self.app.model.Repository.table.c.name, self.app.model.Repository.table.c.user_id)
+                    )
                 else:
-                    return self.sa_session.query(self.app.model.Repository) \
-                                          .filter(or_(*clause_list))
+                    return self.sa_session.query(self.app.model.Repository).filter(or_(*clause_list))
             else:
                 # Return an empty query.
-                return self.sa_session.query(self.app.model.Repository) \
-                                      .filter(self.app.model.Repository.table.c.id == -1)
+                return self.sa_session.query(self.app.model.Repository).filter(
+                    self.app.model.Repository.table.c.id == -1
+                )
         else:
             if order:
-                return self.sa_session.query(self.app.model.Repository) \
-                                      .filter(self.app.model.Repository.table.c.deleted == false()) \
-                                      .order_by(self.app.model.Repository.table.c.name,
-                                                self.app.model.Repository.table.c.user_id)
+                return (
+                    self.sa_session.query(self.app.model.Repository)
+                    .filter(self.app.model.Repository.table.c.deleted == false())
+                    .order_by(self.app.model.Repository.table.c.name, self.app.model.Repository.table.c.user_id)
+                )
             else:
-                return self.sa_session.query(self.app.model.Repository) \
-                                      .filter(self.app.model.Repository.table.c.deleted == false())
+                return self.sa_session.query(self.app.model.Repository).filter(
+                    self.app.model.Repository.table.c.deleted == false()
+                )
 
     def new_datatypes_metadata_required(self, repository_metadata):
         """
         Compare the last saved metadata for each datatype in the repository with the new metadata
         in self.metadata_dict to determine if a new repository_metadata table record is required
         or if the last saved metadata record can be updated for datatypes instead.
         """
         # Datatypes are stored in metadata as a list of dictionaries that looks like:
         # [{'dtype': 'galaxy.datatypes.data:Text', 'subclass': 'True', 'extension': 'acedb'}]
-        if 'datatypes' in self.metadata_dict:
-            current_datatypes = self.metadata_dict['datatypes']
+        if "datatypes" in self.metadata_dict:
+            current_datatypes = self.metadata_dict["datatypes"]
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    if 'datatypes' in metadata:
-                        ancestor_datatypes = metadata['datatypes']
+                    if "datatypes" in metadata:
+                        ancestor_datatypes = metadata["datatypes"]
                         # The saved metadata must be a subset of the new metadata.
                         datatype_comparison = self.compare_datatypes(ancestor_datatypes, current_datatypes)
                         if datatype_comparison == self.NOT_EQUAL_AND_NOT_SUBSET:
                             return True
                         else:
                             return False
                     else:
@@ -509,50 +599,53 @@
         This method compares the last stored repository_metadata record associated with self.repository
         against the contents of self.metadata_dict and returns True or False for the union set of Galaxy
         utilities contained in both metadata dictionaries.  The metadata contained in self.metadata_dict
         may not be a subset of that contained in the last stored repository_metadata record associated with
         self.repository because one or more Galaxy utilities may have been deleted from self.repository in
         the new tip.
         """
-        repository_metadata = metadata_util.get_latest_repository_metadata(self.app,
-                                                                           self.repository.id,
-                                                                           downloadable=False)
+        repository_metadata = metadata_util.get_latest_repository_metadata(
+            self.app, self.repository.id, downloadable=False
+        )
         datatypes_required = self.new_datatypes_metadata_required(repository_metadata)
         # Uncomment the following if we decide that README files should affect how installable
         # repository revisions are defined.  See the NOTE in the compare_readme_files() method.
         # readme_files_required = sewlf.new_readme_files_metadata_required( repository_metadata )
-        repository_dependencies_required = \
-            self.new_repository_dependency_metadata_required(repository_metadata)
+        repository_dependencies_required = self.new_repository_dependency_metadata_required(repository_metadata)
         tools_required = self.new_tool_metadata_required(repository_metadata)
         tool_dependencies_required = self.new_tool_dependency_metadata_required(repository_metadata)
         workflows_required = self.new_workflow_metadata_required(repository_metadata)
-        if datatypes_required or repository_dependencies_required or \
-                tools_required or tool_dependencies_required or workflows_required:
+        if (
+            datatypes_required
+            or repository_dependencies_required
+            or tools_required
+            or tool_dependencies_required
+            or workflows_required
+        ):
             return True
         return False
 
     def new_readme_files_metadata_required(self, repository_metadata):
         """
         Compare the last saved metadata for each readme file in the repository with the new metadata
         in self.metadata_dict to determine if a new repository_metadata table record is required or
         if the last saved metadata record can be updated for readme files instead.
         """
         # Repository README files are kind of a special case because they have no effect on reproducibility.
         # We'll simply inspect the file names to determine if any that exist in the saved metadata are
         # eliminated from the new metadata in self.metadata_dict.
-        if 'readme_files' in self.metadata_dict:
-            current_readme_files = self.metadata_dict['readme_files']
+        if "readme_files" in self.metadata_dict:
+            current_readme_files = self.metadata_dict["readme_files"]
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    if 'readme_files' in metadata:
-                        ancestor_readme_files = metadata['readme_files']
+                    if "readme_files" in metadata:
+                        ancestor_readme_files = metadata["readme_files"]
                         # The saved metadata must be a subset of the new metadata.
-                        readme_file_comparison = self.compare_readme_files(ancestor_readme_files,
-                                                                           current_readme_files)
+                        readme_file_comparison = self.compare_readme_files(ancestor_readme_files, current_readme_files)
                         if readme_file_comparison == self.NOT_EQUAL_AND_NOT_SUBSET:
                             return True
                         else:
                             return False
                     else:
                         # The new metadata includes readme_files, but the stored metadata does not, so
                         # we can update the stored metadata.
@@ -574,19 +667,21 @@
         Compare the last saved metadata for each repository dependency in the repository
         with the new metadata in self.metadata_dict to determine if a new repository_metadata
         table record is required or if the last saved metadata record can be updated for
         repository_dependencies instead.
         """
         if repository_metadata:
             metadata = repository_metadata.metadata
-            if 'repository_dependencies' in metadata:
-                saved_repository_dependencies = metadata['repository_dependencies']['repository_dependencies']
-                new_repository_dependencies_metadata = self.metadata_dict.get('repository_dependencies', None)
+            if "repository_dependencies" in metadata:
+                saved_repository_dependencies = metadata["repository_dependencies"]["repository_dependencies"]
+                new_repository_dependencies_metadata = self.metadata_dict.get("repository_dependencies", None)
                 if new_repository_dependencies_metadata:
-                    new_repository_dependencies = self.metadata_dict['repository_dependencies']['repository_dependencies']
+                    new_repository_dependencies = self.metadata_dict["repository_dependencies"][
+                        "repository_dependencies"
+                    ]
                     # TODO: We used to include the following here to handle the case where repository
                     # dependency definitions were deleted.  However this erroneously returned True in
                     # cases where is should not have done so.  This usually occurred where multiple single
                     # files were uploaded when a single tarball should have been.  We need to implement
                     # support for handling deleted repository dependency definitions so that we can guarantee
                     # reproducibility, but we need to do it in a way that is better than the following.
                     # for new_repository_dependency in new_repository_dependencies:
@@ -594,62 +689,63 @@
                     #         return True
                     # The saved metadata must be a subset of the new metadata.
                     for saved_repository_dependency in saved_repository_dependencies:
                         if saved_repository_dependency not in new_repository_dependencies:
                             # In some cases, the only difference between a dependency definition in the lists
                             # is the changeset_revision value.  We'll check to see if this is the case, and if
                             # the defined dependency is a repository that has metadata set only on its tip.
-                            if not self.different_revision_defines_tip_only_repository_dependency(saved_repository_dependency,
-                                                                                                  new_repository_dependencies):
+                            if not self.different_revision_defines_tip_only_repository_dependency(
+                                saved_repository_dependency, new_repository_dependencies
+                            ):
                                 return True
                     return False
                 else:
                     # The repository_dependencies.xml file must have been deleted, so create a new
                     # repository_metadata record so we always have access to the deleted file.
                     return True
             else:
                 return False
         else:
-            if 'repository_dependencies' in self.metadata_dict:
+            if "repository_dependencies" in self.metadata_dict:
                 # There is no saved repository metadata, so we need to create a new repository_metadata record.
                 return True
             else:
                 # self.metadata_dict includes no metadata for repository dependencies, so a new repository_metadata
                 # record is not needed.
                 return False
 
     def new_tool_metadata_required(self, repository_metadata):
         """
         Compare the last saved metadata for each tool in the repository with the new metadata in
         self.metadata_dict to determine if a new repository_metadata table record is required, or if
         the last saved metadata record can be updated instead.
         """
-        if 'tools' in self.metadata_dict:
+        if "tools" in self.metadata_dict:
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    if 'tools' in metadata:
+                    if "tools" in metadata:
                         saved_tool_ids = []
                         # The metadata for one or more tools was successfully generated in the past
                         # for this repository, so we first compare the version string for each tool id
                         # in self.metadata_dict with what was previously saved to see if we need to create
                         # a new table record or if we can simply update the existing record.
-                        for new_tool_metadata_dict in self.metadata_dict['tools']:
-                            for saved_tool_metadata_dict in metadata['tools']:
-                                if saved_tool_metadata_dict['id'] not in saved_tool_ids:
-                                    saved_tool_ids.append(saved_tool_metadata_dict['id'])
-                                if new_tool_metadata_dict['id'] == saved_tool_metadata_dict['id']:
-                                    if new_tool_metadata_dict['version'] != saved_tool_metadata_dict['version']:
+                        for new_tool_metadata_dict in self.metadata_dict["tools"]:
+                            for saved_tool_metadata_dict in metadata["tools"]:
+                                if saved_tool_metadata_dict["id"] not in saved_tool_ids:
+                                    saved_tool_ids.append(saved_tool_metadata_dict["id"])
+                                if new_tool_metadata_dict["id"] == saved_tool_metadata_dict["id"]:
+                                    if new_tool_metadata_dict["version"] != saved_tool_metadata_dict["version"]:
                                         return True
                         # So far, a new metadata record is not required, but we still have to check to see if
                         # any new tool ids exist in self.metadata_dict that are not in the saved metadata.  We do
                         # this because if a new tarball was uploaded to a repository that included tools, it
                         # may have removed existing tool files if they were not included in the uploaded tarball.
-                        for new_tool_metadata_dict in self.metadata_dict['tools']:
-                            if new_tool_metadata_dict['id'] not in saved_tool_ids:
+                        for new_tool_metadata_dict in self.metadata_dict["tools"]:
+                            if new_tool_metadata_dict["id"] not in saved_tool_ids:
                                 return True
                         return False
                     else:
                         # The new metadata includes tools, but the stored metadata does not, so we can
                         # update the stored metadata.
                         return False
                 else:
@@ -669,17 +765,17 @@
         Compare the last saved metadata for each tool dependency in the repository with the new
         metadata in self.metadata_dict to determine if a new repository_metadata table record is
         required or if the last saved metadata record can be updated for tool_dependencies instead.
         """
         if repository_metadata:
             metadata = repository_metadata.metadata
             if metadata:
-                if 'tool_dependencies' in metadata:
-                    saved_tool_dependencies = metadata['tool_dependencies']
-                    new_tool_dependencies = self.metadata_dict.get('tool_dependencies', None)
+                if "tool_dependencies" in metadata:
+                    saved_tool_dependencies = metadata["tool_dependencies"]
+                    new_tool_dependencies = self.metadata_dict.get("tool_dependencies", None)
                     if new_tool_dependencies:
                         # TODO: We used to include the following here to handle the case where
                         # tool dependency definitions were deleted.  However, this erroneously
                         # returned True in cases where is should not have done so.  This usually
                         # occurred where multiple single files were uploaded when a single tarball
                         # should have been.  We need to implement support for handling deleted
                         # tool dependency definitions so that we can guarantee reproducibility,
@@ -700,45 +796,45 @@
                 else:
                     return False
             else:
                 # We have repository metadata that does not include metadata for any tool dependencies
                 # in the repository, so we can update the existing repository metadata.
                 return False
         else:
-            if 'tool_dependencies' in self.metadata_dict:
+            if "tool_dependencies" in self.metadata_dict:
                 # There is no saved repository metadata, so we need to create a new repository_metadata
                 # record.
                 return True
             else:
                 # self.metadata_dict includes no metadata for tool dependencies, so a new repository_metadata
                 # record is not needed.
                 return False
 
     def new_workflow_metadata_required(self, repository_metadata):
         """
         Currently everything about an exported workflow except the name is hard-coded, so
         there's no real way to differentiate versions of exported workflows.  If this changes
         at some future time, this method should be enhanced accordingly.
         """
-        if 'workflows' in self.metadata_dict:
+        if "workflows" in self.metadata_dict:
             if repository_metadata:
                 # The repository has metadata, so update the workflows value -
                 # no new record is needed.
                 return False
             else:
                 # There is no saved repository metadata, so we need to create a
                 # new repository_metadata table record.
                 return True
         # self.metadata_dict includes no metadata for workflows, so a new
         # repository_metadata table record is not needed.
         return False
 
     def reset_all_metadata_on_repository_in_tool_shed(self):
         """Reset all metadata on a single repository in a tool shed."""
-        log.debug("Resetting all metadata on repository: %s" % self.repository.name)
+        log.debug(f"Resetting all metadata on repository: {self.repository.name}")
         repo = self.repository.hg_repo
         # The list of changeset_revisions refers to repository_metadata records that have been created
         # or updated.  When the following loop completes, we'll delete all repository_metadata records
         # for this repository that do not have a changeset_revision value in this list.
         changeset_revisions = []
         # When a new repository_metadata record is created, it always uses the values of
         # metadata_changeset_revision and metadata_dict.
@@ -764,15 +860,17 @@
                     if ancestor_changeset_revision:
                         # Compare metadata from ancestor and current.  The value of comparison will be one of:
                         # self.NO_METADATA - no metadata for either ancestor or current, so continue from current
                         # self.EQUAL - ancestor metadata is equivalent to current metadata, so continue from current
                         # self.SUBSET - ancestor metadata is a subset of current metadata, so continue from current
                         # self.NOT_EQUAL_AND_NOT_SUBSET - ancestor metadata is neither equal to nor a subset of current
                         # metadata, so persist ancestor metadata.
-                        comparison = self.compare_changeset_revisions(ancestor_changeset_revision, ancestor_metadata_dict)
+                        comparison = self.compare_changeset_revisions(
+                            ancestor_changeset_revision, ancestor_metadata_dict
+                        )
                         if comparison in [self.NO_METADATA, self.EQUAL, self.SUBSET]:
                             ancestor_changeset_revision = self.changeset_revision
                             ancestor_metadata_dict = self.metadata_dict
                         elif comparison == self.NOT_EQUAL_AND_NOT_SUBSET:
                             metadata_changeset_revision = ancestor_changeset_revision
                             metadata_dict = ancestor_metadata_dict
                             self.create_or_update_repository_metadata(metadata_changeset_revision, metadata_dict)
@@ -809,183 +907,186 @@
 
     def reset_all_tool_versions(self, repo):
         """Reset tool version lineage for those changeset revisions that include valid tools."""
         encoded_repository_id = self.app.security.encode_id(self.repository.id)
         changeset_revisions_that_contain_tools = []
         for changeset in repo.changelog:
             changeset_revision = str(repo[changeset])
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app,
-                                                                                              encoded_repository_id,
-                                                                                              changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                self.app, encoded_repository_id, changeset_revision
+            )
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    if metadata.get('tools', None):
+                    if metadata.get("tools", None):
                         changeset_revisions_that_contain_tools.append(changeset_revision)
         # The list of changeset_revisions_that_contain_tools is now filtered to contain only those that
         # are downloadable and contain tools.  If a repository includes tools, build a dictionary of
         # { 'tool id' : 'parent tool id' } pairs for each tool in each changeset revision.
         for index, changeset_revision in enumerate(changeset_revisions_that_contain_tools):
             tool_versions_dict = {}
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app,
-                                                                                              encoded_repository_id,
-                                                                                              changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                self.app, encoded_repository_id, changeset_revision
+            )
             metadata = repository_metadata.metadata
-            tool_dicts = metadata['tools']
+            tool_dicts = metadata["tools"]
             if index == 0:
                 # The first changeset_revision is a special case because it will have no ancestor
                 # changeset_revisions in which to match tools.  The parent tool id for tools in the
                 # first changeset_revision will be the "old_id" in the tool config.
                 for tool_dict in tool_dicts:
-                    tool_versions_dict[tool_dict['guid']] = tool_dict['id']
+                    tool_versions_dict[tool_dict["guid"]] = tool_dict["id"]
             else:
                 for tool_dict in tool_dicts:
-                    parent_id = self.get_parent_id(encoded_repository_id,
-                                                   tool_dict['id'],
-                                                   tool_dict['version'],
-                                                   tool_dict['guid'],
-                                                   changeset_revisions_that_contain_tools[0:index])
-                    tool_versions_dict[tool_dict['guid']] = parent_id
+                    parent_id = self.get_parent_id(
+                        encoded_repository_id,
+                        tool_dict["id"],
+                        tool_dict["version"],
+                        tool_dict["guid"],
+                        changeset_revisions_that_contain_tools[0:index],
+                    )
+                    tool_versions_dict[tool_dict["guid"]] = parent_id
             if tool_versions_dict:
                 repository_metadata.tool_versions = tool_versions_dict
                 self.sa_session.add(repository_metadata)
                 self.sa_session.flush()
 
     def reset_metadata_on_selected_repositories(self, **kwd):
         """
         Inspect the repository changelog to reset metadata for all appropriate changeset revisions.
         This method is called from both Galaxy and the Tool Shed.
         """
-        repository_ids = util.listify(kwd.get('repository_ids', None))
-        message = ''
-        status = 'done'
+        repository_ids = util.listify(kwd.get("repository_ids", None))
+        message = ""
+        status = "done"
         if repository_ids:
             successful_count = 0
             unsuccessful_count = 0
             for repository_id in repository_ids:
                 try:
                     repository = repository_util.get_repository_in_tool_shed(self.app, repository_id)
                     self.set_repository(repository)
                     self.resetting_all_metadata_on_repository = True
                     self.reset_all_metadata_on_repository_in_tool_shed()
                     if self.invalid_file_tups:
-                        message = tool_util.generate_message_for_invalid_tools(self.app,
-                                                                               self.invalid_file_tups,
-                                                                               repository,
-                                                                               None,
-                                                                               as_html=False)
+                        message = tool_util.generate_message_for_invalid_tools(
+                            self.app, self.invalid_file_tups, repository, None, as_html=False
+                        )
                         log.debug(message)
                         unsuccessful_count += 1
                     else:
-                        log.debug("Successfully reset metadata on repository %s owned by %s" %
-                            (str(repository.name), str(repository.user.username)))
+                        log.debug(
+                            "Successfully reset metadata on repository %s owned by %s"
+                            % (str(repository.name), str(repository.user.username))
+                        )
                         successful_count += 1
                 except Exception:
                     log.exception("Error attempting to reset metadata on repository %s", str(repository.name))
                     unsuccessful_count += 1
-            message = "Successfully reset metadata on %d %s.  " % \
-                (successful_count, inflector.cond_plural(successful_count, "repository"))
+            message = "Successfully reset metadata on %d %s.  " % (
+                successful_count,
+                inflector.cond_plural(successful_count, "repository"),
+            )
             if unsuccessful_count:
-                message += "Error setting metadata on %d %s - see the paster log for details.  " % \
-                    (unsuccessful_count, inflector.cond_plural(unsuccessful_count, "repository"))
+                message += "Error setting metadata on %d %s - see the paster log for details.  " % (
+                    unsuccessful_count,
+                    inflector.cond_plural(unsuccessful_count, "repository"),
+                )
         else:
-            message = 'Select at least one repository to on which to reset all metadata.'
-            status = 'error'
+            message = "Select at least one repository to on which to reset all metadata."
+            status = "error"
         return message, status
 
     def set_repository(self, repository):
-        super(RepositoryMetadataManager, self).set_repository(repository)
+        super().set_repository(repository)
         self.repository_clone_url = common_util.generate_clone_url_for_repository_in_tool_shed(self.user, repository)
 
-    def set_repository_metadata(self, host, content_alert_str='', **kwd):
+    def set_repository_metadata(self, host, content_alert_str="", **kwd):
         """
         Set metadata using the self.repository's current disk files, returning specific error
         messages (if any) to alert the repository owner that the changeset has problems.
         """
-        message = ''
-        status = 'done'
+        message = ""
+        status = "done"
         encoded_id = self.app.security.encode_id(self.repository.id)
         repo = self.repository.hg_repo
         self.generate_metadata_for_changeset_revision()
         if self.metadata_dict:
             repository_metadata = None
             repository_type_class = self.app.repository_types_registry.get_class_by_label(self.repository.type)
             tip_only = isinstance(repository_type_class, TipOnly)
             if not tip_only and self.new_metadata_required_for_utilities():
                 # Create a new repository_metadata table row.
-                repository_metadata = self.create_or_update_repository_metadata(self.repository.tip(),
-                                                                                self.metadata_dict)
+                repository_metadata = self.create_or_update_repository_metadata(
+                    self.repository.tip(), self.metadata_dict
+                )
                 # If this is the first record stored for this repository, see if we need to send any email alerts.
                 if len(self.repository.downloadable_revisions) == 1:
-                    suc.handle_email_alerts(self.app,
-                                            host,
-                                            self.repository,
-                                            content_alert_str='',
-                                            new_repo_alert=True,
-                                            admin_only=False)
+                    suc.handle_email_alerts(
+                        self.app, host, self.repository, content_alert_str="", new_repo_alert=True, admin_only=False
+                    )
             else:
                 # Update the latest stored repository metadata with the contents and attributes of self.metadata_dict.
-                repository_metadata = metadata_util.get_latest_repository_metadata(self.app,
-                                                                                   self.repository.id,
-                                                                                   downloadable=False)
+                repository_metadata = metadata_util.get_latest_repository_metadata(
+                    self.app, self.repository.id, downloadable=False
+                )
                 if repository_metadata:
                     downloadable = metadata_util.is_downloadable(self.metadata_dict)
                     # Update the last saved repository_metadata table row.
                     repository_metadata.changeset_revision = self.repository.tip()
                     repository_metadata.metadata = self.metadata_dict
                     repository_metadata.downloadable = downloadable
-                    if 'datatypes' in self.metadata_dict:
-                        repository_metadata.includes_datatypes = True
-                    else:
-                        repository_metadata.includes_datatypes = False
+                    repository_metadata.includes_datatypes = False
                     # We don't store information about the special type of repository dependency that is needed only for
                     # compiling a tool dependency defined for the dependent repository.
-                    repository_dependencies_dict = self.metadata_dict.get('repository_dependencies', {})
-                    repository_dependencies = repository_dependencies_dict.get('repository_dependencies', [])
-                    has_repository_dependencies, has_repository_dependencies_only_if_compiling_contained_td = \
-                        repository_util.get_repository_dependency_types(repository_dependencies)
+                    repository_dependencies_dict = self.metadata_dict.get("repository_dependencies", {})
+                    repository_dependencies = repository_dependencies_dict.get("repository_dependencies", [])
+                    (
+                        has_repository_dependencies,
+                        has_repository_dependencies_only_if_compiling_contained_td,
+                    ) = repository_util.get_repository_dependency_types(repository_dependencies)
                     repository_metadata.has_repository_dependencies = has_repository_dependencies
-                    if 'tool_dependencies' in self.metadata_dict:
+                    if "tool_dependencies" in self.metadata_dict:
                         repository_metadata.includes_tool_dependencies = True
                     else:
                         repository_metadata.includes_tool_dependencies = False
-                    if 'tools' in self.metadata_dict:
+                    if "tools" in self.metadata_dict:
                         repository_metadata.includes_tools = True
                     else:
                         repository_metadata.includes_tools = False
-                    if 'workflows' in self.metadata_dict:
-                        repository_metadata.includes_workflows = True
-                    else:
-                        repository_metadata.includes_workflows = False
+                    repository_metadata.includes_workflows = False
                     repository_metadata.missing_test_components = False
                     self.sa_session.add(repository_metadata)
                     self.sa_session.flush()
                 else:
                     # There are no metadata records associated with the repository.
-                    repository_metadata = self.create_or_update_repository_metadata(self.repository.tip(),
-                                                                                    self.metadata_dict)
-            if 'tools' in self.metadata_dict and repository_metadata and status != 'error':
+                    repository_metadata = self.create_or_update_repository_metadata(
+                        self.repository.tip(), self.metadata_dict
+                    )
+            if "tools" in self.metadata_dict and repository_metadata and status != "error":
                 # Set tool versions on the new downloadable change set.  The order of the list of changesets is
                 # critical, so we use the repo's changelog.
                 changeset_revisions = []
                 for changeset in repo.changelog:
                     changeset_revision = str(repo[changeset])
-                    if metadata_util.get_repository_metadata_by_changeset_revision(self.app, encoded_id, changeset_revision):
+                    if metadata_util.get_repository_metadata_by_changeset_revision(
+                        self.app, encoded_id, changeset_revision
+                    ):
                         changeset_revisions.append(changeset_revision)
                 self.add_tool_versions(encoded_id, repository_metadata, changeset_revisions)
         elif len(repo) == 1 and not self.invalid_file_tups:
-            message = "Revision <b>%s</b> includes no Galaxy utilities for which metadata can " % \
-                str(self.repository.tip())
+            message = "Revision <b>%s</b> includes no Galaxy utilities for which metadata can " % str(
+                self.repository.tip()
+            )
             message += "be defined so this revision cannot be automatically installed into a local Galaxy instance."
             status = "error"
         if self.invalid_file_tups:
-            message = tool_util.generate_message_for_invalid_tools(self.app,
-                                                                   self.invalid_file_tups,
-                                                                   self.repository,
-                                                                   self.metadata_dict)
-            status = 'error'
+            message = tool_util.generate_message_for_invalid_tools(
+                self.app, self.invalid_file_tups, self.repository, self.metadata_dict
+            )
+            status = "error"
         return message, status
 
     def set_repository_metadata_due_to_new_tip(self, host, content_alert_str=None, **kwd):
         """Set metadata on the tip of self.repository in the tool shed."""
         error_message, status = self.set_repository_metadata(host, content_alert_str=content_alert_str, **kwd)
         return status, error_message
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/repository_registry.py` & `galaxy-web-apps-23.0.2/tool_shed/repository_registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,26 @@
 import logging
 
-from sqlalchemy import and_, false, or_
+from sqlalchemy import (
+    and_,
+    false,
+    or_,
+)
 
 import tool_shed.repository_types.util as rt_util
-from tool_shed.util import hg_util
-from tool_shed.util import metadata_util
+from tool_shed.util import (
+    hg_util,
+    metadata_util,
+)
 from tool_shed.webapp import model
 
 log = logging.getLogger(__name__)
 
 
-class Registry(object):
-
+class Registry:
     def __init__(self, app):
         log.debug("Loading the repository registry...")
         self.app = app
         self.certified_level_one_clause_list = self.get_certified_level_one_clause_list()
         # The following lists contain tuples like ( repository.name, repository.user.username, changeset_revision )
         # where the changeset_revision entry is always the latest installable changeset_revision..
         self.certified_level_one_repository_and_suite_tuples = []
@@ -136,55 +141,64 @@
             self.certified_level_one_viewable_suites_by_category[new_name] = val
         else:
             self.certified_level_one_viewable_suites_by_category[new_name] = 0
 
     def get_certified_level_one_clause_list(self):
         certified_level_one_tuples = []
         clause_list = []
-        for repository in self.sa_session.query(model.Repository) \
-                                         .filter(and_(model.Repository.table.c.deleted == false(),
-                                                      model.Repository.table.c.deprecated == false())):
+        for repository in self.sa_session.query(model.Repository).filter(
+            and_(model.Repository.table.c.deleted == false(), model.Repository.table.c.deprecated == false())
+        ):
             certified_level_one_tuple = self.get_certified_level_one_tuple(repository)
             latest_installable_changeset_revision, is_level_one_certified = certified_level_one_tuple
             if is_level_one_certified:
                 certified_level_one_tuples.append(certified_level_one_tuple)
-                clause_list.append(and_(
-                    model.RepositoryMetadata.table.c.repository_id == repository.id,
-                    model.RepositoryMetadata.table.c.changeset_revision == latest_installable_changeset_revision))
+                clause_list.append(
+                    and_(
+                        model.RepositoryMetadata.table.c.repository_id == repository.id,
+                        model.RepositoryMetadata.table.c.changeset_revision == latest_installable_changeset_revision,
+                    )
+                )
         return clause_list
 
     def get_certified_level_one_tuple(self, repository):
         """
         Return True if the latest installable changeset_revision of the received repository is level one certified.
         """
         if repository is None:
             return (None, False)
         if repository.deleted or repository.deprecated:
             return (None, False)
         # Get the latest installable changeset revision since that is all that is currently configured for testing.
-        latest_installable_changeset_revision = metadata_util.get_latest_downloadable_changeset_revision(self.app, repository)
+        latest_installable_changeset_revision = metadata_util.get_latest_downloadable_changeset_revision(
+            self.app, repository
+        )
         if latest_installable_changeset_revision not in [None, hg_util.INITIAL_CHANGELOG_HASH]:
             encoded_repository_id = self.app.security.encode_id(repository.id)
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app,
-                                                                                              encoded_repository_id,
-                                                                                              latest_installable_changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                self.app, encoded_repository_id, latest_installable_changeset_revision
+            )
             if repository_metadata:
                 # No repository_metadata.
                 return (latest_installable_changeset_revision, True)
         else:
             # No installable changeset_revision.
             return (None, False)
 
     def is_level_one_certified(self, repository_metadata):
         if repository_metadata:
             repository = repository_metadata.repository
             if repository:
                 if repository.deprecated or repository.deleted:
                     return False
-                tuple = (str(repository.name), str(repository.user.username), str(repository_metadata.changeset_revision))
+                tuple = (
+                    str(repository.name),
+                    str(repository.user.username),
+                    str(repository_metadata.changeset_revision),
+                )
                 if repository.type in [rt_util.REPOSITORY_SUITE_DEFINITION]:
                     return tuple in self.certified_level_one_suite_tuples
                 else:
                     return tuple in self.certified_level_one_repository_and_suite_tuples
         return False
 
     def is_valid(self, repository):
@@ -216,24 +230,27 @@
                 self.repository_and_suite_tuples.append(tuple)
             if repository.type == rt_util.REPOSITORY_SUITE_DEFINITION:
                 if tuple not in self.suite_tuples:
                     self.suite_tuples.append(tuple)
 
     def load_repository_and_suite_tuples(self):
         # Load self.certified_level_one_repository_and_suite_tuples and self.certified_level_one_suite_tuples.
-        for repository in self.sa_session.query(model.Repository) \
-                                         .join(model.RepositoryMetadata.table) \
-                                         .filter(or_(*self.certified_level_one_clause_list)) \
-                                         .join(model.User.table):
+        for repository in (
+            self.sa_session.query(model.Repository)
+            .join(model.RepositoryMetadata.table)
+            .filter(or_(*self.certified_level_one_clause_list))
+            .join(model.User.table)
+        ):
             self.load_certified_level_one_repository_and_suite_tuple(repository)
         # Load self.repository_and_suite_tuples and self.suite_tuples
-        for repository in self.sa_session.query(model.Repository) \
-                                         .filter(and_(model.Repository.table.c.deleted == false(),
-                                                      model.Repository.table.c.deprecated == false())) \
-                                         .join(model.User.table):
+        for repository in (
+            self.sa_session.query(model.Repository)
+            .filter(and_(model.Repository.table.c.deleted == false(), model.Repository.table.c.deprecated == false()))
+            .join(model.User.table)
+        ):
             self.load_repository_and_suite_tuple(repository)
 
     def load_viewable_repositories_and_suites_by_category(self):
         # Clear all dictionaries just in case they were previously loaded.
         self.certified_level_one_viewable_repositories_and_suites_by_category = {}
         self.certified_level_one_viewable_suites_by_category = {}
         self.certified_level_two_viewable_repositories_and_suites_by_category = {}
@@ -258,17 +275,17 @@
                 self.viewable_valid_suites_by_category[category_name] = 0
             for rca in category.repositories:
                 repository = rca.repository
                 if not repository.deleted and not repository.deprecated:
                     is_valid = self.is_valid(repository)
                     encoded_repository_id = self.app.security.encode_id(repository.id)
                     tip_changeset_hash = repository.tip()
-                    repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app,
-                                                                                                      encoded_repository_id,
-                                                                                                      tip_changeset_hash)
+                    repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                        self.app, encoded_repository_id, tip_changeset_hash
+                    )
                     self.viewable_repositories_and_suites_by_category[category_name] += 1
                     if is_valid:
                         self.viewable_valid_repositories_and_suites_by_category[category_name] += 1
                     if repository.type in [rt_util.REPOSITORY_SUITE_DEFINITION]:
                         self.viewable_suites_by_category[category_name] += 1
                         if is_valid:
                             self.viewable_valid_suites_by_category[category_name] += 1
@@ -323,15 +340,17 @@
                                 if self.viewable_valid_suites_by_category[category_name] > 0:
                                     self.viewable_valid_suites_by_category[category_name] -= 1
                             else:
                                 self.viewable_valid_suites_by_category[category_name] = 0
                     if is_level_one_certified:
                         if category_name in self.certified_level_one_viewable_repositories_and_suites_by_category:
                             if self.certified_level_one_viewable_repositories_and_suites_by_category[category_name] > 0:
-                                self.certified_level_one_viewable_repositories_and_suites_by_category[category_name] -= 1
+                                self.certified_level_one_viewable_repositories_and_suites_by_category[
+                                    category_name
+                                ] -= 1
                         else:
                             self.certified_level_one_viewable_repositories_and_suites_by_category[category_name] = 0
                         if repository.type == rt_util.REPOSITORY_SUITE_DEFINITION:
                             if category_name in self.certified_level_one_viewable_suites_by_category:
                                 if self.certified_level_one_viewable_suites_by_category[category_name] > 0:
                                     self.certified_level_one_viewable_suites_by_category[category_name] -= 1
                             else:
@@ -343,15 +362,15 @@
             # The viewable repository numbers and the categorized (filtered) lists of repository tuples
             # may be slightly skewed, but that is no reason to result in a potential server error.  All
             # will be corrected at next server start.
             log.exception("Handled error removing entry from repository registry")
 
     @property
     def sa_session(self):
-        return self.app.model.context.current
+        return self.app.model.session
 
     def unload_certified_level_one_repository_and_suite_tuple(self, repository):
         # The received repository has been determined to be level one certified.
         name = str(repository.name)
         owner = str(repository.user.username)
         tip_changeset_hash = repository.tip()
         if tip_changeset_hash != hg_util.INITIAL_CHANGELOG_HASH:
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/repository_types/metadata.py` & `galaxy-web-apps-23.0.2/tool_shed/repository_types/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,24 @@
 import logging
 
 log = logging.getLogger(__name__)
 
 
-class Metadata(object):
-
+class Metadata:
     def __init__(self):
         self.type = None
 
     def get_changesets_for_setting_metadata(self, app, repository):
         repo = repository.hg_repo
         return repo.changelog
 
     def is_valid_for_type(self, repository, revisions_to_check=None):
         raise Exception("Unimplemented Method")
 
 
 class TipOnly(Metadata):
-
     def __init__(self):
         self.type = None
 
     def get_changesets_for_setting_metadata(self, app, repository):
         repo = repository.hg_repo
         return [repo.changelog.tip()]
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/repository_types/registry.py` & `galaxy-web-apps-23.0.2/tool_shed/repository_types/registry.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 import logging
-from collections import OrderedDict
 
 from . import (
     repository_suite_definition,
     tool_dependency_definition,
-    unrestricted
+    unrestricted,
 )
 
 log = logging.getLogger(__name__)
 
 
-class Registry(object):
-
+class Registry:
     def __init__(self):
-        self.repository_types_by_label = OrderedDict()
-        self.repository_types_by_label['unrestricted'] = unrestricted.Unrestricted()
-        self.repository_types_by_label['repository_suite_definition'] = repository_suite_definition.RepositorySuiteDefinition()
-        self.repository_types_by_label['tool_dependency_definition'] = tool_dependency_definition.ToolDependencyDefinition()
+        self.repository_types_by_label = {}
+        self.repository_types_by_label["unrestricted"] = unrestricted.Unrestricted()
+        self.repository_types_by_label[
+            "repository_suite_definition"
+        ] = repository_suite_definition.RepositorySuiteDefinition()
+        self.repository_types_by_label[
+            "tool_dependency_definition"
+        ] = tool_dependency_definition.ToolDependencyDefinition()
 
     def get_class_by_label(self, label):
         return self.repository_types_by_label.get(label, None)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/repository_types/repository_suite_definition.py` & `galaxy-web-apps-23.0.2/tool_shed/repository_types/repository_suite_definition.py`

 * *Files 14% similar despite different names*

```diff
@@ -5,19 +5,18 @@
 from tool_shed.repository_types.metadata import TipOnly
 from tool_shed.util import basic_util
 
 log = logging.getLogger(__name__)
 
 
 class RepositorySuiteDefinition(TipOnly):
-
     def __init__(self):
         self.type = rt_util.REPOSITORY_SUITE_DEFINITION
-        self.label = 'Repository suite definition'
-        self.valid_file_names = ['repository_dependencies.xml']
+        self.label = "Repository suite definition"
+        self.valid_file_names = ["repository_dependencies.xml"]
 
     def is_valid_for_type(self, repository, revisions_to_check=None):
         """
         Inspect the received repository's contents to determine if they abide by the rules defined for
         the contents of this type.  If the received revisions_to_check is a list of changeset revisions,
         then inspection will be restricted to the revisions in the list.
         """
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/repository_types/tool_dependency_definition.py` & `galaxy-web-apps-23.0.2/tool_shed/repository_types/tool_dependency_definition.py`

 * *Files 9% similar despite different names*

```diff
@@ -5,19 +5,18 @@
 from tool_shed.repository_types.metadata import TipOnly
 from tool_shed.util import basic_util
 
 log = logging.getLogger(__name__)
 
 
 class ToolDependencyDefinition(TipOnly):
-
     def __init__(self):
         self.type = rt_util.TOOL_DEPENDENCY_DEFINITION
-        self.label = 'Tool dependency definition'
-        self.valid_file_names = ['tool_dependencies.xml']
+        self.label = "Tool dependency definition"
+        self.valid_file_names = ["tool_dependencies.xml"]
 
     def is_valid_for_type(self, repository, revisions_to_check=None):
         """
         Inspect the received repository's contents to determine if they abide by the rules defined for the contents of this type.
         If the received revisions_to_check is a list of changeset revisions, then inspection will be restricted to the revisions
         in the list.
         """
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/repository_types/unrestricted.py` & `galaxy-web-apps-23.0.2/tool_shed/repository_types/unrestricted.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,18 +3,17 @@
 import tool_shed.repository_types.util as rt_util
 from tool_shed.repository_types.metadata import Metadata
 
 log = logging.getLogger(__name__)
 
 
 class Unrestricted(Metadata):
-
     def __init__(self):
         self.type = rt_util.UNRESTRICTED
-        self.label = 'Unrestricted'
+        self.label = "Unrestricted"
 
     def is_valid_for_type(self, repository, revisions_to_check=None):
         """A repository's type can only be changed to the unrestricted type if it is new or has never been installed."""
         if repository.is_new():
             return True
         if repository.times_downloaded == 0:
             return True
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/repository_types/util.py` & `galaxy-web-apps-23.0.2/tool_shed/repository_types/util.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,22 +9,22 @@
     UNRESTRICTED,
 )
 from galaxy.web.form_builder import SelectField
 
 log = logging.getLogger(__name__)
 
 
-def build_repository_type_select_field(trans, repository=None, name='repository_type'):
+def build_repository_type_select_field(trans, repository=None, name="repository_type"):
     """Called from the Tool Shed to generate the current list of supported repository types."""
     if repository:
         selected_type = str(repository.type)
     else:
         selected_type = None
     repository_type_select_field = SelectField(name=name)
-    for type_label, type_class in trans.app.repository_types_registry.repository_types_by_label.items():
+    for type_class in trans.app.repository_types_registry.repository_types_by_label.values():
         option_label = str(type_class.label)
         option_value = str(type_class.type)
         if selected_type and selected_type == option_value:
             selected = True
         else:
             selected = False
         if repository:
@@ -34,36 +34,46 @@
                 repository_type_select_field.add_option(option_label, option_value, selected=selected)
         else:
             repository_type_select_field.add_option(option_label, option_value, selected=selected)
     return repository_type_select_field
 
 
 def generate_message_for_repository_type_change(app, repository):
-    message = ''
+    message = ""
     if repository.can_change_type_to(app, REPOSITORY_SUITE_DEFINITION):
-        repository_suite_definition_type_class = \
-            app.repository_types_registry.get_class_by_label(REPOSITORY_SUITE_DEFINITION)
-        message += "This repository currently contains a single file named <b>%s</b>.  If the intent of this repository is " % \
-            REPOSITORY_DEPENDENCY_DEFINITION_FILENAME
+        repository_suite_definition_type_class = app.repository_types_registry.get_class_by_label(
+            REPOSITORY_SUITE_DEFINITION
+        )
+        message += (
+            "This repository currently contains a single file named <b>%s</b>.  If the intent of this repository is "
+            % REPOSITORY_DEPENDENCY_DEFINITION_FILENAME
+        )
         message += "to define relationships to a collection of repositories that contain related Galaxy utilities with "
-        message += "no plans to add additional files, consider setting its type to <b>%s</b>.<br/>" % \
-            repository_suite_definition_type_class.label
+        message += (
+            "no plans to add additional files, consider setting its type to <b>%s</b>.<br/>"
+            % repository_suite_definition_type_class.label
+        )
     elif repository.can_change_type_to(app, TOOL_DEPENDENCY_DEFINITION):
-        tool_dependency_definition_type_class = \
-            app.repository_types_registry.get_class_by_label(TOOL_DEPENDENCY_DEFINITION)
-        message += "This repository currently contains a single file named <b>%s</b>.  If additional files will " % \
-            TOOL_DEPENDENCY_DEFINITION_FILENAME
-        message += "not be added to this repository, consider setting its type to <b>%s</b>.<br/>" % \
-            tool_dependency_definition_type_class.label
+        tool_dependency_definition_type_class = app.repository_types_registry.get_class_by_label(
+            TOOL_DEPENDENCY_DEFINITION
+        )
+        message += (
+            "This repository currently contains a single file named <b>%s</b>.  If additional files will "
+            % TOOL_DEPENDENCY_DEFINITION_FILENAME
+        )
+        message += (
+            "not be added to this repository, consider setting its type to <b>%s</b>.<br/>"
+            % tool_dependency_definition_type_class.label
+        )
     return message
 
 
 __all__ = (
-    'build_repository_type_select_field',
-    'generate_message_for_repository_type_change',
-    'REPOSITORY_DEPENDENCY_DEFINITION_FILENAME',
-    'REPOSITORY_SUITE_DEFINITION',
-    'TOOL_DEPENDENCY_DEFINITION',
-    'TOOL_DEPENDENCY_DEFINITION_FILENAME',
-    'UNRESTRICTED',
-    'types',
+    "build_repository_type_select_field",
+    "generate_message_for_repository_type_change",
+    "REPOSITORY_DEPENDENCY_DEFINITION_FILENAME",
+    "REPOSITORY_SUITE_DEFINITION",
+    "TOOL_DEPENDENCY_DEFINITION",
+    "TOOL_DEPENDENCY_DEFINITION_FILENAME",
+    "UNRESTRICTED",
+    "types",
 )
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/test/functional/test_1030_install_repository_with_dependency_revisions.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/api/repository_revisions.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,159 +1,221 @@
-from ..base.twilltestcase import common, ShedTwillTestCase
-
-datatypes_repository_name = 'emboss_datatypes_0030'
-datatypes_repository_description = "Galaxy applicable data formats used by Emboss tools."
-datatypes_repository_long_description = "Galaxy applicable data formats used by Emboss tools.  This repository contains no tools."
-
-emboss_repository_name = 'emboss_0030'
-emboss_5_repository_name = 'emboss_5_0030'
-emboss_6_repository_name = 'emboss_6_0030'
-emboss_repository_description = 'Galaxy wrappers for Emboss version 5.0.0 tools for test 0030'
-emboss_repository_long_description = 'Galaxy wrappers for Emboss version 5.0.0 tools for test 0030'
-
-base_datatypes_count = 0
-repository_datatypes_count = 0
-running_standalone = False
-
-
-class RepositoryWithDependencyRevisions(ShedTwillTestCase):
-    '''Test installing a repository with dependency revisions.'''
-
-    def test_0000_initiate_users(self):
-        """Create necessary user accounts."""
-        self.login(email=common.test_user_1_email, username=common.test_user_1_name)
-        test_user_1 = self.test_db_util.get_user(common.test_user_1_email)
-        assert test_user_1 is not None, 'Problem retrieving user with email %s from the database' % common.test_user_1_email
-        self.test_db_util.get_private_role(test_user_1)
-        self.login(email=common.admin_email, username=common.admin_username)
-        admin_user = self.test_db_util.get_user(common.admin_email)
-        assert admin_user is not None, 'Problem retrieving user with email %s from the database' % common.admin_email
-        self.test_db_util.get_private_role(admin_user)
-        self.galaxy_login(email=common.admin_email, username=common.admin_username)
-        galaxy_admin_user = self.test_db_util.get_galaxy_user(common.admin_email)
-        assert galaxy_admin_user is not None, 'Problem retrieving user with email %s from the database' % common.admin_email
-        self.test_db_util.get_galaxy_private_role(galaxy_admin_user)
-
-    def test_0005_ensure_repositories_and_categories_exist(self):
-        '''Create the 0030 category and add repositories to it, if necessary.'''
-        global repository_datatypes_count
-        global running_standalone
-        category = self.create_category(name='Test 0030 Repository Dependency Revisions', description='Test 0030 Repository Dependency Revisions')
-        self.login(email=common.test_user_1_email, username=common.test_user_1_name)
-        datatypes_repository = self.get_or_create_repository(name=datatypes_repository_name,
-                                                             description=datatypes_repository_description,
-                                                             long_description=datatypes_repository_long_description,
-                                                             owner=common.test_user_1_name,
-                                                             category_id=self.security.encode_id(category.id),
-                                                             strings_displayed=[])
-        if self.repository_is_new(datatypes_repository):
-            running_standalone = True
-            self.upload_file(datatypes_repository,
-                             filename='emboss/datatypes/datatypes_conf.xml',
-                             filepath=None,
-                             valid_tools_only=True,
-                             uncompress_file=False,
-                             remove_repo_files_not_in_tar=False,
-                             commit_message='Uploaded datatypes_conf.xml.',
-                             strings_displayed=[],
-                             strings_not_displayed=[])
-            emboss_5_repository = self.get_or_create_repository(name=emboss_5_repository_name,
-                                                                description=emboss_repository_description,
-                                                                long_description=emboss_repository_long_description,
-                                                                owner=common.test_user_1_name,
-                                                                category_id=self.security.encode_id(category.id),
-                                                                strings_displayed=[])
-            self.upload_file(emboss_5_repository,
-                             filename='emboss/emboss.tar',
-                             filepath=None,
-                             valid_tools_only=True,
-                             uncompress_file=False,
-                             remove_repo_files_not_in_tar=False,
-                             commit_message='Uploaded tool tarball.',
-                             strings_displayed=[],
-                             strings_not_displayed=[])
-            repository_dependencies_path = self.generate_temp_path('test_1030', additional_paths=['emboss', '5'])
-            datatypes_tuple = (self.url, datatypes_repository.name, datatypes_repository.user.username, self.get_repository_tip(datatypes_repository))
-            self.create_repository_dependency(repository=emboss_5_repository, repository_tuples=[datatypes_tuple], filepath=repository_dependencies_path)
-            emboss_6_repository = self.get_or_create_repository(name=emboss_6_repository_name,
-                                                                description=emboss_repository_description,
-                                                                long_description=emboss_repository_long_description,
-                                                                owner=common.test_user_1_name,
-                                                                category_id=self.security.encode_id(category.id),
-                                                                strings_displayed=[])
-            self.upload_file(emboss_6_repository,
-                             filename='emboss/emboss.tar',
-                             filepath=None,
-                             valid_tools_only=True,
-                             uncompress_file=False,
-                             remove_repo_files_not_in_tar=False,
-                             commit_message='Uploaded tool tarball.',
-                             strings_displayed=[],
-                             strings_not_displayed=[])
-            repository_dependencies_path = self.generate_temp_path('test_1030', additional_paths=['emboss', '6'])
-            datatypes_tuple = (self.url, datatypes_repository.name, datatypes_repository.user.username, self.get_repository_tip(datatypes_repository))
-            self.create_repository_dependency(repository=emboss_6_repository, repository_tuples=[datatypes_tuple], filepath=repository_dependencies_path)
-            emboss_repository = self.get_or_create_repository(name=emboss_repository_name,
-                                                              description=emboss_repository_description,
-                                                              long_description=emboss_repository_long_description,
-                                                              owner=common.test_user_1_name,
-                                                              category_id=self.security.encode_id(category.id),
-                                                              strings_displayed=[])
-            self.upload_file(emboss_repository,
-                             filename='emboss/emboss.tar',
-                             filepath=None,
-                             valid_tools_only=True,
-                             uncompress_file=False,
-                             remove_repo_files_not_in_tar=False,
-                             commit_message='Uploaded tool tarball.',
-                             strings_displayed=[],
-                             strings_not_displayed=[])
-            repository_dependencies_path = self.generate_temp_path('test_1030', additional_paths=['emboss', '5'])
-            dependency_tuple = (self.url, emboss_5_repository.name, emboss_5_repository.user.username, self.get_repository_tip(emboss_5_repository))
-            self.create_repository_dependency(repository=emboss_repository, repository_tuples=[dependency_tuple], filepath=repository_dependencies_path)
-            dependency_tuple = (self.url, emboss_6_repository.name, emboss_6_repository.user.username, self.get_repository_tip(emboss_6_repository))
-            self.create_repository_dependency(repository=emboss_repository, repository_tuples=[dependency_tuple], filepath=repository_dependencies_path)
-        repository_datatypes_count = int(self.get_repository_datatypes_count(datatypes_repository))
-
-    def test_0010_browse_tool_shed(self):
-        """Browse the available tool sheds in this Galaxy instance and preview the emboss tool."""
-        self.galaxy_login(email=common.admin_email, username=common.admin_username)
-        self.browse_tool_shed(url=self.url, strings_displayed=['Test 0030 Repository Dependency Revisions'])
-        category = self.test_db_util.get_category_by_name('Test 0030 Repository Dependency Revisions')
-        self.browse_category(category, strings_displayed=['emboss_0030'])
-        self.preview_repository_in_tool_shed('emboss_0030', common.test_user_1_name, strings_displayed=['emboss_0030', 'Valid tools'])
-
-    def test_0015_install_emboss_repository(self):
-        '''Install the emboss repository without installing tool dependencies.'''
-        global repository_datatypes_count
-        global base_datatypes_count
-        global running_standalone
-        base_datatypes_count = int(self.get_datatypes_count())
-        strings_displayed = ['Handle', 'Never installed', 'tool dependencies', 'emboss', '5.0.0', 'package']
-        self.install_repository('emboss_0030',
-                                common.test_user_1_name,
-                                'Test 0030 Repository Dependency Revisions',
-                                strings_displayed=strings_displayed,
-                                install_tool_dependencies=False,
-                                new_tool_panel_section_label='test_1030')
-        installed_repository = self.test_db_util.get_installed_repository_by_name_owner('emboss_0030', common.test_user_1_name)
-        strings_displayed = ['emboss_0030',
-                             'Galaxy wrappers for Emboss version 5.0.0 tools for test 0030',
-                             'user1',
-                             self.url.replace('http://', ''),
-                             installed_repository.installed_changeset_revision]
-        self.display_galaxy_browse_repositories_page(strings_displayed=strings_displayed)
-        strings_displayed.extend(['Installed tool shed repository', 'Valid tools', 'antigenic'])
-        self.display_installed_repository_manage_page(installed_repository, strings_displayed=strings_displayed)
-        strings_displayed = ['emboss', '5.0.0', 'package']
-        self.check_installed_repository_tool_dependencies(installed_repository, strings_displayed=strings_displayed, dependencies_installed=False)
-        self.verify_tool_metadata_for_installed_repository(installed_repository)
-        self.update_installed_repository(installed_repository, strings_displayed=["there are no updates available"])
-        current_datatypes = int(self.get_datatypes_count())
-        if running_standalone:
-            assert current_datatypes == base_datatypes_count + repository_datatypes_count, 'Installing emboss did not add new datatypes.'
+import logging
+from typing import (
+    Callable,
+    Dict,
+)
+
+from sqlalchemy import and_
+
+from galaxy import (
+    util,
+    web,
+)
+from galaxy.webapps.base.controller import (
+    BaseAPIController,
+    HTTPBadRequest,
+)
+from tool_shed.util import (
+    metadata_util,
+    repository_util,
+)
+
+log = logging.getLogger(__name__)
+
+
+class RepositoryRevisionsController(BaseAPIController):
+    """RESTful controller for interactions with tool shed repository revisions."""
+
+    def __get_value_mapper(self, trans) -> Dict[str, Callable]:
+        value_mapper = {
+            "id": trans.security.encode_id,
+            "repository_id": trans.security.encode_id,
+            "user_id": trans.security.encode_id,
+        }
+        return value_mapper
+
+    @web.legacy_expose_api_anonymous
+    def index(self, trans, **kwd):
+        """
+        GET /api/repository_revisions
+        Displays a collection (list) of repository revisions.
+        """
+        # Example URL: http://localhost:9009/api/repository_revisions
+        repository_metadata_dicts = []
+        # Build up an anded clause list of filters.
+        clause_list = []
+        # Filter by downloadable if received.
+        downloadable = kwd.get("downloadable", None)
+        if downloadable is not None:
+            clause_list.append(trans.model.RepositoryMetadata.table.c.downloadable == util.asbool(downloadable))
+        # Filter by malicious if received.
+        malicious = kwd.get("malicious", None)
+        if malicious is not None:
+            clause_list.append(trans.model.RepositoryMetadata.table.c.malicious == util.asbool(malicious))
+        # Filter by missing_test_components if received.
+        missing_test_components = kwd.get("missing_test_components", None)
+        if missing_test_components is not None:
+            clause_list.append(
+                trans.model.RepositoryMetadata.table.c.missing_test_components == util.asbool(missing_test_components)
+            )
+        # Filter by includes_tools if received.
+        includes_tools = kwd.get("includes_tools", None)
+        if includes_tools is not None:
+            clause_list.append(trans.model.RepositoryMetadata.table.c.includes_tools == util.asbool(includes_tools))
+        for repository_metadata in (
+            trans.sa_session.query(trans.app.model.RepositoryMetadata)
+            .filter(and_(*clause_list))
+            .order_by(trans.app.model.RepositoryMetadata.table.c.repository_id.desc())
+        ):
+            repository_metadata_dict = repository_metadata.to_dict(
+                view="collection", value_mapper=self.__get_value_mapper(trans)
+            )
+            repository_metadata_dict["url"] = web.url_for(
+                controller="repository_revisions", action="show", id=trans.security.encode_id(repository_metadata.id)
+            )
+            repository_metadata_dicts.append(repository_metadata_dict)
+        return repository_metadata_dicts
+
+    @web.legacy_expose_api_anonymous
+    def repository_dependencies(self, trans, id, **kwd):
+        """
+        GET /api/repository_revisions/{encoded repository_metadata id}/repository_dependencies
+
+        Returns a list of dictionaries that each define a specific downloadable revision of a
+        repository in the Tool Shed.  This method returns dictionaries with more information in
+        them than other methods in this controller.  The information about repository_metdata is
+        enhanced to include information about the repository (e.g., name, owner, etc) associated
+        with the repository_metadata record.
+
+        :param id: the encoded id of the `RepositoryMetadata` object
+        """
+        # Example URL: http://localhost:9009/api/repository_revisions/repository_dependencies/bb125606ff9ea620
+        repository_dependencies_dicts = []
+        repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, id)
+        if repository_metadata is None:
+            log.debug(f"Invalid repository_metadata id received: {id}")
+            return repository_dependencies_dicts
+        metadata = repository_metadata.metadata
+        if metadata is None:
+            log.debug(f"The repository_metadata record with id {id} has no metadata.")
+            return repository_dependencies_dicts
+        if "repository_dependencies" in metadata:
+            rd_tups = metadata["repository_dependencies"]["repository_dependencies"]
+            for rd_tup in rd_tups:
+                tool_shed, name, owner, changeset_revision = rd_tup[0:4]
+                repository_dependency = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
+                if repository_dependency is None:
+                    log.dbug(f"Cannot locate repository dependency {name} owned by {owner}.")
+                    continue
+                repository_dependency_id = trans.security.encode_id(repository_dependency.id)
+                repository_dependency_repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                    trans.app, repository_dependency_id, changeset_revision
+                )
+                if repository_dependency_repository_metadata is None:
+                    # The changeset_revision column in the repository_metadata table has been updated with a new
+                    # value value, so find the changeset_revision to which we need to update.
+                    new_changeset_revision = metadata_util.get_next_downloadable_changeset_revision(
+                        trans.app, repository_dependency, changeset_revision
+                    )
+                    if new_changeset_revision != changeset_revision:
+                        repository_dependency_repository_metadata = (
+                            metadata_util.get_repository_metadata_by_changeset_revision(
+                                trans.app, repository_dependency_id, new_changeset_revision
+                            )
+                        )
+                        changeset_revision = new_changeset_revision
+                    else:
+                        decoded_repository_dependency_id = trans.security.decode_id(repository_dependency_id)
+                        debug_msg = (
+                            "Cannot locate repository_metadata with id %d for repository dependency %s owned by %s "
+                            % (decoded_repository_dependency_id, str(name), str(owner))
+                        )
+                        debug_msg += f"using either of these changeset_revisions: {changeset_revision}, {new_changeset_revision}."
+                        log.debug(debug_msg)
+                        continue
+                repository_dependency_metadata_dict = repository_dependency_repository_metadata.to_dict(
+                    view="element", value_mapper=self.__get_value_mapper(trans)
+                )
+                repository_dependency_dict = repository_dependency.to_dict(
+                    view="element", value_mapper=self.__get_value_mapper(trans)
+                )
+                # We need to be careful with the entries in our repository_dependency_dict here since this Tool Shed API
+                # controller is working with repository_metadata records.  The above to_dict() method returns a dictionary
+                # with an id entry for the repository record.  However, all of the other methods in this controller have
+                # the id entry associated with a repository_metadata record id.  To avoid confusion, we'll update the
+                # repository_dependency_metadata_dict with entries from the repository_dependency_dict without using the
+                # Python dictionary update() method because we do not want to overwrite existing entries.
+                for k, v in repository_dependency_dict.items():
+                    if k not in repository_dependency_metadata_dict:
+                        repository_dependency_metadata_dict[k] = v
+                repository_dependency_metadata_dict["url"] = web.url_for(
+                    controller="repositories", action="show", id=repository_dependency_id
+                )
+                repository_dependencies_dicts.append(repository_dependency_metadata_dict)
+        return repository_dependencies_dicts
+
+    @web.legacy_expose_api_anonymous
+    def show(self, trans, id, **kwd):
+        """
+        GET /api/repository_revisions/{encoded_repository_metadata_id}
+        Displays information about a repository_metadata record in the Tool Shed.
+
+        :param id: the encoded id of the `RepositoryMetadata` object
+        """
+        # Example URL: http://localhost:9009/api/repository_revisions/bb125606ff9ea620
+        repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, id)
+        if repository_metadata is None:
+            log.debug(f"Cannot locate repository_metadata with id {id}")
+            return {}
+        encoded_repository_id = trans.security.encode_id(repository_metadata.repository_id)
+        repository_metadata_dict = repository_metadata.to_dict(
+            view="element", value_mapper=self.__get_value_mapper(trans)
+        )
+        repository_metadata_dict["url"] = web.url_for(
+            controller="repositories", action="show", id=encoded_repository_id
+        )
+        return repository_metadata_dict
+
+    @web.legacy_expose_api
+    def update(self, trans, payload, **kwd):
+        """
+        PUT /api/repository_revisions/{encoded_repository_metadata_id}/{payload}
+        Updates the value of specified columns of the repository_metadata table based on the key / value pairs in payload.
+
+        :param id: the encoded id of the `RepositoryMetadata` object
+        """
+        repository_metadata_id = kwd.get("id", None)
+        if repository_metadata_id is None:
+            raise HTTPBadRequest(detail="Missing required parameter 'id'.")
+        repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, repository_metadata_id)
+        if repository_metadata is None:
+            decoded_repository_metadata_id = trans.security.decode_id(repository_metadata_id)
+            log.debug(f"Cannot locate repository_metadata with id {decoded_repository_metadata_id}")
+            return {}
         else:
-            assert current_datatypes == base_datatypes_count, 'Installing emboss added new datatypes.'
-
-    def test_0025_verify_installed_repository_metadata(self):
-        '''Verify that resetting the metadata on an installed repository does not change the metadata.'''
-        self.verify_installed_repository_metadata_unchanged('emboss_0030', common.test_user_1_name)
+            decoded_repository_metadata_id = repository_metadata.id
+        flush_needed = False
+        for key, new_value in payload.items():
+            if hasattr(repository_metadata, key):
+                # log information when setting attributes associated with the Tool Shed's install and test framework.
+                if key in ["includes_tools", "missing_test_components"]:
+                    log.debug(
+                        "Setting repository_metadata column %s to value %s for changeset_revision %s via the Tool Shed API."
+                        % (str(key), str(new_value), str(repository_metadata.changeset_revision))
+                    )
+                setattr(repository_metadata, key, new_value)
+                flush_needed = True
+        if flush_needed:
+            log.debug(
+                "Updating repository_metadata record with id %s and changeset_revision %s."
+                % (str(decoded_repository_metadata_id), str(repository_metadata.changeset_revision))
+            )
+            trans.sa_session.add(repository_metadata)
+            trans.sa_session.flush()
+            trans.sa_session.refresh(repository_metadata)
+        repository_metadata_dict = repository_metadata.to_dict(
+            view="element", value_mapper=self.__get_value_mapper(trans)
+        )
+        repository_metadata_dict["url"] = web.url_for(
+            controller="repository_revisions", action="show", id=repository_metadata_id
+        )
+        return repository_metadata_dict
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/tools/tool_validator.py` & `galaxy-web-apps-23.0.2/tool_shed/tools/tool_validator.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,74 +2,74 @@
 import logging
 import os
 import tempfile
 
 from galaxy.tool_shed.tools.tool_validator import ToolValidator as GalaxyToolValidator
 from galaxy.tools import Tool
 from galaxy.util import unicodify
+from galaxy.util.tool_shed.xml_util import parse_xml
 from tool_shed.util import (
     basic_util,
     hg_util,
     repository_util,
     tool_util,
-    xml_util
 )
 
 log = logging.getLogger(__name__)
 
 
 class ToolValidator(GalaxyToolValidator):
-
     def can_use_tool_config_disk_file(self, repository, repo, file_path, changeset_revision):
         """
         Determine if repository's tool config file on disk can be used.  This method
         is restricted to tool config files since, with the exception of tool config
         files, multiple files with the same name will likely be in various directories
         in the repository and we're comparing file names only (not relative paths).
         """
         if not file_path or not os.path.exists(file_path):
             # The file no longer exists on disk, so it must have been deleted at some previous
             # point in the change log.
             return False
         if changeset_revision == repository.tip():
             return True
         file_name = basic_util.strip_path(file_path)
-        latest_version_of_file = \
-            self.get_latest_tool_config_revision_from_repository_manifest(repo, file_name, changeset_revision)
+        latest_version_of_file = self.get_latest_tool_config_revision_from_repository_manifest(
+            repo, file_name, changeset_revision
+        )
         can_use_disk_file = filecmp.cmp(file_path, latest_version_of_file)
         try:
             os.unlink(latest_version_of_file)
         except Exception:
             pass
         return can_use_disk_file
 
     def concat_messages(self, msg1, msg2):
         if msg1:
             if msg2:
-                message = '%s  %s' % (msg1, msg2)
+                message = f"{msg1}  {msg2}"
             else:
                 message = msg1
         elif msg2:
             message = msg2
         else:
-            message = ''
+            message = ""
         return message
 
     def copy_disk_sample_files_to_dir(self, repo_files_dir, dest_path):
         """
         Copy all files currently on disk that end with the .sample extension to the
         directory to which dest_path refers.
         """
         sample_files = []
-        for root, dirs, files in os.walk(repo_files_dir):
-            if root.find('.hg') < 0:
+        for root, _dirs, files in os.walk(repo_files_dir):
+            if root.find(".hg") < 0:
                 for name in files:
-                    if name.endswith('.sample'):
+                    if name.endswith(".sample"):
                         relative_path = os.path.join(root, name)
-                        tool_util.copy_sample_file(self.app, relative_path, dest_path=dest_path)
+                        tool_util.copy_sample_file(self.app.config.tool_data_path, relative_path, dest_path=dest_path)
                         sample_files.append(name)
         return sample_files
 
     def get_latest_tool_config_revision_from_repository_manifest(self, repo, filename, changeset_revision):
         """
         Get the latest revision of a tool config file named filename from the repository
         manifest up to the value of changeset_revision.  This method is restricted to tool_config
@@ -85,20 +85,17 @@
                     try:
                         fctx = manifest_ctx[ctx_file]
                     except LookupError:
                         # The ctx_file may have been moved in the change set.  For example,
                         # 'ncbi_blastp_wrapper.xml' was moved to 'tools/ncbi_blast_plus/ncbi_blastp_wrapper.xml',
                         # so keep looking for the file until we find the new location.
                         continue
-                    fh = tempfile.NamedTemporaryFile('wb', prefix="tmp-toolshed-gltcrfrm")
-                    tmp_filename = fh.name
-                    fh.close()
-                    fh = open(tmp_filename, 'wb')
-                    fh.write(fctx.data())
-                    fh.close()
+                    with tempfile.NamedTemporaryFile("wb", prefix="tmp-toolshed-gltcrfrm", delete=False) as fh:
+                        tmp_filename = fh.name
+                        fh.write(fctx.data())
                     return tmp_filename
         return None
 
     def get_list_of_copied_sample_files(self, repo, changeset_revision, dir):
         """
         Find all sample files (files in the repository with the special .sample extension)
         in the reversed repository manifest up to changeset_revision. Copy each discovered file to dir and
@@ -108,132 +105,137 @@
         the tools and generate metadata for them.
         """
         deleted_sample_files = []
         sample_files = []
         for changeset in hg_util.reversed_upper_bounded_changelog(repo, changeset_revision):
             changeset_ctx = repo[changeset]
             for ctx_file in changeset_ctx.files():
-                ctx_file_name = basic_util.strip_path(unicodify(ctx_file))
+                ctx_file = unicodify(ctx_file)
+                ctx_file_name = basic_util.strip_path(ctx_file)
                 # If we decide in the future that files deleted later in the changelog should
                 # not be used, we can use the following if statement. if ctx_file_name.endswith( '.sample' )
                 # and ctx_file_name not in sample_files and ctx_file_name not in deleted_sample_files:
-                if ctx_file_name.endswith('.sample') and ctx_file_name not in sample_files:
+                if ctx_file_name.endswith(".sample") and ctx_file_name not in sample_files:
                     fctx = hg_util.get_file_context_from_ctx(changeset_ctx, ctx_file)
-                    if fctx in ['DELETED']:
+                    if fctx in ["DELETED"]:
                         # Since the possibly future used if statement above is commented out, the
                         # same file that was initially added will be discovered in an earlier changeset
                         # in the change log and fall through to the else block below.  In other words,
                         # if a file named blast2go.loc.sample was added in change set 0 and then deleted
                         # in changeset 3, the deleted file in changeset 3 will be handled here, but the
                         # later discovered file in changeset 0 will be handled in the else block below.
                         # In this way, the file contents will always be found for future tools even though
                         # the file was deleted.
                         if ctx_file_name not in deleted_sample_files:
                             deleted_sample_files.append(ctx_file_name)
                     else:
                         sample_files.append(ctx_file_name)
-                        tmp_ctx_file_name = os.path.join(dir, ctx_file_name.replace('.sample', ''))
-                        fh = open(tmp_ctx_file_name, 'wb')
-                        fh.write(fctx.data())
-                        fh.close()
+                        tmp_ctx_file_name = os.path.join(dir, ctx_file_name.replace(".sample", ""))
+                        with open(tmp_ctx_file_name, "wb") as fh:
+                            fh.write(fctx.data())
         return sample_files, deleted_sample_files
 
-    def handle_sample_files_and_load_tool_from_disk(self, repo_files_dir, repository_id, tool_config_filepath, work_dir):
+    def handle_sample_files_and_load_tool_from_disk(
+        self, repo_files_dir, repository_id, tool_config_filepath, work_dir
+    ):
         """
         Copy all sample files from disk to a temporary directory since the sample files may
         be in multiple directories.
         """
-        message = ''
+        message = ""
         sample_files = self.copy_disk_sample_files_to_dir(repo_files_dir, work_dir)
         if sample_files:
-            if 'tool_data_table_conf.xml.sample' in sample_files:
+            if "tool_data_table_conf.xml.sample" in sample_files:
                 # Load entries into the tool_data_tables if the tool requires them.
-                tool_data_table_config = os.path.join(work_dir, 'tool_data_table_conf.xml')
-                error, message = self.stdtm.handle_sample_tool_data_table_conf_file(tool_data_table_config,
-                                                                                   persist=False)
+                tool_data_table_config = os.path.join(work_dir, "tool_data_table_conf.xml")
+                error, message = self.stdtm.handle_sample_tool_data_table_conf_file(
+                    tool_data_table_config, persist=False
+                )
         tool, valid, message2 = self.load_tool_from_config(repository_id, tool_config_filepath)
         message = self.concat_messages(message, message2)
         return tool, valid, message, sample_files
 
-    def handle_sample_files_and_load_tool_from_tmp_config(self, repo, repository_id, changeset_revision,
-                                                          tool_config_filename, work_dir):
+    def handle_sample_files_and_load_tool_from_tmp_config(
+        self, repo, repository_id, changeset_revision, tool_config_filename, work_dir
+    ):
         tool = None
         valid = False
-        message = ''
+        message = ""
         # We're not currently doing anything with the returned list of deleted_sample_files here.  It is
         # intended to help handle sample files that are in the manifest, but have been deleted from disk.
-        sample_files, deleted_sample_files = self.get_list_of_copied_sample_files(repo, changeset_revision, dir=work_dir)
+        sample_files, deleted_sample_files = self.get_list_of_copied_sample_files(
+            repo, changeset_revision, dir=work_dir
+        )
         if sample_files:
-            if 'tool_data_table_conf.xml.sample' in sample_files:
+            if "tool_data_table_conf.xml.sample" in sample_files:
                 # Load entries into the tool_data_tables if the tool requires them.
-                tool_data_table_config = os.path.join(work_dir, 'tool_data_table_conf.xml')
-                error, message = self.stdtm.handle_sample_tool_data_table_conf_file(tool_data_table_config,
-                                                                                    persist=False)
+                tool_data_table_config = os.path.join(work_dir, "tool_data_table_conf.xml")
+                error, message = self.stdtm.handle_sample_tool_data_table_conf_file(
+                    tool_data_table_config, persist=False
+                )
         manifest_ctx, ctx_file = hg_util.get_ctx_file_path_from_manifest(tool_config_filename, repo, changeset_revision)
         if manifest_ctx and ctx_file:
-            tool, valid, message2 = self.load_tool_from_tmp_config(repo, repository_id, manifest_ctx, ctx_file, work_dir)
+            tool, valid, message2 = self.load_tool_from_tmp_config(
+                repo, repository_id, manifest_ctx, ctx_file, work_dir
+            )
             message = self.concat_messages(message, message2)
         return tool, valid, message, sample_files
 
     def load_tool_from_changeset_revision(self, repository_id, changeset_revision, tool_config_filename):
         """
         Return a loaded tool whose tool config file name (e.g., filtering.xml) is the value
         of tool_config_filename.  The value of changeset_revision is a valid (downloadable)
         changeset revision.  The tool config will be located in the repository manifest between
         the received valid changeset revision and the first changeset revision in the repository,
         searching backwards.
         """
         repository = repository_util.get_repository_in_tool_shed(self.app, repository_id)
         repo_files_dir = repository.repo_path(self.app)
         repo = repository.hg_repo
-        tool_config_filepath = repository_util.get_absolute_path_to_file_in_repository(repo_files_dir, tool_config_filename)
+        tool_config_filepath = repository_util.get_absolute_path_to_file_in_repository(
+            repo_files_dir, tool_config_filename
+        )
         work_dir = tempfile.mkdtemp(prefix="tmp-toolshed-ltfcr")
-        can_use_disk_file = self.can_use_tool_config_disk_file(repository,
-                                                               repo,
-                                                               tool_config_filepath,
-                                                               changeset_revision)
+        can_use_disk_file = self.can_use_tool_config_disk_file(
+            repository, repo, tool_config_filepath, changeset_revision
+        )
         if can_use_disk_file:
-            tool, valid, message, sample_files = \
-                self.handle_sample_files_and_load_tool_from_disk(repo_files_dir,
-                                                                 repository_id,
-                                                                 tool_config_filepath,
-                                                                 work_dir)
+            tool, valid, message, sample_files = self.handle_sample_files_and_load_tool_from_disk(
+                repo_files_dir, repository_id, tool_config_filepath, work_dir
+            )
             if tool is not None:
-                invalid_files_and_errors_tups = \
-                    self.check_tool_input_params(repo_files_dir,
-                                                 tool_config_filename,
-                                                 tool,
-                                                 sample_files)
+                invalid_files_and_errors_tups = self.check_tool_input_params(
+                    repo_files_dir, tool_config_filename, tool, sample_files
+                )
                 if invalid_files_and_errors_tups:
-                    message2 = tool_util.generate_message_for_invalid_tools(self.app,
-                                                                            invalid_files_and_errors_tups,
-                                                                            repository,
-                                                                            metadata_dict=None,
-                                                                            as_html=True,
-                                                                            displaying_invalid_tool=True)
+                    message2 = tool_util.generate_message_for_invalid_tools(
+                        self.app,
+                        invalid_files_and_errors_tups,
+                        repository,
+                        metadata_dict=None,
+                        as_html=True,
+                        displaying_invalid_tool=True,
+                    )
                     message = self.concat_messages(message, message2)
         else:
-            tool, valid, message, sample_files = \
-                self.handle_sample_files_and_load_tool_from_tmp_config(repo,
-                                                                       repository_id,
-                                                                       changeset_revision,
-                                                                       tool_config_filename,
-                                                                       work_dir)
+            tool, valid, message, sample_files = self.handle_sample_files_and_load_tool_from_tmp_config(
+                repo, repository_id, changeset_revision, tool_config_filename, work_dir
+            )
         basic_util.remove_dir(work_dir)
         # Reset the tool_data_tables by loading the empty tool_data_table_conf.xml file.
         self.stdtm.reset_tool_data_tables()
         return repository, tool, valid, message
 
     def load_tool_from_tmp_config(self, repo, repository_id, ctx, ctx_file, work_dir):
         tool = None
         valid = False
-        message = ''
+        message = ""
         tmp_tool_config = hg_util.get_named_tmpfile_from_ctx(ctx, ctx_file, work_dir)
         if tmp_tool_config:
-            tool_element, error_message = xml_util.parse_xml(tmp_tool_config)
+            tool_element, error_message = parse_xml(tmp_tool_config)
             if tool_element is None:
                 return tool, message
             # Look for external files required by the tool config.
             tmp_code_files = []
             external_paths = Tool.get_externally_referenced_paths(tmp_tool_config)
             changeset_revision = str(ctx)
             for path in external_paths:
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/tools/tool_version_manager.py` & `galaxy-web-apps-23.0.2/tool_shed/tools/tool_version_manager.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,39 +1,48 @@
 import logging
 
 from sqlalchemy import and_
 
-from tool_shed.util import hg_util
-from tool_shed.util import metadata_util
-from tool_shed.util import repository_util
+from tool_shed.util import (
+    hg_util,
+    metadata_util,
+    repository_util,
+)
 
 log = logging.getLogger(__name__)
 
 
-class ToolVersionManager(object):
-
+class ToolVersionManager:
     def __init__(self, app):
         self.app = app
 
     def get_tool_version(self, tool_id):
         context = self.app.install_model.context
-        return context.query(self.app.install_model.ToolVersion) \
-                      .filter(self.app.install_model.ToolVersion.table.c.tool_id == tool_id) \
-                      .first()
+        return (
+            context.query(self.app.install_model.ToolVersion)
+            .filter(self.app.install_model.ToolVersion.table.c.tool_id == tool_id)
+            .first()
+        )
 
     def get_tool_version_association(self, parent_tool_version, tool_version):
         """
         Return a ToolVersionAssociation if one exists that associates the two
         received tool_versions. This function is called only from Galaxy.
         """
         context = self.app.install_model.context
-        return context.query(self.app.install_model.ToolVersionAssociation) \
-                      .filter(and_(self.app.install_model.ToolVersionAssociation.table.c.parent_id == parent_tool_version.id,
-                                   self.app.install_model.ToolVersionAssociation.table.c.tool_id == tool_version.id)) \
-                      .first()
+        return (
+            context.query(self.app.install_model.ToolVersionAssociation)
+            .filter(
+                and_(
+                    self.app.install_model.ToolVersionAssociation.table.c.parent_id == parent_tool_version.id,
+                    self.app.install_model.ToolVersionAssociation.table.c.tool_id == tool_version.id,
+                )
+            )
+            .first()
+        )
 
     def get_version_lineage_for_tool(self, repository_id, repository_metadata, guid):
         """
         Return the tool version lineage chain in descendant order for the received
         guid contained in the received repsitory_metadata.tool_versions.  This function
         is called only from the Tool Shed.
         """
@@ -49,17 +58,17 @@
             if rm:
                 parent_guid = rm.tool_versions.get(current_child_guid, None)
                 if parent_guid:
                     version_lineage.append(parent_guid)
                     current_child_guid = parent_guid
         # Get all descendant guids of the received guid.
         current_parent_guid = guid
-        for changeset in hg_util.reversed_lower_upper_bounded_changelog(repo,
-                                                                        repository_metadata.changeset_revision,
-                                                                        repository.tip()):
+        for changeset in hg_util.reversed_lower_upper_bounded_changelog(
+            repo, repository_metadata.changeset_revision, repository.tip()
+        ):
             ctx = repo[changeset]
             rm = metadata_util.get_repository_metadata_by_changeset_revision(self.app, repository_id, str(ctx))
             if rm:
                 tool_versions = rm.tool_versions
                 for child_guid, parent_guid in tool_versions.items():
                     if parent_guid == current_parent_guid:
                         version_lineage.insert(0, child_guid)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/admin_util.py` & `galaxy-web-apps-23.0.2/tool_shed/util/admin_util.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,55 +1,62 @@
 import logging
 import time
+from typing import Optional
 
-from sqlalchemy import false, func
-
-from galaxy import util, web
+from sqlalchemy import (
+    false,
+    func,
+)
+
+from galaxy import (
+    util,
+    web,
+)
 from galaxy.security.validate_user_input import validate_password
 from galaxy.util import inflector
-from galaxy.util.hash_util import new_secure_hash
+from galaxy.util.hash_util import new_secure_hash_v2
 from galaxy.web.form_builder import CheckboxField
+from galaxy.web.legacy_framework.grids import (
+    Grid,
+    GridOperation,
+)
 from tool_shed.util.web_util import escape
 
 log = logging.getLogger(__name__)
-compliance_log = logging.getLogger('COMPLIANCE')
+compliance_log = logging.getLogger("COMPLIANCE")
 
 
-class Admin(object):
+class Admin:
     # Override these
-    user_list_grid = None
-    role_list_grid = None
-    group_list_grid = None
-    delete_operation = None
-    undelete_operation = None
-    purge_operation = None
+    user_list_grid: Optional[Grid] = None
+    role_list_grid: Optional[Grid] = None
+    group_list_grid: Optional[Grid] = None
+    delete_operation: Optional[GridOperation] = None
+    undelete_operation: Optional[GridOperation] = None
+    purge_operation: Optional[GridOperation] = None
 
     @web.expose
     @web.require_admin
     def index(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        return trans.fill_template('/webapps/tool_shed/admin/index.mako',
-                                   message=message,
-                                   status=status)
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        return trans.fill_template("/webapps/tool_shed/admin/index.mako", message=message, status=status)
 
     @web.expose
     @web.require_admin
     def center(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        return trans.fill_template('/webapps/tool_shed/admin/center.mako',
-                                   message=message,
-                                   status=status)
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        return trans.fill_template("/webapps/tool_shed/admin/center.mako", message=message, status=status)
 
     @web.expose
     @web.require_admin
     def roles(self, trans, **kwargs):
-        if 'operation' in kwargs:
-            operation = kwargs['operation'].lower().replace('+', ' ')
+        if "operation" in kwargs:
+            operation = kwargs["operation"].lower().replace("+", " ")
             if operation == "roles":
                 return self.role(trans, **kwargs)
             if operation == "create":
                 return self.create_role(trans, **kwargs)
             if operation == "delete":
                 return self.mark_role_deleted(trans, **kwargs)
             if operation == "undelete":
@@ -66,33 +73,33 @@
         # Render the list view
         return self.role_list_grid(trans, **kwargs)
 
     @web.expose
     @web.require_admin
     def create_role(self, trans, **kwd):
         params = util.Params(kwd)
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
-        name = util.restore_text(params.get('name', ''))
-        description = util.restore_text(params.get('description', ''))
-        in_users = util.listify(params.get('in_users', []))
-        out_users = util.listify(params.get('out_users', []))
-        in_groups = util.listify(params.get('in_groups', []))
-        out_groups = util.listify(params.get('out_groups', []))
-        create_group_for_role = params.get('create_group_for_role', '')
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
+        name = util.restore_text(params.get("name", ""))
+        description = util.restore_text(params.get("description", ""))
+        in_users = util.listify(params.get("in_users", []))
+        out_users = util.listify(params.get("out_users", []))
+        in_groups = util.listify(params.get("in_groups", []))
+        out_groups = util.listify(params.get("out_groups", []))
+        create_group_for_role = params.get("create_group_for_role", "")
         create_group_for_role_checked = CheckboxField.is_checked(create_group_for_role)
         ok = True
-        if params.get('create_role_button', False):
+        if params.get("create_role_button", False):
             if not name or not description:
                 message = "Enter a valid name and a description."
-                status = 'error'
+                status = "error"
                 ok = False
             elif trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.table.c.name == name).first():
                 message = "Role names must be unique and a role with that name already exists, so choose another name."
-                status = 'error'
+                status = "error"
                 ok = False
             else:
                 # Create the role
                 role = trans.app.model.Role(name=name, description=description, type=trans.app.model.Role.types.ADMIN)
                 trans.sa_session.add(role)
                 # Create the UserRoleAssociations
                 for user in [trans.sa_session.query(trans.app.model.User).get(x) for x in in_users]:
@@ -109,101 +116,113 @@
                     # Associate the group with the role
                     gra = trans.model.GroupRoleAssociation(group, role)
                     trans.sa_session.add(gra)
                     num_in_groups = len(in_groups) + 1
                 else:
                     num_in_groups = len(in_groups)
                 trans.sa_session.flush()
-                message = "Role '%s' has been created with %d associated users and %d associated groups.  " \
-                    % (role.name, len(in_users), num_in_groups)
+                message = "Role '%s' has been created with %d associated users and %d associated groups.  " % (
+                    role.name,
+                    len(in_users),
+                    num_in_groups,
+                )
                 if create_group_for_role_checked:
-                    message += 'One of the groups associated with this role is the newly created group with the same name.'
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='roles',
-                                                         message=util.sanitize_text(message),
-                                                         status='done'))
+                    message += (
+                        "One of the groups associated with this role is the newly created group with the same name."
+                    )
+                trans.response.send_redirect(
+                    web.url_for(controller="admin", action="roles", message=util.sanitize_text(message), status="done")
+                )
         if ok:
-            for user in trans.sa_session.query(trans.app.model.User) \
-                                        .filter(trans.app.model.User.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.User.table.c.email):
+            for user in (
+                trans.sa_session.query(trans.app.model.User)
+                .filter(trans.app.model.User.table.c.deleted == false())
+                .order_by(trans.app.model.User.table.c.email)
+            ):
                 out_users.append((user.id, user.email))
-            for group in trans.sa_session.query(trans.app.model.Group) \
-                                         .filter(trans.app.model.Group.table.c.deleted == false()) \
-                                         .order_by(trans.app.model.Group.table.c.name):
+            for group in (
+                trans.sa_session.query(trans.app.model.Group)
+                .filter(trans.app.model.Group.table.c.deleted == false())
+                .order_by(trans.app.model.Group.table.c.name)
+            ):
                 out_groups.append((group.id, group.name))
-        return trans.fill_template('/webapps/tool_shed/admin/dataset_security/role/role_create.mako',
-                                   name=name,
-                                   description=description,
-                                   in_users=in_users,
-                                   out_users=out_users,
-                                   in_groups=in_groups,
-                                   out_groups=out_groups,
-                                   create_group_for_role_checked=create_group_for_role_checked,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/admin/dataset_security/role/role_create.mako",
+            name=name,
+            description=description,
+            in_users=in_users,
+            out_users=out_users,
+            in_groups=in_groups,
+            out_groups=out_groups,
+            create_group_for_role_checked=create_group_for_role_checked,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     @web.require_admin
     def rename_role(self, trans, **kwd):
         params = util.Params(kwd)
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
-        id = params.get('id', None)
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
+        id = params.get("id", None)
         if not id:
             message = "No role ids received for renaming"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='roles',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="roles", message=message, status="error")
+            )
         role = get_role(trans, id)
-        if params.get('rename_role_button', False):
+        if params.get("rename_role_button", False):
             old_name = role.name
             new_name = util.restore_text(params.name)
             new_description = util.restore_text(params.description)
             if not new_name:
-                message = 'Enter a valid name'
-                status = 'error'
+                message = "Enter a valid name"
+                status = "error"
             else:
-                existing_role = trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.table.c.name == new_name).first()
+                existing_role = (
+                    trans.sa_session.query(trans.app.model.Role)
+                    .filter(trans.app.model.Role.table.c.name == new_name)
+                    .first()
+                )
                 if existing_role and existing_role.id != role.id:
-                    message = 'A role with that name already exists'
-                    status = 'error'
+                    message = "A role with that name already exists"
+                    status = "error"
                 else:
                     if not (role.name == new_name and role.description == new_description):
                         role.name = new_name
                         role.description = new_description
                         trans.sa_session.add(role)
                         trans.sa_session.flush()
-                        message = "Role '%s' has been renamed to '%s'" % (old_name, new_name)
-                    return trans.response.send_redirect(web.url_for(controller='admin',
-                                                                    action='roles',
-                                                                    message=util.sanitize_text(message),
-                                                                    status='done'))
-        return trans.fill_template('/webapps/tool_shed/admin/dataset_security/role/role_rename.mako',
-                                   role=role,
-                                   message=message,
-                                   status=status)
+                        message = f"Role '{old_name}' has been renamed to '{new_name}'"
+                    return trans.response.send_redirect(
+                        web.url_for(
+                            controller="admin", action="roles", message=util.sanitize_text(message), status="done"
+                        )
+                    )
+        return trans.fill_template(
+            "/webapps/tool_shed/admin/dataset_security/role/role_rename.mako", role=role, message=message, status=status
+        )
 
     @web.expose
     @web.require_admin
     def manage_users_and_groups_for_role(self, trans, **kwd):
         params = util.Params(kwd)
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
-        id = params.get('id', None)
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
+        id = params.get("id", None)
         if not id:
             message = "No role ids received for managing users and groups"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='roles',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="roles", message=message, status="error")
+            )
         role = get_role(trans, id)
-        if params.get('role_members_edit_button', False):
+        if params.get("role_members_edit_button", False):
             in_users = [trans.sa_session.query(trans.app.model.User).get(x) for x in util.listify(params.in_users)]
-            if trans.webapp.name == 'galaxy':
+            if trans.webapp.name == "galaxy":
                 for ura in role.users:
                     user = trans.sa_session.query(trans.app.model.User).get(ura.user_id)
                     if user not in in_users:
                         # Delete DefaultUserPermissions for previously associated users that have been removed from the role
                         for dup in user.default_permissions:
                             if role == dup.role:
                                 trans.sa_session.delete(dup)
@@ -212,159 +231,164 @@
                             for dhp in history.default_permissions:
                                 if role == dhp.role:
                                     trans.sa_session.delete(dhp)
                         trans.sa_session.flush()
             in_groups = [trans.sa_session.query(trans.app.model.Group).get(x) for x in util.listify(params.in_groups)]
             trans.app.security_agent.set_entity_role_associations(roles=[role], users=in_users, groups=in_groups)
             trans.sa_session.refresh(role)
-            message = "Role '%s' has been updated with %d associated users and %d associated groups" % (role.name, len(in_users), len(in_groups))
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='roles',
-                                                     message=util.sanitize_text(message),
-                                                     status=status))
+            message = "Role '%s' has been updated with %d associated users and %d associated groups" % (
+                role.name,
+                len(in_users),
+                len(in_groups),
+            )
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="roles", message=util.sanitize_text(message), status=status)
+            )
         in_users = []
         out_users = []
         in_groups = []
         out_groups = []
-        for user in trans.sa_session.query(trans.app.model.User) \
-                                    .filter(trans.app.model.User.table.c.deleted == false()) \
-                                    .order_by(trans.app.model.User.table.c.email):
+        for user in (
+            trans.sa_session.query(trans.app.model.User)
+            .filter(trans.app.model.User.table.c.deleted == false())
+            .order_by(trans.app.model.User.table.c.email)
+        ):
             if user in [x.user for x in role.users]:
                 in_users.append((user.id, user.email))
             else:
                 out_users.append((user.id, user.email))
-        for group in trans.sa_session.query(trans.app.model.Group) \
-                                     .filter(trans.app.model.Group.table.c.deleted == false()) \
-                                     .order_by(trans.app.model.Group.table.c.name):
+        for group in (
+            trans.sa_session.query(trans.app.model.Group)
+            .filter(trans.app.model.Group.table.c.deleted == false())
+            .order_by(trans.app.model.Group.table.c.name)
+        ):
             if group in [x.group for x in role.groups]:
                 in_groups.append((group.id, group.name))
             else:
                 out_groups.append((group.id, group.name))
         library_dataset_actions = {}
-        if trans.webapp.name == 'galaxy' and len(role.dataset_actions) < 25:
+        if trans.webapp.name == "galaxy" and len(role.dataset_actions) < 25:
             # Build a list of tuples that are LibraryDatasetDatasetAssociationss followed by a list of actions
             # whose DatasetPermissions is associated with the Role
             # [ ( LibraryDatasetDatasetAssociation [ action, action ] ) ]
             for dp in role.dataset_actions:
-                for ldda in trans.sa_session.query(trans.app.model.LibraryDatasetDatasetAssociation) \
-                                            .filter(trans.app.model.LibraryDatasetDatasetAssociation.dataset_id == dp.dataset_id):
+                for ldda in trans.sa_session.query(trans.app.model.LibraryDatasetDatasetAssociation).filter(
+                    trans.app.model.LibraryDatasetDatasetAssociation.dataset_id == dp.dataset_id
+                ):
                     root_found = False
-                    folder_path = ''
+                    folder_path = ""
                     folder = ldda.library_dataset.folder
                     while not root_found:
-                        folder_path = '%s / %s' % (folder.name, folder_path)
+                        folder_path = f"{folder.name} / {folder_path}"
                         if not folder.parent:
                             root_found = True
                         else:
                             folder = folder.parent
-                    folder_path = '%s %s' % (folder_path, ldda.name)
-                    library = trans.sa_session.query(trans.app.model.Library) \
-                                              .filter(trans.app.model.Library.table.c.root_folder_id == folder.id) \
-                                              .first()
+                    folder_path = f"{folder_path} {ldda.name}"
+                    library = (
+                        trans.sa_session.query(trans.app.model.Library)
+                        .filter(trans.app.model.Library.table.c.root_folder_id == folder.id)
+                        .first()
+                    )
                     if library not in library_dataset_actions:
                         library_dataset_actions[library] = {}
                     try:
                         library_dataset_actions[library][folder_path].append(dp.action)
                     except Exception:
                         library_dataset_actions[library][folder_path] = [dp.action]
         else:
             message = "Not showing associated datasets, there are too many."
-            status = 'info'
-        return trans.fill_template('/webapps/tool_shed/admin/dataset_security/role/role.mako',
-                                   role=role,
-                                   in_users=in_users,
-                                   out_users=out_users,
-                                   in_groups=in_groups,
-                                   out_groups=out_groups,
-                                   library_dataset_actions=library_dataset_actions,
-                                   message=message,
-                                   status=status)
+            status = "info"
+        return trans.fill_template(
+            "/webapps/tool_shed/admin/dataset_security/role/role.mako",
+            role=role,
+            in_users=in_users,
+            out_users=out_users,
+            in_groups=in_groups,
+            out_groups=out_groups,
+            library_dataset_actions=library_dataset_actions,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     @web.require_admin
     def mark_role_deleted(self, trans, **kwd):
-        id = kwd.get('id', None)
+        id = kwd.get("id", None)
         if not id:
             message = "No role ids received for deleting"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='roles',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="roles", message=message, status="error")
+            )
         ids = util.listify(id)
         message = "Deleted %d roles: " % len(ids)
         for role_id in ids:
             role = get_role(trans, role_id)
             role.deleted = True
             trans.sa_session.add(role)
             trans.sa_session.flush()
-            message += " %s " % role.name
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='roles',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+            message += f" {role.name} "
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="roles", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def undelete_role(self, trans, **kwd):
-        id = kwd.get('id', None)
+        id = kwd.get("id", None)
         if not id:
             message = "No role ids received for undeleting"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='roles',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="roles", message=message, status="error")
+            )
         ids = util.listify(id)
         count = 0
         undeleted_roles = ""
         for role_id in ids:
             role = get_role(trans, role_id)
             if not role.deleted:
-                message = "Role '%s' has not been deleted, so it cannot be undeleted." % role.name
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='roles',
-                                                         message=util.sanitize_text(message),
-                                                         status='error'))
+                message = f"Role '{role.name}' has not been deleted, so it cannot be undeleted."
+                trans.response.send_redirect(
+                    web.url_for(controller="admin", action="roles", message=util.sanitize_text(message), status="error")
+                )
             role.deleted = False
             trans.sa_session.add(role)
             trans.sa_session.flush()
             count += 1
-            undeleted_roles += " %s" % role.name
+            undeleted_roles += f" {role.name}"
         message = "Undeleted %d roles: %s" % (count, undeleted_roles)
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='roles',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="roles", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def purge_role(self, trans, **kwd):
         # This method should only be called for a Role that has previously been deleted.
         # Purging a deleted Role deletes all of the following from the database:
         # - UserRoleAssociations where role_id == Role.id
         # - DefaultUserPermissions where role_id == Role.id
         # - DefaultHistoryPermissions where role_id == Role.id
         # - GroupRoleAssociations where role_id == Role.id
         # - DatasetPermissionss where role_id == Role.id
-        id = kwd.get('id', None)
+        id = kwd.get("id", None)
         if not id:
             message = "No role ids received for purging"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='roles',
-                                                     message=util.sanitize_text(message),
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="roles", message=util.sanitize_text(message), status="error")
+            )
         ids = util.listify(id)
         message = "Purged %d roles: " % len(ids)
         for role_id in ids:
             role = get_role(trans, role_id)
             if not role.deleted:
-                message = "Role '%s' has not been deleted, so it cannot be purged." % role.name
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='roles',
-                                                         message=util.sanitize_text(message),
-                                                         status='error'))
+                message = f"Role '{role.name}' has not been deleted, so it cannot be purged."
+                trans.response.send_redirect(
+                    web.url_for(controller="admin", action="roles", message=util.sanitize_text(message), status="error")
+                )
             # Delete UserRoleAssociations
             for ura in role.users:
                 user = trans.sa_session.query(trans.app.model.User).get(ura.user_id)
                 # Delete DefaultUserPermissions for associated users
                 for dup in user.default_permissions:
                     if role == dup.role:
                         trans.sa_session.delete(dup)
@@ -377,25 +401,24 @@
             # Delete GroupRoleAssociations
             for gra in role.groups:
                 trans.sa_session.delete(gra)
             # Delete DatasetPermissionss
             for dp in role.dataset_actions:
                 trans.sa_session.delete(dp)
             trans.sa_session.flush()
-            message += " %s " % role.name
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='roles',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+            message += f" {role.name} "
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="roles", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def groups(self, trans, **kwargs):
-        if 'operation' in kwargs:
-            operation = kwargs['operation'].lower().replace('+', ' ')
+        if "operation" in kwargs:
+            operation = kwargs["operation"].lower().replace("+", " ")
             if operation == "groups":
                 return self.group(trans, **kwargs)
             if operation == "create":
                 return self.create_group(trans, **kwargs)
             if operation == "delete":
                 return self.mark_group_deleted(trans, **kwargs)
             if operation == "undelete":
@@ -409,117 +432,140 @@
         # Render the list view
         return self.group_list_grid(trans, **kwargs)
 
     @web.expose
     @web.require_admin
     def rename_group(self, trans, **kwd):
         params = util.Params(kwd)
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
-        id = params.get('id', None)
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
+        id = params.get("id", None)
         if not id:
             message = "No group ids received for renaming"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='groups',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="groups", message=message, status="error")
+            )
         group = get_group(trans, id)
-        if params.get('rename_group_button', False):
+        if params.get("rename_group_button", False):
             old_name = group.name
             new_name = util.restore_text(params.name)
             if not new_name:
-                message = 'Enter a valid name'
-                status = 'error'
+                message = "Enter a valid name"
+                status = "error"
             else:
-                existing_group = trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.name == new_name).first()
+                existing_group = (
+                    trans.sa_session.query(trans.app.model.Group)
+                    .filter(trans.app.model.Group.table.c.name == new_name)
+                    .first()
+                )
                 if existing_group and existing_group.id != group.id:
-                    message = 'A group with that name already exists'
-                    status = 'error'
+                    message = "A group with that name already exists"
+                    status = "error"
                 else:
                     if group.name != new_name:
                         group.name = new_name
                         trans.sa_session.add(group)
                         trans.sa_session.flush()
-                        message = "Group '%s' has been renamed to '%s'" % (old_name, new_name)
-                    return trans.response.send_redirect(web.url_for(controller='admin',
-                                                                    action='groups',
-                                                                    message=util.sanitize_text(message),
-                                                                    status='done'))
-        return trans.fill_template('/webapps/tool_shed/admin/dataset_security/group/group_rename.mako',
-                                   group=group,
-                                   message=message,
-                                   status=status)
+                        message = f"Group '{old_name}' has been renamed to '{new_name}'"
+                    return trans.response.send_redirect(
+                        web.url_for(
+                            controller="admin", action="groups", message=util.sanitize_text(message), status="done"
+                        )
+                    )
+        return trans.fill_template(
+            "/webapps/tool_shed/admin/dataset_security/group/group_rename.mako",
+            group=group,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     @web.require_admin
     def manage_users_and_roles_for_group(self, trans, **kwd):
         params = util.Params(kwd)
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
         group = get_group(trans, params.id)
-        if params.get('group_roles_users_edit_button', False):
+        if params.get("group_roles_users_edit_button", False):
             in_roles = [trans.sa_session.query(trans.app.model.Role).get(x) for x in util.listify(params.in_roles)]
             in_users = [trans.sa_session.query(trans.app.model.User).get(x) for x in util.listify(params.in_users)]
             trans.app.security_agent.set_entity_group_associations(groups=[group], roles=in_roles, users=in_users)
             trans.sa_session.refresh(group)
-            message += "Group '%s' has been updated with %d associated roles and %d associated users" % (group.name, len(in_roles), len(in_users))
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='groups',
-                                                     message=util.sanitize_text(message),
-                                                     status=status))
+            message += "Group '%s' has been updated with %d associated roles and %d associated users" % (
+                group.name,
+                len(in_roles),
+                len(in_users),
+            )
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="groups", message=util.sanitize_text(message), status=status)
+            )
         in_roles = []
         out_roles = []
         in_users = []
         out_users = []
-        for role in trans.sa_session.query(trans.app.model.Role) \
-                                    .filter(trans.app.model.Role.table.c.deleted == false()) \
-                                    .order_by(trans.app.model.Role.table.c.name):
+        for role in (
+            trans.sa_session.query(trans.app.model.Role)
+            .filter(trans.app.model.Role.table.c.deleted == false())
+            .order_by(trans.app.model.Role.table.c.name)
+        ):
             if role in [x.role for x in group.roles]:
                 in_roles.append((role.id, role.name))
             else:
                 out_roles.append((role.id, role.name))
-        for user in trans.sa_session.query(trans.app.model.User) \
-                                    .filter(trans.app.model.User.table.c.deleted == false()) \
-                                    .order_by(trans.app.model.User.table.c.email):
+        for user in (
+            trans.sa_session.query(trans.app.model.User)
+            .filter(trans.app.model.User.table.c.deleted == false())
+            .order_by(trans.app.model.User.table.c.email)
+        ):
             if user in [x.user for x in group.users]:
                 in_users.append((user.id, user.email))
             else:
                 out_users.append((user.id, user.email))
-        message += 'Group %s is currently associated with %d roles and %d users' % (group.name, len(in_roles), len(in_users))
-        return trans.fill_template('/webapps/tool_shed/admin/dataset_security/group/group.mako',
-                                   group=group,
-                                   in_roles=in_roles,
-                                   out_roles=out_roles,
-                                   in_users=in_users,
-                                   out_users=out_users,
-                                   message=message,
-                                   status=status)
+        message += "Group %s is currently associated with %d roles and %d users" % (
+            group.name,
+            len(in_roles),
+            len(in_users),
+        )
+        return trans.fill_template(
+            "/webapps/tool_shed/admin/dataset_security/group/group.mako",
+            group=group,
+            in_roles=in_roles,
+            out_roles=out_roles,
+            in_users=in_users,
+            out_users=out_users,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     @web.require_admin
     def create_group(self, trans, **kwd):
         params = util.Params(kwd)
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
-        name = util.restore_text(params.get('name', ''))
-        in_users = util.listify(params.get('in_users', []))
-        out_users = util.listify(params.get('out_users', []))
-        in_roles = util.listify(params.get('in_roles', []))
-        out_roles = util.listify(params.get('out_roles', []))
-        create_role_for_group = params.get('create_role_for_group', '')
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
+        name = util.restore_text(params.get("name", ""))
+        in_users = util.listify(params.get("in_users", []))
+        out_users = util.listify(params.get("out_users", []))
+        in_roles = util.listify(params.get("in_roles", []))
+        out_roles = util.listify(params.get("out_roles", []))
+        create_role_for_group = params.get("create_role_for_group", "")
         create_role_for_group_checked = CheckboxField.is_checked(create_role_for_group)
         ok = True
-        if params.get('create_group_button', False):
+        if params.get("create_group_button", False):
             if not name:
                 message = "Enter a valid name."
-                status = 'error'
+                status = "error"
                 ok = False
-            elif trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.name == name).first():
-                message = "Group names must be unique and a group with that name already exists, so choose another name."
-                status = 'error'
+            elif (
+                trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.name == name).first()
+            ):
+                message = (
+                    "Group names must be unique and a group with that name already exists, so choose another name."
+                )
+                status = "error"
                 ok = False
             else:
                 # Create the group
                 group = trans.app.model.Group(name=name)
                 trans.sa_session.add(group)
                 trans.sa_session.flush()
                 # Create the UserRoleAssociations
@@ -528,214 +574,215 @@
                     trans.sa_session.add(uga)
                 # Create the GroupRoleAssociations
                 for role in [trans.sa_session.query(trans.app.model.Role).get(x) for x in in_roles]:
                     gra = trans.app.model.GroupRoleAssociation(group, role)
                     trans.sa_session.add(gra)
                 if create_role_for_group_checked:
                     # Create the role
-                    role = trans.app.model.Role(name=name, description='Role for group %s' % name)
+                    role = trans.app.model.Role(name=name, description=f"Role for group {name}")
                     trans.sa_session.add(role)
                     # Associate the role with the group
                     gra = trans.model.GroupRoleAssociation(group, role)
                     trans.sa_session.add(gra)
                     num_in_roles = len(in_roles) + 1
                 else:
                     num_in_roles = len(in_roles)
                 trans.sa_session.flush()
-                message = "Group '%s' has been created with %d associated users and %d associated roles.  " \
-                    % (group.name, len(in_users), num_in_roles)
+                message = "Group '%s' has been created with %d associated users and %d associated roles.  " % (
+                    group.name,
+                    len(in_users),
+                    num_in_roles,
+                )
                 if create_role_for_group_checked:
-                    message += 'One of the roles associated with this group is the newly created role with the same name.'
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='groups',
-                                                         message=util.sanitize_text(message),
-                                                         status='done'))
+                    message += (
+                        "One of the roles associated with this group is the newly created role with the same name."
+                    )
+                trans.response.send_redirect(
+                    web.url_for(controller="admin", action="groups", message=util.sanitize_text(message), status="done")
+                )
         if ok:
-            for user in trans.sa_session.query(trans.app.model.User) \
-                                        .filter(trans.app.model.User.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.User.table.c.email):
+            for user in (
+                trans.sa_session.query(trans.app.model.User)
+                .filter(trans.app.model.User.table.c.deleted == false())
+                .order_by(trans.app.model.User.table.c.email)
+            ):
                 out_users.append((user.id, user.email))
-            for role in trans.sa_session.query(trans.app.model.Role) \
-                                        .filter(trans.app.model.Role.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.Role.table.c.name):
+            for role in (
+                trans.sa_session.query(trans.app.model.Role)
+                .filter(trans.app.model.Role.table.c.deleted == false())
+                .order_by(trans.app.model.Role.table.c.name)
+            ):
                 out_roles.append((role.id, role.name))
-        return trans.fill_template('/webapps/tool_shed/admin/dataset_security/group/group_create.mako',
-                                   name=name,
-                                   in_users=in_users,
-                                   out_users=out_users,
-                                   in_roles=in_roles,
-                                   out_roles=out_roles,
-                                   create_role_for_group_checked=create_role_for_group_checked,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/admin/dataset_security/group/group_create.mako",
+            name=name,
+            in_users=in_users,
+            out_users=out_users,
+            in_roles=in_roles,
+            out_roles=out_roles,
+            create_role_for_group_checked=create_role_for_group_checked,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     @web.require_admin
     def mark_group_deleted(self, trans, **kwd):
         params = util.Params(kwd)
-        id = params.get('id', None)
+        id = params.get("id", None)
         if not id:
             message = "No group ids received for marking deleted"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='groups',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="groups", message=message, status="error")
+            )
         ids = util.listify(id)
         message = "Deleted %d groups: " % len(ids)
         for group_id in ids:
             group = get_group(trans, group_id)
             group.deleted = True
             trans.sa_session.add(group)
             trans.sa_session.flush()
-            message += " %s " % group.name
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='groups',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+            message += f" {group.name} "
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="groups", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def undelete_group(self, trans, **kwd):
-        id = kwd.get('id', None)
+        id = kwd.get("id", None)
         if not id:
             message = "No group ids received for undeleting"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='groups',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="groups", message=message, status="error")
+            )
         ids = util.listify(id)
         count = 0
         undeleted_groups = ""
         for group_id in ids:
             group = get_group(trans, group_id)
             if not group.deleted:
-                message = "Group '%s' has not been deleted, so it cannot be undeleted." % group.name
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='groups',
-                                                         message=util.sanitize_text(message),
-                                                         status='error'))
+                message = f"Group '{group.name}' has not been deleted, so it cannot be undeleted."
+                trans.response.send_redirect(
+                    web.url_for(
+                        controller="admin", action="groups", message=util.sanitize_text(message), status="error"
+                    )
+                )
             group.deleted = False
             trans.sa_session.add(group)
             trans.sa_session.flush()
             count += 1
-            undeleted_groups += " %s" % group.name
+            undeleted_groups += f" {group.name}"
         message = "Undeleted %d groups: %s" % (count, undeleted_groups)
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='groups',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="groups", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def purge_group(self, trans, **kwd):
         # This method should only be called for a Group that has previously been deleted.
         # Purging a deleted Group simply deletes all UserGroupAssociations and GroupRoleAssociations.
-        id = kwd.get('id', None)
+        id = kwd.get("id", None)
         if not id:
             message = "No group ids received for purging"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='groups',
-                                                     message=util.sanitize_text(message),
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="groups", message=util.sanitize_text(message), status="error")
+            )
         ids = util.listify(id)
         message = "Purged %d groups: " % len(ids)
         for group_id in ids:
             group = get_group(trans, group_id)
             if not group.deleted:
                 # We should never reach here, but just in case there is a bug somewhere...
-                message = "Group '%s' has not been deleted, so it cannot be purged." % group.name
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='groups',
-                                                         message=util.sanitize_text(message),
-                                                         status='error'))
+                message = f"Group '{group.name}' has not been deleted, so it cannot be purged."
+                trans.response.send_redirect(
+                    web.url_for(
+                        controller="admin", action="groups", message=util.sanitize_text(message), status="error"
+                    )
+                )
             # Delete UserGroupAssociations
             for uga in group.users:
                 trans.sa_session.delete(uga)
             # Delete GroupRoleAssociations
             for gra in group.roles:
                 trans.sa_session.delete(gra)
             trans.sa_session.flush()
-            message += " %s " % group.name
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='groups',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+            message += f" {group.name} "
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="groups", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def create_new_user(self, trans, **kwd):
-        return trans.response.send_redirect(web.url_for(controller='user',
-                                                        action='create',
-                                                        cntrller='admin'))
+        return trans.response.send_redirect(web.url_for(controller="user", action="create", cntrller="admin"))
 
     @web.expose
     @web.require_admin
     def reset_user_password(self, trans, **kwd):
-        user_id = kwd.get('id', None)
+        user_id = kwd.get("id", None)
         if not user_id:
             message = "No users received for resetting passwords."
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='users',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="users", message=message, status="error")
+            )
         user_ids = util.listify(user_id)
-        if 'reset_user_password_button' in kwd:
-            message = ''
-            status = ''
+        if "reset_user_password_button" in kwd:
+            message = ""
+            status = ""
             for user_id in user_ids:
                 user = get_user(trans, user_id)
-                password = kwd.get('password', None)
-                confirm = kwd.get('confirm', None)
+                password = kwd.get("password", None)
+                confirm = kwd.get("confirm", None)
                 message = validate_password(trans, password, confirm)
                 if message:
-                    status = 'error'
+                    status = "error"
                     break
                 else:
                     user.set_password_cleartext(password)
                     trans.sa_session.add(user)
                     trans.sa_session.flush()
             if not message and not status:
-                message = "Passwords reset for %d %s." % (len(user_ids), inflector.cond_plural(len(user_ids), 'user'))
-                status = 'done'
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='users',
-                                                     message=util.sanitize_text(message),
-                                                     status=status))
+                message = "Passwords reset for %d %s." % (len(user_ids), inflector.cond_plural(len(user_ids), "user"))
+                status = "done"
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status=status)
+            )
         users = [get_user(trans, user_id) for user_id in user_ids]
         if len(user_ids) > 1:
-            user_id = ','.join(user_ids)
-        return trans.fill_template('/webapps/tool_shed/admin/user/reset_password.mako',
-                                   id=user_id,
-                                   users=users,
-                                   password='',
-                                   confirm='')
+            user_id = ",".join(user_ids)
+        return trans.fill_template(
+            "/webapps/tool_shed/admin/user/reset_password.mako", id=user_id, users=users, password="", confirm=""
+        )
 
     @web.expose
     @web.require_admin
     def mark_user_deleted(self, trans, **kwd):
-        id = kwd.get('id', None)
+        id = kwd.get("id", None)
         if not id:
             message = "No user ids received for deleting"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='users',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="users", message=message, status="error")
+            )
         ids = util.listify(id)
         message = "Deleted %d users: " % len(ids)
         for user_id in ids:
             user = get_user(trans, user_id)
             user.deleted = True
 
-            compliance_log.info('delete-user-event: %s' % user_id)
+            compliance_log.info(f"delete-user-event: {user_id}")
             # See lib/galaxy/webapps/tool_shed/controllers/admin.py
             pseudorandom_value = str(int(time.time()))
-            email_hash = new_secure_hash(user.email + pseudorandom_value)
-            uname_hash = new_secure_hash(user.username + pseudorandom_value)
+            email_hash = new_secure_hash_v2(user.email + pseudorandom_value)
+            uname_hash = new_secure_hash_v2(user.username + pseudorandom_value)
             for role in user.all_roles():
-                print(role, self.app.config.redact_username_during_deletion, self.app.config.redact_email_during_deletion)
+                print(
+                    role, self.app.config.redact_username_during_deletion, self.app.config.redact_email_during_deletion
+                )
                 if self.app.config.redact_username_during_deletion:
                     role.name = role.name.replace(user.username, uname_hash)
                     role.description = role.description.replace(user.username, uname_hash)
 
                 if self.app.config.redact_email_during_deletion:
                     role.name = role.name.replace(user.email, email_hash)
                     role.description = role.description.replace(user.email, email_hash)
@@ -743,51 +790,47 @@
             if self.app.config.redact_email_during_deletion:
                 user.email = email_hash
             if self.app.config.redact_username_during_deletion:
                 user.username = uname_hash
 
             trans.sa_session.add(user)
             trans.sa_session.flush()
-            message += " %s " % user.email
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='users',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+            message += f" {user.email} "
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def undelete_user(self, trans, **kwd):
-        id = kwd.get('id', None)
+        id = kwd.get("id", None)
         if not id:
             message = "No user ids received for undeleting"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='users',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="users", message=message, status="error")
+            )
         ids = util.listify(id)
         count = 0
         undeleted_users = ""
         for user_id in ids:
             user = get_user(trans, user_id)
             if not user.deleted:
-                message = "User '%s' has not been deleted, so it cannot be undeleted." % user.email
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='users',
-                                                         message=util.sanitize_text(message),
-                                                         status='error'))
+                message = f"User '{user.email}' has not been deleted, so it cannot be undeleted."
+                trans.response.send_redirect(
+                    web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status="error")
+                )
             user.deleted = False
             trans.sa_session.add(user)
             trans.sa_session.flush()
             count += 1
-            undeleted_users += " %s" % user.email
+            undeleted_users += f" {user.email}"
         message = "Undeleted %d users: %s" % (count, undeleted_users)
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='users',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def purge_user(self, trans, **kwd):
         # This method should only be called for a User that has previously been deleted.
         # We keep the User in the database ( marked as purged ), and stuff associated
         # with the user's private role in case we want the ability to unpurge the user
@@ -796,32 +839,30 @@
         # - History where user_id = User.id
         #    - HistoryDatasetAssociation where history_id = History.id
         #    - Dataset where HistoryDatasetAssociation.dataset_id = Dataset.id
         # - UserGroupAssociation where user_id == User.id
         # - UserRoleAssociation where user_id == User.id EXCEPT FOR THE PRIVATE ROLE
         # - UserAddress where user_id == User.id
         # Purging Histories and Datasets must be handled via the cleanup_datasets.py script
-        id = kwd.get('id', None)
+        id = kwd.get("id", None)
         if not id:
             message = "No user ids received for purging"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='users',
-                                                     message=util.sanitize_text(message),
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status="error")
+            )
         ids = util.listify(id)
         message = "Purged %d users: " % len(ids)
         for user_id in ids:
             user = get_user(trans, user_id)
             if not user.deleted:
                 # We should never reach here, but just in case there is a bug somewhere...
-                message = "User '%s' has not been deleted, so it cannot be purged." % user.email
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='users',
-                                                         message=util.sanitize_text(message),
-                                                         status='error'))
+                message = f"User '{user.email}' has not been deleted, so it cannot be purged."
+                trans.response.send_redirect(
+                    web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status="error")
+                )
             private_role = trans.app.security_agent.get_private_user_role(user)
             # Delete History
             for h in user.active_histories:
                 trans.sa_session.refresh(h)
                 for hda in h.active_datasets:
                     # Delete HistoryDatasetAssociation
                     d = trans.sa_session.query(trans.app.model.Dataset).get(hda.dataset_id)
@@ -843,25 +884,24 @@
             # Delete UserAddresses
             for address in user.addresses:
                 trans.sa_session.delete(address)
             # Purge the user
             user.purged = True
             trans.sa_session.add(user)
             trans.sa_session.flush()
-            message += "%s " % user.email
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='users',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+            message += f"{user.email} "
+        trans.response.send_redirect(
+            web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status="done")
+        )
 
     @web.expose
     @web.require_admin
     def users(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "roles":
                 return self.user(trans, **kwd)
             elif operation == "reset password":
                 return self.reset_user_password(trans, **kwd)
             elif operation == "delete":
                 return self.mark_user_deleted(trans, **kwd)
             elif operation == "undelete":
@@ -884,114 +924,129 @@
         return self.user_list_grid(trans, **kwd)
 
     @web.expose
     @web.require_admin
     def name_autocomplete_data(self, trans, q=None, limit=None, timestamp=None):
         """Return autocomplete data for user emails"""
         ac_data = ""
-        for user in trans.sa_session.query(trans.app.model.User).filter_by(deleted=False).filter(func.lower(trans.app.model.User.email).like(q.lower() + "%")):
-            ac_data = ac_data + user.email + "\n"
+        for user in (
+            trans.sa_session.query(trans.app.model.User)
+            .filter_by(deleted=False)
+            .filter(func.lower(trans.app.model.User.email).like(f"{q.lower()}%"))
+        ):
+            ac_data = f"{ac_data + user.email}\n"
         return ac_data
 
     @web.expose
     @web.require_admin
     def manage_roles_and_groups_for_user(self, trans, **kwd):
-        user_id = kwd.get('id', None)
-        message = ''
-        status = ''
+        user_id = kwd.get("id", None)
+        message = ""
+        status = ""
         if not user_id:
-            message += "Invalid user id (%s) received" % str(user_id)
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='users',
-                                                     message=util.sanitize_text(message),
-                                                     status='error'))
+            message += f"Invalid user id ({str(user_id)}) received"
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status="error")
+            )
         user = get_user(trans, user_id)
         private_role = trans.app.security_agent.get_private_user_role(user)
-        if kwd.get('user_roles_groups_edit_button', False):
+        if kwd.get("user_roles_groups_edit_button", False):
             # Make sure the user is not dis-associating himself from his private role
-            out_roles = kwd.get('out_roles', [])
+            out_roles = kwd.get("out_roles", [])
             if out_roles:
                 out_roles = [trans.sa_session.query(trans.app.model.Role).get(x) for x in util.listify(out_roles)]
             if private_role in out_roles:
                 message += "You cannot eliminate a user's private role association.  "
-                status = 'error'
-            in_roles = kwd.get('in_roles', [])
+                status = "error"
+            in_roles = kwd.get("in_roles", [])
             if in_roles:
                 in_roles = [trans.sa_session.query(trans.app.model.Role).get(x) for x in util.listify(in_roles)]
-            out_groups = kwd.get('out_groups', [])
+            out_groups = kwd.get("out_groups", [])
             if out_groups:
                 out_groups = [trans.sa_session.query(trans.app.model.Group).get(x) for x in util.listify(out_groups)]
-            in_groups = kwd.get('in_groups', [])
+            in_groups = kwd.get("in_groups", [])
             if in_groups:
                 in_groups = [trans.sa_session.query(trans.app.model.Group).get(x) for x in util.listify(in_groups)]
             if in_roles:
                 trans.app.security_agent.set_entity_user_associations(users=[user], roles=in_roles, groups=in_groups)
                 trans.sa_session.refresh(user)
-                message += "User '%s' has been updated with %d associated roles and %d associated groups (private roles are not displayed)" % \
-                    (user.email, len(in_roles), len(in_groups))
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='users',
-                                                         message=util.sanitize_text(message),
-                                                         status='done'))
+                message += (
+                    "User '%s' has been updated with %d associated roles and %d associated groups (private roles are not displayed)"
+                    % (user.email, len(in_roles), len(in_groups))
+                )
+                trans.response.send_redirect(
+                    web.url_for(controller="admin", action="users", message=util.sanitize_text(message), status="done")
+                )
         in_roles = []
         out_roles = []
         in_groups = []
         out_groups = []
-        for role in trans.sa_session.query(trans.app.model.Role).filter(trans.app.model.Role.table.c.deleted == false()) \
-                .order_by(trans.app.model.Role.table.c.name):
+        for role in (
+            trans.sa_session.query(trans.app.model.Role)
+            .filter(trans.app.model.Role.table.c.deleted == false())
+            .order_by(trans.app.model.Role.table.c.name)
+        ):
             if role in [x.role for x in user.roles]:
                 in_roles.append((role.id, role.name))
             elif role.type != trans.app.model.Role.types.PRIVATE:
                 # There is a 1 to 1 mapping between a user and a PRIVATE role, so private roles should
                 # not be listed in the roles form fields, except for the currently selected user's private
                 # role, which should always be in in_roles.  The check above is added as an additional
                 # precaution, since for a period of time we were including private roles in the form fields.
                 out_roles.append((role.id, role.name))
-        for group in trans.sa_session.query(trans.app.model.Group).filter(trans.app.model.Group.table.c.deleted == false()) \
-                .order_by(trans.app.model.Group.table.c.name):
+        for group in (
+            trans.sa_session.query(trans.app.model.Group)
+            .filter(trans.app.model.Group.table.c.deleted == false())
+            .order_by(trans.app.model.Group.table.c.name)
+        ):
             if group in [x.group for x in user.groups]:
                 in_groups.append((group.id, group.name))
             else:
                 out_groups.append((group.id, group.name))
-        message += "User '%s' is currently associated with %d roles and is a member of %d groups" % \
-            (user.email, len(in_roles), len(in_groups))
+        message += "User '%s' is currently associated with %d roles and is a member of %d groups" % (
+            user.email,
+            len(in_roles),
+            len(in_groups),
+        )
         if not status:
-            status = 'done'
-        return trans.fill_template('/webapps/tool_shed/admin/user/user.mako',
-                                   user=user,
-                                   in_roles=in_roles,
-                                   out_roles=out_roles,
-                                   in_groups=in_groups,
-                                   out_groups=out_groups,
-                                   message=message,
-                                   status=status)
+            status = "done"
+        return trans.fill_template(
+            "/webapps/tool_shed/admin/user/user.mako",
+            user=user,
+            in_roles=in_roles,
+            out_roles=out_roles,
+            in_groups=in_groups,
+            out_groups=out_groups,
+            message=message,
+            status=status,
+        )
 
 
 # ---- Utility methods -------------------------------------------------------
 
 
 def get_user(trans, user_id):
     """Get a User from the database by id."""
     user = trans.sa_session.query(trans.model.User).get(trans.security.decode_id(user_id))
     if not user:
-        return trans.show_error_message("User not found for id (%s)" % str(user_id))
+        return trans.show_error_message(f"User not found for id ({str(user_id)})")
     return user
 
 
 def get_role(trans, id):
     """Get a Role from the database by id."""
     # Load user from database
     id = trans.security.decode_id(id)
     role = trans.sa_session.query(trans.model.Role).get(id)
     if not role:
-        return trans.show_error_message("Role not found for id (%s)" % str(id))
+        return trans.show_error_message(f"Role not found for id ({str(id)})")
     return role
 
 
 def get_group(trans, id):
     """Get a Group from the database by id."""
     # Load user from database
     id = trans.security.decode_id(id)
     group = trans.sa_session.query(trans.model.Group).get(id)
     if not group:
-        return trans.show_error_message("Group not found for id (%s)" % str(id))
+        return trans.show_error_message(f"Group not found for id ({str(id)})")
     return group
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/commit_util.py` & `galaxy-web-apps-23.0.2/tool_shed/util/commit_util.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,175 +1,199 @@
+import bz2
 import gzip
 import json
 import logging
 import os
 import shutil
-import sys
 import tempfile
 from collections import namedtuple
 
 from sqlalchemy.sql.expression import null
 
 import tool_shed.repository_types.util as rt_util
 from galaxy.util import checkers
 from galaxy.util.path import safe_relpath
 from tool_shed.tools.data_table_manager import ShedToolDataTableManager
-from tool_shed.util import basic_util, hg_util, shed_util_common as suc
-
-if sys.version_info < (3, 3):
-    import bz2file as bz2
-else:
-    import bz2
+from tool_shed.util import (
+    basic_util,
+    hg_util,
+    shed_util_common as suc,
+)
 
 log = logging.getLogger(__name__)
 
-UNDESIRABLE_DIRS = ['.hg', '.svn', '.git', '.cvs']
-UNDESIRABLE_FILES = ['.hg_archival.txt', 'hgrc', '.DS_Store', 'tool_test_output.html', 'tool_test_output.json']
+UNDESIRABLE_DIRS = [".hg", ".svn", ".git", ".cvs", ".idea"]
+UNDESIRABLE_FILES = [".hg_archival.txt", "hgrc", ".DS_Store", "tool_test_output.html", "tool_test_output.json"]
 
 
 def check_archive(repository, archive):
     valid = []
     invalid = []
     errors = []
     undesirable_files = []
     undesirable_dirs = []
     for member in archive.getmembers():
         # Allow regular files and directories only
         if not (member.isdir() or member.isfile() or member.islnk()):
-            errors.append("Uploaded archives can only include regular directories and files (no symbolic links, devices, etc).")
+            errors.append(
+                "Uploaded archives can only include regular directories and files (no symbolic links, devices, etc)."
+            )
             invalid.append(member)
             continue
         if not safe_relpath(member.name):
             errors.append("Uploaded archives cannot contain files that would extract outside of the archive.")
             invalid.append(member)
             continue
         if os.path.basename(member.name) in UNDESIRABLE_FILES:
             undesirable_files.append(member)
             continue
         head = tail = member.name
-        try:
-            while tail:
-                head, tail = os.path.split(head)
-                if tail in UNDESIRABLE_DIRS:
-                    undesirable_dirs.append(member)
-                    assert False
-        except AssertionError:
+        found_undesirable_dir = False
+        while tail:
+            head, tail = os.path.split(head)
+            if tail in UNDESIRABLE_DIRS:
+                undesirable_dirs.append(member)
+                found_undesirable_dir = True
+                break
+        if found_undesirable_dir:
             continue
-        if repository.type == rt_util.REPOSITORY_SUITE_DEFINITION and member.name != rt_util.REPOSITORY_DEPENDENCY_DEFINITION_FILENAME:
-            errors.append('Repositories of type <b>Repository suite definition</b> can contain only a single file named <b>repository_dependencies.xml</b>.')
+        if (
+            repository.type == rt_util.REPOSITORY_SUITE_DEFINITION
+            and member.name != rt_util.REPOSITORY_DEPENDENCY_DEFINITION_FILENAME
+        ):
+            errors.append(
+                "Repositories of type <b>Repository suite definition</b> can contain only a single file named <b>repository_dependencies.xml</b>."
+            )
             invalid.append(member)
             continue
-        if repository.type == rt_util.TOOL_DEPENDENCY_DEFINITION and member.name != rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME:
-            errors.append('Repositories of type <b>Tool dependency definition</b> can contain only a single file named <b>tool_dependencies.xml</b>.')
+        if (
+            repository.type == rt_util.TOOL_DEPENDENCY_DEFINITION
+            and member.name != rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME
+        ):
+            errors.append(
+                "Repositories of type <b>Tool dependency definition</b> can contain only a single file named <b>tool_dependencies.xml</b>."
+            )
             invalid.append(member)
             continue
         valid.append(member)
-    ArchiveCheckResults = namedtuple('ArchiveCheckResults', ['valid', 'invalid', 'undesirable_files', 'undesirable_dirs', 'errors'])
+    ArchiveCheckResults = namedtuple(
+        "ArchiveCheckResults", ["valid", "invalid", "undesirable_files", "undesirable_dirs", "errors"]
+    )
     return ArchiveCheckResults(valid, invalid, undesirable_files, undesirable_dirs, errors)
 
 
 def check_file_contents_for_email_alerts(app):
     """
     See if any admin users have chosen to receive email alerts when a repository is updated.
     If so, the file contents of the update must be checked for inappropriate content.
     """
-    sa_session = app.model.context.current
+    sa_session = app.model.session
     admin_users = app.config.get("admin_users", "").split(",")
-    for repository in sa_session.query(app.model.Repository) \
-                                .filter(app.model.Repository.table.c.email_alerts != null()):
+    for repository in sa_session.query(app.model.Repository).filter(
+        app.model.Repository.table.c.email_alerts != null()
+    ):
         email_alerts = json.loads(repository.email_alerts)
         for user_email in email_alerts:
             if user_email in admin_users:
                 return True
     return False
 
 
 def check_file_content_for_html_and_images(file_path):
-    message = ''
+    message = ""
     if checkers.check_html(file_path):
-        message = 'The file "%s" contains HTML content.\n' % str(file_path)
+        message = f'The file "{str(file_path)}" contains HTML content.\n'
     elif checkers.check_image(file_path):
-        message = 'The file "%s" contains image content.\n' % str(file_path)
+        message = f'The file "{str(file_path)}" contains image content.\n'
     return message
 
 
 def get_change_lines_in_file_for_tag(tag, change_dict):
     """
     The received change_dict is the jsonified version of the changes to a file in a
     changeset being pushed to the Tool Shed from the command line. This method cleans
     and returns appropriate lines for inspection.
     """
     cleaned_lines = []
-    data_list = change_dict.get('data', [])
+    data_list = change_dict.get("data", [])
     for data_dict in data_list:
-        block = data_dict.get('block', '')
-        lines = block.split('\\n')
+        block = data_dict.get("block", "")
+        lines = block.split("\\n")
         for line in lines:
             index = line.find(tag)
             if index > -1:
                 line = line[index:]
                 cleaned_lines.append(line)
     return cleaned_lines
 
 
 def get_upload_point(repository, **kwd):
-    upload_point = kwd.get('upload_point', None)
+    upload_point = kwd.get("upload_point", None)
     if upload_point is not None:
         # The value of upload_point will be something like: database/community_files/000/repo_12/1.bed
         if os.path.exists(upload_point):
             if os.path.isfile(upload_point):
                 # Get the parent directory
                 upload_point, not_needed = os.path.split(upload_point)
                 # Now the value of uplaod_point will be something like: database/community_files/000/repo_12/
-            upload_point = upload_point.split('repo_%d' % repository.id)[1]
+            upload_point = upload_point.split("repo_%d" % repository.id)[1]
             if upload_point:
-                upload_point = upload_point.lstrip('/')
-                upload_point = upload_point.rstrip('/')
+                upload_point = upload_point.lstrip("/")
+                upload_point = upload_point.rstrip("/")
             # Now the value of uplaod_point will be something like: /
-            if upload_point == '/':
+            if upload_point == "/":
                 upload_point = None
         else:
             # Must have been an error selecting something that didn't exist, so default to repository root
             upload_point = None
     return upload_point
 
 
 def handle_bz2(repository, uploaded_file_name):
-    fd, uncompressed = tempfile.mkstemp(prefix='repo_%d_upload_bunzip2_' % repository.id,
-                                        dir=os.path.dirname(uploaded_file_name),
-                                        text=False)
-    bzipped_file = bz2.BZ2File(uploaded_file_name, 'rb')
-    while 1:
-        try:
-            chunk = bzipped_file.read(basic_util.CHUNK_SIZE)
-        except IOError:
-            os.close(fd)
-            os.remove(uncompressed)
-            log.exception('Problem uncompressing bz2 data "%s"', uploaded_file_name)
-            return
-        if not chunk:
-            break
-        os.write(fd, chunk)
-    os.close(fd)
-    bzipped_file.close()
-    shutil.move(uncompressed, uploaded_file_name)
-
-
-def handle_directory_changes(app, host, username, repository, full_path, filenames_in_archive, remove_repo_files_not_in_tar,
-                             new_repo_alert, commit_message, undesirable_dirs_removed, undesirable_files_removed):
+    with tempfile.NamedTemporaryFile(
+        mode="wb",
+        prefix=f"repo_{repository.id}_upload_bunzip2_",
+        dir=os.path.dirname(uploaded_file_name),
+        delete=False,
+    ) as uncompressed, bz2.BZ2File(uploaded_file_name, "rb") as bzipped_file:
+        while 1:
+            try:
+                chunk = bzipped_file.read(basic_util.CHUNK_SIZE)
+            except OSError:
+                os.remove(uncompressed.name)
+                log.exception(f'Problem uncompressing bz2 data "{uploaded_file_name}"')
+                return
+            if not chunk:
+                break
+            uncompressed.write(chunk)
+    shutil.move(uncompressed.name, uploaded_file_name)
+
+
+def handle_directory_changes(
+    app,
+    host,
+    username,
+    repository,
+    full_path,
+    filenames_in_archive,
+    remove_repo_files_not_in_tar,
+    new_repo_alert,
+    commit_message,
+    undesirable_dirs_removed,
+    undesirable_files_removed,
+):
     repo_path = repository.repo_path(app)
-    content_alert_str = ''
+    content_alert_str = ""
     files_to_remove = []
-    filenames_in_archive = [os.path.join(full_path, name) for name in filenames_in_archive]
-    repo = repository.hg_repo
+    filenames_in_archive = [os.path.normpath(os.path.join(full_path, name)) for name in filenames_in_archive]
     if remove_repo_files_not_in_tar and not repository.is_new():
         # We have a repository that is not new (it contains files), so discover those files that are in the
         # repository, but not in the uploaded archive.
         for root, dirs, files in os.walk(full_path):
-            if root.find('.hg') < 0 and root.find('hgrc') < 0:
+            if root.find(".hg") < 0 and root.find("hgrc") < 0:
                 for undesirable_dir in UNDESIRABLE_DIRS:
                     if undesirable_dir in dirs:
                         dirs.remove(undesirable_dir)
                         undesirable_dirs_removed += 1
                 for undesirable_file in UNDESIRABLE_FILES:
                     if undesirable_file in files:
                         files.remove(undesirable_file)
@@ -178,86 +202,67 @@
                     full_name = os.path.join(root, name)
                     if full_name not in filenames_in_archive:
                         files_to_remove.append(full_name)
         for repo_file in files_to_remove:
             # Remove files in the repository (relative to the upload point) that are not in
             # the uploaded archive.
             try:
-                hg_util.remove_file(repo_path, repo_file, force=True)
+                hg_util.remove_path(repo_path, repo_file)
             except Exception as e:
-                log.debug("Error removing files using the mercurial API, so trying a different approach, the error was: %s" % str(e))
-                relative_selected_file = repo_file.split('repo_%d' % repository.id)[1].lstrip('/')
-                repo.dirstate.remove(relative_selected_file)
-                repo.dirstate.write()
-                absolute_selected_file = os.path.abspath(repo_file)
-                if os.path.isdir(absolute_selected_file):
-                    try:
-                        os.rmdir(absolute_selected_file)
-                    except OSError:
-                        # The directory is not empty.
-                        pass
-                elif os.path.isfile(absolute_selected_file):
-                    os.remove(absolute_selected_file)
-                    dir = os.path.split(absolute_selected_file)[0]
-                    try:
-                        os.rmdir(dir)
-                    except OSError:
-                        # The directory is not empty.
-                        pass
+                error_message = f"Error removing file {repo_file} in mercurial repo:\n{e}"
+                log.debug(error_message)
+                return "error", error_message, files_to_remove, content_alert_str, 0, 0
     # See if any admin users have chosen to receive email alerts when a repository is updated.
     # If so, check every uploaded file to ensure content is appropriate.
     check_contents = check_file_contents_for_email_alerts(app)
     for filename_in_archive in filenames_in_archive:
         # Check file content to ensure it is appropriate.
         if check_contents and os.path.isfile(filename_in_archive):
             content_alert_str += check_file_content_for_html_and_images(filename_in_archive)
         hg_util.add_changeset(repo_path, filename_in_archive)
-        if filename_in_archive.endswith('tool_data_table_conf.xml.sample'):
+        if filename_in_archive.endswith("tool_data_table_conf.xml.sample"):
             # Handle the special case where a tool_data_table_conf.xml.sample file is being uploaded
             # by parsing the file and adding new entries to the in-memory app.tool_data_tables
             # dictionary.
             stdtm = ShedToolDataTableManager(app)
             error, message = stdtm.handle_sample_tool_data_table_conf_file(filename_in_archive, persist=False)
             if error:
-                return False, message, files_to_remove, content_alert_str, undesirable_dirs_removed, undesirable_files_removed
-    hg_util.commit_changeset(repo_path,
-                             full_path_to_changeset=full_path,
-                             username=username,
-                             message=commit_message)
+                return (
+                    False,
+                    message,
+                    files_to_remove,
+                    content_alert_str,
+                    undesirable_dirs_removed,
+                    undesirable_files_removed,
+                )
+    hg_util.commit_changeset(repo_path, full_path_to_changeset=full_path, username=username, message=commit_message)
     admin_only = len(repository.downloadable_revisions) != 1
-    suc.handle_email_alerts(app,
-                            host,
-                            repository,
-                            content_alert_str=content_alert_str,
-                            new_repo_alert=new_repo_alert,
-                            admin_only=admin_only)
-    return True, '', files_to_remove, content_alert_str, undesirable_dirs_removed, undesirable_files_removed
+    suc.handle_email_alerts(
+        app, host, repository, content_alert_str=content_alert_str, new_repo_alert=new_repo_alert, admin_only=admin_only
+    )
+    return True, "", files_to_remove, content_alert_str, undesirable_dirs_removed, undesirable_files_removed
 
 
 def handle_gzip(repository, uploaded_file_name):
-    fd, uncompressed = tempfile.mkstemp(prefix='repo_%d_upload_gunzip_' % repository.id,
-                                        dir=os.path.dirname(uploaded_file_name),
-                                        text=False)
-    gzipped_file = gzip.GzipFile(uploaded_file_name, 'rb')
-    while 1:
-        try:
-            chunk = gzipped_file.read(basic_util.CHUNK_SIZE)
-        except IOError:
-            os.close(fd)
-            os.remove(uncompressed)
-            log.exception('Problem uncompressing gz data "%s"', uploaded_file_name)
-            return
-        if not chunk:
-            break
-        os.write(fd, chunk)
-    os.close(fd)
-    gzipped_file.close()
-    shutil.move(uncompressed, uploaded_file_name)
+    with tempfile.NamedTemporaryFile(
+        mode="wb", prefix=f"repo_{repository.id}_upload_gunzip_", dir=os.path.dirname(uploaded_file_name), delete=False
+    ) as uncompressed, gzip.GzipFile(uploaded_file_name, "rb") as gzipped_file:
+        while 1:
+            try:
+                chunk = gzipped_file.read(basic_util.CHUNK_SIZE)
+            except OSError:
+                os.remove(uncompressed.name)
+                log.exception(f'Problem uncompressing gz data "{uploaded_file_name}"')
+                return
+            if not chunk:
+                break
+            uncompressed.write(chunk)
+    shutil.move(uncompressed.name, uploaded_file_name)
 
 
 def uncompress(repository, uploaded_file_name, uploaded_file_filename, isgzip=False, isbz2=False):
     if isgzip:
         handle_gzip(repository, uploaded_file_name)
-        return uploaded_file_filename.rstrip('.gz')
+        return uploaded_file_filename.rstrip(".gz")
     if isbz2:
         handle_bz2(repository, uploaded_file_name)
-        return uploaded_file_filename.rstrip('.bz2')
+        return uploaded_file_filename.rstrip(".bz2")
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/hg_util.py` & `galaxy-web-apps-23.0.2/tool_shed/util/hg_util.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import logging
 import os
+import shutil
 import subprocess
 import tempfile
 from datetime import datetime
 from time import gmtime
 
 from galaxy.tool_shed.util import basic_util
 from galaxy.tool_shed.util.hg_util import (
@@ -18,75 +19,81 @@
     reversed_upper_bounded_changelog,
     update_repository,
 )
 from galaxy.util import unicodify
 
 log = logging.getLogger(__name__)
 
-INITIAL_CHANGELOG_HASH = '000000000000'
+INITIAL_CHANGELOG_HASH = "000000000000"
 
 
 def add_changeset(repo_path, path_to_filename_in_archive):
     try:
-        subprocess.check_output(['hg', 'add', path_to_filename_in_archive], stderr=subprocess.STDOUT, cwd=repo_path)
+        subprocess.check_output(["hg", "add", path_to_filename_in_archive], stderr=subprocess.STDOUT, cwd=repo_path)
     except Exception as e:
-        error_message = "Error adding '%s' to repository: %s" % (path_to_filename_in_archive, unicodify(e))
+        error_message = f"Error adding '{path_to_filename_in_archive}' to repository: {unicodify(e)}"
         if isinstance(e, subprocess.CalledProcessError):
-            error_message += "\nOutput was:\n%s" % unicodify(e.output)
+            error_message += f"\nOutput was:\n{unicodify(e.output)}"
         raise Exception(error_message)
 
 
 def archive_repository_revision(app, repository, archive_dir, changeset_revision):
-    '''Create an un-versioned archive of a repository.'''
+    """Create an un-versioned archive of a repository."""
     repo_path = repository.repo_path(app)
     try:
-        subprocess.check_output(['hg', 'archive', '-r', changeset_revision, archive_dir], stderr=subprocess.STDOUT, cwd=repo_path)
+        subprocess.check_output(
+            ["hg", "archive", "-r", changeset_revision, archive_dir], stderr=subprocess.STDOUT, cwd=repo_path
+        )
     except Exception as e:
-        error_message = "Error attempting to archive revision '%s' of repository '%s': %s" % (changeset_revision, repository.name, unicodify(e))
+        error_message = f"Error attempting to archive revision '{changeset_revision}' of repository '{repository.name}': {unicodify(e)}"
         if isinstance(e, subprocess.CalledProcessError):
-            error_message += "\nOutput was:\n%s" % unicodify(e.output)
+            error_message += f"\nOutput was:\n{unicodify(e.output)}"
         log.exception(error_message)
         raise Exception(error_message)
 
 
 def commit_changeset(repo_path, full_path_to_changeset, username, message):
     try:
-        subprocess.check_output(['hg', 'commit', '-u', username, '-m', message, full_path_to_changeset], stderr=subprocess.STDOUT, cwd=repo_path)
+        subprocess.check_output(
+            ["hg", "commit", "-u", username, "-m", message, full_path_to_changeset],
+            stderr=subprocess.STDOUT,
+            cwd=repo_path,
+        )
     except Exception as e:
-        error_message = "Error committing '%s' to repository: %s" % (full_path_to_changeset, unicodify(e))
+        error_message = f"Error committing '{full_path_to_changeset}' to repository: {unicodify(e)}"
         if isinstance(e, subprocess.CalledProcessError):
-            if e.returncode == 1 and 'nothing changed' in unicodify(e.output):
+            if e.returncode == 1 and "nothing changed" in unicodify(e.output):
                 return
-            error_message += "\nOutput was:\n%s" % unicodify(e.output)
+            error_message += f"\nOutput was:\n{unicodify(e.output)}"
         raise Exception(error_message)
 
 
 def get_hgrc_path(repo_path):
-    return os.path.join(repo_path, '.hg', 'hgrc')
+    return os.path.join(repo_path, ".hg", "hgrc")
 
 
 def create_hgrc_file(app, repository):
     # Since we support both http and https, we set `push_ssl` to False to
     # override the default (which is True) in the Mercurial API.
     # The hg purge extension purges all files and directories not being tracked
     # by Mercurial in the current repository. It will remove unknown files and
     # empty directories. This is not currently used because it is not supported
     # in the Mercurial API.
     repo_path = repository.repo_path(app)
     hgrc_path = get_hgrc_path(repo_path)
-    with open(hgrc_path, 'w') as fp:
-        fp.write('[paths]\n')
-        fp.write('default = .\n')
-        fp.write('default-push = .\n')
-        fp.write('[web]\n')
-        fp.write('allow_push = %s\n' % repository.user.username)
-        fp.write('name = %s\n' % repository.name)
-        fp.write('push_ssl = false\n')
-        fp.write('[extensions]\n')
-        fp.write('hgext.purge=')
+    with open(hgrc_path, "w") as fp:
+        fp.write("[paths]\n")
+        fp.write("default = .\n")
+        fp.write("default-push = .\n")
+        fp.write("[web]\n")
+        fp.write(f"allow_push = {repository.user.username}\n")
+        fp.write(f"name = {repository.name}\n")
+        fp.write("push_ssl = false\n")
+        fp.write("[extensions]\n")
+        fp.write("hgext.purge=")
 
 
 def get_named_tmpfile_from_ctx(ctx, filename, dir):
     """
     Return a named temporary file created from a specified file with a given name included in a repository
     changeset revision.
     """
@@ -98,18 +105,18 @@
                 # If the file was moved, its destination file contents will be returned here.
                 fctx = ctx[ctx_file]
             except LookupError:
                 # Continue looking in case the file was moved.
                 fctx = None
                 continue
             if fctx:
-                fh = tempfile.NamedTemporaryFile('wb', prefix="tmp-toolshed-gntfc", dir=dir)
+                fh = tempfile.NamedTemporaryFile("wb", prefix="tmp-toolshed-gntfc", dir=dir)
                 tmp_filename = fh.name
                 fh.close()
-                fh = open(tmp_filename, 'wb')
+                fh = open(tmp_filename, "wb")
                 fh.write(fctx.data())
                 fh.close()
                 return tmp_filename
     return None
 
 
 def get_readable_ctx_date(ctx):
@@ -141,139 +148,147 @@
     """
     repo = repository.hg_repo
     ctx = get_changectx_for_changeset(repo, changeset_revision)
     if ctx:
         return get_revision_label_from_ctx(ctx, include_date=include_date, include_hash=include_hash)
     else:
         if include_hash:
-            return "-1:%s" % changeset_revision
+            return f"-1:{changeset_revision}"
         else:
             return "-1"
 
 
-def get_rev_label_changeset_revision_from_repository_metadata(app, repository_metadata, repository=None,
-                                                              include_date=True, include_hash=True):
+def get_rev_label_changeset_revision_from_repository_metadata(
+    app, repository_metadata, repository=None, include_date=True, include_hash=True
+):
     if repository is None:
         repository = repository_metadata.repository
     repo = repository.hg_repo
     changeset_revision = repository_metadata.changeset_revision
     ctx = get_changectx_for_changeset(repo, changeset_revision)
     if ctx:
-        rev = '%04d' % ctx.rev()
+        rev = "%04d" % ctx.rev()
         if include_date:
             changeset_revision_date = get_readable_ctx_date(ctx)
             if include_hash:
-                label = "%s:%s (%s)" % (str(ctx.rev()), changeset_revision, changeset_revision_date)
+                label = f"{ctx.rev()}:{changeset_revision} ({changeset_revision_date})"
             else:
-                label = "%s (%s)" % (str(ctx.rev()), changeset_revision_date)
+                label = f"{ctx.rev()} ({changeset_revision_date})"
         else:
             if include_hash:
-                label = "%s:%s" % (str(ctx.rev()), changeset_revision)
+                label = f"{ctx.rev()}:{changeset_revision}"
             else:
-                label = "%s" % str(ctx.rev())
+                label = f"{ctx.rev()}"
     else:
-        rev = '-1'
+        rev = "-1"
         if include_hash:
-            label = "-1:%s" % changeset_revision
+            label = f"-1:{changeset_revision}"
         else:
             label = "-1"
     return rev, label, changeset_revision
 
 
 def get_revision_label_from_ctx(ctx, include_date=True, include_hash=True):
     if include_date:
         if include_hash:
-            return '%s:%s <i><font color="#666666">(%s)</font></i>' % \
-                (str(ctx.rev()), str(ctx), str(get_readable_ctx_date(ctx)))
+            return f'{ctx.rev()}:{ctx} <i><font color="#666666">({get_readable_ctx_date(ctx)})</font></i>'
         else:
-            return '%s <i><font color="#666666">(%s)</font></i>' % \
-                (str(ctx.rev()), str(get_readable_ctx_date(ctx)))
+            return f'{ctx.rev()} <i><font color="#666666">({get_readable_ctx_date(ctx)})</font></i>'
     else:
         if include_hash:
-            return '%s:%s' % (str(ctx.rev()), str(ctx))
+            return f"{ctx.rev()}:{ctx}"
         else:
             return str(ctx.rev())
 
 
 def get_rev_label_from_changeset_revision(repo, changeset_revision, include_date=True, include_hash=True):
     """
     Given a changeset revision hash, return two strings, the changeset rev and the changeset revision hash
     which includes the revision date if the receive include_date is True.
     """
     ctx = get_changectx_for_changeset(repo, changeset_revision)
     if ctx:
-        rev = '%04d' % ctx.rev()
+        rev = "%04d" % ctx.rev()
         label = get_revision_label_from_ctx(ctx, include_date=include_date)
     else:
-        rev = '-1'
-        label = "-1:%s" % changeset_revision
+        rev = "-1"
+        label = f"-1:{changeset_revision}"
     return rev, label
 
 
-def remove_file(repo_path, selected_file, force=True):
-    cmd = ['hg', 'remove']
-    if force:
-        cmd.append('--force')
-    cmd.append(selected_file)
+def remove_path(repo_path, selected_file):
+    cmd = ["hg", "remove", "--force", selected_file]
     try:
         subprocess.check_output(cmd, stderr=subprocess.STDOUT, cwd=repo_path)
     except Exception as e:
-        error_message = "Error removing file '%s': %s" % (selected_file, unicodify(e))
+        error_message = f"Error removing path '{selected_file}': {unicodify(e)}"
         if isinstance(e, subprocess.CalledProcessError):
-            error_message += "\nOutput was:\n%s" % unicodify(e.output)
+            output = unicodify(e.output)
+            if "is untracked" in output:
+                # That's ok, happens if we add a new file or directory via tarball upload,
+                # just delete the file or dir on disk
+                selected_file_path = os.path.join(repo_path, selected_file)
+                if os.path.isdir(selected_file_path):
+                    shutil.rmtree(selected_file_path)
+                else:
+                    os.remove(selected_file_path)
+                return
+            error_message += f"\nOutput was:\n{output}"
         raise Exception(error_message)
 
 
 def init_repository(repo_path):
     """
     Create a new Mercurial repository in the given directory.
     """
     try:
-        subprocess.check_output(['hg', 'init'], stderr=subprocess.STDOUT, cwd=repo_path)
+        subprocess.check_output(["hg", "init"], stderr=subprocess.STDOUT, cwd=repo_path)
     except Exception as e:
-        error_message = 'Error initializing repository: %s' % unicodify(e)
+        error_message = f"Error initializing repository: {unicodify(e)}"
         if isinstance(e, subprocess.CalledProcessError):
-            error_message += "\nOutput was:\n%s" % unicodify(e.output)
+            error_message += f"\nOutput was:\n{unicodify(e.output)}"
         raise Exception(error_message)
 
 
 def changeset2rev(repo_path, changeset_revision):
     """
     Return the revision number (as an int) corresponding to a specified changeset revision.
     """
     try:
-        rev = subprocess.check_output(['hg', 'id', '-r', changeset_revision, '-n'], stderr=subprocess.STDOUT, cwd=repo_path)
+        rev = subprocess.check_output(
+            ["hg", "id", "-r", changeset_revision, "-n"], stderr=subprocess.STDOUT, cwd=repo_path
+        )
     except Exception as e:
-        error_message = "Error looking for changeset '%s': %s" % (changeset_revision, unicodify(e))
+        error_message = f"Error looking for changeset '{changeset_revision}': {unicodify(e)}"
         if isinstance(e, subprocess.CalledProcessError):
-            error_message += "\nOutput was:\n%s" % unicodify(e.output)
+            error_message += f"\nOutput was:\n{unicodify(e.output)}"
         raise Exception(error_message)
     return int(rev.strip())
 
 
 __all__ = (
-    'add_changeset',
-    'archive_repository_revision',
-    'clone_repository',
-    'commit_changeset',
-    'copy_file_from_manifest',
-    'create_hgrc_file',
-    'get_changectx_for_changeset',
-    'get_config_from_disk',
-    'get_ctx_file_path_from_manifest',
-    'get_file_context_from_ctx',
-    'get_named_tmpfile_from_ctx',
-    'get_readable_ctx_date',
-    'get_repository_heads',
-    'get_reversed_changelog_changesets',
-    'get_revision_label',
-    'get_rev_label_changeset_revision_from_repository_metadata',
-    'get_revision_label_from_ctx',
-    'get_rev_label_from_changeset_revision',
-    'pull_repository',
-    'remove_file',
-    'reversed_lower_upper_bounded_changelog',
-    'reversed_upper_bounded_changelog',
-    'update_repository',
-    'init_repository',
-    'changeset2rev',
+    "add_changeset",
+    "archive_repository_revision",
+    "clone_repository",
+    "commit_changeset",
+    "copy_file_from_manifest",
+    "create_hgrc_file",
+    "get_changectx_for_changeset",
+    "get_config_from_disk",
+    "get_ctx_file_path_from_manifest",
+    "get_file_context_from_ctx",
+    "get_named_tmpfile_from_ctx",
+    "get_readable_ctx_date",
+    "get_repository_heads",
+    "get_reversed_changelog_changesets",
+    "get_revision_label",
+    "get_rev_label_changeset_revision_from_repository_metadata",
+    "get_revision_label_from_ctx",
+    "get_rev_label_from_changeset_revision",
+    "pull_repository",
+    "remove_path",
+    "reversed_lower_upper_bounded_changelog",
+    "reversed_upper_bounded_changelog",
+    "update_repository",
+    "init_repository",
+    "changeset2rev",
 )
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/hgweb_config.py` & `galaxy-web-apps-23.0.2/tool_shed/util/hgweb_config.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,107 +1,108 @@
+import configparser
 import logging
 import os
 import shutil
 import threading
 from datetime import date
 
-from six.moves import configparser
-
 from galaxy.util import unicodify
 
 log = logging.getLogger(__name__)
 
 new_hgweb_config_template = """
 [paths]
 
 """
 
 
-class HgWebConfigManager(object):
+class HgWebConfigManager:
     def __init__(self):
         self.hgweb_config_dir = None
         self.in_memory_config = None
 
     def add_entry(self, lhs, rhs):
         """Add an entry in the hgweb.config file for a new repository."""
         lock = threading.Lock()
         lock.acquire(True)
         try:
             # Since we're changing the config, make sure the latest is loaded into memory.
             self.read_config(force_read=True)
             # An entry looks something like: repos/test/mira_assembler = database/community_files/000/repo_123.
-            if rhs.startswith('./'):
-                rhs = rhs.replace('./', '', 1)
+            if rhs.startswith("./"):
+                rhs = rhs.replace("./", "", 1)
             self.make_backup()
             # Add the new entry into memory.
-            self.in_memory_config.set('paths', lhs, rhs)
+            self.in_memory_config.set("paths", lhs, rhs)
             # Persist our in-memory configuration.
             self.write_config()
         except Exception as e:
             log.debug("Exception in HgWebConfigManager.add_entry(): %s", unicodify(e))
         finally:
             lock.release()
 
     def change_entry(self, old_lhs, new_lhs, new_rhs):
         """Change an entry in the hgweb.config file for a repository - this only happens when the owner changes the name of the repository."""
         lock = threading.Lock()
         lock.acquire(True)
         try:
             self.make_backup()
             # Remove the old entry.
-            self.in_memory_config.remove_option('paths', old_lhs)
+            self.in_memory_config.remove_option("paths", old_lhs)
             # Add the new entry.
-            self.in_memory_config.set('paths', new_lhs, new_rhs)
+            self.in_memory_config.set("paths", new_lhs, new_rhs)
             # Persist our in-memory configuration.
             self.write_config()
         except Exception as e:
             log.debug("Exception in HgWebConfigManager.change_entry(): %s", unicodify(e))
         finally:
             lock.release()
 
     def get_entry(self, lhs):
         """Return an entry in the hgweb.config file for a repository"""
         self.read_config()
         try:
-            entry = self.in_memory_config.get('paths', lhs)
+            entry = self.in_memory_config.get("paths", lhs)
         except configparser.NoOptionError:
             try:
                 # We have a multi-threaded front-end, so one of the threads may not have the latest version of the hgweb.config file.
                 self.read_config(force_read=True)
-                entry = self.in_memory_config.get('paths', lhs)
+                entry = self.in_memory_config.get("paths", lhs)
             except configparser.NoOptionError:
-                raise Exception("Entry for repository %s missing in file %s." % (lhs, self.hgweb_config))
+                raise Exception(f"Entry for repository {lhs} missing in file {self.hgweb_config}.")
         return entry
 
     @property
     def hgweb_config(self):
-        hgweb_config = os.path.join(self.hgweb_config_dir, 'hgweb.config')
+        hgweb_config = os.path.join(self.hgweb_config_dir, "hgweb.config")
         if not os.path.exists(hgweb_config):
             # We used to raise an exception here...
             # raise Exception( "Required file %s does not exist - check config setting for hgweb_config_dir." % hgweb_config )
             # ...but now we just log the missing file and create a new empty one.
-            log.debug("Required file %s does not exist, so creating a new, empty file.  Check your config setting for hgweb_config_dir." % hgweb_config)
-            with open(hgweb_config, 'w') as hgweb_config_file:
+            log.debug(
+                f"Required file {hgweb_config} does not exist, so creating a new, empty file.  Check your config setting for hgweb_config_dir."
+            )
+            with open(hgweb_config, "w") as hgweb_config_file:
                 hgweb_config_file.write(new_hgweb_config_template)
         return os.path.abspath(hgweb_config)
 
     def make_backup(self):
         # Make a backup of the hgweb.config file.
         today = date.today()
         backup_date = today.strftime("%Y_%m_%d")
-        hgweb_config_backup_filename = 'hgweb.config_%s_backup' % backup_date
+        hgweb_config_backup_filename = f"hgweb.config_{backup_date}_backup"
         hgweb_config_copy = os.path.join(self.hgweb_config_dir, hgweb_config_backup_filename)
         shutil.copy(os.path.abspath(self.hgweb_config), os.path.abspath(hgweb_config_copy))
 
     def read_config(self, force_read=False):
         if force_read or self.in_memory_config is None:
             config = configparser.ConfigParser()
             config.read(self.hgweb_config)
             self.in_memory_config = config
 
     def write_config(self):
         """Writing the in-memory configuration to the hgweb.config file on disk."""
-        with open(self.hgweb_config, 'w') as config_file:
+        with open(self.hgweb_config, "w") as config_file:
             self.in_memory_config.write(config_file)
 
 
 hgweb_config_manager = HgWebConfigManager()
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/metadata_util.py` & `galaxy-web-apps-23.0.2/tool_shed/util/metadata_util.py`

 * *Files 16% similar despite different names*

```diff
@@ -10,117 +10,124 @@
 from galaxy.tool_shed.util.repository_util import get_repository_by_name_and_owner
 from galaxy.util.tool_shed.common_util import parse_repository_dependency_tuple
 from tool_shed.util.hg_util import changeset2rev
 
 log = logging.getLogger(__name__)
 
 
-def get_all_dependencies(app, metadata_entry, processed_dependency_links=[]):
+def get_all_dependencies(app, metadata_entry, processed_dependency_links=None):
+    processed_dependency_links = processed_dependency_links or []
     encoder = app.security.encode_id
-    value_mapper = {'repository_id': encoder, 'id': encoder, 'user_id': encoder}
-    metadata = metadata_entry.to_dict(value_mapper=value_mapper, view='element')
-    db = app.model.context.current
+    value_mapper = {"repository_id": encoder, "id": encoder, "user_id": encoder}
+    metadata = metadata_entry.to_dict(value_mapper=value_mapper, view="element")
+    db = app.model.session
     returned_dependencies = []
     required_metadata = get_dependencies_for_metadata_revision(app, metadata)
     if required_metadata is None:
         return metadata
     for dependency_metadata in required_metadata:
-        dependency_dict = dependency_metadata.to_dict(value_mapper=value_mapper, view='element')
-        dependency_link = (metadata['id'], dependency_dict['id'])
+        dependency_dict = dependency_metadata.to_dict(value_mapper=value_mapper, view="element")
+        dependency_link = (metadata["id"], dependency_dict["id"])
         if dependency_link in processed_dependency_links:
             continue
         processed_dependency_links.append(dependency_link)
-        repository = db.query(app.model.Repository).get(app.security.decode_id(dependency_dict['repository_id']))
-        dependency_dict['repository'] = repository.to_dict(value_mapper=value_mapper)
+        repository = db.query(app.model.Repository).get(app.security.decode_id(dependency_dict["repository_id"]))
+        dependency_dict["repository"] = repository.to_dict(value_mapper=value_mapper)
         if dependency_metadata.includes_tools:
-            dependency_dict['tools'] = dependency_metadata.metadata['tools']
-        dependency_dict['repository_dependencies'] = []
-        if dependency_dict['includes_tool_dependencies']:
-            dependency_dict['tool_dependencies'] = repository.get_tool_dependencies(app, dependency_dict['changeset_revision'])
-        if dependency_dict['has_repository_dependencies']:
-            dependency_dict['repository_dependencies'] = get_all_dependencies(app, dependency_metadata, processed_dependency_links)
+            dependency_dict["tools"] = dependency_metadata.metadata["tools"]
+        dependency_dict["repository_dependencies"] = []
+        if dependency_dict["includes_tool_dependencies"]:
+            dependency_dict["tool_dependencies"] = repository.get_tool_dependencies(
+                app, dependency_dict["changeset_revision"]
+            )
+        if dependency_dict["has_repository_dependencies"]:
+            dependency_dict["repository_dependencies"] = get_all_dependencies(
+                app, dependency_metadata, processed_dependency_links
+            )
         else:
-            dependency_dict['repository_dependencies'] = []
+            dependency_dict["repository_dependencies"] = []
         returned_dependencies.append(dependency_dict)
     return returned_dependencies
 
 
 def get_current_repository_metadata_for_changeset_revision(app, repository, changeset_revision):
     encoded_repository_id = app.security.encode_id(repository.id)
-    repository_metadata = get_repository_metadata_by_changeset_revision(app,
-                                                                        encoded_repository_id,
-                                                                        changeset_revision)
+    repository_metadata = get_repository_metadata_by_changeset_revision(app, encoded_repository_id, changeset_revision)
     if repository_metadata:
         return repository_metadata
     # The installable changeset_revision may have been changed because it was "moved ahead"
     # in the repository changelog.
-    updated_changeset_revision = get_next_downloadable_changeset_revision(app, repository, after_changeset_revision=changeset_revision)
+    updated_changeset_revision = get_next_downloadable_changeset_revision(
+        app, repository, after_changeset_revision=changeset_revision
+    )
     if updated_changeset_revision and updated_changeset_revision != changeset_revision:
-        repository_metadata = get_repository_metadata_by_changeset_revision(app,
-                                                                            encoded_repository_id,
-                                                                            updated_changeset_revision)
+        repository_metadata = get_repository_metadata_by_changeset_revision(
+            app, encoded_repository_id, updated_changeset_revision
+        )
         if repository_metadata:
             return repository_metadata
     return None
 
 
 def get_dependencies_for_metadata_revision(app, metadata):
     dependencies = []
-    for shed, name, owner, changeset, prior, _ in metadata['repository_dependencies']:
+    for _shed, name, owner, changeset, _prior, _ in metadata["repository_dependencies"]:
         required_repository = get_repository_by_name_and_owner(app, name, owner)
         updated_changeset = get_next_downloadable_changeset_revision(app, required_repository, changeset)
         if updated_changeset is None:
             continue
-        metadata_entry = get_repository_metadata_by_changeset_revision(app, app.security.encode_id(required_repository.id), updated_changeset)
+        metadata_entry = get_repository_metadata_by_changeset_revision(
+            app, app.security.encode_id(required_repository.id), updated_changeset
+        )
         dependencies.append(metadata_entry)
     return dependencies
 
 
 def get_latest_changeset_revision(app, repository):
     repository_tip = repository.tip()
-    repository_metadata = get_repository_metadata_by_changeset_revision(app,
-                                                                        app.security.encode_id(repository.id),
-                                                                        repository_tip)
+    repository_metadata = get_repository_metadata_by_changeset_revision(
+        app, app.security.encode_id(repository.id), repository_tip
+    )
     if repository_metadata and repository_metadata.downloadable:
         return repository_tip
     changeset_revisions = [revision[1] for revision in get_metadata_revisions(app, repository)]
     if changeset_revisions:
         return changeset_revisions[-1]
     return INITIAL_CHANGELOG_HASH
 
 
 def get_latest_downloadable_changeset_revision(app, repository):
     repository_tip = repository.tip()
-    repository_metadata = get_repository_metadata_by_changeset_revision(app, app.security.encode_id(repository.id), repository_tip)
+    repository_metadata = get_repository_metadata_by_changeset_revision(
+        app, app.security.encode_id(repository.id), repository_tip
+    )
     if repository_metadata and repository_metadata.downloadable:
         return repository_tip
     changeset_revisions = [revision[1] for revision in get_metadata_revisions(app, repository)]
     if changeset_revisions:
         return changeset_revisions[-1]
     return INITIAL_CHANGELOG_HASH
 
 
 def get_latest_repository_metadata(app, decoded_repository_id, downloadable=False):
     """Get last metadata defined for a specified repository from the database."""
-    sa_session = app.model.context.current
+    sa_session = app.model.session
     repository = sa_session.query(app.model.Repository).get(decoded_repository_id)
     if downloadable:
         changeset_revision = get_latest_downloadable_changeset_revision(app, repository)
     else:
         changeset_revision = get_latest_changeset_revision(app, repository)
-    return get_repository_metadata_by_changeset_revision(app,
-                                                         app.security.encode_id(repository.id),
-                                                         changeset_revision)
+    return get_repository_metadata_by_changeset_revision(app, app.security.encode_id(repository.id), changeset_revision)
 
 
 def get_metadata_revisions(app, repository, sort_revisions=True, reverse=False, downloadable=True):
     """
     Return a list of changesets for the provided repository.
     """
-    sa_session = app.model.context.current
+    sa_session = app.model.session
     if downloadable:
         metadata_revisions = repository.downloadable_revisions
     else:
         metadata_revisions = repository.metadata_revisions
     repo_path = repository.repo_path(app)
     changeset_tups = []
     for repository_metadata in metadata_revisions:
@@ -193,94 +200,82 @@
     Return a list of of tuples defining repository objects required by the received repository.  The returned
     list defines the entire repository dependency tree.  This method is called only from the Tool Shed.
     """
     dependency_tups = []
     if repository_metadata is not None:
         metadata = repository_metadata.metadata
         if metadata:
-            repository_dependencies_dict = metadata.get('repository_dependencies', None)
+            repository_dependencies_dict = metadata.get("repository_dependencies", None)
             if repository_dependencies_dict is not None:
-                repository_dependency_tups = repository_dependencies_dict.get('repository_dependencies', None)
+                repository_dependency_tups = repository_dependencies_dict.get("repository_dependencies", None)
                 if repository_dependency_tups is not None:
                     # The value of repository_dependency_tups is a list of repository dependency tuples like this:
                     # ['http://localhost:9009', 'package_samtools_0_1_18', 'devteam', 'ef37fc635cb9', 'False', 'False']
                     for repository_dependency_tup in repository_dependency_tups:
-                        toolshed, name, owner, changeset_revision, pir, oicct = \
-                            parse_repository_dependency_tuple(repository_dependency_tup)
+                        toolshed, name, owner, changeset_revision, pir, oicct = parse_repository_dependency_tuple(
+                            repository_dependency_tup
+                        )
                         repository = get_repository_by_name_and_owner(app, name, owner)
                         if repository:
                             if deprecated_only:
                                 if repository.deprecated:
                                     dependency_tups.append(repository_dependency_tup)
                             else:
                                 dependency_tups.append(repository_dependency_tup)
                         else:
-                            log.debug("Cannot locate repository %s owned by %s for inclusion in repository dependency tups." %
-                                (name, owner))
+                            log.debug(
+                                "Cannot locate repository %s owned by %s for inclusion in repository dependency tups."
+                                % (name, owner)
+                            )
     return dependency_tups
 
 
 def get_repository_metadata_by_changeset_revision(app, id, changeset_revision):
     """Get metadata for a specified repository change set from the database."""
     # Make sure there are no duplicate records, and return the single unique record for the changeset_revision.
     # Duplicate records were somehow created in the past.  The cause of this issue has been resolved, but we'll
     # leave this method as is for a while longer to ensure all duplicate records are removed.
-    sa_session = app.model.context.current
-    all_metadata_records = sa_session.query(app.model.RepositoryMetadata) \
-                                     .filter(and_(app.model.RepositoryMetadata.table.c.repository_id == app.security.decode_id(id),
-                                                  app.model.RepositoryMetadata.table.c.changeset_revision == changeset_revision)) \
-                                     .all()
+    sa_session = app.model.session
+    all_metadata_records = (
+        sa_session.query(app.model.RepositoryMetadata)
+        .filter(
+            and_(
+                app.model.RepositoryMetadata.table.c.repository_id == app.security.decode_id(id),
+                app.model.RepositoryMetadata.table.c.changeset_revision == changeset_revision,
+            )
+        )
+        .all()
+    )
     if len(all_metadata_records) > 1:
         # Delete all records older than the last one updated.
         for repository_metadata in all_metadata_records[1:]:
             sa_session.delete(repository_metadata)
             sa_session.flush()
         return all_metadata_records[0]
     elif all_metadata_records:
         return all_metadata_records[0]
     return None
 
 
 def get_repository_metadata_by_id(app, id):
     """Get repository metadata from the database"""
-    sa_session = app.model.context.current
+    sa_session = app.model.session
     return sa_session.query(app.model.RepositoryMetadata).get(app.security.decode_id(id))
 
 
 def get_repository_metadata_by_repository_id_changeset_revision(app, id, changeset_revision, metadata_only=False):
     """Get a specified metadata record for a specified repository in the tool shed."""
     if metadata_only:
         repository_metadata = get_repository_metadata_by_changeset_revision(app, id, changeset_revision)
         if repository_metadata and repository_metadata.metadata:
             return repository_metadata.metadata
         return None
     return get_repository_metadata_by_changeset_revision(app, id, changeset_revision)
 
 
-def get_repository_metadata_revisions_for_review(repository, reviewed=True):
-    repository_metadata_revisions = []
-    metadata_changeset_revision_hashes = []
-    if reviewed:
-        for metadata_revision in repository.metadata_revisions:
-            metadata_changeset_revision_hashes.append(metadata_revision.changeset_revision)
-        for review in repository.reviews:
-            if review.changeset_revision in metadata_changeset_revision_hashes:
-                rmcr_hashes = [rmr.changeset_revision for rmr in repository_metadata_revisions]
-                if review.changeset_revision not in rmcr_hashes:
-                    repository_metadata_revisions.append(review.repository_metadata)
-    else:
-        for review in repository.reviews:
-            if review.changeset_revision not in metadata_changeset_revision_hashes:
-                metadata_changeset_revision_hashes.append(review.changeset_revision)
-        for metadata_revision in repository.metadata_revisions:
-            if metadata_revision.changeset_revision not in metadata_changeset_revision_hashes:
-                repository_metadata_revisions.append(metadata_revision)
-    return repository_metadata_revisions
-
-
 def get_updated_changeset_revisions(app, name, owner, changeset_revision):
     """
     Return a string of comma-separated changeset revision hashes for all available updates to the received changeset
     revision for the repository defined by the received name and owner.
     """
     repository = get_repository_by_name_and_owner(app, name, owner)
     # Get the upper bound changeset revision.
@@ -290,36 +285,36 @@
     repo = repository.hg_repo
     changeset_hashes = []
     for changeset in reversed_lower_upper_bounded_changelog(repo, changeset_revision, upper_bound_changeset_revision):
         # Make sure to exclude upper_bound_changeset_revision.
         if changeset != upper_bound_changeset_revision:
             changeset_hashes.append(str(repo[changeset]))
     if changeset_hashes:
-        changeset_hashes_str = ','.join(changeset_hashes)
+        changeset_hashes_str = ",".join(changeset_hashes)
         return changeset_hashes_str
-    return ''
+    return ""
 
 
 def is_downloadable(metadata_dict):
     # NOTE: although repository README files are considered Galaxy utilities, they have no
     # effect on determining if a revision is installable.  See the comments in the
     # compare_readme_files() method.
-    if 'datatypes' in metadata_dict:
+    if "datatypes" in metadata_dict:
         # We have proprietary datatypes.
         return True
-    if 'repository_dependencies' in metadata_dict:
+    if "repository_dependencies" in metadata_dict:
         # We have repository_dependencies.
         return True
-    if 'tools' in metadata_dict:
+    if "tools" in metadata_dict:
         # We have tools.
         return True
-    if 'tool_dependencies' in metadata_dict:
+    if "tool_dependencies" in metadata_dict:
         # We have tool dependencies, and perhaps only tool dependencies!
         return True
-    if 'workflows' in metadata_dict:
+    if "workflows" in metadata_dict:
         # We have exported workflows.
         return True
     return False
 
 
 def is_malicious(app, id, changeset_revision, **kwd):
     """Check the malicious flag in repository metadata for a specified change set revision."""
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/readme_util.py` & `galaxy-web-apps-23.0.2/tool_shed/util/readme_util.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,105 +1,126 @@
-import io
 import json
 import logging
 import os
 
 from mako.template import Template
 
-import tool_shed.util.shed_util_common as suc
 from galaxy import web
+from galaxy.tool_shed.util import basic_util
+from galaxy.tool_shed.util.hg_util import (
+    get_changectx_for_changeset,
+    get_file_context_from_ctx,
+)
+from galaxy.tool_shed.util.repository_util import get_repo_info_tuple_contents
+from galaxy.tool_shed.util.shed_util_common import set_image_paths
 from galaxy.util import (
     rst_to_html,
     unicodify,
     url_get,
 )
-from tool_shed.util import (
-    basic_util,
-    common_util,
-    hg_util,
-    metadata_util,
-    repository_util,
-)
+from galaxy.util.tool_shed.common_util import get_tool_shed_url_from_tool_shed_registry
+from tool_shed.util.metadata_util import get_latest_downloadable_changeset_revision
 
 log = logging.getLogger(__name__)
 
 
 def build_readme_files_dict(app, repository, changeset_revision, metadata, tool_path=None):
     """
     Return a dictionary of valid readme file name <-> readme file content pairs for all readme files defined in the received metadata.  Since the
     received changeset_revision (which is associated with the received metadata) may not be the latest installable changeset revision, the README
     file contents may not be available on disk.  This method is used by both Galaxy and the Tool Shed.
     """
-    if app.name == 'galaxy':
+    if app.name == "galaxy":
         can_use_disk_files = True
     else:
-        latest_downloadable_changeset_revision = metadata_util.get_latest_downloadable_changeset_revision(app, repository)
+        latest_downloadable_changeset_revision = get_latest_downloadable_changeset_revision(app, repository)
         can_use_disk_files = changeset_revision == latest_downloadable_changeset_revision
     readme_files_dict = {}
     if metadata:
-        if 'readme_files' in metadata:
-            for relative_path_to_readme_file in metadata['readme_files']:
+        if "readme_files" in metadata:
+            for relative_path_to_readme_file in metadata["readme_files"]:
                 readme_file_name = os.path.split(relative_path_to_readme_file)[1]
                 if can_use_disk_files:
                     if tool_path:
-                        full_path_to_readme_file = os.path.abspath(os.path.join(tool_path, relative_path_to_readme_file))
+                        full_path_to_readme_file = os.path.abspath(
+                            os.path.join(tool_path, relative_path_to_readme_file)
+                        )
                     else:
                         full_path_to_readme_file = os.path.abspath(relative_path_to_readme_file)
                     text = None
                     try:
-                        with io.open(full_path_to_readme_file, 'r', encoding='utf-8') as f:
+                        with open(full_path_to_readme_file, encoding="utf-8") as f:
                             text = f.read()
                     except Exception:
                         log.exception("Error reading README file '%s' from disk", relative_path_to_readme_file)
                         text = None
                     if text:
                         text_of_reasonable_length = basic_util.size_string(text)
-                        if text_of_reasonable_length.find('.. image:: ') >= 0:
+                        if text_of_reasonable_length.find(".. image:: ") >= 0:
                             # Handle image display for README files that are contained in repositories in the tool shed or installed into Galaxy.
                             try:
-                                text_of_reasonable_length = suc.set_image_paths(app,
-                                                                                text_of_reasonable_length,
-                                                                                encoded_repository_id=app.security.encode_id(repository.id))
+                                text_of_reasonable_length = set_image_paths(
+                                    app,
+                                    text_of_reasonable_length,
+                                    encoded_repository_id=app.security.encode_id(repository.id),
+                                )
                             except Exception:
-                                log.exception("Exception in build_readme_files_dict, so images may not be properly displayed")
-                        if readme_file_name.endswith('.rst'):
-                            text_of_reasonable_length = Template(rst_to_html(text_of_reasonable_length),
-                                                                 input_encoding='utf-8',
-                                                                 default_filters=['decode.utf8'],
-                                                                 encoding_errors='replace')
-                            text_of_reasonable_length = text_of_reasonable_length.render(static_path=web.url_for('/static'),
-                                                                                         host_url=web.url_for('/', qualified=True))
+                                log.exception(
+                                    "Exception in build_readme_files_dict, so images may not be properly displayed"
+                                )
+                        if readme_file_name.endswith(".rst"):
+                            text_of_reasonable_length = Template(
+                                rst_to_html(text_of_reasonable_length),
+                                input_encoding="utf-8",
+                                default_filters=["decode.utf8"],
+                                encoding_errors="replace",
+                            )
+                            text_of_reasonable_length = text_of_reasonable_length.render(
+                                static_path=web.url_for("/static"), host_url=web.url_for("/", qualified=True)
+                            )
                             text_of_reasonable_length = unicodify(text_of_reasonable_length)
                         else:
                             text_of_reasonable_length = basic_util.to_html_string(text_of_reasonable_length)
                         readme_files_dict[readme_file_name] = text_of_reasonable_length
                 else:
                     # We must be in the tool shed and have an old changeset_revision, so we need to retrieve the file contents from the repository manifest.
                     repo = repository.hg_repo
-                    ctx = hg_util.get_changectx_for_changeset(repo, changeset_revision)
+                    ctx = get_changectx_for_changeset(repo, changeset_revision)
                     if ctx:
-                        fctx = hg_util.get_file_context_from_ctx(ctx, readme_file_name)
-                        if fctx and fctx not in ['DELETED']:
+                        fctx = get_file_context_from_ctx(ctx, readme_file_name)
+                        if fctx and fctx not in ["DELETED"]:
                             try:
                                 text = unicodify(fctx.data())
                                 readme_files_dict[readme_file_name] = basic_util.size_string(text)
                             except Exception:
-                                log.exception("Error reading README file '%s' from repository manifest", relative_path_to_readme_file)
+                                log.exception(
+                                    "Error reading README file '%s' from repository manifest",
+                                    relative_path_to_readme_file,
+                                )
     return readme_files_dict
 
 
 def get_readme_files_dict_for_display(app, tool_shed_url, repo_info_dict):
     """
     Return a dictionary of README files contained in the single repository being installed so they can be displayed on the tool panel section
     selection page.
     """
     name = next(iter(repo_info_dict))
     repo_info_tuple = repo_info_dict[name]
-    description, repository_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, installed_td = \
-        repository_util.get_repo_info_tuple_contents(repo_info_tuple)
+    (
+        description,
+        repository_clone_url,
+        changeset_revision,
+        ctx_rev,
+        repository_owner,
+        repository_dependencies,
+        installed_td,
+    ) = get_repo_info_tuple_contents(repo_info_tuple)
     # Handle changing HTTP protocols over time.
-    tool_shed_url = common_util.get_tool_shed_url_from_tool_shed_registry(app, tool_shed_url)
+    tool_shed_url = get_tool_shed_url_from_tool_shed_registry(app, tool_shed_url)
     params = dict(name=name, owner=repository_owner, changeset_revision=changeset_revision)
-    pathspec = ['repository', 'get_readme_files']
-    raw_text = url_get(tool_shed_url, auth=app.tool_shed_registry.url_auth(tool_shed_url), pathspec=pathspec, params=params)
+    pathspec = ["repository", "get_readme_files"]
+    raw_text = url_get(
+        tool_shed_url, auth=app.tool_shed_registry.url_auth(tool_shed_url), pathspec=pathspec, params=params
+    )
     readme_files_dict = json.loads(raw_text)
     return readme_files_dict
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/repository_content_util.py` & `galaxy-web-apps-23.0.2/tool_shed/util/repository_content_util.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,26 +4,35 @@
 import tool_shed.repository_types.util as rt_util
 from tool_shed.util import (
     commit_util,
     xml_util,
 )
 
 
-def upload_tar(trans, rdah, tdah, repository, tar, uploaded_file, upload_point, remove_repo_files_not_in_tar,
-               commit_message, new_repo_alert):
+def upload_tar(
+    trans,
+    rdah,
+    tdah,
+    repository,
+    tar,
+    uploaded_file,
+    upload_point,
+    remove_repo_files_not_in_tar,
+    commit_message,
+    new_repo_alert,
+):
     # Upload a tar archive of files.
     undesirable_dirs_removed = 0
     undesirable_files_removed = 0
     check_results = commit_util.check_archive(repository, tar)
     if check_results.invalid:
         tar.close()
         uploaded_file.close()
-        message = '%s Invalid paths were: %s' % (
-            ' '.join(check_results.errors), ', '.join(check_results.invalid))
-        return False, message, [], '', undesirable_dirs_removed, undesirable_files_removed
+        message = "{} Invalid paths were: {}".format(" ".join(check_results.errors), ", ".join(check_results.invalid))
+        return False, message, [], "", undesirable_dirs_removed, undesirable_files_removed
     else:
         repository.hg_repo
         repo_dir = repository.repo_path(trans.app)
         if upload_point is not None:
             full_path = os.path.abspath(os.path.join(repo_dir, upload_point))
         else:
             full_path = os.path.abspath(repo_dir)
@@ -37,31 +46,33 @@
         for filename in filenames_in_archive:
             uploaded_file_name = os.path.join(full_path, filename)
             if os.path.split(uploaded_file_name)[-1] == rt_util.REPOSITORY_DEPENDENCY_DEFINITION_FILENAME:
                 # Inspect the contents of the file to see if toolshed or changeset_revision attributes
                 # are missing and if so, set them appropriately.
                 altered, root_elem, error_message = rdah.handle_tag_attributes(uploaded_file_name)
                 if error_message:
-                    return False, error_message, [], '', [], []
+                    return False, error_message, [], "", [], []
                 elif altered:
                     tmp_filename = xml_util.create_and_write_tmp_file(root_elem)
                     shutil.move(tmp_filename, uploaded_file_name)
             elif os.path.split(uploaded_file_name)[-1] == rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME:
                 # Inspect the contents of the file to see if toolshed or changeset_revision
                 # attributes are missing and if so, set them appropriately.
                 altered, root_elem, error_message = tdah.handle_tag_attributes(uploaded_file_name)
                 if error_message:
-                    return False, error_message, [], '', [], []
+                    return False, error_message, [], "", [], []
                 if altered:
                     tmp_filename = xml_util.create_and_write_tmp_file(root_elem)
                     shutil.move(tmp_filename, uploaded_file_name)
-        return commit_util.handle_directory_changes(trans.app,
-                                                    trans.request.host,
-                                                    trans.user.username,
-                                                    repository,
-                                                    full_path,
-                                                    filenames_in_archive,
-                                                    remove_repo_files_not_in_tar,
-                                                    new_repo_alert,
-                                                    commit_message,
-                                                    undesirable_dirs_removed,
-                                                    undesirable_files_removed)
+        return commit_util.handle_directory_changes(
+            trans.app,
+            trans.request.host,
+            trans.user.username,
+            repository,
+            full_path,
+            filenames_in_archive,
+            remove_repo_files_not_in_tar,
+            new_repo_alert,
+            commit_message,
+            undesirable_dirs_removed,
+            undesirable_files_removed,
+        )
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/search_util.py` & `galaxy-web-apps-23.0.2/tool_shed/util/search_util.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,70 +1,86 @@
 import logging
 
-from sqlalchemy import and_, false, true
+from sqlalchemy import (
+    and_,
+    false,
+    true,
+)
 
 log = logging.getLogger(__name__)
 
 
 def in_tool_dict(tool_dict, exact_matches_checked, tool_id=None, tool_name=None, tool_version=None):
     found = False
     if tool_id and not tool_name and not tool_version:
-        tool_dict_tool_id = tool_dict['id'].lower()
-        found = (tool_id == tool_dict_tool_id) or \
-                (not exact_matches_checked and tool_dict_tool_id.find(tool_id) >= 0)
+        tool_dict_tool_id = tool_dict["id"].lower()
+        found = (tool_id == tool_dict_tool_id) or (not exact_matches_checked and tool_dict_tool_id.find(tool_id) >= 0)
     elif tool_name and not tool_id and not tool_version:
-        tool_dict_tool_name = tool_dict['name'].lower()
-        found = (tool_name == tool_dict_tool_name) or \
-                (not exact_matches_checked and tool_dict_tool_name.find(tool_name) >= 0)
+        tool_dict_tool_name = tool_dict["name"].lower()
+        found = (tool_name == tool_dict_tool_name) or (
+            not exact_matches_checked and tool_dict_tool_name.find(tool_name) >= 0
+        )
     elif tool_version and not tool_id and not tool_name:
-        tool_dict_tool_version = tool_dict['version'].lower()
-        found = (tool_version == tool_dict_tool_version) or \
-                (not exact_matches_checked and tool_dict_tool_version.find(tool_version) >= 0)
+        tool_dict_tool_version = tool_dict["version"].lower()
+        found = (tool_version == tool_dict_tool_version) or (
+            not exact_matches_checked and tool_dict_tool_version.find(tool_version) >= 0
+        )
     elif tool_id and tool_name and not tool_version:
-        tool_dict_tool_id = tool_dict['id'].lower()
-        tool_dict_tool_name = tool_dict['name'].lower()
-        found = (tool_id == tool_dict_tool_id and tool_name == tool_dict_tool_name) or \
-                (not exact_matches_checked and tool_dict_tool_id.find(tool_id) >= 0 and tool_dict_tool_name.find(tool_name) >= 0)
+        tool_dict_tool_id = tool_dict["id"].lower()
+        tool_dict_tool_name = tool_dict["name"].lower()
+        found = (tool_id == tool_dict_tool_id and tool_name == tool_dict_tool_name) or (
+            not exact_matches_checked
+            and tool_dict_tool_id.find(tool_id) >= 0
+            and tool_dict_tool_name.find(tool_name) >= 0
+        )
     elif tool_id and tool_version and not tool_name:
-        tool_dict_tool_id = tool_dict['id'].lower()
-        tool_dict_tool_version = tool_dict['version'].lower()
-        found = (tool_id == tool_dict_tool_id and tool_version == tool_dict_tool_version) or \
-                (not exact_matches_checked and tool_dict_tool_id.find(tool_id) >= 0 and tool_dict_tool_version.find(tool_version) >= 0)
+        tool_dict_tool_id = tool_dict["id"].lower()
+        tool_dict_tool_version = tool_dict["version"].lower()
+        found = (tool_id == tool_dict_tool_id and tool_version == tool_dict_tool_version) or (
+            not exact_matches_checked
+            and tool_dict_tool_id.find(tool_id) >= 0
+            and tool_dict_tool_version.find(tool_version) >= 0
+        )
     elif tool_version and tool_name and not tool_id:
-        tool_dict_tool_version = tool_dict['version'].lower()
-        tool_dict_tool_name = tool_dict['name'].lower()
-        found = (tool_version == tool_dict_tool_version and tool_name == tool_dict_tool_name) or \
-                (not exact_matches_checked and tool_dict_tool_version.find(tool_version) >= 0 and tool_dict_tool_name.find(tool_name) >= 0)
+        tool_dict_tool_version = tool_dict["version"].lower()
+        tool_dict_tool_name = tool_dict["name"].lower()
+        found = (tool_version == tool_dict_tool_version and tool_name == tool_dict_tool_name) or (
+            not exact_matches_checked
+            and tool_dict_tool_version.find(tool_version) >= 0
+            and tool_dict_tool_name.find(tool_name) >= 0
+        )
     elif tool_version and tool_name and tool_id:
-        tool_dict_tool_version = tool_dict['version'].lower()
-        tool_dict_tool_name = tool_dict['name'].lower()
-        tool_dict_tool_id = tool_dict['id'].lower()
-        found = (tool_version == tool_dict_tool_version and
-                 tool_name == tool_dict_tool_name and
-                 tool_id == tool_dict_tool_id) or \
-                (not exact_matches_checked and
-                 tool_dict_tool_version.find(tool_version) >= 0 and
-                 tool_dict_tool_name.find(tool_name) >= 0 and
-                 tool_dict_tool_id.find(tool_id) >= 0)
+        tool_dict_tool_version = tool_dict["version"].lower()
+        tool_dict_tool_name = tool_dict["name"].lower()
+        tool_dict_tool_id = tool_dict["id"].lower()
+        found = (
+            tool_version == tool_dict_tool_version and tool_name == tool_dict_tool_name and tool_id == tool_dict_tool_id
+        ) or (
+            not exact_matches_checked
+            and tool_dict_tool_version.find(tool_version) >= 0
+            and tool_dict_tool_name.find(tool_name) >= 0
+            and tool_dict_tool_id.find(tool_id) >= 0
+        )
     return found
 
 
 def in_workflow_dict(workflow_dict, exact_matches_checked, workflow_name):
-    workflow_dict_workflow_name = workflow_dict['name'].lower()
-    return (workflow_name == workflow_dict_workflow_name) or \
-           (not exact_matches_checked and workflow_dict_workflow_name.find(workflow_name) >= 0)
+    workflow_dict_workflow_name = workflow_dict["name"].lower()
+    return (workflow_name == workflow_dict_workflow_name) or (
+        not exact_matches_checked and workflow_dict_workflow_name.find(workflow_name) >= 0
+    )
 
 
 def make_same_length(list1, list2):
     # If either list is 1 item, we'll append to it until its length is the same as the other.
     if len(list1) == 1:
-        for i in range(1, len(list2)):
+        for _ in range(1, len(list2)):
             list1.append(list1[0])
     elif len(list2) == 1:
-        for i in range(1, len(list1)):
+        for _ in range(1, len(list1)):
             list2.append(list2[0])
     return list1, list2
 
 
 def search_ids_names(tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_ids, tool_names):
     for i, tool_id in enumerate(tool_ids):
         tool_name = tool_names[i]
@@ -77,95 +93,138 @@
     for i, tool_id in enumerate(tool_ids):
         tool_version = tool_versions[i]
         if in_tool_dict(tool_dict, exact_matches_checked, tool_id=tool_id, tool_version=tool_version):
             match_tuples.append((repository_metadata.repository_id, repository_metadata.changeset_revision))
     return match_tuples
 
 
-def search_names_versions(tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_names, tool_versions):
+def search_names_versions(
+    tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_names, tool_versions
+):
     for i, tool_name in enumerate(tool_names):
         tool_version = tool_versions[i]
         if in_tool_dict(tool_dict, exact_matches_checked, tool_name=tool_name, tool_version=tool_version):
             match_tuples.append((repository_metadata.repository_id, repository_metadata.changeset_revision))
     return match_tuples
 
 
-def search_repository_metadata(app, exact_matches_checked, tool_ids='', tool_names='', tool_versions='',
-                               workflow_names='', all_workflows=False):
-    sa_session = app.model.context.current
+def search_repository_metadata(app, exact_matches_checked, tool_ids="", tool_names="", tool_versions=""):
+    sa_session = app.model.session
     match_tuples = []
     ok = True
     if tool_ids or tool_names or tool_versions:
-        for repository_metadata in sa_session.query(app.model.RepositoryMetadata) \
-                                             .filter(app.model.RepositoryMetadata.table.c.includes_tools == true()) \
-                                             .join(app.model.Repository) \
-                                             .filter(and_(app.model.Repository.table.c.deleted == false(),
-                                                          app.model.Repository.table.c.deprecated == false())):
+        for repository_metadata in (
+            sa_session.query(app.model.RepositoryMetadata)
+            .filter(app.model.RepositoryMetadata.table.c.includes_tools == true())
+            .join(app.model.Repository)
+            .filter(
+                and_(
+                    app.model.Repository.table.c.deleted == false(), app.model.Repository.table.c.deprecated == false()
+                )
+            )
+        ):
             metadata = repository_metadata.metadata
             if metadata:
-                tools = metadata.get('tools', [])
+                tools = metadata.get("tools", [])
                 for tool_dict in tools:
                     if tool_ids and not tool_names and not tool_versions:
                         for tool_id in tool_ids:
                             if in_tool_dict(tool_dict, exact_matches_checked, tool_id=tool_id):
-                                match_tuples.append((repository_metadata.repository_id, repository_metadata.changeset_revision))
+                                match_tuples.append(
+                                    (repository_metadata.repository_id, repository_metadata.changeset_revision)
+                                )
                     elif tool_names and not tool_ids and not tool_versions:
                         for tool_name in tool_names:
                             if in_tool_dict(tool_dict, exact_matches_checked, tool_name=tool_name):
-                                match_tuples.append((repository_metadata.repository_id, repository_metadata.changeset_revision))
+                                match_tuples.append(
+                                    (repository_metadata.repository_id, repository_metadata.changeset_revision)
+                                )
                     elif tool_versions and not tool_ids and not tool_names:
                         for tool_version in tool_versions:
                             if in_tool_dict(tool_dict, exact_matches_checked, tool_version=tool_version):
-                                match_tuples.append((repository_metadata.repository_id, repository_metadata.changeset_revision))
+                                match_tuples.append(
+                                    (repository_metadata.repository_id, repository_metadata.changeset_revision)
+                                )
                     elif tool_ids and tool_names and not tool_versions:
                         if len(tool_ids) == len(tool_names):
-                            match_tuples = search_ids_names(tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_ids, tool_names)
+                            match_tuples = search_ids_names(
+                                tool_dict,
+                                exact_matches_checked,
+                                match_tuples,
+                                repository_metadata,
+                                tool_ids,
+                                tool_names,
+                            )
                         elif len(tool_ids) == 1 or len(tool_names) == 1:
                             tool_ids, tool_names = make_same_length(tool_ids, tool_names)
-                            match_tuples = search_ids_names(tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_ids, tool_names)
+                            match_tuples = search_ids_names(
+                                tool_dict,
+                                exact_matches_checked,
+                                match_tuples,
+                                repository_metadata,
+                                tool_ids,
+                                tool_names,
+                            )
                         else:
                             ok = False
                     elif tool_ids and tool_versions and not tool_names:
                         if len(tool_ids) == len(tool_versions):
-                            match_tuples = search_ids_versions(tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_ids, tool_versions)
+                            match_tuples = search_ids_versions(
+                                tool_dict,
+                                exact_matches_checked,
+                                match_tuples,
+                                repository_metadata,
+                                tool_ids,
+                                tool_versions,
+                            )
                         elif len(tool_ids) == 1 or len(tool_versions) == 1:
                             tool_ids, tool_versions = make_same_length(tool_ids, tool_versions)
-                            match_tuples = search_ids_versions(tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_ids, tool_versions)
+                            match_tuples = search_ids_versions(
+                                tool_dict,
+                                exact_matches_checked,
+                                match_tuples,
+                                repository_metadata,
+                                tool_ids,
+                                tool_versions,
+                            )
                         else:
                             ok = False
                     elif tool_versions and tool_names and not tool_ids:
                         if len(tool_versions) == len(tool_names):
-                            match_tuples = search_names_versions(tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_names, tool_versions)
+                            match_tuples = search_names_versions(
+                                tool_dict,
+                                exact_matches_checked,
+                                match_tuples,
+                                repository_metadata,
+                                tool_names,
+                                tool_versions,
+                            )
                         elif len(tool_versions) == 1 or len(tool_names) == 1:
                             tool_versions, tool_names = make_same_length(tool_versions, tool_names)
-                            match_tuples = search_names_versions(tool_dict, exact_matches_checked, match_tuples, repository_metadata, tool_names, tool_versions)
+                            match_tuples = search_names_versions(
+                                tool_dict,
+                                exact_matches_checked,
+                                match_tuples,
+                                repository_metadata,
+                                tool_names,
+                                tool_versions,
+                            )
                         else:
                             ok = False
                     elif tool_versions and tool_names and tool_ids:
                         if len(tool_versions) == len(tool_names) and len(tool_names) == len(tool_ids):
                             for i, tool_version in enumerate(tool_versions):
                                 tool_name = tool_names[i]
                                 tool_id = tool_ids[i]
-                                if in_tool_dict(tool_dict, exact_matches_checked, tool_id=tool_id, tool_name=tool_name, tool_version=tool_version):
-                                    match_tuples.append((repository_metadata.repository_id, repository_metadata.changeset_revision))
+                                if in_tool_dict(
+                                    tool_dict,
+                                    exact_matches_checked,
+                                    tool_id=tool_id,
+                                    tool_name=tool_name,
+                                    tool_version=tool_version,
+                                ):
+                                    match_tuples.append(
+                                        (repository_metadata.repository_id, repository_metadata.changeset_revision)
+                                    )
                         else:
                             ok = False
-    elif workflow_names or all_workflows:
-        for repository_metadata in sa_session.query(app.model.RepositoryMetadata) \
-                                             .filter(app.model.RepositoryMetadata.table.c.includes_workflows == true()) \
-                                             .join(app.model.Repository) \
-                                             .filter(and_(app.model.Repository.table.c.deleted == false(),
-                                                          app.model.Repository.table.c.deprecated == false())):
-            metadata = repository_metadata.metadata
-            if metadata:
-                # metadata[ 'workflows' ] is a list of tuples where each contained tuple is
-                # [ <relative path to the .ga file in the repository>, <exported workflow dict> ]
-                if workflow_names:
-                    workflow_tups = metadata.get('workflows', [])
-                    workflows = [workflow_tup[1] for workflow_tup in workflow_tups]
-                    for workflow_dict in workflows:
-                        for workflow_name in workflow_names:
-                            if in_workflow_dict(workflow_dict, exact_matches_checked, workflow_name):
-                                match_tuples.append((repository_metadata.repository_id, repository_metadata.changeset_revision))
-                elif all_workflows:
-                    match_tuples.append((repository_metadata.repository_id, repository_metadata.changeset_revision))
     return ok, match_tuples
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/shed_index.py` & `galaxy-web-apps-23.0.2/tool_shed/util/shed_index.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,74 +1,82 @@
 import logging
 import os
 
-from mercurial import hg, ui
+from mercurial import (
+    hg,
+    ui,
+)
 from whoosh.writing import AsyncWriter
 
 import tool_shed.webapp.model.mapping as ts_mapping
 from galaxy.tool_util.loader_directory import load_tool_elements_from_path
 from galaxy.tools.search import get_or_create_index
 from galaxy.util import (
     directory_hash_id,
     ExecutionTimer,
     pretty_print_time_interval,
-    unicodify
+    unicodify,
 )
 from tool_shed.util.hgweb_config import hgweb_config_manager
 from tool_shed.webapp import model
 from tool_shed.webapp.search.repo_search import schema as repo_schema
 from tool_shed.webapp.search.tool_search import schema as tool_schema
 
 log = logging.getLogger(__name__)
 
 
 def _get_or_create_index(whoosh_index_dir):
-    tool_index_dir = os.path.join(whoosh_index_dir, 'tools')
+    tool_index_dir = os.path.join(whoosh_index_dir, "tools")
     if not os.path.exists(whoosh_index_dir):
         os.makedirs(whoosh_index_dir)
     if not os.path.exists(tool_index_dir):
         os.makedirs(tool_index_dir)
     return get_or_create_index(whoosh_index_dir, repo_schema), get_or_create_index(tool_index_dir, tool_schema)
 
 
 def build_index(whoosh_index_dir, file_path, hgweb_config_dir, dburi, **kwargs):
     """
     Build two search indexes simultaneously
     One is for repositories and the other for tools.
 
     Returns a tuple with number of repos and tools that were indexed.
     """
-    model = ts_mapping.init(file_path, dburi, engine_options={}, create_tables=False)
-    sa_session = model.context.current
+    model = ts_mapping.init(dburi, engine_options={}, create_tables=False)
+    sa_session = model.session
     repo_index, tool_index = _get_or_create_index(whoosh_index_dir)
 
     repo_index_writer = AsyncWriter(repo_index)
     tool_index_writer = AsyncWriter(tool_index)
     repos_indexed = 0
     tools_indexed = 0
 
     execution_timer = ExecutionTimer()
     with repo_index.searcher() as searcher:
         for repo in get_repos(sa_session, file_path, hgweb_config_dir, **kwargs):
-            tools_list = repo.pop('tools_list')
-            repo_id = repo['id']
+            tools_list = repo.pop("tools_list")
+            repo_id = repo["id"]
             indexed_document = searcher.document(id=repo_id)
             if indexed_document:
-                if indexed_document['full_last_updated'] == repo.get('full_last_updated'):
+                if indexed_document["full_last_updated"] == repo.get("full_last_updated"):
                     # We're done, since we sorted repos by update time
                     break
                 else:
                     # Got an update, delete the previous document
-                    repo_index_writer.delete_by_term('id', repo_id)
+                    repo_index_writer.delete_by_term("id", repo_id)
 
             repo_index_writer.add_document(**repo)
 
             #  Tools get their own index
+            tool_index_writer.delete_by_term("repo_id", repo_id)
             for tool in tools_list:
-                tool_index_writer.add_document(**tool)
+                tool_contents = tool.copy()
+                tool_contents["repo_owner_username"] = repo.get("repo_owner_username")
+                tool_contents["repo_name"] = repo.get("name")
+                tool_contents["repo_id"] = repo_id
+                tool_index_writer.add_document(**tool_contents)
                 tools_indexed += 1
 
             repos_indexed += 1
 
     tool_index_writer.commit()
     repo_index_writer.commit()
 
@@ -80,100 +88,111 @@
 def get_repos(sa_session, file_path, hgweb_config_dir, **kwargs):
     """
     Load repos from DB and included tools from .xml configs.
     """
     hgwcm = hgweb_config_manager
     hgwcm.hgweb_config_dir = hgweb_config_dir
     # Do not index deleted, deprecated, or "tool_dependency_definition" type repositories.
-    q = sa_session.query(model.Repository).filter_by(deleted=False).filter_by(deprecated=False).order_by(model.Repository.update_time.desc())
-    q = q.filter(model.Repository.type != 'tool_dependency_definition')
+    q = (
+        sa_session.query(model.Repository)
+        .filter_by(deleted=False)
+        .filter_by(deprecated=False)
+        .order_by(model.Repository.update_time.desc())
+    )
+    q = q.filter(model.Repository.type != "tool_dependency_definition")
     for repo in q:
         category_names = []
-        for rca in sa_session.query(model.RepositoryCategoryAssociation).filter(model.RepositoryCategoryAssociation.repository_id == repo.id):
+        for rca in sa_session.query(model.RepositoryCategoryAssociation).filter(
+            model.RepositoryCategoryAssociation.repository_id == repo.id
+        ):
             for category in sa_session.query(model.Category).filter(model.Category.id == rca.category.id):
                 category_names.append(category.name.lower())
         categories = (",").join(category_names)
         repo_id = repo.id
         name = repo.name
         description = repo.description
         long_description = repo.long_description
         homepage_url = repo.homepage_url
         remote_repository_url = repo.remote_repository_url
 
         times_downloaded = repo.times_downloaded or 0
 
-        repo_owner_username = ''
+        repo_owner_username = ""
         if repo.user_id is not None:
             user = sa_session.query(model.User).filter(model.User.id == repo.user_id).one()
             repo_owner_username = user.username.lower()
 
-        approved = 'no'
-        for review in repo.reviews:
-            if review.approved == 'yes':
-                approved = 'yes'
-                break
-
         last_updated = pretty_print_time_interval(repo.update_time)
         full_last_updated = repo.update_time.strftime("%Y-%m-%d %I:%M %p")
 
         # Load all changesets of the repo for lineage.
-        repo_path = os.path.join(hgweb_config_dir, hgwcm.get_entry(os.path.join("repos", repo.user.username, repo.name)))
-        hg_repo = hg.repository(ui.ui(), repo_path.encode('utf-8'))
+        repo_path = os.path.join(
+            hgweb_config_dir, hgwcm.get_entry(os.path.join("repos", repo.user.username, repo.name))
+        )
+        hg_repo = hg.repository(ui.ui(), repo_path.encode("utf-8"))
         lineage = []
         for changeset in hg_repo.changelog:
-            lineage.append(unicodify(changeset) + ":" + unicodify(hg_repo[changeset]))
+            lineage.append(f"{unicodify(changeset)}:{unicodify(hg_repo[changeset])}")
         repo_lineage = str(lineage)
 
         #  Parse all the tools within repo for a separate index.
         tools_list = []
         path = os.path.join(file_path, *directory_hash_id(repo.id))
         path = os.path.join(path, "repo_%d" % repo.id)
         if os.path.exists(path):
             tools_list.extend(load_one_dir(path))
-            for root, dirs, files in os.walk(path):
-                if '.hg' in dirs:
-                    dirs.remove('.hg')
+            for root, dirs, _files in os.walk(path):
+                if ".hg" in dirs:
+                    dirs.remove(".hg")
                 for dirname in dirs:
                     tools_in_dir = load_one_dir(os.path.join(root, dirname))
                     tools_list.extend(tools_in_dir)
 
-        yield (dict(id=unicodify(repo_id),
-                    name=unicodify(name),
-                    description=unicodify(description),
-                    long_description=unicodify(long_description),
-                    homepage_url=unicodify(homepage_url),
-                    remote_repository_url=unicodify(remote_repository_url),
-                    repo_owner_username=unicodify(repo_owner_username),
-                    times_downloaded=unicodify(times_downloaded),
-                    approved=unicodify(approved),
-                    last_updated=unicodify(last_updated),
-                    full_last_updated=unicodify(full_last_updated),
-                    tools_list=tools_list,
-                    repo_lineage=unicodify(repo_lineage),
-                    categories=unicodify(categories)))
+        yield (
+            dict(
+                id=unicodify(repo_id),
+                name=unicodify(name),
+                description=unicodify(description),
+                long_description=unicodify(long_description),
+                homepage_url=unicodify(homepage_url),
+                remote_repository_url=unicodify(remote_repository_url),
+                repo_owner_username=unicodify(repo_owner_username),
+                times_downloaded=unicodify(times_downloaded),
+                approved=unicodify("no"),
+                last_updated=unicodify(last_updated),
+                full_last_updated=unicodify(full_last_updated),
+                tools_list=tools_list,
+                repo_lineage=unicodify(repo_lineage),
+                categories=unicodify(categories),
+            )
+        )
 
 
 def debug_handler(path, exc_info):
     """
     By default the underlying tool parsing logs warnings for each exception.
     This is very chatty hence this metod changes it to debug level.
     """
-    log.debug("Failed to load tool with path %s." % path, exc_info=exc_info)
+    log.debug(f"Failed to load tool with path {path}.", exc_info=exc_info)
 
 
 def load_one_dir(path):
     tools_in_dir = []
     tool_elems = load_tool_elements_from_path(path, load_exception_handler=debug_handler)
     if tool_elems:
         for elem in tool_elems:
             root = elem[1].getroot()
-            if root.tag == 'tool':
+            if root.tag == "tool":
                 tool = {}
-                if root.find('help') is not None:
-                    tool.update(dict(help=unicodify(root.find('help').text)))
-                if root.find('description') is not None:
-                    tool.update(dict(description=unicodify(root.find('description').text)))
-                tool.update(dict(id=unicodify(root.attrib.get('id')),
-                                 name=unicodify(root.attrib.get('name')),
-                                 version=unicodify(root.attrib.get('version'))))
+                if root.find("help") is not None:
+                    tool.update(dict(help=unicodify(root.find("help").text)))
+                if root.find("description") is not None:
+                    tool.update(dict(description=unicodify(root.find("description").text)))
+                tool.update(
+                    dict(
+                        id=unicodify(root.attrib.get("id")),
+                        name=unicodify(root.attrib.get("name")),
+                        version=unicodify(root.attrib.get("version")),
+                    )
+                )
                 tools_in_dir.append(tool)
     return tools_in_dir
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/shed_util_common.py` & `galaxy-web-apps-23.0.2/tool_shed/util/shed_util_common.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,44 +1,50 @@
 import json
 import logging
 import os
 import socket
 import string
 
 import sqlalchemy.orm.exc
-from sqlalchemy import and_, false, true
+from sqlalchemy import (
+    and_,
+    false,
+    true,
+)
 
 import galaxy.tool_util.deps.requirements
 from galaxy import util
 from galaxy.tool_shed.util.shed_util_common import (
     can_eliminate_repository_dependency,
-    can_eliminate_tool_dependency,
     clean_dependency_relationships,
     generate_tool_guid,
     get_ctx_rev,
     get_next_prior_import_or_install_required_dict_entry,
     get_tool_panel_config_tool_path_install_dir,
     get_user,
     have_shed_tool_conf_for_install,
     set_image_paths,
     tool_shed_is_this_tool_shed,
 )
-from galaxy.util import checkers
+from galaxy.util import (
+    checkers,
+    unicodify,
+)
 from tool_shed.util import (
     basic_util,
     common_util,
     hg_util,
-    repository_util
+    repository_util,
 )
 
 log = logging.getLogger(__name__)
 
 MAX_CONTENT_SIZE = 1048576
-DATATYPES_CONFIG_FILENAME = 'datatypes_conf.xml'
-REPOSITORY_DATA_MANAGER_CONFIG_FILENAME = 'data_manager_conf.xml'
+DATATYPES_CONFIG_FILENAME = "datatypes_conf.xml"
+REPOSITORY_DATA_MANAGER_CONFIG_FILENAME = "data_manager_conf.xml"
 
 new_repo_email_alert_template = """
 Sharable link:         ${sharable_link}
 Repository name:       ${repository_name}
 Revision:              ${revision}
 Change description:
 ${description}
@@ -90,38 +96,42 @@
 -----------------------------------------------------------------------------
 This message was sent from the Galaxy Tool Shed instance hosted on the server
 '${host}'
 """
 
 
 def count_repositories_in_category(app, category_id):
-    sa_session = app.model.context.current
-    return sa_session.query(app.model.RepositoryCategoryAssociation) \
-                     .filter(app.model.RepositoryCategoryAssociation.table.c.category_id == app.security.decode_id(category_id)) \
-                     .count()
+    sa_session = app.model.session
+    return (
+        sa_session.query(app.model.RepositoryCategoryAssociation)
+        .filter(app.model.RepositoryCategoryAssociation.table.c.category_id == app.security.decode_id(category_id))
+        .count()
+    )
 
 
 def get_categories(app):
     """Get all categories from the database."""
-    sa_session = app.model.context.current
-    return sa_session.query(app.model.Category) \
-                     .filter(app.model.Category.table.c.deleted == false()) \
-                     .order_by(app.model.Category.table.c.name) \
-                     .all()
+    sa_session = app.model.session
+    return (
+        sa_session.query(app.model.Category)
+        .filter(app.model.Category.table.c.deleted == false())
+        .order_by(app.model.Category.table.c.name)
+        .all()
+    )
 
 
 def get_category(app, id):
     """Get a category from the database."""
-    sa_session = app.model.context.current
+    sa_session = app.model.session
     return sa_session.query(app.model.Category).get(app.security.decode_id(id))
 
 
 def get_category_by_name(app, name):
     """Get a category from the database via name."""
-    sa_session = app.model.context.current
+    sa_session = app.model.session
     try:
         return sa_session.query(app.model.Category).filter_by(name=name).one()
     except sqlalchemy.orm.exc.NoResultFound:
         return None
 
 
 def get_tool_shed_repo_requirements(app, tool_shed_url, repositories=None, repo_info_dicts=None):
@@ -130,111 +140,117 @@
     Returns a list of requirements, where each requirement is a dictionary with name and version as keys.
     """
     if not repositories and not repo_info_dicts:
         raise Exception("Need to pass either repository or repo_info_dicts")
     if repositories:
         if not isinstance(repositories, list):
             repositories = [repositories]
-        repository_params = [{'name': repository.name,
-                             'owner': repository.owner,
-                             'changeset_revision': repository.changeset_revision} for repository in repositories]
+        repository_params = [
+            {"name": repository.name, "owner": repository.owner, "changeset_revision": repository.changeset_revision}
+            for repository in repositories
+        ]
     else:
         if not isinstance(repo_info_dicts, list):
             repo_info_dicts = [repo_info_dicts]
         repository_params = []
         for repo_info_dict in repo_info_dicts:
             for name, repo_info_tuple in repo_info_dict.items():
                 # repo_info_tuple is a list, but keep terminology
                 owner = repo_info_tuple[4]
                 changeset_revision = repo_info_tuple[2]
-                repository_params.append({'name': name,
-                                          'owner': owner,
-                                          'changeset_revision': changeset_revision})
+                repository_params.append({"name": name, "owner": owner, "changeset_revision": changeset_revision})
     pathspec = ["api", "repositories", "get_repository_revision_install_info"]
     tools = []
     for params in repository_params:
-        response = util.url_get(tool_shed_url,
-                                auth=app.tool_shed_registry.url_auth(tool_shed_url),
-                                pathspec=pathspec,
-                                params=params
-                                )
+        response = util.url_get(
+            tool_shed_url, auth=app.tool_shed_registry.url_auth(tool_shed_url), pathspec=pathspec, params=params
+        )
         json_response = json.loads(response)
-        valid_tools = json_response[1].get('valid_tools', [])
+        valid_tools = json_response[1].get("valid_tools", [])
         if valid_tools:
             tools.extend(valid_tools)
     return get_requirements_from_tools(tools)
 
 
 def get_requirements_from_tools(tools):
-    return {tool['id']: galaxy.tool_util.deps.requirements.ToolRequirements.from_list(tool['requirements']) for tool in tools}
+    return {
+        tool["id"]: galaxy.tool_util.deps.requirements.ToolRequirements.from_list(tool["requirements"])
+        for tool in tools
+    }
 
 
 def get_requirements_from_repository(repository):
     if not repository.includes_tools:
         return {}
     else:
-        return get_requirements_from_tools(repository.metadata.get('tools', []))
+        return get_requirements_from_tools(repository.metadata_.get("tools", []))
 
 
 def get_repository_categories(app, id):
     """Get categories of a repository on the tool shed side from the database via id"""
-    sa_session = app.model.context.current
-    return sa_session.query(app.model.RepositoryCategoryAssociation) \
-        .filter(app.model.RepositoryCategoryAssociation.table.c.repository_id == app.security.decode_id(id))
+    sa_session = app.model.session
+    return sa_session.query(app.model.RepositoryCategoryAssociation).filter(
+        app.model.RepositoryCategoryAssociation.table.c.repository_id == app.security.decode_id(id)
+    )
 
 
 def get_repository_file_contents(app, file_path, repository_id, is_admin=False):
     """Return the display-safe contents of a repository file for display in a browser."""
-    safe_str = ''
+    safe_str = ""
     if not is_path_browsable(app, file_path, repository_id, is_admin):
-        log.warning('Request tries to access a file outside of the repository location. File path: %s', file_path)
-        return 'Invalid file path'
+        log.warning("Request tries to access a file outside of the repository location. File path: %s", file_path)
+        return "Invalid file path"
     # Symlink targets are checked by is_path_browsable
     if os.path.islink(file_path):
-        safe_str = 'link to: ' + basic_util.to_html_string(os.readlink(file_path))
+        safe_str = f"link to: {basic_util.to_html_string(os.readlink(file_path))}"
         return safe_str
     elif checkers.is_gzip(file_path):
-        return '<br/>gzip compressed file<br/>'
+        return "<br/>gzip compressed file<br/>"
     elif checkers.is_bz2(file_path):
-        return '<br/>bz2 compressed file<br/>'
+        return "<br/>bz2 compressed file<br/>"
     elif checkers.is_zip(file_path):
-        return '<br/>zip compressed file<br/>'
+        return "<br/>zip compressed file<br/>"
     elif checkers.check_binary(file_path):
-        return '<br/>Binary file<br/>'
+        return "<br/>Binary file<br/>"
     else:
-        for i, line in enumerate(open(file_path)):
-            safe_str = '%s%s' % (safe_str, basic_util.to_html_string(line))
-            # Stop reading after string is larger than MAX_CONTENT_SIZE.
-            if len(safe_str) > MAX_CONTENT_SIZE:
-                large_str = \
-                    '<br/>File contents truncated because file size is larger than maximum viewing size of %s<br/>' % \
-                    util.nice_size(MAX_CONTENT_SIZE)
-                safe_str = '%s%s' % (safe_str, large_str)
-                break
+        with open(file_path) as fh:
+            for line in fh:
+                safe_str = f"{safe_str}{basic_util.to_html_string(line)}"
+                # Stop reading after string is larger than MAX_CONTENT_SIZE.
+                if len(safe_str) > MAX_CONTENT_SIZE:
+                    large_str = (
+                        "<br/>File contents truncated because file size is larger than maximum viewing size of %s<br/>"
+                        % util.nice_size(MAX_CONTENT_SIZE)
+                    )
+                    safe_str = f"{safe_str}{large_str}"
+                    break
 
         if len(safe_str) > basic_util.MAX_DISPLAY_SIZE:
             # Eliminate the middle of the file to display a file no larger than basic_util.MAX_DISPLAY_SIZE.
             # This may not be ideal if the file is larger than MAX_CONTENT_SIZE.
-            join_by_str = \
-                "<br/><br/>...some text eliminated here because file size is larger than maximum viewing size of %s...<br/><br/>" % \
-                util.nice_size(basic_util.MAX_DISPLAY_SIZE)
-            safe_str = util.shrink_string_by_size(safe_str,
-                                                  basic_util.MAX_DISPLAY_SIZE,
-                                                  join_by=join_by_str,
-                                                  left_larger=True,
-                                                  beginning_on_size_error=True)
+            join_by_str = (
+                "<br/><br/>...some text eliminated here because file size is larger than maximum viewing size of %s...<br/><br/>"
+                % util.nice_size(basic_util.MAX_DISPLAY_SIZE)
+            )
+            safe_str = util.shrink_string_by_size(
+                safe_str,
+                basic_util.MAX_DISPLAY_SIZE,
+                join_by=join_by_str,
+                left_larger=True,
+                beginning_on_size_error=True,
+            )
         return safe_str
 
 
 def get_repository_files(folder_path):
     """Return the file hierarchy of a tool shed repository."""
     contents = []
     for item in os.listdir(folder_path):
         # Skip .hg directories
-        if item.startswith('.hg'):
+        if item.startswith(".hg"):
             continue
         contents.append(item)
     if contents:
         contents.sort()
     return contents
 
 
@@ -242,15 +258,15 @@
     # The changeset_revision_select_field in several grids performs a refresh_on_change which sends in request parameters like
     # changeset_revison_1, changeset_revision_2, etc.  One of the many select fields on the grid performed the refresh_on_change,
     # so we loop through all of the received values to see which value is not the repository tip.  If we find it, we know the
     # refresh_on_change occurred and we have the necessary repository id and change set revision to pass on.
     repository_id = None
     v = None
     for k, v in kwd.items():
-        changeset_revision_str = 'changeset_revision_'
+        changeset_revision_str = "changeset_revision_"
         if k.startswith(changeset_revision_str):
             repository_id = app.security.encode_id(int(k.lstrip(changeset_revision_str)))
             repository = repository_util.get_repository_in_tool_shed(app, repository_id)
             if repository.tip() != v:
                 return v, repository
     # This should never be reached - raise an exception?
     return v, None
@@ -259,127 +275,143 @@
 def get_repository_type_from_tool_shed(app, tool_shed_url, name, owner):
     """
     Send a request to the tool shed to retrieve the type for a repository defined by the
     combination of a name and owner.
     """
     tool_shed_url = common_util.get_tool_shed_url_from_tool_shed_registry(app, tool_shed_url)
     params = dict(name=name, owner=owner)
-    pathspec = ['repository', 'get_repository_type']
-    repository_type = util.url_get(tool_shed_url, auth=app.tool_shed_registry.url_auth(tool_shed_url), pathspec=pathspec, params=params)
+    pathspec = ["repository", "get_repository_type"]
+    repository_type = util.url_get(
+        tool_shed_url, auth=app.tool_shed_registry.url_auth(tool_shed_url), pathspec=pathspec, params=params
+    )
     return repository_type
 
 
 def get_tool_dependency_definition_metadata_from_tool_shed(app, tool_shed_url, name, owner):
     """
     Send a request to the tool shed to retrieve the current metadata for a
     repository of type tool_dependency_definition defined by the combination
     of a name and owner.
     """
     tool_shed_url = common_util.get_tool_shed_url_from_tool_shed_registry(app, tool_shed_url)
     params = dict(name=name, owner=owner)
-    pathspec = ['repository', 'get_tool_dependency_definition_metadata']
-    metadata = util.url_get(tool_shed_url, auth=app.tool_shed_registry.url_auth(tool_shed_url), pathspec=pathspec, params=params)
+    pathspec = ["repository", "get_tool_dependency_definition_metadata"]
+    metadata = util.url_get(
+        tool_shed_url, auth=app.tool_shed_registry.url_auth(tool_shed_url), pathspec=pathspec, params=params
+    )
     return metadata
 
 
 def get_tool_path_by_shed_tool_conf_filename(app, shed_tool_conf):
     """
     Return the tool_path config setting for the received shed_tool_conf file by searching the tool box's in-memory list of shed_tool_confs for the
     dictionary whose config_filename key has a value matching the received shed_tool_conf.
     """
     for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
-        config_filename = shed_tool_conf_dict['config_filename']
+        config_filename = shed_tool_conf_dict["config_filename"]
         if config_filename == shed_tool_conf:
-            return shed_tool_conf_dict['tool_path']
+            return shed_tool_conf_dict["tool_path"]
         else:
             file_name = basic_util.strip_path(config_filename)
             if file_name == shed_tool_conf:
-                return shed_tool_conf_dict['tool_path']
+                return shed_tool_conf_dict["tool_path"]
     return None
 
 
-def handle_email_alerts(app, host, repository, content_alert_str='', new_repo_alert=False, admin_only=False):
+def handle_email_alerts(app, host, repository, content_alert_str="", new_repo_alert=False, admin_only=False):
     """
     There are 2 complementary features that enable a tool shed user to receive email notification:
+
     1. Within User Preferences, they can elect to receive email when the first (or first valid)
        change set is produced for a new repository.
     2. When viewing or managing a repository, they can check the box labeled "Receive email alerts"
        which caused them to receive email alerts when updates to the repository occur.  This same feature
        is available on a per-repository basis on the repository grid within the tool shed.
 
     There are currently 4 scenarios for sending email notification when a change is made to a repository:
+
     1. An admin user elects to receive email when the first change set is produced for a new repository
        from User Preferences.  The change set does not have to include any valid content.  This allows for
        the capture of inappropriate content being uploaded to new repositories.
     2. A regular user elects to receive email when the first valid change set is produced for a new repository
        from User Preferences.  This differs from 1 above in that the user will not receive email until a
-       change set tha tincludes valid content is produced.
+       change set that includes valid content is produced.
     3. An admin user checks the "Receive email alerts" check box on the manage repository page.  Since the
        user is an admin user, the email will include information about both HTML and image content that was
        included in the change set.
     4. A regular user checks the "Receive email alerts" check box on the manage repository page.  Since the
        user is not an admin user, the email will not include any information about both HTML and image content
        that was included in the change set.
+
     """
-    sa_session = app.model.context.current
+    sa_session = app.model.session
     repo = repository.hg_repo
-    sharable_link = repository_util.generate_sharable_link_for_repository_in_tool_shed(repository, changeset_revision=None)
+    sharable_link = repository_util.generate_sharable_link_for_repository_in_tool_shed(
+        repository, changeset_revision=None
+    )
     smtp_server = app.config.smtp_server
     if smtp_server and (new_repo_alert or repository.email_alerts):
         # Send email alert to users that want them.
         if app.config.email_from is not None:
             email_from = app.config.email_from
-        elif host.split(':')[0] in ['localhost', '127.0.0.1', '0.0.0.0']:
-            email_from = 'galaxy-no-reply@' + socket.getfqdn()
+        elif host.split(":")[0] in ["localhost", "127.0.0.1", "0.0.0.0"]:
+            email_from = f"galaxy-no-reply@{socket.getfqdn()}"
         else:
-            email_from = 'galaxy-no-reply@' + host.split(':')[0]
+            email_from = f"galaxy-no-reply@{host.split(':')[0]}"
         ctx = repo[repo.changelog.tip()]
+        username = unicodify(ctx.user())
         try:
-            username = ctx.user().split()[0]
+            username = username.split()[0]
         except Exception:
-            username = ctx.user()
+            pass
         # We'll use 2 template bodies because we only want to send content
         # alerts to tool shed admin users.
         if new_repo_alert:
             template = new_repo_email_alert_template
         else:
             template = email_alert_template
         display_date = hg_util.get_readable_ctx_date(ctx)
-        admin_body = string.Template(template).safe_substitute(host=host,
-                                                               sharable_link=sharable_link,
-                                                               repository_name=repository.name,
-                                                               revision='%s:%s' % (str(ctx.rev()), ctx),
-                                                               display_date=display_date,
-                                                               description=ctx.description(),
-                                                               username=username,
-                                                               content_alert_str=content_alert_str)
-        body = string.Template(template).safe_substitute(host=host,
-                                                         sharable_link=sharable_link,
-                                                         repository_name=repository.name,
-                                                         revision='%s:%s' % (str(ctx.rev()), ctx),
-                                                         display_date=display_date,
-                                                         description=ctx.description(),
-                                                         username=username,
-                                                         content_alert_str='')
+        description = unicodify(ctx.description())
+        revision = f"{ctx.rev()}:{ctx}"
+        admin_body = string.Template(template).safe_substitute(
+            host=host,
+            sharable_link=sharable_link,
+            repository_name=repository.name,
+            revision=revision,
+            display_date=display_date,
+            description=description,
+            username=username,
+            content_alert_str=content_alert_str,
+        )
+        body = string.Template(template).safe_substitute(
+            host=host,
+            sharable_link=sharable_link,
+            repository_name=repository.name,
+            revision=revision,
+            display_date=display_date,
+            description=description,
+            username=username,
+            content_alert_str="",
+        )
         admin_users = app.config.get("admin_users", "").split(",")
         frm = email_from
         if new_repo_alert:
-            subject = "Galaxy tool shed alert for new repository named %s" % str(repository.name)
+            subject = f"Galaxy tool shed alert for new repository named {str(repository.name)}"
             subject = subject[:80]
             email_alerts = []
-            for user in sa_session.query(app.model.User) \
-                                  .filter(and_(app.model.User.table.c.deleted == false(),
-                                               app.model.User.table.c.new_repo_alert == true())):
+            for user in sa_session.query(app.model.User).filter(
+                and_(app.model.User.table.c.deleted == false(), app.model.User.table.c.new_repo_alert == true())
+            ):
                 if admin_only:
                     if user.email in admin_users:
                         email_alerts.append(user.email)
                 else:
                     email_alerts.append(user.email)
         else:
-            subject = "Galaxy tool shed update alert for repository named %s" % str(repository.name)
+            subject = f"Galaxy tool shed update alert for repository named {str(repository.name)}"
             email_alerts = json.loads(repository.email_alerts)
         for email in email_alerts:
             to = email.strip()
             # Send it
             try:
                 if to in admin_users:
                     util.send_mail(frm, to, subject, admin_body, app.config)
@@ -403,15 +435,15 @@
 def is_path_within_dependency_dir(app, path):
     """
     Detect whether the given path is within the tool_dependency_dir folder on the disk.
     (Specified by the config option). Use to filter malicious symlinks targeting outside paths.
     """
     allowed = False
     resolved_path = os.path.realpath(path)
-    tool_dependency_dir = app.config.get('tool_dependency_dir', None)
+    tool_dependency_dir = app.config.get("tool_dependency_dir", None)
     if tool_dependency_dir:
         dependency_path = os.path.abspath(tool_dependency_dir)
         allowed = os.path.commonprefix([dependency_path, resolved_path]) == dependency_path
     return allowed
 
 
 def is_path_within_repo(app, path, repository_id):
@@ -426,68 +458,71 @@
 
 def open_repository_files_folder(app, folder_path, repository_id, is_admin=False):
     """
     Return a list of dictionaries, each of which contains information for a file or directory contained
     within a directory in a repository file hierarchy.
     """
     if not is_path_browsable(app, folder_path, repository_id, is_admin):
-        log.warning('Request tries to access a folder outside of the allowed locations. Folder path: %s', folder_path)
+        log.warning("Request tries to access a folder outside of the allowed locations. Folder path: %s", folder_path)
         return []
     try:
         files_list = get_repository_files(folder_path)
     except OSError as e:
-        if str(e).find('No such file or directory') >= 0:
+        if str(e).find("No such file or directory") >= 0:
             # We have a repository with no contents.
             return []
     folder_contents = []
     for filename in files_list:
         is_folder = False
         full_path = os.path.join(folder_path, filename)
         is_link = os.path.islink(full_path)
         path_is_browsable = is_path_browsable(app, full_path, repository_id)
         if is_link and not path_is_browsable:
-            log.warning('Valid folder contains a symlink outside of the repository location. Link found in: ' + str(full_path))
+            log.warning(
+                f"Valid folder contains a symlink outside of the repository location. Link found in: {str(full_path)}"
+            )
         if filename:
             if os.path.isdir(full_path) and path_is_browsable:
                 # Append a '/' character so that our jquery dynatree will function properly.
-                filename = '%s/' % filename
-                full_path = '%s/' % full_path
+                filename = f"{filename}/"
+                full_path = f"{full_path}/"
                 is_folder = True
-            node = {"title": filename,
-                    "isFolder": is_folder,
-                    "isLazy": is_folder,
-                    "tooltip": full_path,
-                    "key": full_path}
+            node = {
+                "title": filename,
+                "isFolder": is_folder,
+                "isLazy": is_folder,
+                "tooltip": full_path,
+                "key": full_path,
+            }
             folder_contents.append(node)
     return folder_contents
 
 
 __all__ = (
-    'can_eliminate_repository_dependency',
-    'can_eliminate_tool_dependency',
-    'clean_dependency_relationships',
-    'count_repositories_in_category',
-    'generate_tool_guid',
-    'get_categories',
-    'get_category',
-    'get_category_by_name',
-    'get_requirements_from_tools',
-    'get_requirements_from_repository',
-    'get_tool_shed_repo_requirements',
-    'get_ctx_rev',
-    'get_next_prior_import_or_install_required_dict_entry',
-    'get_repository_categories',
-    'get_repository_file_contents',
-    'get_repository_type_from_tool_shed',
-    'get_tool_dependency_definition_metadata_from_tool_shed',
-    'get_tool_panel_config_tool_path_install_dir',
-    'get_tool_path_by_shed_tool_conf_filename',
-    'get_user',
-    'handle_email_alerts',
-    'have_shed_tool_conf_for_install',
-    'is_path_browsable',
-    'is_path_within_dependency_dir',
-    'is_path_within_repo',
-    'open_repository_files_folder',
-    'set_image_paths',
-    'tool_shed_is_this_tool_shed',
+    "can_eliminate_repository_dependency",
+    "clean_dependency_relationships",
+    "count_repositories_in_category",
+    "generate_tool_guid",
+    "get_categories",
+    "get_category",
+    "get_category_by_name",
+    "get_requirements_from_tools",
+    "get_requirements_from_repository",
+    "get_tool_shed_repo_requirements",
+    "get_ctx_rev",
+    "get_next_prior_import_or_install_required_dict_entry",
+    "get_repository_categories",
+    "get_repository_file_contents",
+    "get_repository_type_from_tool_shed",
+    "get_tool_dependency_definition_metadata_from_tool_shed",
+    "get_tool_panel_config_tool_path_install_dir",
+    "get_tool_path_by_shed_tool_conf_filename",
+    "get_user",
+    "handle_email_alerts",
+    "have_shed_tool_conf_for_install",
+    "is_path_browsable",
+    "is_path_within_dependency_dir",
+    "is_path_within_repo",
+    "open_repository_files_folder",
+    "set_image_paths",
+    "tool_shed_is_this_tool_shed",
 )
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/util/web_util.py` & `galaxy-web-apps-23.0.2/tool_shed/util/web_util.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,22 +1,20 @@
-# -*- coding: utf-8 -*-
 from markupsafe import escape as raw_escape
-from six import text_type
 
 ALLOWED_ELEMENTS = ["<b>", "</b>", "<br/>"]
-ALLOWED_MAP = dict((x, raw_escape(x)) for x in ALLOWED_ELEMENTS)
+ALLOWED_MAP = {x: raw_escape(x) for x in ALLOWED_ELEMENTS}
 
 
 def escape(string):
-    """ A tool shed variant of markupsafe.escape that allows a select few
+    """A tool shed variant of markupsafe.escape that allows a select few
     HTML elements that are repeatedly used in messages created deep
     in the toolshed components. Ideally abstract things would be produced
     in these components and messages in the views or client side - this is
     what should be worked toward - but for now - we have this hack.
 
     >>> assert escape(u"A <b>cómplǐcḁtëd strĩñg</b>") == u'A <b>cómplǐcḁtëd strĩñg</b>'
     """
-    escaped = text_type(raw_escape(string))
+    escaped = str(raw_escape(string))
     # Unescape few selected tags.
     for key, value in ALLOWED_MAP.items():
         escaped = escaped.replace(value, key)
     return escaped
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/utility_containers/__init__.py` & `galaxy-web-apps-23.0.2/tool_shed/utility_containers/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,360 +1,365 @@
 import logging
 import threading
 
 from galaxy import util
-from tool_shed.util import (
-    common_util,
-    container_util,
-    readme_util
+from galaxy.tool_shed.util import utility_container_manager
+from galaxy.tool_shed.util.container_util import (
+    generate_repository_dependencies_key_for_repository,
+    STRSEP,
 )
-from . import utility_container_manager
+from galaxy.util.tool_shed.common_util import parse_repository_dependency_tuple
+from tool_shed.util.readme_util import build_readme_files_dict
 
 log = logging.getLogger(__name__)
 
 
-class FailedTest(object):
+class FailedTest:
     """Failed tool tests object"""
 
     def __init__(self, id=None, stderr=None, test_id=None, tool_id=None, tool_version=None, traceback=None):
         self.id = id
         self.stderr = stderr
         self.test_id = test_id
         self.tool_id = tool_id
         self.tool_version = tool_version
         self.traceback = traceback
 
 
-class InvalidRepositoryDependency(object):
+class InvalidRepositoryDependency:
     """Invalid repository dependency definition object"""
 
-    def __init__(self, id=None, toolshed=None, repository_name=None, repository_owner=None, changeset_revision=None,
-                 prior_installation_required=False, only_if_compiling_contained_td=False, error=None):
+    def __init__(
+        self,
+        id=None,
+        toolshed=None,
+        repository_name=None,
+        repository_owner=None,
+        changeset_revision=None,
+        prior_installation_required=False,
+        only_if_compiling_contained_td=False,
+        error=None,
+    ):
         self.id = id
         self.toolshed = toolshed
         self.repository_name = repository_name
         self.repository_owner = repository_owner
         self.changeset_revision = changeset_revision
         self.prior_installation_required = prior_installation_required
         self.only_if_compiling_contained_td = only_if_compiling_contained_td
         self.error = error
 
 
-class InvalidToolDependency(object):
+class InvalidToolDependency:
     """Invalid tool dependency definition object"""
 
     def __init__(self, id=None, name=None, version=None, type=None, error=None):
         self.id = id
         self.name = name
         self.version = version
         self.type = type
         self.error = error
 
 
-class MissingTestComponent(object):
+class MissingTestComponent:
     """Missing tool test components object"""
 
     def __init__(self, id=None, missing_components=None, tool_guid=None, tool_id=None, tool_version=None):
         self.id = id
         self.missing_components = missing_components
         self.tool_guid = tool_guid
         self.tool_id = tool_id
         self.tool_version = tool_version
 
 
-class NotTested(object):
+class NotTested:
     """NotTested object"""
 
     def __init__(self, id=None, reason=None):
         self.id = id
         self.reason = reason
 
 
-class PassedTest(object):
+class PassedTest:
     """Passed tool tests object"""
 
     def __init__(self, id=None, test_id=None, tool_id=None, tool_version=None):
         self.id = id
         self.test_id = test_id
         self.tool_id = tool_id
         self.tool_version = tool_version
 
 
-class RepositoryInstallationError(object):
+class RepositoryInstallationError:
     """Repository installation error object"""
 
     def __init__(self, id=None, tool_shed=None, name=None, owner=None, changeset_revision=None, error_message=None):
         self.id = id
         self.tool_shed = tool_shed
         self.name = name
         self.owner = owner
         self.changeset_revision = changeset_revision
         self.error_message = error_message
 
 
-class RepositorySuccessfulInstallation(object):
+class RepositorySuccessfulInstallation:
     """Repository installation object"""
 
     def __init__(self, id=None, tool_shed=None, name=None, owner=None, changeset_revision=None):
         self.id = id
         self.tool_shed = tool_shed
         self.name = name
         self.owner = owner
         self.changeset_revision = changeset_revision
 
 
-class ToolDependencyInstallationError(object):
+class ToolDependencyInstallationError:
     """Tool dependency installation error object"""
 
     def __init__(self, id=None, type=None, name=None, version=None, error_message=None):
         self.id = id
         self.type = type
         self.name = name
         self.version = version
         self.error_message = error_message
 
 
-class ToolDependencySuccessfulInstallation(object):
+class ToolDependencySuccessfulInstallation:
     """Tool dependency installation object"""
 
     def __init__(self, id=None, type=None, name=None, version=None, installation_directory=None):
         self.id = id
         self.type = type
         self.name = name
         self.version = version
         self.installation_directory = installation_directory
 
 
 class ToolShedUtilityContainerManager(utility_container_manager.UtilityContainerManager):
-
     def __init__(self, app):
         self.app = app
 
     def build_invalid_repository_dependencies_root_folder(self, folder_id, invalid_repository_dependencies_dict):
         """Return a folder hierarchy containing invalid repository dependencies."""
-        label = 'Invalid repository dependencies'
+        label = "Invalid repository dependencies"
         if invalid_repository_dependencies_dict:
             invalid_repository_dependency_id = 0
             folder_id += 1
-            invalid_repository_dependencies_root_folder = \
-                utility_container_manager.Folder(id=folder_id,
-                                                 key='root',
-                                                 label='root',
-                                                 parent=None)
+            invalid_repository_dependencies_root_folder = utility_container_manager.Folder(
+                id=folder_id, key="root", label="root", parent=None
+            )
             folder_id += 1
-            invalid_repository_dependencies_folder = \
-                utility_container_manager.Folder(id=folder_id,
-                                                 key='invalid_repository_dependencies',
-                                                 label=label,
-                                                 parent=invalid_repository_dependencies_root_folder)
+            invalid_repository_dependencies_folder = utility_container_manager.Folder(
+                id=folder_id,
+                key="invalid_repository_dependencies",
+                label=label,
+                parent=invalid_repository_dependencies_root_folder,
+            )
             invalid_repository_dependencies_root_folder.folders.append(invalid_repository_dependencies_folder)
-            invalid_repository_dependencies = invalid_repository_dependencies_dict['repository_dependencies']
+            invalid_repository_dependencies = invalid_repository_dependencies_dict["repository_dependencies"]
             for invalid_repository_dependency in invalid_repository_dependencies:
                 folder_id += 1
                 invalid_repository_dependency_id += 1
-                toolshed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td, error = \
-                    common_util.parse_repository_dependency_tuple(invalid_repository_dependency, contains_error=True)
-                key = container_util.generate_repository_dependencies_key_for_repository(toolshed,
-                                                                                         name,
-                                                                                         owner,
-                                                                                         changeset_revision,
-                                                                                         prior_installation_required,
-                                                                                         only_if_compiling_contained_td)
-                label = "Repository <b>%s</b> revision <b>%s</b> owned by <b>%s</b>" % (name, changeset_revision, owner)
-                folder = utility_container_manager.Folder(id=folder_id,
-                                                          key=key,
-                                                          label=label,
-                                                          parent=invalid_repository_dependencies_folder)
-                ird = InvalidRepositoryDependency(id=invalid_repository_dependency_id,
-                                                  toolshed=toolshed,
-                                                  repository_name=name,
-                                                  repository_owner=owner,
-                                                  changeset_revision=changeset_revision,
-                                                  prior_installation_required=util.asbool(prior_installation_required),
-                                                  only_if_compiling_contained_td=util.asbool(only_if_compiling_contained_td),
-                                                  error=error)
+                (
+                    toolshed,
+                    name,
+                    owner,
+                    changeset_revision,
+                    prior_installation_required,
+                    only_if_compiling_contained_td,
+                    error,
+                ) = parse_repository_dependency_tuple(invalid_repository_dependency, contains_error=True)
+                key = generate_repository_dependencies_key_for_repository(
+                    toolshed,
+                    name,
+                    owner,
+                    changeset_revision,
+                    prior_installation_required,
+                    only_if_compiling_contained_td,
+                )
+                label = f"Repository <b>{name}</b> revision <b>{changeset_revision}</b> owned by <b>{owner}</b>"
+                folder = utility_container_manager.Folder(
+                    id=folder_id, key=key, label=label, parent=invalid_repository_dependencies_folder
+                )
+                ird = InvalidRepositoryDependency(
+                    id=invalid_repository_dependency_id,
+                    toolshed=toolshed,
+                    repository_name=name,
+                    repository_owner=owner,
+                    changeset_revision=changeset_revision,
+                    prior_installation_required=util.asbool(prior_installation_required),
+                    only_if_compiling_contained_td=util.asbool(only_if_compiling_contained_td),
+                    error=error,
+                )
                 folder.invalid_repository_dependencies.append(ird)
                 invalid_repository_dependencies_folder.folders.append(folder)
         else:
             invalid_repository_dependencies_root_folder = None
         return folder_id, invalid_repository_dependencies_root_folder
 
     def build_invalid_tool_dependencies_root_folder(self, folder_id, invalid_tool_dependencies_dict):
         """Return a folder hierarchy containing invalid tool dependencies."""
         # # INvalid tool dependencies are always packages like:
         # {"R/2.15.1": {"name": "R", "readme": "some string", "type": "package", "version": "2.15.1" "error" : "some sting" }
-        label = 'Invalid tool dependencies'
+        label = "Invalid tool dependencies"
         if invalid_tool_dependencies_dict:
             invalid_tool_dependency_id = 0
             folder_id += 1
-            invalid_tool_dependencies_root_folder = \
-                utility_container_manager.Folder(id=folder_id, key='root', label='root', parent=None)
+            invalid_tool_dependencies_root_folder = utility_container_manager.Folder(
+                id=folder_id, key="root", label="root", parent=None
+            )
             folder_id += 1
-            invalid_tool_dependencies_folder = \
-                utility_container_manager.Folder(id=folder_id,
-                                                 key='invalid_tool_dependencies',
-                                                 label=label,
-                                                 parent=invalid_tool_dependencies_root_folder)
+            invalid_tool_dependencies_folder = utility_container_manager.Folder(
+                id=folder_id, key="invalid_tool_dependencies", label=label, parent=invalid_tool_dependencies_root_folder
+            )
             invalid_tool_dependencies_root_folder.folders.append(invalid_tool_dependencies_folder)
-            for td_key, requirements_dict in invalid_tool_dependencies_dict.items():
+            for requirements_dict in invalid_tool_dependencies_dict.values():
                 folder_id += 1
                 invalid_tool_dependency_id += 1
                 try:
-                    name = requirements_dict['name']
-                    type = requirements_dict['type']
-                    version = requirements_dict['version']
-                    error = requirements_dict['error']
+                    name = requirements_dict["name"]
+                    type = requirements_dict["type"]
+                    version = requirements_dict["version"]
+                    error = requirements_dict["error"]
                 except Exception as e:
-                    name = 'unknown'
-                    type = 'unknown'
-                    version = 'unknown'
+                    name = "unknown"
+                    type = "unknown"
+                    version = "unknown"
                     error = str(e)
                 key = self.generate_tool_dependencies_key(name, version, type)
-                label = "Version <b>%s</b> of the <b>%s</b> <b>%s</b>" % (version, name, type)
-                folder = utility_container_manager.Folder(id=folder_id,
-                                                          key=key,
-                                                          label=label,
-                                                          parent=invalid_tool_dependencies_folder)
-                itd = InvalidToolDependency(id=invalid_tool_dependency_id,
-                                            name=name,
-                                            version=version,
-                                            type=type,
-                                            error=error)
+                label = f"Version <b>{version}</b> of the <b>{name}</b> <b>{type}</b>"
+                folder = utility_container_manager.Folder(
+                    id=folder_id, key=key, label=label, parent=invalid_tool_dependencies_folder
+                )
+                itd = InvalidToolDependency(
+                    id=invalid_tool_dependency_id, name=name, version=version, type=type, error=error
+                )
                 folder.invalid_tool_dependencies.append(itd)
                 invalid_tool_dependencies_folder.folders.append(folder)
         else:
             invalid_tool_dependencies_root_folder = None
         return folder_id, invalid_tool_dependencies_root_folder
 
-    def build_repository_containers(self, repository, changeset_revision, repository_dependencies, repository_metadata,
-                                    exclude=None):
+    def build_repository_containers(
+        self, repository, changeset_revision, repository_dependencies, repository_metadata, exclude=None
+    ):
         """
         Return a dictionary of containers for the received repository's dependencies and
         contents for display in the Tool Shed.
         """
         if exclude is None:
             exclude = []
-        containers_dict = dict(datatypes=None,
-                               invalid_tools=None,
-                               readme_files=None,
-                               repository_dependencies=None,
-                               tool_dependencies=None,
-                               valid_tools=None,
-                               workflows=None,
-                               valid_data_managers=None)
+        containers_dict = dict(
+            datatypes=None,
+            invalid_tools=None,
+            readme_files=None,
+            repository_dependencies=None,
+            tool_dependencies=None,
+            valid_tools=None,
+            valid_data_managers=None,
+        )
         if repository_metadata:
             metadata = repository_metadata.metadata
             lock = threading.Lock()
             lock.acquire(True)
             try:
                 folder_id = 0
-                # Datatypes container.
-                if metadata:
-                    if 'datatypes' not in exclude and 'datatypes' in metadata:
-                        datatypes = metadata['datatypes']
-                        folder_id, datatypes_root_folder = self.build_datatypes_folder(folder_id, datatypes)
-                        containers_dict['datatypes'] = datatypes_root_folder
                 # Invalid repository dependencies container.
                 if metadata:
-                    if 'invalid_repository_dependencies' not in exclude and 'invalid_repository_dependencies' in metadata:
-                        invalid_repository_dependencies = metadata['invalid_repository_dependencies']
-                        folder_id, invalid_repository_dependencies_root_folder = \
-                            self.build_invalid_repository_dependencies_root_folder(folder_id,
-                                                                                   invalid_repository_dependencies)
-                        containers_dict['invalid_repository_dependencies'] = invalid_repository_dependencies_root_folder
+                    if (
+                        "invalid_repository_dependencies" not in exclude
+                        and "invalid_repository_dependencies" in metadata
+                    ):
+                        invalid_repository_dependencies = metadata["invalid_repository_dependencies"]
+                        (
+                            folder_id,
+                            invalid_repository_dependencies_root_folder,
+                        ) = self.build_invalid_repository_dependencies_root_folder(
+                            folder_id, invalid_repository_dependencies
+                        )
+                        containers_dict["invalid_repository_dependencies"] = invalid_repository_dependencies_root_folder
                 # Invalid tool dependencies container.
                 if metadata:
-                    if 'invalid_tool_dependencies' not in exclude and 'invalid_tool_dependencies' in metadata:
-                        invalid_tool_dependencies = metadata['invalid_tool_dependencies']
-                        folder_id, invalid_tool_dependencies_root_folder = \
-                            self.build_invalid_tool_dependencies_root_folder(folder_id,
-                                                                             invalid_tool_dependencies)
-                        containers_dict['invalid_tool_dependencies'] = invalid_tool_dependencies_root_folder
+                    if "invalid_tool_dependencies" not in exclude and "invalid_tool_dependencies" in metadata:
+                        invalid_tool_dependencies = metadata["invalid_tool_dependencies"]
+                        (
+                            folder_id,
+                            invalid_tool_dependencies_root_folder,
+                        ) = self.build_invalid_tool_dependencies_root_folder(folder_id, invalid_tool_dependencies)
+                        containers_dict["invalid_tool_dependencies"] = invalid_tool_dependencies_root_folder
                 # Invalid tools container.
                 if metadata:
-                    if 'invalid_tools' not in exclude and 'invalid_tools' in metadata:
-                        invalid_tool_configs = metadata['invalid_tools']
-                        folder_id, invalid_tools_root_folder = \
-                            self.build_invalid_tools_folder(folder_id,
-                                                            invalid_tool_configs,
-                                                            changeset_revision,
-                                                            repository=repository,
-                                                            label='Invalid tools')
-                        containers_dict['invalid_tools'] = invalid_tools_root_folder
+                    if "invalid_tools" not in exclude and "invalid_tools" in metadata:
+                        invalid_tool_configs = metadata["invalid_tools"]
+                        folder_id, invalid_tools_root_folder = self.build_invalid_tools_folder(
+                            folder_id,
+                            invalid_tool_configs,
+                            changeset_revision,
+                            repository=repository,
+                            label="Invalid tools",
+                        )
+                        containers_dict["invalid_tools"] = invalid_tools_root_folder
                 # Readme files container.
                 if metadata:
-                    if 'readme_files' not in exclude and 'readme_files' in metadata:
-                        readme_files_dict = readme_util.build_readme_files_dict(self.app, repository, changeset_revision, metadata)
-                        folder_id, readme_files_root_folder = self.build_readme_files_folder(folder_id, readme_files_dict)
-                        containers_dict['readme_files'] = readme_files_root_folder
-                if 'repository_dependencies' not in exclude:
+                    if "readme_files" not in exclude and "readme_files" in metadata:
+                        readme_files_dict = build_readme_files_dict(self.app, repository, changeset_revision, metadata)
+                        folder_id, readme_files_root_folder = self.build_readme_files_folder(
+                            folder_id, readme_files_dict
+                        )
+                        containers_dict["readme_files"] = readme_files_root_folder
+                if "repository_dependencies" not in exclude:
                     # Repository dependencies container.
-                    folder_id, repository_dependencies_root_folder = \
-                        self.build_repository_dependencies_folder(folder_id=folder_id,
-                                                                  repository_dependencies=repository_dependencies,
-                                                                  label='Repository dependencies',
-                                                                  installed=False)
+                    folder_id, repository_dependencies_root_folder = self.build_repository_dependencies_folder(
+                        folder_id=folder_id,
+                        repository_dependencies=repository_dependencies,
+                        label="Repository dependencies",
+                        installed=False,
+                    )
                     if repository_dependencies_root_folder:
-                        containers_dict['repository_dependencies'] = repository_dependencies_root_folder
+                        containers_dict["repository_dependencies"] = repository_dependencies_root_folder
                 # Tool dependencies container.
                 if metadata:
-                    if 'tool_dependencies' not in exclude and 'tool_dependencies' in metadata:
-                        tool_dependencies = metadata['tool_dependencies']
-                        if 'orphan_tool_dependencies' in metadata:
+                    if "tool_dependencies" not in exclude and "tool_dependencies" in metadata:
+                        tool_dependencies = metadata["tool_dependencies"]
+                        if "orphan_tool_dependencies" in metadata:
                             # The use of the orphan_tool_dependencies category in metadata has been deprecated,
                             # but we still need to check in case the metadata is out of date.
-                            orphan_tool_dependencies = metadata['orphan_tool_dependencies']
+                            orphan_tool_dependencies = metadata["orphan_tool_dependencies"]
                             tool_dependencies.update(orphan_tool_dependencies)
                         # Tool dependencies can be categorized as orphans only if the repository contains tools.
-                        if 'tools' not in exclude:
-                            tools = metadata.get('tools', [])
-                            tools.extend(metadata.get('invalid_tools', []))
-                        folder_id, tool_dependencies_root_folder = \
-                            self.build_tool_dependencies_folder(folder_id,
-                                                                tool_dependencies,
-                                                                missing=False,
-                                                                new_install=False)
-                        containers_dict['tool_dependencies'] = tool_dependencies_root_folder
+                        if "tools" not in exclude:
+                            tools = metadata.get("tools", [])
+                            tools.extend(metadata.get("invalid_tools", []))
+                        folder_id, tool_dependencies_root_folder = self.build_tool_dependencies_folder(
+                            folder_id, tool_dependencies, missing=False, new_install=False
+                        )
+                        containers_dict["tool_dependencies"] = tool_dependencies_root_folder
                 # Valid tools container.
                 if metadata:
-                    if 'tools' not in exclude and 'tools' in metadata:
-                        valid_tools = metadata['tools']
-                        folder_id, valid_tools_root_folder = self.build_tools_folder(folder_id,
-                                                                                     valid_tools,
-                                                                                     repository,
-                                                                                     changeset_revision,
-                                                                                     label='Valid tools')
-                        containers_dict['valid_tools'] = valid_tools_root_folder
-                # Workflows container.
-                if metadata:
-                    if 'workflows' not in exclude and 'workflows' in metadata:
-                        workflows = metadata['workflows']
-                        folder_id, workflows_root_folder = \
-                            self.build_workflows_folder(folder_id=folder_id,
-                                                        workflows=workflows,
-                                                        repository_metadata_id=repository_metadata.id,
-                                                        repository_id=None,
-                                                        label='Workflows')
-                        containers_dict['workflows'] = workflows_root_folder
+                    if "tools" not in exclude and "tools" in metadata:
+                        valid_tools = metadata["tools"]
+                        folder_id, valid_tools_root_folder = self.build_tools_folder(
+                            folder_id, valid_tools, repository, changeset_revision, label="Valid tools"
+                        )
+                        containers_dict["valid_tools"] = valid_tools_root_folder
                 # Valid Data Managers container
                 if metadata:
-                    if 'data_manager' not in exclude and 'data_manager' in metadata:
-                        data_managers = metadata['data_manager'].get('data_managers', None)
-                        folder_id, data_managers_root_folder = \
-                            self.build_data_managers_folder(folder_id, data_managers, label="Data Managers")
-                        containers_dict['valid_data_managers'] = data_managers_root_folder
-                        error_messages = metadata['data_manager'].get('error_messages', None)
-                        data_managers = metadata['data_manager'].get('invalid_data_managers', None)
-                        folder_id, data_managers_root_folder = \
-                            self.build_invalid_data_managers_folder(folder_id,
-                                                                    data_managers,
-                                                                    error_messages,
-                                                                    label="Invalid Data Managers")
-                        containers_dict['invalid_data_managers'] = data_managers_root_folder
+                    if "data_manager" not in exclude and "data_manager" in metadata:
+                        data_managers = metadata["data_manager"].get("data_managers", None)
+                        folder_id, data_managers_root_folder = self.build_data_managers_folder(
+                            folder_id, data_managers, label="Data Managers"
+                        )
+                        containers_dict["valid_data_managers"] = data_managers_root_folder
+                        error_messages = metadata["data_manager"].get("error_messages", None)
+                        data_managers = metadata["data_manager"].get("invalid_data_managers", None)
+                        folder_id, data_managers_root_folder = self.build_invalid_data_managers_folder(
+                            folder_id, data_managers, error_messages, label="Invalid Data Managers"
+                        )
+                        containers_dict["invalid_data_managers"] = data_managers_root_folder
             except Exception:
                 log.exception("Exception in build_repository_containers")
             finally:
                 lock.release()
         return containers_dict
 
     def generate_tool_dependencies_key(self, name, version, type):
-        return '%s%s%s%s%s' % (str(name), container_util.STRSEP, str(version), container_util.STRSEP, str(type))
+        return f"{str(name)}{STRSEP}{str(version)}{STRSEP}{str(type)}"
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/api/authenticate.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/api/authenticate.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,34 +1,43 @@
 """
 API key retrieval through BaseAuth
 Sample usage:
 
-curl --user zipzap@foo.com:password http://localhost:9009/api/authenticate/baseauth
+.. code-block::
+
+    curl --user zipzap@foo.com:password http://localhost:9009/api/authenticate/baseauth
+
+Returns
+
+.. code-block:: json
+
+    {
+        "api_key": "<some api key>"
+    }
 
-Returns:
-{
-    "api_key": <some api key>
-}
 """
 import logging
 
-from galaxy.web import expose_api_raw_anonymous_and_sessionless
-from galaxy.webapps.galaxy.api.authenticate import AuthenticationController
+from galaxy.web import expose_api_anonymous_and_sessionless
+from galaxy.webapps.galaxy.api import depends
+from galaxy.webapps.galaxy.services.authenticate import AuthenticationService
+from . import BaseShedAPIController
 
 log = logging.getLogger(__name__)
 
 
-class ToolShedAuthenticationController(AuthenticationController):
+class ToolShedAuthenticationController(BaseShedAPIController):
+    authentication_service = depends(AuthenticationService)
 
-    @expose_api_raw_anonymous_and_sessionless
+    @expose_api_anonymous_and_sessionless
     def get_tool_shed_api_key(self, trans, **kwd):
         """
-        def get_api_key( self, trans, **kwd )
-        * GET /api/authenticate/baseauth
+        GET /api/authenticate/baseauth
+
         returns an API key for authenticated user based on BaseAuth headers
 
         :returns: api_key in json format
         :rtype:   dict
 
         :raises: ObjectNotFound, HTTPBadRequest
         """
-        return self.get_api_key(trans, **kwd)
+        return self.authentication_service.get_api_key(trans.environ, trans.request)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/api/categories.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/api/categories.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,34 +1,35 @@
 import logging
+from typing import (
+    Callable,
+    Dict,
+)
 
 import tool_shed.util.shed_util_common as suc
 from galaxy import (
     exceptions,
     util,
-    web
+    web,
 )
 from galaxy.web import (
     expose_api,
     expose_api_anonymous_and_sessionless,
-    require_admin as require_admin
+    require_admin,
 )
 from galaxy.webapps.base.controller import BaseAPIController
 from tool_shed.util import repository_util
 
 log = logging.getLogger(__name__)
 
 
 class CategoriesController(BaseAPIController):
     """RESTful controller for interactions with categories in the Tool Shed."""
 
-    def __get_repository_count(self, trans, category_name):
-        return self.app.repository_registry.viewable_repositories_and_suites_by_category.get(category_name, 0)
-
-    def __get_value_mapper(self, trans):
-        value_mapper = {'id': trans.security.encode_id}
+    def __get_value_mapper(self, trans) -> Dict[str, Callable]:
+        value_mapper = {"id": trans.security.encode_id}
         return value_mapper
 
     @expose_api
     @require_admin
     def create(self, trans, payload, **kwd):
         """
         POST /api/categories
@@ -38,34 +39,33 @@
         :param name (required): the name of the category
         :param description (optional): the description of the category (if not provided, the name will be used)
 
         Example: POST /api/categories/?key=XXXYYYXXXYYY
         Content-Disposition: form-data; name="name" Category_Name
         Content-Disposition: form-data; name="description" Category_Description
         """
-        category_dict = dict(message='', status='ok')
-        name = payload.get('name', '')
+        category_dict = dict(message="", status="ok")
+        name = payload.get("name", "")
         if name:
-            description = payload.get('description', '')
+            description = payload.get("description", "")
             if not description:
                 # Default the description to the name.
                 description = name
             if suc.get_category_by_name(self.app, name):
-                raise exceptions.Conflict('A category with that name already exists.')
+                raise exceptions.Conflict("A category with that name already exists.")
             else:
                 # Create the category
                 category = self.app.model.Category(name=name, description=description)
                 trans.sa_session.add(category)
                 trans.sa_session.flush()
-                category_dict = category.to_dict(view='element',
-                                                 value_mapper=self.__get_value_mapper(trans))
-                category_dict['message'] = "Category '%s' has been created" % str(category.name)
-                category_dict['url'] = web.url_for(controller='categories',
-                                                   action='show',
-                                                   id=trans.security.encode_id(category.id))
+                category_dict = category.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
+                category_dict["message"] = f"Category '{str(category.name)}' has been created"
+                category_dict["url"] = web.url_for(
+                    controller="categories", action="show", id=trans.security.encode_id(category.id)
+                )
         else:
             raise exceptions.RequestParameterMissingException('Missing required parameter "name".')
         return category_dict
 
     @expose_api_anonymous_and_sessionless
     def get_repositories(self, trans, category_id, **kwd):
         """
@@ -75,36 +75,31 @@
         :param id: the encoded id of the Category object
         :param sort_key: the field by which the repositories should be sorted
         :param sort_order: ascending or descending sort
         :param page: the page number to return
 
         Example: GET localhost:9009/api/categories/f9cad7b01a472135/repositories
         """
-        installable = util.asbool(kwd.get('installable', 'false'))
-        sort_key = kwd.get('sort_key', 'name')
-        sort_order = kwd.get('sort_order', 'asc')
-        page = kwd.get('page', None)
+        installable = util.asbool(kwd.get("installable", "false"))
+        sort_key = kwd.get("sort_key", "name")
+        sort_order = kwd.get("sort_order", "asc")
+        page = kwd.get("page", None)
         category = suc.get_category(self.app, category_id)
         if category is None:
-            category_dict = dict(message='Unable to locate category record for id %s.' % (str(id)),
-                                 status='error')
+            category_dict = dict(message=f"Unable to locate category record for id {str(id)}.", status="error")
             return category_dict
-        category_dict = category.to_dict(view='element',
-                                         value_mapper=self.__get_value_mapper(trans))
-        category_dict['repository_count'] = suc.count_repositories_in_category(self.app, category_id)
-        category_dict['url'] = web.url_for(controller='categories',
-                                           action='show',
-                                           id=trans.security.encode_id(category.id))
-        repositories = repository_util.get_repositories_by_category(self.app,
-                                                                    category.id,
-                                                                    installable=installable,
-                                                                    sort_order=sort_order,
-                                                                    sort_key=sort_key,
-                                                                    page=page)
-        category_dict['repositories'] = repositories
+        category_dict = category.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
+        category_dict["repository_count"] = suc.count_repositories_in_category(self.app, category_id)
+        category_dict["url"] = web.url_for(
+            controller="categories", action="show", id=trans.security.encode_id(category.id)
+        )
+        repositories = repository_util.get_repositories_by_category(
+            self.app, category.id, installable=installable, sort_order=sort_order, sort_key=sort_key, page=page
+        )
+        category_dict["repositories"] = repositories
         return category_dict
 
     @expose_api_anonymous_and_sessionless
     def index(self, trans, deleted=False, **kwd):
         """
         GET /api/categories
         Return a list of dictionaries that contain information about each Category.
@@ -112,24 +107,27 @@
         :param deleted: flag used to include deleted categories
 
         Example: GET localhost:9009/api/categories
         """
         category_dicts = []
         deleted = util.asbool(deleted)
         if deleted and not trans.user_is_admin:
-            raise exceptions.AdminRequiredException('Only administrators can query deleted categories.')
-        for category in trans.sa_session.query(self.app.model.Category) \
-                                        .filter(self.app.model.Category.table.c.deleted == deleted) \
-                                        .order_by(self.app.model.Category.table.c.name):
-            category_dict = category.to_dict(view='collection',
-                                             value_mapper=self.__get_value_mapper(trans))
-            category_dict['url'] = web.url_for(controller='categories',
-                                               action='show',
-                                               id=trans.security.encode_id(category.id))
-            category_dict['repositories'] = self.app.repository_registry.viewable_repositories_and_suites_by_category.get(category.name, 0)
+            raise exceptions.AdminRequiredException("Only administrators can query deleted categories.")
+        for category in (
+            trans.sa_session.query(self.app.model.Category)
+            .filter(self.app.model.Category.table.c.deleted == deleted)
+            .order_by(self.app.model.Category.table.c.name)
+        ):
+            category_dict = category.to_dict(view="collection", value_mapper=self.__get_value_mapper(trans))
+            category_dict["url"] = web.url_for(
+                controller="categories", action="show", id=trans.security.encode_id(category.id)
+            )
+            category_dict[
+                "repositories"
+            ] = self.app.repository_registry.viewable_repositories_and_suites_by_category.get(category.name, 0)
             category_dicts.append(category_dict)
         return category_dicts
 
     @expose_api_anonymous_and_sessionless
     def show(self, trans, id, **kwd):
         """
         GET /api/categories/{encoded_category_id}
@@ -137,16 +135,14 @@
 
         :param id: the encoded id of the Category object
 
         Example: GET localhost:9009/api/categories/f9cad7b01a472135
         """
         category = suc.get_category(self.app, id)
         if category is None:
-            category_dict = dict(message='Unable to locate category record for id %s.' % (str(id)),
-                                 status='error')
+            category_dict = dict(message=f"Unable to locate category record for id {str(id)}.", status="error")
             return category_dict
-        category_dict = category.to_dict(view='element',
-                                         value_mapper=self.__get_value_mapper(trans))
-        category_dict['url'] = web.url_for(controller='categories',
-                                           action='show',
-                                           id=trans.security.encode_id(category.id))
+        category_dict = category.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
+        category_dict["url"] = web.url_for(
+            controller="categories", action="show", id=trans.security.encode_id(category.id)
+        )
         return category_dict
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/api/configuration.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/api/configuration.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,17 +7,16 @@
 from galaxy.web import expose_api_anonymous_and_sessionless
 from galaxy.webapps.base.controller import BaseAPIController
 
 log = logging.getLogger(__name__)
 
 
 class ConfigurationController(BaseAPIController):
-
     def __init__(self, app):
-        super(ConfigurationController, self).__init__(app)
+        super().__init__(app)
 
     @expose_api_anonymous_and_sessionless
     def version(self, trans, **kwds):
         """
         GET /api/version
         Return a description of the version_major and version of Galaxy Tool Shed
         (e.g. 15.07 and 15.07.dev).
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/api/groups.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/api/groups.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,39 +1,43 @@
 import logging
+from typing import (
+    Callable,
+    Dict,
+)
 
 from galaxy import (
     util,
-    web
+    web,
 )
 from galaxy.exceptions import (
     AdminRequiredException,
     ObjectNotFound,
-    RequestParameterMissingException
+    RequestParameterMissingException,
 )
 from galaxy.util import pretty_print_time_interval
 from galaxy.web import (
     expose_api,
     expose_api_anonymous_and_sessionless,
-    require_admin as require_admin
+    require_admin,
 )
 from galaxy.webapps.base.controller import BaseAPIController
 from tool_shed.managers import groups
 
 log = logging.getLogger(__name__)
 
 
 class GroupsController(BaseAPIController):
     """RESTful controller for interactions with groups in the Tool Shed."""
 
     def __init__(self, app):
-        super(GroupsController, self).__init__(app)
+        super().__init__(app)
         self.group_manager = groups.GroupManager()
 
-    def __get_value_mapper(self, trans):
-        value_mapper = {'id' : trans.security.encode_id}
+    def __get_value_mapper(self, trans) -> Dict[str, Callable]:
+        value_mapper = {"id": trans.security.encode_id}
         return value_mapper
 
     @expose_api_anonymous_and_sessionless
     def index(self, trans, deleted=False, **kwd):
         """
         GET /api/groups
         Return a list of dictionaries that contain information about each Group.
@@ -41,15 +45,15 @@
         :param deleted: flag used to include deleted groups
 
         Example: GET localhost:9009/api/groups
         """
         group_dicts = []
         deleted = util.asbool(deleted)
         if deleted and not trans.user_is_admin:
-            raise AdminRequiredException('Only administrators can query deleted groups.')
+            raise AdminRequiredException("Only administrators can query deleted groups.")
         for group in self.group_manager.list(trans, deleted):
             group_dicts.append(self._populate(trans, group))
         return group_dicts
 
     @expose_api
     @require_admin
     def create(self, trans, payload, **kwd):
@@ -61,23 +65,25 @@
         :param name (required): the name of the group
         :param description (optional): the description of the group
 
         Example: POST /api/groups/?key=XXXYYYXXXYYY
         Content-Disposition: form-data; name="name" Group_Name
         Content-Disposition: form-data; name="description" Group_Description
         """
-        group_dict = dict(message='', status='ok')
-        name = payload.get('name', '')
+        group_dict = dict(message="", status="ok")
+        name = payload.get("name", "")
         if name:
-            description = payload.get('description', '')
+            description = payload.get("description", "")
             if not description:
-                description = ''
+                description = ""
             else:
                 # TODO add description field to the model
-                group_dict = self.group_manager.create(trans, name=name).to_dict(view='element', value_mapper=self.__get_value_mapper(trans))
+                group_dict = self.group_manager.create(trans, name=name).to_dict(
+                    view="element", value_mapper=self.__get_value_mapper(trans)
+                )
         else:
             raise RequestParameterMissingException('Missing required parameter "name".')
         return group_dict
 
     @expose_api_anonymous_and_sessionless
     def show(self, trans, encoded_id, **kwd):
         """
@@ -87,70 +93,75 @@
         :param id: the encoded id of the Group object
 
         Example: GET localhost:9009/api/groups/f9cad7b01a472135
         """
         decoded_id = trans.security.decode_id(encoded_id)
         group = self.group_manager.get(trans, decoded_id)
         if group is None:
-            raise ObjectNotFound('Unable to locate group record for id %s.' % (str(encoded_id)))
+            raise ObjectNotFound("Unable to locate group record with the given id.")
         return self._populate(trans, group)
 
     def _populate(self, trans, group):
         """
         Turn the given group information from DB into a dict
         and add other characteristics like members and repositories.
         """
         model = trans.app.model
-        group_dict = group.to_dict(view='collection', value_mapper=self.__get_value_mapper(trans))
+        group_dict = group.to_dict(view="collection", value_mapper=self.__get_value_mapper(trans))
         group_members = []
         group_repos = []
         total_downloads = 0
         for uga in group.users:
             user = trans.sa_session.query(model.User).filter(model.User.table.c.id == uga.user_id).one()
             user_repos_count = 0
-            for repo in trans.sa_session.query(model.Repository) \
-                    .filter(model.Repository.table.c.user_id == uga.user_id) \
-                    .join(model.RepositoryMetadata.table) \
-                    .join(model.User.table) \
-                    .outerjoin(model.RepositoryCategoryAssociation.table) \
-                    .outerjoin(model.Category.table):
+            for repo in (
+                trans.sa_session.query(model.Repository)
+                .filter(model.Repository.table.c.user_id == uga.user_id)
+                .join(model.RepositoryMetadata.table)
+                .join(model.User.table)
+                .outerjoin(model.RepositoryCategoryAssociation.table)
+                .outerjoin(model.Category.table)
+            ):
                 categories = []
                 for rca in repo.categories:
                     cat_dict = dict(name=rca.category.name, id=trans.app.security.encode_id(rca.category.id))
                     categories.append(cat_dict)
                 time_repo_created_full = repo.create_time.strftime("%Y-%m-%d %I:%M %p")
                 time_repo_updated_full = repo.update_time.strftime("%Y-%m-%d %I:%M %p")
                 time_repo_created = pretty_print_time_interval(repo.create_time, True)
                 time_repo_updated = pretty_print_time_interval(repo.update_time, True)
-                approved = ''
-                ratings = []
-                for review in repo.reviews:
-                    if review.rating:
-                        ratings.append(review.rating)
-                    if review.approved == 'yes':
-                        approved = 'yes'
                 # TODO add user ratings
-                ratings_mean = str(float(sum(ratings)) / len(ratings)) if len(ratings) > 0 else ''
                 total_downloads += repo.times_downloaded
-                group_repos.append({'name': repo.name,
-                                    'times_downloaded': repo.times_downloaded,
-                                    'owner': repo.user.username,
-                                    'time_created_full': time_repo_created_full,
-                                    'time_created': time_repo_created,
-                                    'time_updated_full': time_repo_updated_full,
-                                    'time_updated': time_repo_updated,
-                                    'description': repo.description,
-                                    'approved': approved,
-                                    'ratings_mean': ratings_mean,
-                                    'categories' : categories})
+                group_repos.append(
+                    {
+                        "name": repo.name,
+                        "times_downloaded": repo.times_downloaded,
+                        "owner": repo.user.username,
+                        "time_created_full": time_repo_created_full,
+                        "time_created": time_repo_created,
+                        "time_updated_full": time_repo_updated_full,
+                        "time_updated": time_repo_updated,
+                        "description": repo.description,
+                        "categories": categories,
+                    }
+                )
                 user_repos_count += 1
             encoded_user_id = trans.app.security.encode_id(user.id)
-            user_repos_url = web.url_for(controller='repository', action='browse_repositories_by_user', user_id=encoded_user_id)
+            user_repos_url = web.url_for(
+                controller="repository", action="browse_repositories_by_user", user_id=encoded_user_id
+            )
             time_created = pretty_print_time_interval(user.create_time, True)
-            member_dict = {'id': encoded_user_id, 'username': user.username, 'user_repos_url': user_repos_url, 'user_repos_count': user_repos_count, 'user_tools_count': 'unknown', 'time_created': time_created}
+            member_dict = {
+                "id": encoded_user_id,
+                "username": user.username,
+                "user_repos_url": user_repos_url,
+                "user_repos_count": user_repos_count,
+                "user_tools_count": "unknown",
+                "time_created": time_created,
+            }
             group_members.append(member_dict)
-        group_dict['members'] = group_members
-        group_dict['total_members'] = len(group_members)
-        group_dict['repositories'] = group_repos
-        group_dict['total_repos'] = len(group_repos)
-        group_dict['total_downloads'] = total_downloads
+        group_dict["members"] = group_members
+        group_dict["total_members"] = len(group_members)
+        group_dict["repositories"] = group_repos
+        group_dict["total_repos"] = len(group_repos)
+        group_dict["total_downloads"] = total_downloads
         return group_dict
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/api/repositories.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/api/repositories.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,53 +1,60 @@
 import json
 import logging
 import os
 import tarfile
 from collections import namedtuple
+from io import StringIO
 from time import strftime
+from typing import (
+    Callable,
+    Dict,
+)
 
-from six import StringIO
-from sqlalchemy import and_, false
+from sqlalchemy import (
+    and_,
+    false,
+)
 from webob.compat import cgi_FieldStorage
 
 from galaxy import (
     util,
-    web
+    web,
 )
 from galaxy.exceptions import (
     ActionInputError,
     ConfigDoesNotAllowException,
     InsufficientPermissionsException,
-    MalformedId,
     ObjectNotFound,
     RequestParameterInvalidException,
-    RequestParameterMissingException
+    RequestParameterMissingException,
 )
 from galaxy.util import checkers
 from galaxy.web import (
     expose_api,
     expose_api_anonymous_and_sessionless,
-    expose_api_raw_anonymous_and_sessionless
+    expose_api_raw_anonymous_and_sessionless,
 )
 from galaxy.webapps.base.controller import (
     BaseAPIController,
-    HTTPBadRequest
+    HTTPBadRequest,
 )
 from tool_shed.dependencies import attribute_handlers
 from tool_shed.metadata import repository_metadata_manager
 from tool_shed.repository_types import util as rt_util
 from tool_shed.util import (
     commit_util,
     encoding_util,
     hg_util,
     metadata_util,
     repository_content_util,
     repository_util,
-    tool_util
+    tool_util,
 )
+from tool_shed.webapp import model
 from tool_shed.webapp.search.repo_search import RepoSearch
 
 log = logging.getLogger(__name__)
 
 
 class RepositoriesController(BaseAPIController):
     """RESTful controller for interactions with repositories in the Tool Shed."""
@@ -63,39 +70,40 @@
         The following parameters are included in the payload.
         :param tool_shed_url (required): the base URL of the Tool Shed containing the Repository
         :param name (required): the name of the Repository
         :param owner (required): the owner of the Repository
         """
         response_dict = {}
         if not trans.user_is_admin:
-            response_dict['status'] = 'error'
-            response_dict['message'] = "You are not authorized to add entries to this Tool Shed's repository registry."
+            response_dict["status"] = "error"
+            response_dict["message"] = "You are not authorized to add entries to this Tool Shed's repository registry."
             return response_dict
-        tool_shed_url = payload.get('tool_shed_url', '')
+        tool_shed_url = payload.get("tool_shed_url", "")
         if not tool_shed_url:
             raise HTTPBadRequest(detail="Missing required parameter 'tool_shed_url'.")
-        tool_shed_url = tool_shed_url.rstrip('/')
-        name = payload.get('name', '')
+        tool_shed_url = tool_shed_url.rstrip("/")
+        name = payload.get("name", "")
         if not name:
             raise HTTPBadRequest(detail="Missing required parameter 'name'.")
-        owner = payload.get('owner', '')
+        owner = payload.get("owner", "")
         if not owner:
             raise HTTPBadRequest(detail="Missing required parameter 'owner'.")
         repository = repository_util.get_repository_by_name_and_owner(self.app, name, owner)
         if repository is None:
-            error_message = 'Cannot locate repository with name %s and owner %s,' % (str(name), str(owner))
+            error_message = f"Cannot locate repository with name {name} and owner {owner},"
             log.debug(error_message)
-            response_dict['status'] = 'error'
-            response_dict['message'] = error_message
+            response_dict["status"] = "error"
+            response_dict["message"] = error_message
             return response_dict
         # Update the repository registry.
         self.app.repository_registry.add_entry(repository)
-        response_dict['status'] = 'ok'
-        response_dict['message'] = 'Entries for repository %s owned by %s have been added to the Tool Shed repository registry.' \
-            % (name, owner)
+        response_dict["status"] = "ok"
+        response_dict[
+            "message"
+        ] = f"Entries for repository {name} owned by {owner} have been added to the Tool Shed repository registry."
         return response_dict
 
     @web.legacy_expose_api_anonymous
     def get_ordered_installable_revisions(self, trans, name=None, owner=None, **kwd):
         """
         GET /api/repositories/get_ordered_installable_revisions
 
@@ -103,27 +111,31 @@
         :param owner: the owner of the Repository
 
         Returns the ordered list of changeset revision hash strings that are associated with installable revisions.
         As in the changelog, the list is ordered oldest to newest.
         """
         # Example URL: http://localhost:9009/api/repositories/get_ordered_installable_revisions?name=add_column&owner=test
         if name is None:
-            name = kwd.get('name', None)
+            name = kwd.get("name", None)
         if owner is None:
-            owner = kwd.get('owner', None)
-        tsr_id = kwd.get('tsr_id', None)
-        eagerload_columns = ['downloadable_revisions']
+            owner = kwd.get("owner", None)
+        tsr_id = kwd.get("tsr_id", None)
+        eagerload_columns = [model.Repository.downloadable_revisions]
         if None not in [name, owner]:
             # Get the repository information.
-            repository = repository_util.get_repository_by_name_and_owner(self.app, name, owner, eagerload_columns=eagerload_columns)
+            repository = repository_util.get_repository_by_name_and_owner(
+                self.app, name, owner, eagerload_columns=eagerload_columns
+            )
             if repository is None:
                 trans.response.status = 404
-                return {'status': 'error', 'message': 'No repository named %s found with owner %s' % (name, owner)}
+                return {"status": "error", "message": f"No repository named {name} found with owner {owner}"}
         elif tsr_id is not None:
-            repository = repository_util.get_repository_in_tool_shed(self.app, tsr_id, eagerload_columns=eagerload_columns)
+            repository = repository_util.get_repository_in_tool_shed(
+                self.app, tsr_id, eagerload_columns=eagerload_columns
+            )
         else:
             error_message = "Error in the Tool Shed repositories API in get_ordered_installable_revisions: "
             error_message += "invalid parameters received."
             log.debug(error_message)
             return []
         return [revision[1] for revision in repository.installable_revisions(self.app, sort_revisions=True)]
 
@@ -132,154 +144,166 @@
         """
         GET /api/repositories/get_repository_revision_install_info
 
         :param name: the name of the Repository
         :param owner: the owner of the Repository
         :param changeset_revision: the changeset_revision of the RepositoryMetadata object associated with the Repository
 
-        Returns a list of the following dictionaries::
-        - a dictionary defining the Repository.  For example:
-        {
-            "deleted": false,
-            "deprecated": false,
-            "description": "add_column hello",
-            "id": "f9cad7b01a472135",
-            "long_description": "add_column hello",
-            "name": "add_column",
-            "owner": "test",
-            "private": false,
-            "times_downloaded": 6,
-            "url": "/api/repositories/f9cad7b01a472135",
-            "user_id": "f9cad7b01a472135"
-        }
-        - a dictionary defining the Repository revision (RepositoryMetadata).  For example:
-        {
-            "changeset_revision": "3a08cc21466f",
-            "downloadable": true,
-            "has_repository_dependencies": false,
-            "has_repository_dependencies_only_if_compiling_contained_td": false,
-            "id": "f9cad7b01a472135",
-            "includes_datatypes": false,
-            "includes_tool_dependencies": false,
-            "includes_tools": true,
-            "includes_tools_for_display_in_tool_panel": true,
-            "includes_workflows": false,
-            "malicious": false,
-            "repository_id": "f9cad7b01a472135",
-            "url": "/api/repository_revisions/f9cad7b01a472135",
-            "valid_tools": [{u'add_to_tool_panel': True,
-                u'description': u'data on any column using simple expressions',
-                u'guid': u'localhost:9009/repos/enis/sample_repo_1/Filter1/2.2.0',
-                u'id': u'Filter1',
-                u'name': u'Filter',
-                u'requirements': [],
-                u'tests': [{u'inputs': [[u'input', u'1.bed'], [u'cond', u"c1=='chr22'"]],
-                  u'name': u'Test-1',
-                  u'outputs': [[u'out_file1', u'filter1_test1.bed']],
-                  u'required_files': [u'1.bed', u'filter1_test1.bed']}],
-                u'tool_config': u'database/community_files/000/repo_1/filtering.xml',
-                u'tool_type': u'default',
-                u'version': u'2.2.0',
-                u'version_string_cmd': None}]
-        }
-        - a dictionary including the additional information required to install the repository.  For example:
-        {
-            "add_column": [
-                "add_column hello",
-                "http://test@localhost:9009/repos/test/add_column",
-                "3a08cc21466f",
-                "1",
-                "test",
-                {},
-                {}
-            ]
-        }
+        Returns a list of the following dictionaries
+
+        - a dictionary defining the Repository.  For example::
+
+            {
+                "deleted": false,
+                "deprecated": false,
+                "description": "add_column hello",
+                "id": "f9cad7b01a472135",
+                "long_description": "add_column hello",
+                "name": "add_column",
+                "owner": "test",
+                "private": false,
+                "times_downloaded": 6,
+                "url": "/api/repositories/f9cad7b01a472135",
+                "user_id": "f9cad7b01a472135"
+            }
+
+        - a dictionary defining the Repository revision (RepositoryMetadata).  For example::
+
+            {
+                "changeset_revision": "3a08cc21466f",
+                "downloadable": true,
+                "has_repository_dependencies": false,
+                "has_repository_dependencies_only_if_compiling_contained_td": false,
+                "id": "f9cad7b01a472135",
+                "includes_datatypes": false,
+                "includes_tool_dependencies": false,
+                "includes_tools": true,
+                "includes_tools_for_display_in_tool_panel": true,
+                "includes_workflows": false,
+                "malicious": false,
+                "repository_id": "f9cad7b01a472135",
+                "url": "/api/repository_revisions/f9cad7b01a472135",
+                "valid_tools": [{u'add_to_tool_panel': True,
+                    u'description': u'data on any column using simple expressions',
+                    u'guid': u'localhost:9009/repos/enis/sample_repo_1/Filter1/2.2.0',
+                    u'id': u'Filter1',
+                    u'name': u'Filter',
+                    u'requirements': [],
+                    u'tests': [{u'inputs': [[u'input', u'1.bed'], [u'cond', u"c1=='chr22'"]],
+                    u'name': u'Test-1',
+                    u'outputs': [[u'out_file1', u'filter1_test1.bed']],
+                    u'required_files': [u'1.bed', u'filter1_test1.bed']}],
+                    u'tool_config': u'database/community_files/000/repo_1/filtering.xml',
+                    u'tool_type': u'default',
+                    u'version': u'2.2.0',
+                    u'version_string_cmd': None}]
+            }
+
+        - a dictionary including the additional information required to install the repository.  For example::
+
+            {
+                "add_column": [
+                    "add_column hello",
+                    "http://test@localhost:9009/repos/test/add_column",
+                    "3a08cc21466f",
+                    "1",
+                    "test",
+                    {},
+                    {}
+                ]
+            }
+
         """
         # Example URL:
         # http://<xyz>/api/repositories/get_repository_revision_install_info?name=<n>&owner=<o>&changeset_revision=<cr>
         if name and owner and changeset_revision:
             # Get the repository information.
-            repository = repository_util.get_repository_by_name_and_owner(self.app, name, owner, eagerload_columns=['downloadable_revisions'])
+            repository = repository_util.get_repository_by_name_and_owner(
+                self.app, name, owner, eagerload_columns=[model.Repository.downloadable_revisions]
+            )
             if repository is None:
-                log.debug('Cannot locate repository %s owned by %s' % (str(name), str(owner)))
+                log.debug(f"Cannot locate repository {name} owned by {owner}")
                 return {}, {}, {}
             encoded_repository_id = trans.security.encode_id(repository.id)
-            repository_dict = repository.to_dict(view='element',
-                                                 value_mapper=self.__get_value_mapper(trans))
-            repository_dict['url'] = web.url_for(controller='repositories',
-                                                 action='show',
-                                                 id=encoded_repository_id)
+            repository_dict = repository.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
+            repository_dict["url"] = web.url_for(controller="repositories", action="show", id=encoded_repository_id)
             # Get the repository_metadata information.
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app,
-                                                                                              encoded_repository_id,
-                                                                                              changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                self.app, encoded_repository_id, changeset_revision
+            )
             if repository_metadata is None:
                 # The changeset_revision column in the repository_metadata table has been updated with a new
                 # value value, so find the changeset_revision to which we need to update.
-                new_changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, changeset_revision)
-                repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app,
-                                                                                                  encoded_repository_id,
-                                                                                                  new_changeset_revision)
+                new_changeset_revision = metadata_util.get_next_downloadable_changeset_revision(
+                    self.app, repository, changeset_revision
+                )
+                repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                    self.app, encoded_repository_id, new_changeset_revision
+                )
                 changeset_revision = new_changeset_revision
             if repository_metadata is not None:
                 encoded_repository_metadata_id = trans.security.encode_id(repository_metadata.id)
-                repository_metadata_dict = repository_metadata.to_dict(view='collection',
-                                                                       value_mapper=self.__get_value_mapper(trans))
-                repository_metadata_dict['url'] = web.url_for(controller='repository_revisions',
-                                                              action='show',
-                                                              id=encoded_repository_metadata_id)
-                if 'tools' in repository_metadata.metadata:
-                    repository_metadata_dict['valid_tools'] = repository_metadata.metadata['tools']
+                repository_metadata_dict = repository_metadata.to_dict(
+                    view="collection", value_mapper=self.__get_value_mapper(trans)
+                )
+                repository_metadata_dict["url"] = web.url_for(
+                    controller="repository_revisions", action="show", id=encoded_repository_metadata_id
+                )
+                if "tools" in repository_metadata.metadata:
+                    repository_metadata_dict["valid_tools"] = repository_metadata.metadata["tools"]
                 # Get the repo_info_dict for installing the repository.
-                repo_info_dict, \
-                    includes_tools, \
-                    includes_tool_dependencies, \
-                    includes_tools_for_display_in_tool_panel, \
-                    has_repository_dependencies, \
-                    has_repository_dependencies_only_if_compiling_contained_td = \
-                    repository_util.get_repo_info_dict(self.app,
-                                                       trans.user,
-                                                       encoded_repository_id,
-                                                       changeset_revision)
+                (
+                    repo_info_dict,
+                    includes_tools,
+                    includes_tool_dependencies,
+                    includes_tools_for_display_in_tool_panel,
+                    has_repository_dependencies,
+                    has_repository_dependencies_only_if_compiling_contained_td,
+                ) = repository_util.get_repo_info_dict(self.app, trans.user, encoded_repository_id, changeset_revision)
                 return repository_dict, repository_metadata_dict, repo_info_dict
             else:
-                log.debug("Unable to locate repository_metadata record for repository id %s and changeset_revision %s" %
-                          (str(repository.id), str(changeset_revision)))
+                log.debug(
+                    "Unable to locate repository_metadata record for repository id %s and changeset_revision %s"
+                    % (str(repository.id), str(changeset_revision))
+                )
                 return repository_dict, {}, {}
         else:
             debug_msg = "Error in the Tool Shed repositories API in get_repository_revision_install_info: "
-            debug_msg += "Invalid name %s or owner %s or changeset_revision %s received." % \
-                (str(name), str(owner), str(changeset_revision))
+            debug_msg += f"Invalid name {name} or owner {owner} or changeset_revision {changeset_revision} received."
             log.debug(debug_msg)
             return {}, {}, {}
 
     @web.legacy_expose_api_anonymous
     def get_installable_revisions(self, trans, **kwd):
         """
         GET /api/repositories/get_installable_revisions
 
         :param tsr_id: the encoded toolshed ID of the repository
 
         Returns a list of lists of changesets, in the format [ [ 0, fbb391dc803c ], [ 1, 9d9ec4d9c03e ], [ 2, 9b5b20673b89 ], [ 3, e8c99ce51292 ] ].
         """
         # Example URL: http://localhost:9009/api/repositories/get_installable_revisions?tsr_id=9d37e53072ff9fa4
-        tsr_id = kwd.get('tsr_id', None)
+        tsr_id = kwd.get("tsr_id", None)
         if tsr_id is not None:
-            repository = repository_util.get_repository_in_tool_shed(self.app, tsr_id, eagerload_columns=['downloadable_revisions'])
+            repository = repository_util.get_repository_in_tool_shed(
+                self.app, tsr_id, eagerload_columns=[model.Repository.downloadable_revisions]
+            )
         else:
             error_message = "Error in the Tool Shed repositories API in get_ordered_installable_revisions: "
             error_message += "missing or invalid parameter received."
             log.debug(error_message)
             return []
         return repository.installable_revisions(self.app)
 
-    def __get_value_mapper(self, trans):
-        value_mapper = {'id': trans.security.encode_id,
-                        'repository_id': trans.security.encode_id,
-                        'user_id': trans.security.encode_id}
+    def __get_value_mapper(self, trans) -> Dict[str, Callable]:
+        value_mapper = {
+            "id": trans.security.encode_id,
+            "repository_id": trans.security.encode_id,
+            "user_id": trans.security.encode_id,
+        }
         return value_mapper
 
     @expose_api_raw_anonymous_and_sessionless
     def index(self, trans, deleted=False, owner=None, name=None, **kwd):
         """
         GET /api/repositories
         Displays a collection of repositories with optional criteria.
@@ -316,130 +340,161 @@
 
         Examples:
             GET http://localhost:9009/api/repositories
             GET http://localhost:9009/api/repositories?q=fastq
         """
         repository_dicts = []
         deleted = util.asbool(deleted)
-        q = kwd.get('q', '')
+        q = kwd.get("q", "")
         if q:
-            page = kwd.get('page', 1)
-            page_size = kwd.get('page_size', 10)
+            page = kwd.get("page", 1)
+            page_size = kwd.get("page_size", 10)
             try:
                 page = int(page)
                 page_size = int(page_size)
             except ValueError:
                 raise RequestParameterInvalidException('The "page" and "page_size" parameters have to be integers.')
-            return_jsonp = util.asbool(kwd.get('jsonp', False))
-            callback = kwd.get('callback', 'callback')
+            return_jsonp = util.asbool(kwd.get("jsonp", False))
+            callback = kwd.get("callback", "callback")
             search_results = self._search(trans, q, page, page_size)
             if return_jsonp:
-                response = str('%s(%s);' % (callback, json.dumps(search_results)))
+                response = str(f"{callback}({json.dumps(search_results)});")
             else:
                 response = json.dumps(search_results)
             return response
-        tool_ids = kwd.get('tool_ids', None)
+        tool_ids = kwd.get("tool_ids", None)
         if tool_ids is not None:
             tool_ids = util.listify(tool_ids)
             repository_found = []
             all_metadata = dict()
             for tool_id in tool_ids:
                 # A valid GUID looks like toolshed.g2.bx.psu.edu/repos/bgruening/deeptools/deeptools_computeMatrix/1.1.0
-                shed, _, owner, name, tool, version = tool_id.split('/')
-                clause_list = [and_(self.app.model.Repository.table.c.deprecated == false(),
-                                    self.app.model.Repository.table.c.deleted == false(),
-                                    self.app.model.Repository.table.c.name == name,
-                                    self.app.model.User.table.c.username == owner,
-                                    self.app.model.Repository.table.c.user_id == self.app.model.User.table.c.id)]
+                shed, _, owner, name, tool, version = tool_id.split("/")
+                clause_list = [
+                    and_(
+                        self.app.model.Repository.table.c.deprecated == false(),
+                        self.app.model.Repository.table.c.deleted == false(),
+                        self.app.model.Repository.table.c.name == name,
+                        self.app.model.User.table.c.username == owner,
+                        self.app.model.Repository.table.c.user_id == self.app.model.User.table.c.id,
+                    )
+                ]
                 repository = trans.sa_session.query(self.app.model.Repository).filter(*clause_list).first()
+                if not repository:
+                    log.warning(f"Repository {owner}/{name} does not exist, skipping")
+                    continue
                 for changeset, changehash in repository.installable_revisions(self.app):
-                    metadata = metadata_util.get_current_repository_metadata_for_changeset_revision(self.app, repository, changehash)
-                    tools = metadata.metadata['tools']
+                    metadata = metadata_util.get_current_repository_metadata_for_changeset_revision(
+                        self.app, repository, changehash
+                    )
+                    tools = metadata.metadata.get("tools")
+                    if not tools:
+                        log.warning(f"Repository {owner}/{name}/{changehash} does not contain valid tools, skipping")
+                        continue
                     for tool in tools:
-                        if tool['guid'] in tool_ids:
-                            repository_found.append('%d:%s' % (int(changeset), changehash))
-                    metadata = metadata_util.get_current_repository_metadata_for_changeset_revision(self.app, repository, changehash)
+                        if tool["guid"] in tool_ids:
+                            repository_found.append("%d:%s" % (int(changeset), changehash))
+                    metadata = metadata_util.get_current_repository_metadata_for_changeset_revision(
+                        self.app, repository, changehash
+                    )
                     if metadata is None:
                         continue
-                    metadata_dict = metadata.to_dict(value_mapper={'id': self.app.security.encode_id, 'repository_id': self.app.security.encode_id})
-                    metadata_dict['repository'] = repository.to_dict(value_mapper={'id': self.app.security.encode_id})
+                    metadata_dict = metadata.to_dict(
+                        value_mapper={"id": self.app.security.encode_id, "repository_id": self.app.security.encode_id}
+                    )
+                    metadata_dict["repository"] = repository.to_dict(value_mapper={"id": self.app.security.encode_id})
                     if metadata.has_repository_dependencies:
-                        metadata_dict['repository_dependencies'] = metadata_util.get_all_dependencies(self.app, metadata, processed_dependency_links=[])
+                        metadata_dict["repository_dependencies"] = metadata_util.get_all_dependencies(
+                            self.app, metadata, processed_dependency_links=[]
+                        )
                     else:
-                        metadata_dict['repository_dependencies'] = []
+                        metadata_dict["repository_dependencies"] = []
                     if metadata.includes_tool_dependencies:
-                        metadata_dict['tool_dependencies'] = repository.get_tool_dependencies(self.app, changehash)
+                        metadata_dict["tool_dependencies"] = repository.get_tool_dependencies(self.app, changehash)
                     else:
-                        metadata_dict['tool_dependencies'] = {}
+                        metadata_dict["tool_dependencies"] = {}
                     if metadata.includes_tools:
-                        metadata_dict['tools'] = metadata.metadata['tools']
-                    all_metadata['%s:%s' % (int(changeset), changehash)] = metadata_dict
-            if repository_found is not None:
-                all_metadata['current_changeset'] = repository_found[0]
+                        metadata_dict["tools"] = metadata.metadata["tools"]
+                    all_metadata[f"{int(changeset)}:{changehash}"] = metadata_dict
+            if repository_found:
+                all_metadata["current_changeset"] = repository_found[0]
                 # all_metadata[ 'found_changesets' ] = repository_found
                 return json.dumps(all_metadata)
-            return '{}'
+            return "{}"
 
-        clause_list = [and_(self.app.model.Repository.table.c.deprecated == false(),
-                            self.app.model.Repository.table.c.deleted == deleted)]
+        clause_list = [
+            and_(
+                self.app.model.Repository.table.c.deprecated == false(),
+                self.app.model.Repository.table.c.deleted == deleted,
+            )
+        ]
         if owner is not None:
-            clause_list.append(and_(self.app.model.User.table.c.username == owner,
-                                    self.app.model.Repository.table.c.user_id == self.app.model.User.table.c.id))
+            clause_list.append(
+                and_(
+                    self.app.model.User.table.c.username == owner,
+                    self.app.model.Repository.table.c.user_id == self.app.model.User.table.c.id,
+                )
+            )
         if name is not None:
             clause_list.append(self.app.model.Repository.table.c.name == name)
-        for repository in trans.sa_session.query(self.app.model.Repository) \
-                                          .filter(*clause_list) \
-                                          .order_by(self.app.model.Repository.table.c.name):
-            repository_dict = repository.to_dict(view='collection',
-                                                 value_mapper=self.__get_value_mapper(trans))
-            repository_dict['category_ids'] = \
-                [trans.security.encode_id(x.category.id) for x in repository.categories]
+        for repository in (
+            trans.sa_session.query(self.app.model.Repository)
+            .filter(*clause_list)
+            .order_by(self.app.model.Repository.table.c.name)
+        ):
+            repository_dict = repository.to_dict(view="collection", value_mapper=self.__get_value_mapper(trans))
+            repository_dict["category_ids"] = [trans.security.encode_id(x.category.id) for x in repository.categories]
             repository_dicts.append(repository_dict)
         return json.dumps(repository_dicts)
 
     def _search(self, trans, q, page=1, page_size=10):
         """
         Perform the search over TS repositories.
         Note that search works over the Whoosh index which you have
         to pre-create with scripts/tool_shed/build_ts_whoosh_index.sh manually.
         Also TS config option toolshed_search_on has to be True and
         whoosh_index_dir has to be specified.
         """
         conf = self.app.config
         if not conf.toolshed_search_on:
-            raise ConfigDoesNotAllowException('Searching the TS through the API is turned off for this instance.')
+            raise ConfigDoesNotAllowException("Searching the TS through the API is turned off for this instance.")
         if not conf.whoosh_index_dir:
-            raise ConfigDoesNotAllowException('There is no directory for the search index specified. Please contact the administrator.')
+            raise ConfigDoesNotAllowException(
+                "There is no directory for the search index specified. Please contact the administrator."
+            )
         search_term = q.strip()
         if len(search_term) < 1:
-            raise RequestParameterInvalidException('The search term has to be at least one character long.')
+            raise RequestParameterInvalidException("The search term has to be at least one character long.")
 
         repo_search = RepoSearch()
 
-        Boosts = namedtuple('Boosts', ['repo_name_boost',
-                                       'repo_description_boost',
-                                       'repo_long_description_boost',
-                                       'repo_homepage_url_boost',
-                                       'repo_remote_repository_url_boost',
-                                       'categories_boost',
-                                       'repo_owner_username_boost'])
-        boosts = Boosts(float(conf.get('repo_name_boost', 0.9)),
-                        float(conf.get('repo_description_boost', 0.6)),
-                        float(conf.get('repo_long_description_boost', 0.5)),
-                        float(conf.get('repo_homepage_url_boost', 0.3)),
-                        float(conf.get('repo_remote_repository_url_boost', 0.2)),
-                        float(conf.get('categories_boost', 0.5)),
-                        float(conf.get('repo_owner_username_boost', 0.3)))
-
-        results = repo_search.search(trans,
-                                     search_term,
-                                     page,
-                                     page_size,
-                                     boosts)
-        results['hostname'] = web.url_for('/', qualified=True)
+        Boosts = namedtuple(
+            "Boosts",
+            [
+                "repo_name_boost",
+                "repo_description_boost",
+                "repo_long_description_boost",
+                "repo_homepage_url_boost",
+                "repo_remote_repository_url_boost",
+                "categories_boost",
+                "repo_owner_username_boost",
+            ],
+        )
+        boosts = Boosts(
+            float(conf.get("repo_name_boost", 0.9)),
+            float(conf.get("repo_description_boost", 0.6)),
+            float(conf.get("repo_long_description_boost", 0.5)),
+            float(conf.get("repo_homepage_url_boost", 0.3)),
+            float(conf.get("repo_remote_repository_url_boost", 0.2)),
+            float(conf.get("categories_boost", 0.5)),
+            float(conf.get("repo_owner_username_boost", 0.3)),
+        )
+
+        results = repo_search.search(trans, search_term, page, page_size, boosts)
+        results["hostname"] = web.url_for("/", qualified=True)
         return results
 
     @web.legacy_expose_api
     def remove_repository_registry_entry(self, trans, payload, **kwd):
         """
         POST /api/repositories/remove_repository_registry_entry
         Removes appropriate entries from the repository registry for the repository defined by the received name and owner.
@@ -449,218 +504,193 @@
         The following parameters are included in the payload.
         :param tool_shed_url (required): the base URL of the Tool Shed containing the Repository
         :param name (required): the name of the Repository
         :param owner (required): the owner of the Repository
         """
         response_dict = {}
         if not trans.user_is_admin:
-            response_dict['status'] = 'error'
-            response_dict['message'] = "You are not authorized to remove entries from this Tool Shed's repository registry."
+            response_dict["status"] = "error"
+            response_dict[
+                "message"
+            ] = "You are not authorized to remove entries from this Tool Shed's repository registry."
             return response_dict
-        tool_shed_url = payload.get('tool_shed_url', '')
+        tool_shed_url = payload.get("tool_shed_url", "")
         if not tool_shed_url:
             raise HTTPBadRequest(detail="Missing required parameter 'tool_shed_url'.")
-        tool_shed_url = tool_shed_url.rstrip('/')
-        name = payload.get('name', '')
+        tool_shed_url = tool_shed_url.rstrip("/")
+        name = payload.get("name", "")
         if not name:
             raise HTTPBadRequest(detail="Missing required parameter 'name'.")
-        owner = payload.get('owner', '')
+        owner = payload.get("owner", "")
         if not owner:
             raise HTTPBadRequest(detail="Missing required parameter 'owner'.")
         repository = repository_util.get_repository_by_name_and_owner(self.app, name, owner)
         if repository is None:
-            error_message = 'Cannot locate repository with name %s and owner %s,' % (str(name), str(owner))
+            error_message = f"Cannot locate repository with name {name} and owner {owner},"
             log.debug(error_message)
-            response_dict['status'] = 'error'
-            response_dict['message'] = error_message
+            response_dict["status"] = "error"
+            response_dict["message"] = error_message
             return response_dict
         # Update the repository registry.
         self.app.repository_registry.remove_entry(repository)
-        response_dict['status'] = 'ok'
-        response_dict['message'] = 'Entries for repository %s owned by %s have been removed from the Tool Shed repository registry.' \
-            % (name, owner)
+        response_dict["status"] = "ok"
+        response_dict[
+            "message"
+        ] = f"Entries for repository {name} owned by {owner} have been removed from the Tool Shed repository registry."
         return response_dict
 
     @web.legacy_expose_api
-    def repository_ids_for_setting_metadata(self, trans, my_writable=False, **kwd):
-        """
-        GET /api/repository_ids_for_setting_metadata
-
-        Displays a collection (list) of repository ids ordered for setting metadata.
-
-        :param key: the API key of the Tool Shed user.
-        :param my_writable (optional): if the API key is associated with an admin user in the Tool Shed, setting this param value
-                                       to True will restrict resetting metadata to only repositories that are writable by the user
-                                       in addition to those repositories of type tool_dependency_definition.  This param is ignored
-                                       if the current user is not an admin user, in which case this same restriction is automatic.
-        """
-        if trans.user_is_admin:
-            my_writable = util.asbool(my_writable)
-        else:
-            my_writable = True
-        handled_repository_ids = []
-        repository_ids = []
-        rmm = repository_metadata_manager.RepositoryMetadataManager(self.app, trans.user)
-        query = rmm.get_query_for_setting_metadata_on_repositories(my_writable=my_writable, order=False)
-        # Make sure repositories of type tool_dependency_definition are first in the list.
-        for repository in query:
-            if repository.type == rt_util.TOOL_DEPENDENCY_DEFINITION and repository.id not in handled_repository_ids:
-                repository_ids.append(trans.security.encode_id(repository.id))
-        # Now add all remaining repositories to the list.
-        for repository in query:
-            if repository.type != rt_util.TOOL_DEPENDENCY_DEFINITION and repository.id not in handled_repository_ids:
-                repository_ids.append(trans.security.encode_id(repository.id))
-        return repository_ids
-
-    @web.legacy_expose_api
     def reset_metadata_on_repositories(self, trans, payload, **kwd):
         """
         PUT /api/repositories/reset_metadata_on_repositories
 
         Resets all metadata on all repositories in the Tool Shed in an "orderly fashion".  Since there are currently only two
         repository types (tool_dependecy_definition and unrestricted), the order in which metadata is reset is repositories of
         type tool_dependecy_definition first followed by repositories of type unrestricted, and only one pass is necessary.  If
         a new repository type is introduced, the process will undoubtedly need to be revisited.  To facilitate this order, an
         in-memory list of repository ids that have been processed is maintained.
 
         :param key: the API key of the Tool Shed user.
+        :param my_writable (optional):
+            if the API key is associated with an admin user in the Tool Shed, setting this param value
+            to True will restrict resetting metadata to only repositories that are writable by the user
+            in addition to those repositories of type tool_dependency_definition.  This param is ignored
+            if the current user is not an admin user, in which case this same restriction is automatic.
 
-        The following parameters can optionally be included in the payload.
-        :param my_writable (optional): if the API key is associated with an admin user in the Tool Shed, setting this param value
-                                       to True will restrict resetting metadata to only repositories that are writable by the user
-                                       in addition to those repositories of type tool_dependency_definition.  This param is ignored
-                                       if the current user is not an admin user, in which case this same restriction is automatic.
         :param encoded_ids_to_skip (optional): a list of encoded repository ids for repositories that should not be processed.
-        :param skip_file (optional): A local file name that contains the encoded repository ids associated with repositories to skip.
-                                     This param can be used as an alternative to the above encoded_ids_to_skip.
+        :param skip_file (optional):
+            A local file name that contains the encoded repository ids associated with repositories to skip.
+            This param can be used as an alternative to the above encoded_ids_to_skip.
+
         """
 
         def handle_repository(trans, rmm, repository, results):
-            log.debug("Resetting metadata on repository %s" % str(repository.name))
+            log.debug(f"Resetting metadata on repository {repository.name}")
             try:
                 rmm.set_repository(repository)
                 rmm.reset_all_metadata_on_repository_in_tool_shed()
                 rmm_invalid_file_tups = rmm.get_invalid_file_tups()
                 if rmm_invalid_file_tups:
-                    message = tool_util.generate_message_for_invalid_tools(self.app,
-                                                                           rmm_invalid_file_tups,
-                                                                           repository,
-                                                                           None,
-                                                                           as_html=False)
-                    results['unsuccessful_count'] += 1
+                    message = tool_util.generate_message_for_invalid_tools(
+                        self.app, rmm_invalid_file_tups, repository, None, as_html=False
+                    )
+                    results["unsuccessful_count"] += 1
                 else:
-                    message = "Successfully reset metadata on repository %s owned by %s" % \
-                        (str(repository.name), str(repository.user.username))
-                    results['successful_count'] += 1
+                    message = f"Successfully reset metadata on repository {repository.name} owned by {repository.user.username}"
+                    results["successful_count"] += 1
             except Exception as e:
-                message = "Error resetting metadata on repository %s owned by %s: %s" % \
-                    (str(repository.name), str(repository.user.username), util.unicodify(e))
-                results['unsuccessful_count'] += 1
-            status = '%s : %s' % (str(repository.name), message)
-            results['repository_status'].append(status)
+                message = (
+                    f"Error resetting metadata on repository {repository.name} owned by {repository.user.username}: {e}"
+                )
+                results["unsuccessful_count"] += 1
+            status = f"{repository.name} : {message}"
+            results["repository_status"].append(status)
             return results
-        rmm = repository_metadata_manager.RepositoryMetadataManager(app=self.app,
-                                                                    user=trans.user,
-                                                                    resetting_all_metadata_on_repository=True,
-                                                                    updating_installed_repository=False,
-                                                                    persist=False)
+
+        rmm = repository_metadata_manager.RepositoryMetadataManager(
+            app=self.app,
+            user=trans.user,
+            resetting_all_metadata_on_repository=True,
+            updating_installed_repository=False,
+            persist=False,
+        )
         start_time = strftime("%Y-%m-%d %H:%M:%S")
-        results = dict(start_time=start_time,
-                       repository_status=[],
-                       successful_count=0,
-                       unsuccessful_count=0)
+        results = dict(start_time=start_time, repository_status=[], successful_count=0, unsuccessful_count=0)
         handled_repository_ids = []
-        encoded_ids_to_skip = payload.get('encoded_ids_to_skip', [])
-        skip_file = payload.get('skip_file', None)
+        encoded_ids_to_skip = payload.get("encoded_ids_to_skip", [])
+        skip_file = payload.get("skip_file", None)
         if skip_file and os.path.exists(skip_file) and not encoded_ids_to_skip:
             # Load the list of encoded_ids_to_skip from the skip_file.
             # Contents of file must be 1 encoded repository id per line.
-            lines = open(skip_file, 'rb').readlines()
+            lines = open(skip_file, "rb").readlines()
             for line in lines:
-                if line.startswith('#'):
+                if line.startswith("#"):
                     # Skip comments.
                     continue
-                encoded_ids_to_skip.append(line.rstrip('\n'))
+                encoded_ids_to_skip.append(line.rstrip("\n"))
         if trans.user_is_admin:
-            my_writable = util.asbool(payload.get('my_writable', False))
+            my_writable = util.asbool(payload.get("my_writable", False))
         else:
             my_writable = True
         query = rmm.get_query_for_setting_metadata_on_repositories(my_writable=my_writable, order=False)
         # First reset metadata on all repositories of type repository_dependency_definition.
         for repository in query:
             encoded_id = trans.security.encode_id(repository.id)
             if encoded_id in encoded_ids_to_skip:
-                log.debug("Skipping repository with id %s because it is in encoded_ids_to_skip %s" %
-                          (str(repository.id), str(encoded_ids_to_skip)))
+                log.debug(
+                    "Skipping repository with id %s because it is in encoded_ids_to_skip %s"
+                    % (str(repository.id), str(encoded_ids_to_skip))
+                )
             elif repository.type == rt_util.TOOL_DEPENDENCY_DEFINITION and repository.id not in handled_repository_ids:
                 results = handle_repository(trans, rmm, repository, results)
         # Now reset metadata on all remaining repositories.
         for repository in query:
             encoded_id = trans.security.encode_id(repository.id)
             if encoded_id in encoded_ids_to_skip:
-                log.debug("Skipping repository with id %s because it is in encoded_ids_to_skip %s" %
-                          (str(repository.id), str(encoded_ids_to_skip)))
+                log.debug(
+                    "Skipping repository with id %s because it is in encoded_ids_to_skip %s"
+                    % (str(repository.id), str(encoded_ids_to_skip))
+                )
             elif repository.type != rt_util.TOOL_DEPENDENCY_DEFINITION and repository.id not in handled_repository_ids:
                 results = handle_repository(trans, rmm, repository, results)
         stop_time = strftime("%Y-%m-%d %H:%M:%S")
-        results['stop_time'] = stop_time
+        results["stop_time"] = stop_time
         return json.dumps(results, sort_keys=True, indent=4)
 
     @web.legacy_expose_api
     def reset_metadata_on_repository(self, trans, payload, **kwd):
         """
-        PUT /api/repositories/reset_metadata_on_repository
+        POST /api/repositories/reset_metadata_on_repository
 
         Resets all metadata on a specified repository in the Tool Shed.
 
         :param key: the API key of the Tool Shed user.
 
         The following parameters must be included in the payload.
         :param repository_id: the encoded id of the repository on which metadata is to be reset.
         """
 
         def handle_repository(trans, start_time, repository):
-            results = dict(start_time=start_time,
-                           repository_status=[])
+            results = dict(start_time=start_time, repository_status=[])
             try:
-                rmm = repository_metadata_manager.RepositoryMetadataManager(app=self.app,
-                                                                            user=trans.user,
-                                                                            repository=repository,
-                                                                            resetting_all_metadata_on_repository=True,
-                                                                            updating_installed_repository=False,
-                                                                            persist=False)
+                rmm = repository_metadata_manager.RepositoryMetadataManager(
+                    app=self.app,
+                    user=trans.user,
+                    repository=repository,
+                    resetting_all_metadata_on_repository=True,
+                    updating_installed_repository=False,
+                    persist=False,
+                )
                 rmm.reset_all_metadata_on_repository_in_tool_shed()
                 rmm_invalid_file_tups = rmm.get_invalid_file_tups()
                 if rmm_invalid_file_tups:
-                    message = tool_util.generate_message_for_invalid_tools(self.app,
-                                                                           rmm_invalid_file_tups,
-                                                                           repository,
-                                                                           None,
-                                                                           as_html=False)
-                    results['status'] = 'warning'
+                    message = tool_util.generate_message_for_invalid_tools(
+                        self.app, rmm_invalid_file_tups, repository, None, as_html=False
+                    )
+                    results["status"] = "warning"
                 else:
-                    message = "Successfully reset metadata on repository %s owned by %s" % \
-                        (str(repository.name), str(repository.user.username))
-                    results['status'] = 'ok'
+                    message = f"Successfully reset metadata on repository {repository.name} owned by {repository.user.username}"
+                    results["status"] = "ok"
             except Exception as e:
-                message = "Error resetting metadata on repository %s owned by %s: %s" % \
-                    (str(repository.name), str(repository.user.username), util.unicodify(e))
-                results['status'] = 'error'
-            status = '%s : %s' % (str(repository.name), message)
-            results['repository_status'].append(status)
+                message = (
+                    f"Error resetting metadata on repository {repository.name} owned by {repository.user.username}: {e}"
+                )
+                results["status"] = "error"
+            status = f"{repository.name} : {message}"
+            results["repository_status"].append(status)
             return results
 
-        repository_id = payload.get('repository_id', None)
+        repository_id = payload.get("repository_id", None)
         if repository_id is not None:
             repository = repository_util.get_repository_in_tool_shed(self.app, repository_id)
             start_time = strftime("%Y-%m-%d %H:%M:%S")
-            log.debug("%s...resetting metadata on repository %s" % (start_time, str(repository.name)))
+            log.debug(f"{start_time}...resetting metadata on repository {repository.name}")
             results = handle_repository(trans, start_time, repository)
             stop_time = strftime("%Y-%m-%d %H:%M:%S")
-            results['stop_time'] = stop_time
+            results["stop_time"] = stop_time
         return results
 
     @expose_api_anonymous_and_sessionless
     def show(self, trans, id, **kwd):
         """
         GET /api/repositories/{encoded_repository_id}
         Returns information about a repository in the Tool Shed.
@@ -671,27 +701,20 @@
         :type  id: encoded str
 
         :returns:   detailed repository information
         :rtype:     dict
 
         :raises:  ObjectNotFound, MalformedId
         """
-        try:
-            trans.security.decode_id(id)
-        except Exception:
-            raise MalformedId('The given id is invalid.')
-
         repository = repository_util.get_repository_in_tool_shed(self.app, id)
         if repository is None:
-            raise ObjectNotFound('Unable to locate repository for the given id.')
-        repository_dict = repository.to_dict(view='element',
-                                             value_mapper=self.__get_value_mapper(trans))
+            raise ObjectNotFound("Unable to locate repository for the given id.")
+        repository_dict = repository.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
         # TODO the following property would be better suited in the to_dict method
-        repository_dict['category_ids'] = \
-            [trans.security.encode_id(x.category.id) for x in repository.categories]
+        repository_dict["category_ids"] = [trans.security.encode_id(x.category.id) for x in repository.categories]
         return repository_dict
 
     @expose_api_raw_anonymous_and_sessionless
     def updates(self, trans, **kwd):
         """
         GET /api/repositories/updates
         Return a dictionary with boolean values for whether there are updates available
@@ -706,160 +729,178 @@
         :type  changeset_revision: str
         :param hexlify: flag whether to hexlify the response (for backward compatibility)
         :type  changeset: boolean
 
         :returns:   information about repository deprecations, updates, and upgrades
         :rtype:     dict
         """
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
-        changeset_revision = kwd.get('changeset_revision', None)
-        hexlify_this = util.asbool(kwd.get('hexlify', True))
-        repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner, eagerload_columns=['downloadable_revisions'])
+        name = kwd.get("name", None)
+        owner = kwd.get("owner", None)
+        changeset_revision = kwd.get("changeset_revision", None)
+        hexlify_this = util.asbool(kwd.get("hexlify", True))
+        repository = repository_util.get_repository_by_name_and_owner(
+            trans.app, name, owner, eagerload_columns=[model.Repository.downloadable_revisions]
+        )
         if repository and repository.downloadable_revisions:
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                              trans.security.encode_id(repository.id),
-                                                                                              changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                trans.app, trans.security.encode_id(repository.id), changeset_revision
+            )
             tool_shed_status_dict = {}
             # Handle repository deprecation.
-            tool_shed_status_dict['repository_deprecated'] = str(repository.deprecated)
+            tool_shed_status_dict["repository_deprecated"] = str(repository.deprecated)
             tip_revision = repository.downloadable_revisions[0]
             # Handle latest installable revision.
             if changeset_revision == tip_revision:
-                tool_shed_status_dict['latest_installable_revision'] = 'True'
+                tool_shed_status_dict["latest_installable_revision"] = "True"
             else:
-                next_installable_revision = metadata_util.get_next_downloadable_changeset_revision(trans.app, repository, changeset_revision)
+                next_installable_revision = metadata_util.get_next_downloadable_changeset_revision(
+                    trans.app, repository, changeset_revision
+                )
                 if repository_metadata is None:
                     if next_installable_revision and next_installable_revision != changeset_revision:
-                        tool_shed_status_dict['latest_installable_revision'] = 'True'
+                        tool_shed_status_dict["latest_installable_revision"] = "True"
                     else:
-                        tool_shed_status_dict['latest_installable_revision'] = 'False'
+                        tool_shed_status_dict["latest_installable_revision"] = "False"
                 else:
                     if next_installable_revision and next_installable_revision != changeset_revision:
-                        tool_shed_status_dict['latest_installable_revision'] = 'False'
+                        tool_shed_status_dict["latest_installable_revision"] = "False"
                     else:
-                        tool_shed_status_dict['latest_installable_revision'] = 'True'
+                        tool_shed_status_dict["latest_installable_revision"] = "True"
             # Handle revision updates.
             if changeset_revision == tip_revision:
-                tool_shed_status_dict['revision_update'] = 'False'
+                tool_shed_status_dict["revision_update"] = "False"
             else:
                 if repository_metadata is None:
-                    tool_shed_status_dict['revision_update'] = 'True'
+                    tool_shed_status_dict["revision_update"] = "True"
                 else:
-                    tool_shed_status_dict['revision_update'] = 'False'
+                    tool_shed_status_dict["revision_update"] = "False"
             # Handle revision upgrades.
-            metadata_revisions = [revision[1] for revision in metadata_util.get_metadata_revisions(trans.app, repository)]
+            metadata_revisions = [
+                revision[1] for revision in metadata_util.get_metadata_revisions(trans.app, repository)
+            ]
             num_metadata_revisions = len(metadata_revisions)
             for index, metadata_revision in enumerate(metadata_revisions):
                 if index == num_metadata_revisions:
-                    tool_shed_status_dict['revision_upgrade'] = 'False'
+                    tool_shed_status_dict["revision_upgrade"] = "False"
                     break
                 if metadata_revision == changeset_revision:
                     if num_metadata_revisions - index > 1:
-                        tool_shed_status_dict['revision_upgrade'] = 'True'
+                        tool_shed_status_dict["revision_upgrade"] = "True"
                     else:
-                        tool_shed_status_dict['revision_upgrade'] = 'False'
+                        tool_shed_status_dict["revision_upgrade"] = "False"
                     break
-            return encoding_util.tool_shed_encode(tool_shed_status_dict) if hexlify_this else json.dumps(tool_shed_status_dict)
+            return (
+                encoding_util.tool_shed_encode(tool_shed_status_dict)
+                if hexlify_this
+                else json.dumps(tool_shed_status_dict)
+            )
         return encoding_util.tool_shed_encode({}) if hexlify_this else json.dumps({})
 
     @expose_api_anonymous_and_sessionless
     def show_tools(self, trans, id, changeset, **kwd):
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app,
-                                                                                          id,
-                                                                                          changeset)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(self.app, id, changeset)
         if repository_metadata is not None:
             encoded_repository_metadata_id = trans.security.encode_id(repository_metadata.id)
-            repository_metadata_dict = repository_metadata.to_dict(view='collection',
-                                                                   value_mapper=self.__get_value_mapper(trans))
-            repository_metadata_dict['url'] = web.url_for(controller='repository_revisions',
-                                                          action='show',
-                                                          id=encoded_repository_metadata_id)
-            if 'tools' in repository_metadata.metadata:
-                repository_metadata_dict['valid_tools'] = repository_metadata.metadata['tools']
+            repository_metadata_dict = repository_metadata.to_dict(
+                view="collection", value_mapper=self.__get_value_mapper(trans)
+            )
+            repository_metadata_dict["url"] = web.url_for(
+                controller="repository_revisions", action="show", id=encoded_repository_metadata_id
+            )
+            if "tools" in repository_metadata.metadata:
+                repository_metadata_dict["valid_tools"] = repository_metadata.metadata["tools"]
             return repository_metadata_dict
         else:
-            log.debug("Unable to locate repository_metadata record for repository id %s and changeset_revision %s" %
-                      (str(id), str(changeset)))
+            log.debug(
+                "Unable to locate repository_metadata record for repository id %s and changeset_revision %s"
+                % (str(id), str(changeset))
+            )
             return {}
 
     @expose_api_anonymous_and_sessionless
     def metadata(self, trans, id, **kwd):
         """
         GET /api/repositories/{encoded_repository_id}/metadata
         Returns information about a repository in the Tool Shed.
 
         Example URL: http://localhost:9009/api/repositories/f9cad7b01a472135/metadata
 
         :param id: the encoded id of the Repository object
 
+        :param downloadable_only: Return only downloadable revisions (defaults to True).
+                                  Added for test cases - shouldn't be considered part of the stable API.
+
         :returns:   A dictionary containing the specified repository's metadata, by changeset,
                     recursively including dependencies and their metadata.
 
         :not found:  Empty dictionary.
         """
-        try:
-            trans.security.decode_id(id)
-        except Exception:
-            raise MalformedId('The given id is invalid.')
-        recursive = util.asbool(kwd.get('recursive', 'True'))
+        recursive = util.asbool(kwd.get("recursive", "True"))
+        downloadable_only = util.asbool(kwd.get("downloadable_only", "True"))
         all_metadata = {}
-        repository = repository_util.get_repository_in_tool_shed(self.app, id, eagerload_columns=['downloadable_revisions'])
-        for changeset, changehash in repository.installable_revisions(self.app):
-            metadata = metadata_util.get_current_repository_metadata_for_changeset_revision(self.app, repository, changehash)
+        repository = repository_util.get_repository_in_tool_shed(
+            self.app, id, eagerload_columns=[model.Repository.downloadable_revisions]
+        )
+        for changeset, changehash in metadata_util.get_metadata_revisions(
+            self.app, repository, sort_revisions=True, downloadable=downloadable_only
+        ):
+            metadata = metadata_util.get_current_repository_metadata_for_changeset_revision(
+                self.app, repository, changehash
+            )
             if metadata is None:
                 continue
-            metadata_dict = metadata.to_dict(value_mapper={'id': self.app.security.encode_id, 'repository_id': self.app.security.encode_id})
-            metadata_dict['repository'] = repository.to_dict(value_mapper={'id': self.app.security.encode_id})
+            metadata_dict = metadata.to_dict(
+                value_mapper={"id": self.app.security.encode_id, "repository_id": self.app.security.encode_id}
+            )
+            metadata_dict["repository"] = repository.to_dict(value_mapper={"id": self.app.security.encode_id})
             if metadata.has_repository_dependencies and recursive:
-                metadata_dict['repository_dependencies'] = metadata_util.get_all_dependencies(self.app, metadata, processed_dependency_links=[])
+                metadata_dict["repository_dependencies"] = metadata_util.get_all_dependencies(
+                    self.app, metadata, processed_dependency_links=[]
+                )
             else:
-                metadata_dict['repository_dependencies'] = []
-            if metadata.includes_tool_dependencies and recursive:
-                metadata_dict['tool_dependencies'] = repository.get_tool_dependencies(self.app, changehash)
-            else:
-                metadata_dict['tool_dependencies'] = {}
+                metadata_dict["repository_dependencies"] = []
             if metadata.includes_tools:
-                metadata_dict['tools'] = metadata.metadata['tools']
-            all_metadata['%s:%s' % (int(changeset), changehash)] = metadata_dict
+                metadata_dict["tools"] = metadata.metadata["tools"]
+            all_metadata[f"{int(changeset)}:{changehash}"] = metadata_dict
         return all_metadata
 
     @expose_api
     def update(self, trans, id, **kwd):
         """
         PATCH /api/repositories/{encoded_repository_id}
         Updates information about a repository in the Tool Shed.
 
         :param id: the encoded id of the Repository object
 
-        :param payload: dictionary structure containing::
+        :param payload: dictionary structure containing
+
             'name':                  repo's name (optional)
             'synopsis':              repo's synopsis (optional)
             'description':           repo's description (optional)
             'remote_repository_url': repo's remote repo (optional)
             'homepage_url':          repo's homepage url (optional)
-            'category_ids':          list of existing encoded TS category ids
-                                     the updated repo should be associated with (optional)
+            'category_ids':          list of existing encoded TS category ids the updated repo should be associated with (optional)
+
         :type payload: dict
 
         :returns:   detailed repository information
         :rtype:     dict
 
         :raises: RequestParameterInvalidException, InsufficientPermissionsException
         """
-        payload = kwd.get('payload', None)
+        payload = kwd.get("payload", None)
         if not payload:
             raise RequestParameterMissingException("You did not specify any payload.")
 
-        name = payload.get('name', None)
-        synopsis = payload.get('synopsis', None)
-        description = payload.get('description', None)
-        remote_repository_url = payload.get('remote_repository_url', None)
-        homepage_url = payload.get('homepage_url', None)
-        category_ids = payload.get('category_ids', None)
+        name = payload.get("name", None)
+        synopsis = payload.get("synopsis", None)
+        description = payload.get("description", None)
+        remote_repository_url = payload.get("remote_repository_url", None)
+        homepage_url = payload.get("homepage_url", None)
+        category_ids = payload.get("category_ids", None)
         if category_ids is not None:
             # We need to know if it was actually passed, and listify turns None into []
             category_ids = util.listify(category_ids)
 
         update_kwds = dict(
             name=name,
             description=synopsis,
@@ -872,80 +913,80 @@
         repo, message = repository_util.update_repository(app=self.app, trans=trans, id=id, **update_kwds)
         if repo is None:
             if "You are not the owner" in message:
                 raise InsufficientPermissionsException(message)
             else:
                 raise ActionInputError(message)
 
-        repository_dict = repo.to_dict(view='element', value_mapper=self.__get_value_mapper(trans))
-        repository_dict['category_ids'] = \
-            [trans.security.encode_id(x.category.id) for x in repo.categories]
+        repository_dict = repo.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
+        repository_dict["category_ids"] = [trans.security.encode_id(x.category.id) for x in repo.categories]
         return repository_dict
 
     @expose_api
     def create(self, trans, **kwd):
         """
-        create( self, trans, payload, **kwd )
-        * POST /api/repositories:
-            Creates a new repository.
-            Only ``name`` and ``synopsis`` parameters are required.
+        POST /api/repositories:
+
+        Creates a new repository.
+        Only ``name`` and ``synopsis`` parameters are required.
+
+        :param payload: dictionary structure containing
 
-        :param payload: dictionary structure containing::
             'name':                  new repo's name (required)
             'synopsis':              new repo's synopsis (required)
             'description':           new repo's description (optional)
             'remote_repository_url': new repo's remote repo (optional)
             'homepage_url':          new repo's homepage url (optional)
-            'category_ids[]':        list of existing encoded TS category ids
-                                     the new repo should be associated with (optional)
+            'category_ids[]':        list of existing encoded TS category ids the new repo should be associated with (optional)
             'type':                  new repo's type, defaults to ``unrestricted`` (optional)
 
         :type payload: dict
 
         :returns:   detailed repository information
         :rtype:     dict
 
         :raises: RequestParameterMissingException, RequestParameterInvalidException
         """
-        payload = kwd.get('payload', None)
+        payload = kwd.get("payload", None)
         if not payload:
             raise RequestParameterMissingException("You did not specify any payload.")
-        name = payload.get('name', None)
+        name = payload.get("name", None)
         if not name:
             raise RequestParameterMissingException("Missing required parameter 'name'.")
-        synopsis = payload.get('synopsis', None)
+        synopsis = payload.get("synopsis", None)
         if not synopsis:
             raise RequestParameterMissingException("Missing required parameter 'synopsis'.")
 
-        description = payload.get('description', '')
-        remote_repository_url = payload.get('remote_repository_url', '')
-        homepage_url = payload.get('homepage_url', '')
-        category_ids = util.listify(payload.get('category_ids[]', ''))
+        description = payload.get("description", "")
+        remote_repository_url = payload.get("remote_repository_url", "")
+        homepage_url = payload.get("homepage_url", "")
+        category_ids = util.listify(payload.get("category_ids[]", ""))
 
-        repo_type = payload.get('type', rt_util.UNRESTRICTED)
+        repo_type = payload.get("type", rt_util.UNRESTRICTED)
         if repo_type not in rt_util.types:
-            raise RequestParameterInvalidException('This repository type is not valid')
+            raise RequestParameterInvalidException("This repository type is not valid")
 
         invalid_message = repository_util.validate_repository_name(self.app, name, trans.user)
         if invalid_message:
             raise RequestParameterInvalidException(invalid_message)
 
-        repo, message = repository_util.create_repository(app=self.app,
-                                                          name=name,
-                                                          type=repo_type,
-                                                          description=synopsis,
-                                                          long_description=description,
-                                                          user_id=trans.user.id,
-                                                          category_ids=category_ids,
-                                                          remote_repository_url=remote_repository_url,
-                                                          homepage_url=homepage_url)
-
-        repository_dict = repo.to_dict(view='element', value_mapper=self.__get_value_mapper(trans))
-        repository_dict['category_ids'] = \
-            [trans.security.encode_id(x.category.id) for x in repo.categories]
+        repo, message = repository_util.create_repository(
+            app=self.app,
+            name=name,
+            type=repo_type,
+            description=synopsis,
+            long_description=description,
+            user_id=trans.user.id,
+            category_ids=category_ids,
+            remote_repository_url=remote_repository_url,
+            homepage_url=homepage_url,
+        )
+
+        repository_dict = repo.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
+        repository_dict["category_ids"] = [trans.security.encode_id(x.category.id) for x in repo.categories]
         return repository_dict
 
     @web.legacy_expose_api
     def create_changeset_revision(self, trans, id, payload, **kwd):
         """
         POST /api/repositories/{encoded_repository_id}/changeset_revision
 
@@ -962,96 +1003,98 @@
 
         # Example URL: http://localhost:9009/api/repositories/f9cad7b01a472135
         rdah = attribute_handlers.RepositoryDependencyAttributeHandler(self.app, unpopulate=False)
         tdah = attribute_handlers.ToolDependencyAttributeHandler(self.app, unpopulate=False)
 
         repository = repository_util.get_repository_in_tool_shed(self.app, id)
 
-        if not (trans.user_is_admin or
-                self.app.security_agent.user_can_administer_repository(trans.user, repository) or
-                self.app.security_agent.can_push(self.app, trans.user, repository)):
+        if not (
+            trans.user_is_admin
+            or self.app.security_agent.user_can_administer_repository(trans.user, repository)
+            or self.app.security_agent.can_push(self.app, trans.user, repository)
+        ):
             trans.response.status = 400
             return {
                 "err_msg": "You do not have permission to update this repository.",
             }
 
         repo_dir = repository.repo_path(self.app)
 
         upload_point = commit_util.get_upload_point(repository, **kwd)
         tip = repository.tip()
 
-        file_data = payload.get('file')
+        file_data = payload.get("file")
         # Code stolen from gx's upload_common.py
         if isinstance(file_data, cgi_FieldStorage):
             assert not isinstance(file_data.file, StringIO)
-            assert file_data.file.name != '<fdopen>'
-            local_filename = util.mkstemp_ln(file_data.file.name, 'upload_file_data_')
+            assert file_data.file.name != "<fdopen>"
+            local_filename = util.mkstemp_ln(file_data.file.name, "upload_file_data_")
             file_data.file.close()
-            file_data = dict(filename=file_data.filename,
-                             local_filename=local_filename)
-        elif type(file_data) == dict and 'local_filename' not in file_data:
-            raise Exception('Uploaded file was encoded in a way not understood.')
+            file_data = dict(filename=file_data.filename, local_filename=local_filename)
+        elif type(file_data) == dict and "local_filename" not in file_data:
+            raise Exception("Uploaded file was encoded in a way not understood.")
 
-        commit_message = kwd.get('commit_message', 'Uploaded')
+        commit_message = kwd.get("commit_message", "Uploaded")
 
-        uploaded_file = open(file_data['local_filename'], 'rb')
-        uploaded_file_name = file_data['local_filename']
+        uploaded_file = open(file_data["local_filename"], "rb")
+        uploaded_file_name = file_data["local_filename"]
 
         isgzip = False
         isbz2 = False
         isgzip = checkers.is_gzip(uploaded_file_name)
         if not isgzip:
             isbz2 = checkers.is_bz2(uploaded_file_name)
-        if (isgzip or isbz2):
+        if isgzip or isbz2:
             # Open for reading with transparent compression.
-            tar = tarfile.open(uploaded_file_name, 'r:*')
+            tar = tarfile.open(uploaded_file_name, "r:*")
         else:
             tar = tarfile.open(uploaded_file_name)
 
         new_repo_alert = False
         remove_repo_files_not_in_tar = True
 
-        ok, message, files_to_remove, content_alert_str, undesirable_dirs_removed, undesirable_files_removed = \
-            repository_content_util.upload_tar(
-                trans,
-                rdah,
-                tdah,
-                repository,
-                tar,
-                uploaded_file,
-                upload_point,
-                remove_repo_files_not_in_tar,
-                commit_message,
-                new_repo_alert
-            )
+        (
+            ok,
+            message,
+            files_to_remove,
+            content_alert_str,
+            undesirable_dirs_removed,
+            undesirable_files_removed,
+        ) = repository_content_util.upload_tar(
+            trans,
+            rdah,
+            tdah,
+            repository,
+            tar,
+            uploaded_file,
+            upload_point,
+            remove_repo_files_not_in_tar,
+            commit_message,
+            new_repo_alert,
+        )
         if ok:
             # Update the repository files for browsing.
             hg_util.update_repository(repo_dir)
             # Get the new repository tip.
             if tip == repository.tip():
                 trans.response.status = 400
-                message = 'No changes to repository.'
+                message = "No changes to repository."
                 ok = False
             else:
-                rmm = repository_metadata_manager.RepositoryMetadataManager(app=self.app,
-                                                                            user=trans.user,
-                                                                            repository=repository)
-                status, error_message = \
-                    rmm.set_repository_metadata_due_to_new_tip(trans.request.host,
-                                                               content_alert_str=content_alert_str,
-                                                               **kwd)
+                rmm = repository_metadata_manager.RepositoryMetadataManager(
+                    app=self.app, user=trans.user, repository=repository
+                )
+                status, error_message = rmm.set_repository_metadata_due_to_new_tip(
+                    trans.request.host, content_alert_str=content_alert_str, **kwd
+                )
                 if error_message:
                     ok = False
                     trans.response.status = 500
                     message = error_message
         else:
             trans.response.status = 500
         if os.path.exists(uploaded_file_name):
             os.remove(uploaded_file_name)
         if not ok:
-            return {
-                "err_msg": message
-            }
+            return {"err_msg": message}
         else:
-            return {
-                "message": message
-            }
+            return {"message": message}
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/api/users.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/api/users.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,65 +1,55 @@
 import logging
 
 import tool_shed.util.shed_util_common as suc
 from galaxy import (
+    exceptions,
     util,
-    web
+    web,
 )
 from galaxy.security.validate_user_input import (
     validate_email,
     validate_password,
-    validate_publicname
+    validate_publicname,
 )
 from galaxy.webapps.base.controller import BaseAPIController
 
 log = logging.getLogger(__name__)
 
 
 class UsersController(BaseAPIController):
     """RESTful controller for interactions with users in the Tool Shed."""
 
-    @web.legacy_expose_api
+    @web.expose_api
     @web.require_admin
     def create(self, trans, payload, **kwd):
         """
-        POST /api/users
-        Returns a dictionary of information about the created user.
+                POST /api/users
+                Returns a dictionary of information about the created user.
 
-:       param key: the current Galaxy admin user's API key
+        :       param key: the current Galaxy admin user's API key
 
-        The following parameters are included in the payload.
-        :param email (required): the email address of the user
-        :param password (required): the password of the user
-        :param username (required): the public username of the user
+                The following parameters are included in the payload.
+                :param email (required): the email address of the user
+                :param password (required): the password of the user
+                :param username (required): the public username of the user
         """
-        user_dict = dict(message='',
-                         status='ok')
         # Get the information about the user to be created from the payload.
-        email = payload.get('email', '')
-        password = payload.get('password', '')
-        username = payload.get('username', '')
-        message = self.__validate(trans,
-                                  email=email,
-                                  password=password,
-                                  confirm=password,
-                                  username=username)
+        email = payload.get("email", "")
+        password = payload.get("password", "")
+        username = payload.get("username", "")
+        message = self.__validate(trans, email=email, password=password, confirm=password, username=username)
         if message:
-            message = 'email: %s, username: %s - %s' % (email, username, message)
-            user_dict['message'] = message
-            user_dict['status'] = 'error'
-        else:
-            # Create the user.
-            user = self.__create_user(trans, email, username, password)
-            user_dict = user.to_dict(view='element',
-                                     value_mapper=self.__get_value_mapper(trans))
-            user_dict['message'] = "User '%s' has been created." % str(user.username)
-            user_dict['url'] = web.url_for(controller='users',
-                                           action='show',
-                                           id=trans.security.encode_id(user.id))
+            raise exceptions.RequestParameterInvalidException(message)
+
+        # Create the user.
+        user = self.__create_user(trans, email, username, password)
+        user_dict = user.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
+        user_dict["message"] = f"User '{str(user.username)}' has been created."
+        user_dict["url"] = web.url_for(controller="users", action="show", id=trans.security.encode_id(user.id))
         return user_dict
 
     def __create_user(self, trans, email, username, password):
         user = trans.app.model.User(email=email)
         user.set_password_cleartext(password)
         user.username = username
         if trans.app.config.user_activation_on:
@@ -68,60 +58,59 @@
             user.active = True  # Activation is off, every new user is active by default.
         trans.sa_session.add(user)
         trans.sa_session.flush()
         trans.app.security_agent.create_private_user_role(user)
         return user
 
     def __get_value_mapper(self, trans):
-        value_mapper = {'id' : trans.security.encode_id}
+        value_mapper = {"id": trans.security.encode_id}
         return value_mapper
 
-    @web.legacy_expose_api_anonymous
+    @web.expose_api_anonymous
     def index(self, trans, deleted=False, **kwd):
         """
         GET /api/users
         Returns a list of dictionaries that contain information about each user.
         """
         # Example URL: http://localhost:9009/api/users
         user_dicts = []
         deleted = util.asbool(deleted)
-        for user in trans.sa_session.query(trans.app.model.User) \
-                                    .filter(trans.app.model.User.table.c.deleted == deleted) \
-                                    .order_by(trans.app.model.User.table.c.username):
-            user_dict = user.to_dict(view='collection',
-                                     value_mapper=self.__get_value_mapper(trans))
-            user_dict['url'] = web.url_for(controller='users',
-                                           action='show',
-                                           id=trans.security.encode_id(user.id))
+        for user in (
+            trans.sa_session.query(trans.app.model.User)
+            .filter(trans.app.model.User.table.c.deleted == deleted)
+            .order_by(trans.app.model.User.table.c.username)
+        ):
+            user_dict = user.to_dict(view="collection", value_mapper=self.__get_value_mapper(trans))
+            user_dict["url"] = web.url_for(controller="users", action="show", id=trans.security.encode_id(user.id))
             user_dicts.append(user_dict)
         return user_dicts
 
-    @web.legacy_expose_api_anonymous
+    @web.expose_api_anonymous
     def show(self, trans, id, **kwd):
         """
         GET /api/users/{encoded_user_id}
         GET /api/users/current
         Returns a dictionary of information about a user.
 
         :param id: the encoded id of the User object.
         """
         user = None
         # user is requesting data about themselves
-        user = trans.user if id == 'current' else suc.get_user(trans.app, id)
+        user = trans.user if id == "current" else suc.get_user(trans.app, id)
         if user is None:
-            user_dict = dict(message='Unable to locate user record for id %s.' % (str(id)),
-                             status='error')
+            user_dict = dict(message=f"Unable to locate user record for id {str(id)}.", status="error")
             return user_dict
-        user_dict = user.to_dict(view='element',
-                                 value_mapper=self.__get_value_mapper(trans))
-        user_dict['url'] = web.url_for(controller='users',
-                                       action='show',
-                                       id=trans.security.encode_id(user.id))
+        user_dict = user.to_dict(view="element", value_mapper=self.__get_value_mapper(trans))
+        user_dict["url"] = web.url_for(controller="users", action="show", id=trans.security.encode_id(user.id))
         return user_dict
 
     def __validate(self, trans, email, password, confirm, username):
-        if username in ['repos']:
-            return "The term '%s' is a reserved word in the Tool Shed, so it cannot be used as a public user name." % username
-        message = "\n".join((validate_email(trans, email),
-                             validate_password(trans, password, confirm),
-                             validate_publicname(trans, username))).rstrip()
+        if username in ["repos"]:
+            return f"The term '{username}' is a reserved word in the Tool Shed, so it cannot be used as a public user name."
+        message = "\n".join(
+            (
+                validate_email(trans, email),
+                validate_password(trans, password, confirm),
+                validate_publicname(trans, username),
+            )
+        ).rstrip()
         return message
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/app.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/app.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,85 +1,109 @@
 import logging
 import sys
 import time
+from typing import Any
+
+from sqlalchemy.orm.scoping import scoped_session
 
 import galaxy.datatypes.registry
-import galaxy.quota
 import galaxy.tools.data
 import tool_shed.repository_registry
 import tool_shed.repository_types.registry
 import tool_shed.webapp.model
+from galaxy import auth
+from galaxy.app import (
+    HaltableContainer,
+    SentryClientMixin,
+)
 from galaxy.config import configure_logging
+from galaxy.managers.api_keys import ApiKeyManager
+from galaxy.managers.citations import CitationsManager
+from galaxy.managers.users import UserManager
+from galaxy.model.base import SharedModelMapping
 from galaxy.model.tags import CommunityTagHandler
+from galaxy.quota import (
+    NoQuotaAgent,
+    QuotaAgent,
+)
 from galaxy.security import idencoding
+from galaxy.structured_app import BasicSharedApp
 from galaxy.util.dbkeys import GenomeBuilds
 from galaxy.web_stack import application_stack_instance
 from tool_shed.grids.repository_grid_filter_manager import RepositoryGridFilterManager
+from tool_shed.structured_app import ToolShedApp
 from tool_shed.util.hgweb_config import hgweb_config_manager
 from . import config
 
 log = logging.getLogger(__name__)
 
 
-class UniverseApplication(object):
+class UniverseApplication(ToolShedApp, SentryClientMixin, HaltableContainer):
     """Encapsulates the state of a Universe application"""
 
-    def __init__(self, **kwd):
+    def __init__(self, **kwd) -> None:
+        super().__init__()
+        self[BasicSharedApp] = self
+        self[ToolShedApp] = self
         log.debug("python path is: %s", ", ".join(sys.path))
         self.name = "tool_shed"
         # will be overwritten when building WSGI app
         self.is_webapp = False
         # Read the tool_shed.ini configuration file and check for errors.
-        self.config = config.Configuration(**kwd)
+        self.config: Any = config.Configuration(**kwd)
         self.config.check()
         configure_logging(self.config)
         self.application_stack = application_stack_instance()
         # Initialize the  Galaxy datatypes registry.
         self.datatypes_registry = galaxy.datatypes.registry.Registry()
         self.datatypes_registry.load_datatypes(self.config.root, self.config.datatypes_config)
         # Initialize the Tool Shed repository_types registry.
         self.repository_types_registry = tool_shed.repository_types.registry.Registry()
         # Initialize the RepositoryGridFilterManager.
         self.repository_grid_filter_manager = RepositoryGridFilterManager()
         # Determine the Tool Shed database connection string.
         if self.config.database_connection:
             db_url = self.config.database_connection
         else:
-            db_url = "sqlite:///%s?isolation_level=IMMEDIATE" % self.config.database
+            db_url = f"sqlite:///{self.config.database}?isolation_level=IMMEDIATE"
         # Initialize the Tool Shed database and check for appropriate schema version.
         from tool_shed.webapp.model.migrate.check import create_or_verify_database
+
         create_or_verify_database(db_url, self.config.database_engine_options)
         # Set up the Tool Shed database engine and ORM.
         from tool_shed.webapp.model import mapping
-        self.model = mapping.init(self.config.file_path,
-                                  db_url,
-                                  self.config.database_engine_options)
+
+        model: mapping.ToolShedModelMapping = mapping.init(db_url, self.config.database_engine_options)
+        self.model = model
         self.security = idencoding.IdEncodingHelper(id_secret=self.config.id_secret)
+        self._register_singleton(idencoding.IdEncodingHelper, self.security)
+        self._register_singleton(SharedModelMapping, model)
+        self._register_singleton(mapping.ToolShedModelMapping, model)
+        self._register_singleton(scoped_session, self.model.context)
+        self.user_manager = self._register_singleton(UserManager, UserManager)
+        self.api_keys_manager = self._register_singleton(ApiKeyManager)
         # initialize the Tool Shed tag handler.
         self.tag_handler = CommunityTagHandler(self)
         # Initialize the Tool Shed tool data tables.  Never pass a configuration file here
         # because the Tool Shed should always have an empty dictionary!
         self.tool_data_tables = galaxy.tools.data.ToolDataTableManager(self.config.tool_data_path)
         self.genome_builds = GenomeBuilds(self)
-        from galaxy import auth
-        self.auth_manager = auth.AuthManager(self)
+        self.auth_manager = self._register_singleton(auth.AuthManager, auth.AuthManager(self.config))
         # Citation manager needed to load tools.
-        from galaxy.managers.citations import CitationsManager
-        self.citations_manager = CitationsManager(self)
+        self.citations_manager = self._register_singleton(CitationsManager, CitationsManager(self))
         self.use_tool_dependency_resolution = False
         # Initialize the Tool Shed security agent.
-        self.security_agent = self.model.security_agent
+        self.security_agent = model.security_agent
         # The Tool Shed makes no use of a quota, but this attribute is still required.
-        self.quota_agent = galaxy.quota.NoQuotaAgent(self.model)
+        self.quota_agent = self._register_singleton(QuotaAgent, NoQuotaAgent())
         # Initialize the baseline Tool Shed statistics component.
-        self.shed_counter = self.model.shed_counter
+        self.shed_counter = model.shed_counter
         # Let the Tool Shed's HgwebConfigManager know where the hgweb.config file is located.
         self.hgweb_config_manager = hgweb_config_manager
         self.hgweb_config_manager.hgweb_config_dir = self.config.hgweb_config_dir
         # Initialize the repository registry.
         self.repository_registry = tool_shed.repository_registry.Registry(self)
+        # Configure Sentry client if configured
+        self.configure_sentry_client()
         #  used for cachebusting -- refactor this into a *SINGLE* UniverseApplication base.
         self.server_starttime = int(time.time())
         log.debug("Tool shed hgweb.config file is: %s", self.hgweb_config_manager.hgweb_config)
-
-    def shutdown(self):
-        pass
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/config.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/config.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,145 +1,146 @@
 """
 Universe configuration builder.
 """
+import configparser
 import logging
 import logging.config
 import os
 from datetime import timedelta
 
-from six.moves import configparser
-
 from galaxy.config import (
     BaseAppConfiguration,
     CommonConfigurationMixin,
+    expand_pretty_datetime_format,
     get_database_engine_options,
+    TOOL_SHED_CONFIG_SCHEMA_PATH,
 )
 from galaxy.config.schema import AppSchema
 from galaxy.exceptions import ConfigurationError
 from galaxy.util import string_as_bool
-from galaxy.version import VERSION, VERSION_MAJOR
-from galaxy.web.formatting import expand_pretty_datetime_format
+from galaxy.version import (
+    VERSION,
+    VERSION_MAJOR,
+    VERSION_MINOR,
+)
 
 log = logging.getLogger(__name__)
 
-ts_webapp_path = os.path.abspath(os.path.dirname(__file__))
-templates_path = os.path.join(ts_webapp_path, 'templates')
-
-TOOLSHED_APP_NAME = 'tool_shed'
-TOOLSHED_CONFIG_SCHEMA_PATH = 'lib/tool_shed/webapp/config_schema.yml'
+TOOLSHED_APP_NAME = "tool_shed"
 
 
 class ToolShedAppConfiguration(BaseAppConfiguration, CommonConfigurationMixin):
-    default_config_file_name = 'tool_shed.yml'
+    default_config_file_name = "tool_shed.yml"
+
+    add_sample_file_to_defaults = {"datatypes_config_file"}
 
     def _load_schema(self):
-        return AppSchema(TOOLSHED_CONFIG_SCHEMA_PATH, TOOLSHED_APP_NAME)
+        return AppSchema(TOOL_SHED_CONFIG_SCHEMA_PATH, TOOLSHED_APP_NAME)
 
     def __init__(self, **kwargs):
-        super(ToolShedAppConfiguration, self).__init__(**kwargs)
+        super().__init__(**kwargs)
         self._process_config(kwargs)
 
     @property
     def shed_tool_data_path(self):
         return self.tool_data_path
 
-    def parse_config_file_options(self, kwargs):
-        defaults = dict(
-            auth_config_file=[self._in_config_dir('config/auth_conf.xml')],
-            datatypes_config_file=[self._in_config_dir('datatypes_conf.xml'), self._in_sample_dir('datatypes_conf.xml.sample')],
-            shed_tool_data_table_config=[self._in_managed_config_dir('shed_tool_data_table_conf.xml')],
-        )
-        self._parse_config_file_options(defaults, dict(), kwargs)
-        # Backwards compatibility for names used in too many places to fix
-        self.datatypes_config = self.datatypes_config_file
-
     def check(self):
         # Check that required directories exist; attempt to create otherwise
         paths_to_check = [
             self.file_path,
             self.hgweb_config_dir,
-            self.template_path,
             self.tool_data_path,
             self.template_cache_path,
-            os.path.join(self.tool_data_path, 'shared', 'jars'),
+            os.path.join(self.tool_data_path, "shared", "jars"),
         ]
         for path in paths_to_check:
             self._ensure_directory(path)
         # Check that required files exist.
         if not os.path.isfile(self.datatypes_config):
-            raise ConfigurationError('File not found: %s' % self.datatypes_config)
+            raise ConfigurationError(f"File not found: {self.datatypes_config}")
 
     def _process_config(self, kwargs):
-        # Resolve paths of other config files
-        self.parse_config_file_options(kwargs)
+        # Backwards compatibility for names used in too many places to fix
+        self.datatypes_config = self.datatypes_config_file
         # Collect the umask and primary gid from the environment
         self.umask = os.umask(0o77)  # get the current umask
         os.umask(self.umask)  # can't get w/o set, so set it back
         self.gid = os.getgid()  # if running under newgrp(1) we'll need to fix the group of data created on the cluster
         self.version_major = VERSION_MAJOR
+        self.version_minor = VERSION_MINOR
         self.version = VERSION
         # Database related configuration
         if not self.database_connection:  # Provide default if not supplied by user
-            self.database_connection = 'sqlite:///%s?isolation_level=IMMEDIATE' % self._in_data_dir('community.sqlite')
+            self.database_connection = f"sqlite:///{self._in_data_dir('community.sqlite')}?isolation_level=IMMEDIATE"
         self.database_engine_options = get_database_engine_options(kwargs)
-        self.database_create_tables = string_as_bool(kwargs.get('database_create_tables', 'True'))
+        self.database_create_tables = string_as_bool(kwargs.get("database_create_tables", "True"))
         # Where dataset files are stored
         self.file_path = self._in_root_dir(self.file_path)
         self.new_file_path = self._in_root_dir(self.new_file_path)
-        self.cookie_path = kwargs.get('cookie_path')
-        self.cookie_domain = kwargs.get('cookie_domain')
-        self.enable_quotas = string_as_bool(kwargs.get('enable_quotas', False))
+        self.cookie_path = kwargs.get("cookie_path")
+        self.cookie_domain = kwargs.get("cookie_domain")
+        self.enable_quotas = string_as_bool(kwargs.get("enable_quotas", False))
         # Tool stuff
-        self.tool_path = self._in_root_dir(kwargs.get('tool_path', 'tools'))
-        self.tool_secret = kwargs.get('tool_secret', '')
-        self.tool_data_path = os.path.join(os.getcwd(), kwargs.get('tool_data_path', 'shed-tool-data'))
+        self.tool_path = self._in_root_dir(kwargs.get("tool_path", "tools"))
+        self.tool_secret = kwargs.get("tool_secret", "")
+        self.tool_data_path = os.path.join(os.getcwd(), kwargs.get("tool_data_path", "shed-tool-data"))
         self.tool_data_table_config_path = None
-        self.integrated_tool_panel_config = self._in_root_dir(kwargs.get('integrated_tool_panel_config', 'integrated_tool_panel.xml'))
-        self.builds_file_path = self._in_root_dir(kwargs.get('builds_file_path', os.path.join(self.tool_data_path, 'shared', 'ucsc', 'builds.txt')))
-        self.len_file_path = self._in_root_dir(kwargs.get('len_file_path', os.path.join(self.tool_data_path, 'shared', 'ucsc', 'chrom')))
-        self.ftp_upload_dir = kwargs.get('ftp_upload_dir')
+        self.integrated_tool_panel_config = self._in_root_dir(
+            kwargs.get("integrated_tool_panel_config", "integrated_tool_panel.xml")
+        )
+        self.builds_file_path = self._in_root_dir(
+            kwargs.get("builds_file_path", os.path.join(self.tool_data_path, "shared", "ucsc", "builds.txt"))
+        )
+        self.len_file_path = self._in_root_dir(
+            kwargs.get("len_file_path", os.path.join(self.tool_data_path, "shared", "ucsc", "chrom"))
+        )
+        self.ftp_upload_dir = kwargs.get("ftp_upload_dir")
         self.update_integrated_tool_panel = False
         # Galaxy flavor Docker Image
         self.user_activation_on = None
-        self.registration_warning_message = kwargs.get('registration_warning_message')
-        self.blacklist_location = kwargs.get('blacklist_file')
-        self.blacklist_content = None
-        self.whitelist_location = kwargs.get('whitelist_file')
-        self.whitelist_content = None
-        self.template_path = templates_path
-        self.template_cache_path = self._in_root_dir(kwargs.get('template_cache_path', 'database/compiled_templates/community'))
-        self.error_email_to = kwargs.get('error_email_to')
+        self.registration_warning_message = kwargs.get("registration_warning_message")
+        self.email_domain_blocklist_content = None
+        self.email_domain_allowlist_content = None
+        self.template_cache_path = self._in_root_dir(
+            kwargs.get("template_cache_path", "database/compiled_templates/community")
+        )
+        self.error_email_to = kwargs.get("error_email_to")
         self.pretty_datetime_format = expand_pretty_datetime_format(self.pretty_datetime_format)
         # Configuration for the message box directly below the masthead.
-        self.wiki_url = kwargs.get('wiki_url', 'https://galaxyproject.org/')
-        self.blog_url = kwargs.get('blog_url')
-        self.screencasts_url = kwargs.get('screencasts_url')
+        self.wiki_url = kwargs.get("wiki_url", "https://galaxyproject.org/")
+        self.blog_url = kwargs.get("blog_url")
+        self.screencasts_url = kwargs.get("screencasts_url")
         self.log_events = False
         self.cloud_controller_instance = False
-        self.server_name = ''
+        self.server_name = ""
         # Where the tool shed hgweb.config file is stored - the default is the Galaxy installation directory.
         self.hgweb_config_dir = self._in_root_dir(self.hgweb_config_dir) or self.root
         # Proxy features
-        self.drmaa_external_runjob_script = kwargs.get('drmaa_external_runjob_script')
+        self.drmaa_external_runjob_script = kwargs.get("drmaa_external_runjob_script")
         # Parse global_conf and save the parser
-        global_conf = kwargs.get('global_conf')
+        global_conf = kwargs.get("global_conf")
         global_conf_parser = configparser.ConfigParser()
         self.global_conf_parser = global_conf_parser
-        if global_conf and '__file__' in global_conf and '.yml' not in global_conf['__file__']:
-            global_conf_parser.read(global_conf['__file__'])
-        self.running_functional_tests = string_as_bool(kwargs.get('running_functional_tests', False))
-        self.citation_cache_data_dir = self._in_root_dir(kwargs.get('citation_cache_data_dir', 'database/tool_shed_citations/data'))
-        self.citation_cache_lock_dir = self._in_root_dir(kwargs.get('citation_cache_lock_dir', 'database/tool_shed_citations/locks'))
+        if global_conf and "__file__" in global_conf and ".yml" not in global_conf["__file__"]:
+            global_conf_parser.read(global_conf["__file__"])
+        self.running_functional_tests = string_as_bool(kwargs.get("running_functional_tests", False))
+        self.citation_cache_data_dir = self._in_root_dir(
+            kwargs.get("citation_cache_data_dir", "database/tool_shed_citations/data")
+        )
+        self.citation_cache_lock_dir = self._in_root_dir(
+            kwargs.get("citation_cache_lock_dir", "database/tool_shed_citations/locks")
+        )
         self.password_expiration_period = timedelta(days=int(self.password_expiration_period))
 
         # Security/Policy Compliance
         self.redact_username_during_deletion = False
         self.redact_email_during_deletion = False
         self.redact_username_in_logs = False
-        self.enable_beta_gdpr = string_as_bool(kwargs.get('enable_beta_gdpr', False))
+        self.enable_beta_gdpr = string_as_bool(kwargs.get("enable_beta_gdpr", False))
         if self.enable_beta_gdpr:
             self.redact_username_during_deletion = True
             self.redact_email_during_deletion = True
             self.redact_username_in_logs = True
             self.allow_user_deletion = True
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/admin.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/admin.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,173 +1,181 @@
 import logging
 
 import tool_shed.grids.admin_grids as admin_grids
 from galaxy import (
     util,
-    web
+    web,
 )
 from galaxy.util import inflector
-from galaxy.web.framework.helpers import grids
+from galaxy.web.legacy_framework import grids
 from galaxy.webapps.base.controller import BaseUIController
 from tool_shed.metadata import repository_metadata_manager
 from tool_shed.util import (
     metadata_util,
     repository_util,
-    shed_util_common as suc
+    shed_util_common as suc,
 )
 from tool_shed.util.admin_util import Admin
 from tool_shed.util.web_util import escape
 
 log = logging.getLogger(__name__)
 
 
 class AdminController(BaseUIController, Admin):
-
     user_list_grid = admin_grids.UserGrid()
     role_list_grid = admin_grids.RoleGrid()
     group_list_grid = admin_grids.GroupGrid()
     manage_category_grid = admin_grids.ManageCategoryGrid()
     repository_grid = admin_grids.AdminRepositoryGrid()
     repository_metadata_grid = admin_grids.RepositoryMetadataGrid()
 
     delete_operation = grids.GridOperation("Delete", condition=(lambda item: not item.deleted), allow_multiple=True)
-    undelete_operation = grids.GridOperation("Undelete", condition=(lambda item: item.deleted and not item.purged), allow_multiple=True)
-    purge_operation = grids.GridOperation("Purge", condition=(lambda item: item.deleted and not item.purged), allow_multiple=True)
+    undelete_operation = grids.GridOperation(
+        "Undelete", condition=(lambda item: item.deleted and not item.purged), allow_multiple=True
+    )
+    purge_operation = grids.GridOperation(
+        "Purge", condition=(lambda item: item.deleted and not item.purged), allow_multiple=True
+    )
 
     @web.expose
     @web.require_admin
     def browse_repositories(self, trans, **kwd):
         # We add parameters to the keyword dict in this method in order to rename the param
         # with an "f-" prefix, simulating filtering by clicking a search link.  We have
         # to take this approach because the "-" character is illegal in HTTP requests.
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='browse_repositories',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="browse_repositories", **kwd)
+                )
             elif operation == "edit_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='edit_repository',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="edit_repository", **kwd)
+                )
             elif operation == "repositories_by_user":
                 # Eliminate the current filters if any exist.
-                for k, v in list(kwd.items()):
-                    if k.startswith('f-'):
+                for k in list(kwd.keys()):
+                    if k.startswith("f-"):
                         del kwd[k]
-                if 'user_id' in kwd:
-                    user = suc.get_user(trans.app, kwd['user_id'])
-                    kwd['f-email'] = user.email
-                    del kwd['user_id']
+                if "user_id" in kwd:
+                    user = suc.get_user(trans.app, kwd["user_id"])
+                    kwd["f-email"] = user.email
+                    del kwd["user_id"]
                 else:
                     # The received id is the repository id, so we need to get the id of the user
                     # that uploaded the repository.
-                    repository_id = kwd.get('id', None)
+                    repository_id = kwd.get("id", None)
                     repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
-                    kwd['f-email'] = repository.user.email
+                    kwd["f-email"] = repository.user.email
             elif operation == "repositories_by_category":
                 # Eliminate the current filters if any exist.
-                for k, v in list(kwd.items()):
-                    if k.startswith('f-'):
+                for k in list(kwd.keys()):
+                    if k.startswith("f-"):
                         del kwd[k]
-                category_id = kwd.get('id', None)
+                category_id = kwd.get("id", None)
                 category = suc.get_category(trans.app, category_id)
-                kwd['f-Category.name'] = category.name
+                kwd["f-Category.name"] = category.name
             elif operation == "receive email alerts":
-                if kwd['id']:
-                    kwd['caller'] = 'browse_repositories'
-                    return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                    action='set_email_alerts',
-                                                                    **kwd))
+                if kwd["id"]:
+                    kwd["caller"] = "browse_repositories"
+                    return trans.response.send_redirect(
+                        web.url_for(controller="repository", action="set_email_alerts", **kwd)
+                    )
                 else:
-                    del kwd['operation']
-            elif operation == 'delete':
+                    del kwd["operation"]
+            elif operation == "delete":
                 return self.delete_repository(trans, **kwd)
             elif operation == "undelete":
                 return self.undelete_repository(trans, **kwd)
         # The changeset_revision_select_field in the RepositoryGrid performs a refresh_on_change
         # which sends in request parameters like changeset_revison_1, changeset_revision_2, etc.  One
         # of the many select fields on the grid performed the refresh_on_change, so we loop through
         # all of the received values to see which value is not the repository tip.  If we find it, we
         # know the refresh_on_change occurred, and we have the necessary repository id and change set
         # revision to pass on.
         for k, v in kwd.items():
-            changeset_revision_str = 'changeset_revision_'
+            changeset_revision_str = "changeset_revision_"
             if k.startswith(changeset_revision_str):
                 repository_id = trans.security.encode_id(int(k.lstrip(changeset_revision_str)))
                 repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
                 if repository.tip() != v:
-                    return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                    action='browse_repositories',
-                                                                    operation='view_or_manage_repository',
-                                                                    id=trans.security.encode_id(repository.id),
-                                                                    changeset_revision=v))
+                    return trans.response.send_redirect(
+                        web.url_for(
+                            controller="repository",
+                            action="browse_repositories",
+                            operation="view_or_manage_repository",
+                            id=trans.security.encode_id(repository.id),
+                            changeset_revision=v,
+                        )
+                    )
         # Render the list view
         return self.repository_grid(trans, **kwd)
 
     @web.expose
     @web.require_admin
     def browse_repository_metadata(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "delete":
                 return self.delete_repository_metadata(trans, **kwd)
             if operation == "view_or_manage_repository_revision":
                 # The received id is a RepositoryMetadata object id, so we need to get the
                 # associated Repository and redirect to view_or_manage_repository with the
                 # changeset_revision.
-                repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, kwd['id'])
+                repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, kwd["id"])
                 repository = repository_metadata.repository
-                kwd['id'] = trans.security.encode_id(repository.id)
-                kwd['changeset_revision'] = repository_metadata.changeset_revision
-                kwd['operation'] = 'view_or_manage_repository'
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='browse_repositories',
-                                                                **kwd))
+                kwd["id"] = trans.security.encode_id(repository.id)
+                kwd["changeset_revision"] = repository_metadata.changeset_revision
+                kwd["operation"] = "view_or_manage_repository"
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="browse_repositories", **kwd)
+                )
         return self.repository_metadata_grid(trans, **kwd)
 
     @web.expose
     @web.require_admin
     def create_category(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        name = kwd.get('name', '').strip()
-        description = kwd.get('description', '').strip()
-        if kwd.get('create_category_button', False):
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        name = kwd.get("name", "").strip()
+        description = kwd.get("description", "").strip()
+        if kwd.get("create_category_button", False):
             if not name or not description:
-                message = 'Enter a valid name and a description'
-                status = 'error'
+                message = "Enter a valid name and a description"
+                status = "error"
             elif suc.get_category_by_name(trans.app, name):
-                message = 'A category with that name already exists'
-                status = 'error'
+                message = "A category with that name already exists"
+                status = "error"
             else:
                 # Create the category
                 category = trans.app.model.Category(name=name, description=description)
                 trans.sa_session.add(category)
                 trans.sa_session.flush()
                 # Update the Tool Shed's repository registry.
                 trans.app.repository_registry.add_category_entry(category)
-                message = "Category '%s' has been created" % escape(category.name)
-                status = 'done'
-                trans.response.send_redirect(web.url_for(controller='admin',
-                                                         action='manage_categories',
-                                                         message=message,
-                                                         status=status))
-        return trans.fill_template('/webapps/tool_shed/category/create_category.mako',
-                                   name=name,
-                                   description=description,
-                                   message=message,
-                                   status=status)
+                message = f"Category '{escape(category.name)}' has been created"
+                status = "done"
+                trans.response.send_redirect(
+                    web.url_for(controller="admin", action="manage_categories", message=message, status=status)
+                )
+        return trans.fill_template(
+            "/webapps/tool_shed/category/create_category.mako",
+            name=name,
+            description=description,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     @web.require_admin
     def delete_repository(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        id = kwd.get('id', None)
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        id = kwd.get("id", None)
         if id:
             # Deleting multiple items is currently not allowed (allow_multiple=False), so there will only be 1 id.
             ids = util.listify(id)
             count = 0
             deleted_repositories = ""
             for repository_id in ids:
                 repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
@@ -184,202 +192,196 @@
                             trans.sa_session.add(repository_admin_role)
                         repository.deleted = True
                         trans.sa_session.add(repository)
                         trans.sa_session.flush()
                         # Update the repository registry.
                         trans.app.repository_registry.remove_entry(repository)
                         count += 1
-                        deleted_repositories += " %s " % repository.name
+                        deleted_repositories += f" {repository.name} "
             if count:
-                message = "Deleted %d %s: %s" % (count, inflector.cond_plural(len(ids), "repository"), escape(deleted_repositories))
+                message = "Deleted %d %s: %s" % (
+                    count,
+                    inflector.cond_plural(len(ids), "repository"),
+                    escape(deleted_repositories),
+                )
             else:
                 message = "All selected repositories were already marked deleted."
         else:
             message = "No repository ids received for deleting."
-            status = 'error'
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='browse_repositories',
-                                                 message=util.sanitize_text(message),
-                                                 status=status))
+            status = "error"
+        trans.response.send_redirect(
+            web.url_for(
+                controller="admin", action="browse_repositories", message=util.sanitize_text(message), status=status
+            )
+        )
 
     @web.expose
     @web.require_admin
     def delete_repository_metadata(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        id = kwd.get('id', None)
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        id = kwd.get("id", None)
         if id:
             ids = util.listify(id)
             count = 0
             for repository_metadata_id in ids:
                 repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, repository_metadata_id)
                 trans.sa_session.delete(repository_metadata)
                 trans.sa_session.flush()
                 count += 1
             if count:
                 message = "Deleted %d repository metadata %s" % (count, inflector.cond_plural(len(ids), "record"))
         else:
             message = "No repository metadata ids received for deleting."
-            status = 'error'
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='browse_repository_metadata',
-                                                 message=util.sanitize_text(message),
-                                                 status=status))
+            status = "error"
+        trans.response.send_redirect(
+            web.url_for(
+                controller="admin",
+                action="browse_repository_metadata",
+                message=util.sanitize_text(message),
+                status=status,
+            )
+        )
 
     @web.expose
     @web.require_admin
     def edit_category(self, trans, **kwd):
-        '''Handle requests to edit TS category name or description'''
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        id = kwd.get('id', None)
+        """Handle requests to edit TS category name or description"""
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        id = kwd.get("id", None)
         if not id:
             message = "No category ids received for editing"
-            trans.response.send_redirect(web.url_for(controller='admin',
-                                                     action='manage_categories',
-                                                     message=message,
-                                                     status='error'))
+            trans.response.send_redirect(
+                web.url_for(controller="admin", action="manage_categories", message=message, status="error")
+            )
         category = suc.get_category(trans.app, id)
         original_category_name = str(category.name)
         original_category_description = str(category.description)
-        if kwd.get('edit_category_button', False):
+        if kwd.get("edit_category_button", False):
             flush_needed = False
-            new_name = kwd.get('name', '').strip()
-            new_description = kwd.get('description', '').strip()
+            new_name = kwd.get("name", "").strip()
+            new_description = kwd.get("description", "").strip()
             if original_category_name != new_name:
                 if not new_name:
-                    message = 'Enter a valid name'
-                    status = 'error'
+                    message = "Enter a valid name"
+                    status = "error"
                 elif original_category_name != new_name and suc.get_category_by_name(trans.app, new_name):
-                    message = 'A category with that name already exists'
-                    status = 'error'
+                    message = "A category with that name already exists"
+                    status = "error"
                 else:
                     category.name = new_name
                     flush_needed = True
             if original_category_description != new_description:
                 category.description = new_description
                 if not flush_needed:
                     flush_needed = True
             if flush_needed:
                 trans.sa_session.add(category)
                 trans.sa_session.flush()
                 if original_category_name != new_name:
                     # Update the Tool Shed's repository registry.
                     trans.app.repository_registry.edit_category_entry(original_category_name, new_name)
-                message = "The information has been saved for category '%s'" % escape(category.name)
-                status = 'done'
-                return trans.response.send_redirect(web.url_for(controller='admin',
-                                                                action='manage_categories',
-                                                                message=message,
-                                                                status=status))
-        return trans.fill_template('/webapps/tool_shed/category/edit_category.mako',
-                                   category=category,
-                                   message=message,
-                                   status=status)
+                message = f"The information has been saved for category '{escape(category.name)}'"
+                status = "done"
+                return trans.response.send_redirect(
+                    web.url_for(controller="admin", action="manage_categories", message=message, status=status)
+                )
+        return trans.fill_template(
+            "/webapps/tool_shed/category/edit_category.mako", category=category, message=message, status=status
+        )
 
     @web.expose
     @web.require_admin
     def manage_categories(self, trans, **kwd):
-        if 'f-free-text-search' in kwd:
+        if "f-free-text-search" in kwd:
             # Trick to enable searching repository name, description from the CategoryGrid.
             # What we've done is rendered the search box for the RepositoryGrid on the grid.mako
             # template for the CategoryGrid.  See ~/templates/webapps/tool_shed/category/grid.mako.
             # Since we are searching repositories and not categories, redirect to browse_repositories().
-            return trans.response.send_redirect(web.url_for(controller='admin',
-                                                            action='browse_repositories',
-                                                            **kwd))
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+            return trans.response.send_redirect(web.url_for(controller="admin", action="browse_repositories", **kwd))
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "create":
-                return trans.response.send_redirect(web.url_for(controller='admin',
-                                                                action='create_category',
-                                                                **kwd))
+                return trans.response.send_redirect(web.url_for(controller="admin", action="create_category", **kwd))
             elif operation == "delete":
-                return trans.response.send_redirect(web.url_for(controller='admin',
-                                                                action='mark_category_deleted',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="admin", action="mark_category_deleted", **kwd)
+                )
             elif operation == "undelete":
-                return trans.response.send_redirect(web.url_for(controller='admin',
-                                                                action='undelete_category',
-                                                                **kwd))
+                return trans.response.send_redirect(web.url_for(controller="admin", action="undelete_category", **kwd))
             elif operation == "purge":
-                return trans.response.send_redirect(web.url_for(controller='admin',
-                                                                action='purge_category',
-                                                                **kwd))
+                return trans.response.send_redirect(web.url_for(controller="admin", action="purge_category", **kwd))
             elif operation == "edit":
-                return trans.response.send_redirect(web.url_for(controller='admin',
-                                                                action='edit_category',
-                                                                **kwd))
+                return trans.response.send_redirect(web.url_for(controller="admin", action="edit_category", **kwd))
         return self.manage_category_grid(trans, **kwd)
 
     @web.expose
     @web.require_admin
     def regenerate_statistics(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        if 'regenerate_statistics_button' in kwd:
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        if "regenerate_statistics_button" in kwd:
             trans.app.shed_counter.generate_statistics()
             message = "Successfully regenerated statistics"
-        return trans.fill_template('/webapps/tool_shed/admin/statistics.mako',
-                                   message=message,
-                                   status=status)
+        return trans.fill_template("/webapps/tool_shed/admin/statistics.mako", message=message, status=status)
 
     @web.expose
     @web.require_admin
     def manage_role_associations(self, trans, **kwd):
         """Manage users, groups and repositories associated with a role."""
-        role_id = kwd.get('id', None)
+        role_id = kwd.get("id", None)
         role = repository_util.get_role_by_id(trans.app, role_id)
         # We currently only have a single role associated with a repository, the repository admin role.
         repository_role_association = role.repositories[0]
         repository = repository_role_association.repository
-        associations_dict = repository_util.handle_role_associations(trans.app,
-                                                                     role,
-                                                                     repository,
-                                                                     **kwd)
-        in_users = associations_dict.get('in_users', [])
-        out_users = associations_dict.get('out_users', [])
-        in_groups = associations_dict.get('in_groups', [])
-        out_groups = associations_dict.get('out_groups', [])
-        message = associations_dict.get('message', '')
-        status = associations_dict.get('status', 'done')
-        return trans.fill_template('/webapps/tool_shed/role/role.mako',
-                                   in_admin_controller=True,
-                                   repository=repository,
-                                   role=role,
-                                   in_users=in_users,
-                                   out_users=out_users,
-                                   in_groups=in_groups,
-                                   out_groups=out_groups,
-                                   message=message,
-                                   status=status)
+        associations_dict = repository_util.handle_role_associations(trans.app, role, repository, **kwd)
+        in_users = associations_dict.get("in_users", [])
+        out_users = associations_dict.get("out_users", [])
+        in_groups = associations_dict.get("in_groups", [])
+        out_groups = associations_dict.get("out_groups", [])
+        message = associations_dict.get("message", "")
+        status = associations_dict.get("status", "done")
+        return trans.fill_template(
+            "/webapps/tool_shed/role/role.mako",
+            in_admin_controller=True,
+            repository=repository,
+            role=role,
+            in_users=in_users,
+            out_users=out_users,
+            in_groups=in_groups,
+            out_groups=out_groups,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     @web.require_admin
     def reset_metadata_on_selected_repositories_in_tool_shed(self, trans, **kwd):
         rmm = repository_metadata_manager.RepositoryMetadataManager(trans.app, trans.user)
-        if 'reset_metadata_on_selected_repositories_button' in kwd:
+        if "reset_metadata_on_selected_repositories_button" in kwd:
             message, status = rmm.reset_metadata_on_selected_repositories(**kwd)
         else:
-            message = escape(util.restore_text(kwd.get('message', '')))
-            status = kwd.get('status', 'done')
-        repositories_select_field = rmm.build_repository_ids_select_field(name='repository_ids',
-                                                                          multiple=True,
-                                                                          display='checkboxes',
-                                                                          my_writable=False)
-        return trans.fill_template('/webapps/tool_shed/common/reset_metadata_on_selected_repositories.mako',
-                                   repositories_select_field=repositories_select_field,
-                                   message=message,
-                                   status=status)
+            message = escape(util.restore_text(kwd.get("message", "")))
+            status = kwd.get("status", "done")
+        repositories_select_field = rmm.build_repository_ids_select_field(
+            name="repository_ids", multiple=True, display="checkboxes", my_writable=False
+        )
+        return trans.fill_template(
+            "/webapps/tool_shed/common/reset_metadata_on_selected_repositories.mako",
+            repositories_select_field=repositories_select_field,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     @web.require_admin
     def undelete_repository(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        id = kwd.get('id', None)
+        message = escape(kwd.get("message", ""))
+        id = kwd.get("id", None)
         if id:
             # Undeleting multiple items is currently not allowed (allow_multiple=False), so there will only be 1 id.
             ids = util.listify(id)
             count = 0
             undeleted_repositories = ""
             for repository_id in ids:
                 repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
@@ -401,100 +403,108 @@
                         repository.deleted = False
                         trans.sa_session.add(repository)
                         trans.sa_session.flush()
                         if not repository.deprecated:
                             # Update the repository registry.
                             trans.app.repository_registry.add_entry(repository)
                         count += 1
-                        undeleted_repositories += " %s" % repository.name
+                        undeleted_repositories += f" {repository.name}"
             if count:
-                message = "Undeleted %d %s: %s" % (count, inflector.cond_plural(count, "repository"), undeleted_repositories)
+                message = "Undeleted %d %s: %s" % (
+                    count,
+                    inflector.cond_plural(count, "repository"),
+                    undeleted_repositories,
+                )
             else:
                 message = "No selected repositories were marked deleted, so they could not be undeleted."
         else:
             message = "No repository ids received for undeleting."
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='browse_repositories',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+        trans.response.send_redirect(
+            web.url_for(
+                controller="admin", action="browse_repositories", message=util.sanitize_text(message), status="done"
+            )
+        )
 
     @web.expose
     @web.require_admin
     def mark_category_deleted(self, trans, **kwd):
         # TODO: We should probably eliminate the Category.deleted column since it really makes no
         # sense to mark a category as deleted (category names and descriptions can be changed instead).
         # If we do this, and the following 2 methods can be eliminated.
-        message = escape(kwd.get('message', ''))
-        id = kwd.get('id', None)
+        message = escape(kwd.get("message", ""))
+        id = kwd.get("id", None)
         if id:
             ids = util.listify(id)
             message = "Deleted %d categories: " % len(ids)
             for category_id in ids:
                 category = suc.get_category(trans.app, category_id)
                 category.deleted = True
                 trans.sa_session.add(category)
                 trans.sa_session.flush()
                 # Update the Tool Shed's repository registry.
                 trans.app.repository_registry.remove_category_entry(category)
-                message += " %s " % escape(category.name)
+                message += f" {escape(category.name)} "
         else:
             message = "No category ids received for deleting."
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='manage_categories',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+        trans.response.send_redirect(
+            web.url_for(
+                controller="admin", action="manage_categories", message=util.sanitize_text(message), status="done"
+            )
+        )
 
     @web.expose
     @web.require_admin
     def purge_category(self, trans, **kwd):
         # This method should only be called for a Category that has previously been deleted.
         # Purging a deleted Category deletes all of the following from the database:
         # - RepoitoryCategoryAssociations where category_id == Category.id
-        message = escape(kwd.get('message', ''))
-        id = kwd.get('id', None)
+        message = escape(kwd.get("message", ""))
+        id = kwd.get("id", None)
         if id:
             ids = util.listify(id)
             count = 0
             purged_categories = ""
             message = "Purged %d categories: " % len(ids)
             for category_id in ids:
                 category = suc.get_category(trans.app, category_id)
                 if category.deleted:
                     # Delete RepositoryCategoryAssociations
                     for rca in category.repositories:
                         trans.sa_session.delete(rca)
                     trans.sa_session.flush()
-                    purged_categories += " %s " % category.name
+                    purged_categories += f" {category.name} "
             message = "Purged %d categories: %s" % (count, escape(purged_categories))
         else:
             message = "No category ids received for purging."
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='manage_categories',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+        trans.response.send_redirect(
+            web.url_for(
+                controller="admin", action="manage_categories", message=util.sanitize_text(message), status="done"
+            )
+        )
 
     @web.expose
     @web.require_admin
     def undelete_category(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        id = kwd.get('id', None)
+        message = escape(kwd.get("message", ""))
+        id = kwd.get("id", None)
         if id:
             ids = util.listify(id)
             count = 0
             undeleted_categories = ""
             for category_id in ids:
                 category = suc.get_category(trans.app, category_id)
                 if category.deleted:
                     category.deleted = False
                     trans.sa_session.add(category)
                     trans.sa_session.flush()
                     # Update the Tool Shed's repository registry.
                     trans.app.repository_registry.add_category_entry(category)
                     count += 1
-                    undeleted_categories += " %s" % category.name
+                    undeleted_categories += f" {category.name}"
             message = "Undeleted %d categories: %s" % (count, escape(undeleted_categories))
         else:
             message = "No category ids received for undeleting."
-        trans.response.send_redirect(web.url_for(controller='admin',
-                                                 action='manage_categories',
-                                                 message=util.sanitize_text(message),
-                                                 status='done'))
+        trans.response.send_redirect(
+            web.url_for(
+                controller="admin", action="manage_categories", message=util.sanitize_text(message), status="done"
+            )
+        )
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/hg.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/hg.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,49 @@
 import logging
 
 from mercurial.hgweb.hgwebdir_mod import hgwebdir
-from mercurial.hgweb.request import wsgiapplication
 
 from galaxy import web
+from galaxy.exceptions import ObjectNotFound
 from galaxy.webapps.base.controller import BaseUIController
 from tool_shed.util.repository_util import get_repository_by_name_and_owner
 
 log = logging.getLogger(__name__)
 
 
+class PortAsStringMiddleware:
+    def __init__(self, application):
+        self.application = application
+
+    def __call__(self, environ, start_response):
+        environ["SERVER_PORT"] = str(environ["SERVER_PORT"])
+        return self.application(environ, start_response)
+
+
 class HgController(BaseUIController):
     @web.expose
     def handle_request(self, trans, **kwd):
         # The os command that results in this method being called will look something like:
         # hg clone http://test@127.0.0.1:9009/repos/test/convert_characters1
         hgweb_config = trans.app.hgweb_config_manager.hgweb_config
-        cmd = kwd.get('cmd', None)
 
         def make_web_app():
-            hgwebapp = hgwebdir(hgweb_config.encode('utf-8'))
+            hgwebapp = hgwebdir(hgweb_config.encode("utf-8"))
             return hgwebapp
-        wsgi_app = wsgiapplication(make_web_app)
-        if cmd == 'getbundle':
-            path_info = kwd.get('path_info', None)
-            if path_info:
-                owner, name = path_info.split('/')
-                repository = get_repository_by_name_and_owner(trans.app, name, owner)
-                if repository:
-                    times_downloaded = repository.times_downloaded
-                    times_downloaded += 1
-                    repository.times_downloaded = times_downloaded
-                    trans.sa_session.add(repository)
-                    trans.sa_session.flush()
-        return wsgi_app
+
+        wsgi_app = make_web_app()
+        repository = None
+        path_info = kwd.get("path_info", None)
+        if path_info and len(path_info.split("/")) == 2:
+            owner, name = path_info.split("/")
+            repository = get_repository_by_name_and_owner(trans.app, name, owner)
+        if repository:
+            if repository.deprecated:
+                raise ObjectNotFound("Requested repository not found or deprecated.")
+            cmd = kwd.get("cmd", None)
+            if cmd == "getbundle":
+                times_downloaded = repository.times_downloaded
+                times_downloaded += 1
+                repository.times_downloaded = times_downloaded
+                trans.sa_session.add(repository)
+                trans.sa_session.flush()
+        return PortAsStringMiddleware(wsgi_app)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/repository.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/repository.py`

 * *Files 18% similar despite different names*

```diff
@@ -5,80 +5,90 @@
 import tempfile
 from datetime import date
 
 from mercurial import (
     cmdutil,
     commands,
     mdiff,
-    patch
+    patch,
+)
+from sqlalchemy import (
+    and_,
+    false,
+    null,
 )
-from sqlalchemy import and_, false, null
 
 import tool_shed.grids.repository_grids as repository_grids
 import tool_shed.grids.util as grids_util
 import tool_shed.repository_types.util as rt_util
 from galaxy import (
     util,
-    web
+    web,
 )
+from galaxy.tool_shed.util import dependency_display
 from galaxy.tools.repositories import ValidationContext
-from galaxy.web.form_builder import CheckboxField, SelectField
+from galaxy.web.form_builder import (
+    CheckboxField,
+    SelectField,
+)
+from galaxy.web.legacy_framework import grids
 from galaxy.webapps.base.controller import BaseUIController
-from galaxy.webapps.reports.framework import grids
 from tool_shed.dependencies.repository import relation_builder
-from tool_shed.galaxy_install import dependency_display
 from tool_shed.metadata import repository_metadata_manager
 from tool_shed.tools import (
     tool_validator,
-    tool_version_manager
+    tool_version_manager,
 )
 from tool_shed.util import (
     basic_util,
     common_util,
     encoding_util,
     hg_util,
     metadata_util,
     readme_util,
     repository_util,
     search_util,
     shed_util_common as suc,
-    tool_util
+    tool_util,
 )
 from tool_shed.util.web_util import escape
 from tool_shed.utility_containers import ToolShedUtilityContainerManager
+from tool_shed.webapp.framework.decorators import require_login
 from tool_shed.webapp.util import ratings_util
 
 log = logging.getLogger(__name__)
 
 malicious_error = "  This changeset cannot be downloaded because it potentially produces malicious behavior or contains inappropriate content."
 malicious_error_can_push = "  Correct this changeset as soon as possible, it potentially produces malicious behavior or contains inappropriate content."
 
 
 def get_mercurial_default_options_dict(command):
-    '''Borrowed from repoman - get default parameters for a mercurial command.'''
+    """Borrowed from repoman - get default parameters for a mercurial command."""
     possible = cmdutil.findpossible(command, commands.table)
     # Mercurial >= 3.4 returns a tuple whose first element is the old return dict
     if type(possible) is tuple:
         possible = possible[0]
     if len(possible) != 1:
-        raise Exception('unable to find mercurial command "%s"' % command)
-    return dict((r[1].replace(b'-', b'_'), r[2]) for r in next(iter(possible.values()))[1][1])
+        raise Exception(f'unable to find mercurial command "{command}"')
+    return {r[1].replace(b"-", b"_"): r[2] for r in next(iter(possible.values()))[1][1]}
 
 
 class RepositoryController(BaseUIController, ratings_util.ItemRatings):
-
     category_grid = repository_grids.CategoryGrid()
     datatypes_grid = repository_grids.DatatypesGrid()
     deprecated_repositories_i_own_grid = repository_grids.DeprecatedRepositoriesIOwnGrid()
     email_alerts_repository_grid = repository_grids.EmailAlertsRepositoryGrid()
     docker_image_grid = repository_grids.DockerImageGrid()
     install_matched_repository_grid = repository_grids.InstallMatchedRepositoryGrid()
     matched_repository_grid = repository_grids.MatchedRepositoryGrid()
     my_writable_repositories_grid = repository_grids.MyWritableRepositoriesGrid()
-    my_writable_repositories_missing_tool_test_components_grid = repository_grids.MyWritableRepositoriesMissingToolTestComponentsGrid()
+    my_writable_repositories_missing_tool_test_components_grid = (
+        repository_grids.MyWritableRepositoriesMissingToolTestComponentsGrid()
+    )
+    my_writable_repositories_with_invalid_tools_grid = repository_grids.MyWritableRepositoriesWithInvalidToolsGrid()
     repositories_by_user_grid = repository_grids.RepositoriesByUserGrid()
     repositories_i_own_grid = repository_grids.RepositoriesIOwnGrid()
     repositories_i_can_administer_grid = repository_grids.RepositoriesICanAdministerGrid()
     repositories_in_category_grid = repository_grids.RepositoriesInCategoryGrid()
     repositories_missing_tool_test_components_grid = repository_grids.RepositoriesMissingToolTestComponentsGrid()
     repositories_with_invalid_tools_grid = repository_grids.RepositoriesWithInvalidToolsGrid()
     repository_dependencies_grid = repository_grids.RepositoryDependenciesGrid()
@@ -87,836 +97,817 @@
     repository_metadata_grid = repository_grids.RepositoryMetadataGrid()
     tool_dependencies_grid = repository_grids.ToolDependenciesGrid()
     tools_grid = repository_grids.ToolsGrid()
     valid_category_grid = repository_grids.ValidCategoryGrid()
     valid_repository_grid = repository_grids.ValidRepositoryGrid()
 
     def _redirect_if_necessary(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **kwd)
+                )
             elif operation == "repositories_by_user":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='browse_repositories_by_user',
-                                                                **kwd))
-            elif operation in ['mark as deprecated', 'mark as not deprecated']:
-                kwd['mark_deprecated'] = operation == 'mark as deprecated'
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='deprecate',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="browse_repositories_by_user", **kwd)
+                )
+            elif operation in ["mark as deprecated", "mark as not deprecated"]:
+                kwd["mark_deprecated"] = operation == "mark as deprecated"
+                return trans.response.send_redirect(web.url_for(controller="repository", action="deprecate", **kwd))
 
     @web.expose
     def browse_categories(self, trans, **kwd):
         # The request came from the tool shed.
-        if 'f-free-text-search' in kwd:
+        if "f-free-text-search" in kwd:
             # Trick to enable searching repository name, description from the CategoryGrid.
             # What we've done is rendered the search box for the RepositoryGrid on the grid.mako
             # template for the CategoryGrid.  See ~/templates/webapps/tool_shed/category/grid.mako.
             # Since we are searching repositories and not categories, redirect to browse_repositories().
-            if 'id' in kwd and 'f-free-text-search' in kwd and kwd['id'] == kwd['f-free-text-search']:
+            if "id" in kwd and "f-free-text-search" in kwd and kwd["id"] == kwd["f-free-text-search"]:
                 # The value of 'id' has been set to the search string, which is a repository name.
                 # We'll try to get the desired encoded repository id to pass on.
                 try:
-                    repository_name = kwd['id']
+                    repository_name = kwd["id"]
                     repository = repository_util.get_repository_by_name(trans.app, repository_name)
-                    kwd['id'] = trans.security.encode_id(repository.id)
+                    kwd["id"] = trans.security.encode_id(repository.id)
                 except Exception:
                     pass
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            **kwd))
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+            return trans.response.send_redirect(
+                web.url_for(controller="repository", action="browse_repositories", **kwd)
+            )
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation in ["repositories_by_category", "repositories_by_user"]:
                 # Eliminate the current filters if any exist.
-                for k, v in list(kwd.items()):
-                    if k.startswith('f-'):
+                for k in list(kwd.keys()):
+                    if k.startswith("f-"):
                         del kwd[k]
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='browse_repositories',
-                                                                **kwd))
-        title = trans.app.repository_grid_filter_manager.get_grid_title(trans,
-                                                                        trailing_string='by Category',
-                                                                        default='Repositories')
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="browse_repositories", **kwd)
+                )
+        title = trans.app.repository_grid_filter_manager.get_grid_title(
+            trans, trailing_string="by Category", default="Repositories"
+        )
         self.category_grid.title = title
         return self.category_grid(trans, **kwd)
 
     @web.expose
     def browse_datatypes(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             # The received id is a RepositoryMetadata id.
-            repository_metadata_id = kwd['id']
+            repository_metadata_id = kwd["id"]
             repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, repository_metadata_id)
             repository_id = trans.security.encode_id(repository_metadata.repository_id)
             changeset_revision = repository_metadata.changeset_revision
-            new_kwd = dict(id=repository_id,
-                           changeset_revision=changeset_revision)
+            new_kwd = dict(id=repository_id, changeset_revision=changeset_revision)
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **new_kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **new_kwd)
+                )
         return self.datatypes_grid(trans, **kwd)
 
     @web.expose
     def browse_deprecated_repositories_i_own(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **kwd)
+                )
         selected_changeset_revision, repository = suc.get_repository_from_refresh_on_change(trans.app, **kwd)
         if repository:
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            operation='view_or_manage_repository',
-                                                            id=trans.security.encode_id(repository.id),
-                                                            changeset_revision=selected_changeset_revision))
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="browse_repositories",
+                    operation="view_or_manage_repository",
+                    id=trans.security.encode_id(repository.id),
+                    changeset_revision=selected_changeset_revision,
+                )
+            )
         return self.deprecated_repositories_i_own_grid(trans, **kwd)
 
     @web.expose
     def browse_my_writable_repositories(self, trans, **kwd):
         _redir = self._redirect_if_necessary(trans, **kwd)
         if _redir is not None:
             return _redir
 
         selected_changeset_revision, repository = suc.get_repository_from_refresh_on_change(trans.app, **kwd)
         if repository:
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            operation='view_or_manage_repository',
-                                                            id=trans.security.encode_id(repository.id),
-                                                            changeset_revision=selected_changeset_revision))
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="browse_repositories",
+                    operation="view_or_manage_repository",
+                    id=trans.security.encode_id(repository.id),
+                    changeset_revision=selected_changeset_revision,
+                )
+            )
         return self.my_writable_repositories_grid(trans, **kwd)
 
     @web.expose
     def browse_my_writable_repositories_missing_tool_test_components(self, trans, **kwd):
         _redir = self._redirect_if_necessary(trans, **kwd)
         if _redir is not None:
             return _redir
 
-        if 'message' not in kwd:
-            message = 'This list contains repositories that match the following criteria:<br>'
-            message += '<ul>'
-            message += '<li>you are authorized to update them</li>'
-            message += '<li>the latest installable revision contains at least 1 tool with no defined tests <b>OR</b>:</li>'
-            message += '<li>the latest installable revision contains at least 1 tool with a test that requires a missing test data file</li>'
-            message += '</ul>'
-            kwd['message'] = message
-            kwd['status'] = 'warning'
+        if "message" not in kwd:
+            message = "This list contains repositories that match the following criteria:<br>"
+            message += "<ul>"
+            message += "<li>you are authorized to update them</li>"
+            message += (
+                "<li>the latest installable revision contains at least 1 tool with no defined tests <b>OR</b>:</li>"
+            )
+            message += "<li>the latest installable revision contains at least 1 tool with a test that requires a missing test data file</li>"
+            message += "</ul>"
+            kwd["message"] = message
+            kwd["status"] = "warning"
         return self.my_writable_repositories_missing_tool_test_components_grid(trans, **kwd)
 
     @web.expose
     def browse_my_writable_repositories_with_invalid_tools(self, trans, **kwd):
         _redir = self._redirect_if_necessary(trans, **kwd)
         if _redir is not None:
             return _redir
 
-        if 'message' not in kwd:
-            message = 'This list contains repositories that match the following criteria:<br>'
-            message += '<ul>'
-            message += '<li>you are authorized to update them</li>'
-            message += '<li>the latest metadata revision contains at least 1 invalid tool</li>'
-            message += '</ul>'
-            message += 'Click the tool config file name to see why the tool is invalid.'
-            kwd['message'] = message
-            kwd['status'] = 'warning'
+        if "message" not in kwd:
+            message = "This list contains repositories that match the following criteria:<br>"
+            message += "<ul>"
+            message += "<li>you are authorized to update them</li>"
+            message += "<li>the latest metadata revision contains at least 1 invalid tool</li>"
+            message += "</ul>"
+            message += "Click the tool config file name to see why the tool is invalid."
+            kwd["message"] = message
+            kwd["status"] = "warning"
         return self.my_writable_repositories_with_invalid_tools_grid(trans, **kwd)
 
     @web.expose
     def browse_repositories(self, trans, **kwd):
         # We add params to the keyword dict in this method in order to rename the param with an "f-" prefix,
         # simulating filtering by clicking a search link.  We have to take this approach because the "-"
         # character is illegal in HTTP requests.
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **kwd)
+                )
             elif operation == "edit_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='edit_repository',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="edit_repository", **kwd)
+                )
             elif operation == "repositories_by_user":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='browse_repositories_by_user',
-                                                                **kwd))
-            elif operation == "reviewed_repositories_i_own":
-                return trans.response.send_redirect(web.url_for(controller='repository_review',
-                                                                action='reviewed_repositories_i_own'))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="browse_repositories_by_user", **kwd)
+                )
             elif operation == "repositories_by_category":
-                category_id = kwd.get('id', None)
-                message = escape(kwd.get('message', ''))
-                status = kwd.get('status', 'done')
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='browse_repositories_in_category',
-                                                                id=category_id,
-                                                                message=message,
-                                                                status=status))
+                category_id = kwd.get("id", None)
+                message = escape(kwd.get("message", ""))
+                status = kwd.get("status", "done")
+                return trans.response.send_redirect(
+                    web.url_for(
+                        controller="repository",
+                        action="browse_repositories_in_category",
+                        id=category_id,
+                        message=message,
+                        status=status,
+                    )
+                )
             elif operation == "receive email alerts":
                 if trans.user:
-                    if kwd['id']:
-                        kwd['caller'] = 'browse_repositories'
-                        return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                        action='set_email_alerts',
-                                                                        **kwd))
+                    if kwd["id"]:
+                        kwd["caller"] = "browse_repositories"
+                        return trans.response.send_redirect(
+                            web.url_for(controller="repository", action="set_email_alerts", **kwd)
+                        )
                 else:
-                    kwd['message'] = 'You must be logged in to set email alerts.'
-                    kwd['status'] = 'error'
-                    del kwd['operation']
+                    kwd["message"] = "You must be logged in to set email alerts."
+                    kwd["status"] = "error"
+                    del kwd["operation"]
         selected_changeset_revision, repository = suc.get_repository_from_refresh_on_change(trans.app, **kwd)
         if repository:
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            operation='view_or_manage_repository',
-                                                            id=trans.security.encode_id(repository.id),
-                                                            changeset_revision=selected_changeset_revision))
-        title = trans.app.repository_grid_filter_manager.get_grid_title(trans,
-                                                                        trailing_string='',
-                                                                        default='Repositories')
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="browse_repositories",
+                    operation="view_or_manage_repository",
+                    id=trans.security.encode_id(repository.id),
+                    changeset_revision=selected_changeset_revision,
+                )
+            )
+        title = trans.app.repository_grid_filter_manager.get_grid_title(
+            trans, trailing_string="", default="Repositories"
+        )
         self.repository_grid.title = title
         return self.repository_grid(trans, **kwd)
 
     @web.expose
     def browse_repositories_by_user(self, trans, **kwd):
         """Display the list of repositories owned by a specified user."""
         # Eliminate the current search filters if any exist.
-        for k, v in list(kwd.items()):
-            if k.startswith('f-'):
+        for k in list(kwd.keys()):
+            if k.startswith("f-"):
                 del kwd[k]
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **kwd))
-        user_id = kwd.get('user_id', None)
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **kwd)
+                )
+        user_id = kwd.get("user_id", None)
         if user_id is None:
             # The received id is the repository id, so we need to get the id of the user that owns the repository.
-            repository_id = kwd.get('id', None)
+            repository_id = kwd.get("id", None)
             if repository_id:
                 repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
                 user_id = trans.security.encode_id(repository.user.id)
-                kwd['user_id'] = user_id
+                kwd["user_id"] = user_id
             else:
                 # The user selected a repository revision which results in a refresh_on_change.
                 selected_changeset_revision, repository = suc.get_repository_from_refresh_on_change(trans.app, **kwd)
                 if repository:
-                    return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                    action='view_or_manage_repository',
-                                                                    id=trans.security.encode_id(repository.id),
-                                                                    changeset_revision=selected_changeset_revision))
+                    return trans.response.send_redirect(
+                        web.url_for(
+                            controller="repository",
+                            action="view_or_manage_repository",
+                            id=trans.security.encode_id(repository.id),
+                            changeset_revision=selected_changeset_revision,
+                        )
+                    )
         if user_id:
             user = suc.get_user(trans.app, user_id)
-            trailing_string = ''
-            default = 'Repositories Owned by %s' % str(user.username)
+            trailing_string = ""
+            default = f"Repositories Owned by {str(user.username)}"
         else:
-            trailing_string = ''
-            default = 'Repositories'
-        title = trans.app.repository_grid_filter_manager.get_grid_title(trans,
-                                                                        trailing_string=trailing_string,
-                                                                        default=default)
+            trailing_string = ""
+            default = "Repositories"
+        title = trans.app.repository_grid_filter_manager.get_grid_title(
+            trans, trailing_string=trailing_string, default=default
+        )
         self.repositories_by_user_grid.title = title
         return self.repositories_by_user_grid(trans, **kwd)
 
     @web.expose
     def browse_repositories_i_can_administer(self, trans, **kwd):
         _redir = self._redirect_if_necessary(trans, **kwd)
         if _redir is not None:
             return _redir
 
         selected_changeset_revision, repository = suc.get_repository_from_refresh_on_change(trans.app, **kwd)
         if repository:
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            operation='view_or_manage_repository',
-                                                            id=trans.security.encode_id(repository.id),
-                                                            changeset_revision=selected_changeset_revision))
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="browse_repositories",
+                    operation="view_or_manage_repository",
+                    id=trans.security.encode_id(repository.id),
+                    changeset_revision=selected_changeset_revision,
+                )
+            )
         return self.repositories_i_can_administer_grid(trans, **kwd)
 
     @web.expose
     def browse_repositories_i_own(self, trans, **kwd):
         _redir = self._redirect_if_necessary(trans, **kwd)
         if _redir is not None:
             return _redir
 
         selected_changeset_revision, repository = suc.get_repository_from_refresh_on_change(trans.app, **kwd)
         if repository:
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            operation='view_or_manage_repository',
-                                                            id=trans.security.encode_id(repository.id),
-                                                            changeset_revision=selected_changeset_revision))
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="browse_repositories",
+                    operation="view_or_manage_repository",
+                    id=trans.security.encode_id(repository.id),
+                    changeset_revision=selected_changeset_revision,
+                )
+            )
         return self.repositories_i_own_grid(trans, **kwd)
 
     @web.expose
     def browse_repositories_in_category(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **kwd))
-            if operation == 'repositories_by_user':
-                user_id = kwd.get('user_id', None)
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **kwd)
+                )
+            if operation == "repositories_by_user":
+                user_id = kwd.get("user_id", None)
                 if user_id is None:
                     # The received id is the repository id, so we need to get the id of the user that owns the repository.
-                    repository_id = kwd.get('id', None)
+                    repository_id = kwd.get("id", None)
                     if repository_id:
                         repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
                         user_id = trans.security.encode_id(repository.user.id)
                         user = suc.get_user(trans.app, user_id)
-                        self.repositories_by_user_grid.title = "Repositories owned by %s" % user.username
-                        kwd['user_id'] = user_id
+                        self.repositories_by_user_grid.title = f"Repositories owned by {user.username}"
+                        kwd["user_id"] = user_id
                         return self.repositories_by_user_grid(trans, **kwd)
         selected_changeset_revision, repository = suc.get_repository_from_refresh_on_change(trans.app, **kwd)
         if repository:
             # The user selected a repository revision which results in a refresh_on_change.
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='view_or_manage_repository',
-                                                            id=trans.security.encode_id(repository.id),
-                                                            changeset_revision=selected_changeset_revision))
-        category_id = kwd.get('id', None)
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="view_or_manage_repository",
+                    id=trans.security.encode_id(repository.id),
+                    changeset_revision=selected_changeset_revision,
+                )
+            )
+        category_id = kwd.get("id", None)
         if category_id:
             category = suc.get_category(trans.app, category_id)
             if category:
-                trailing_string = 'in Category %s' % str(category.name)
+                trailing_string = f"in Category {str(category.name)}"
             else:
-                trailing_string = 'in Category'
+                trailing_string = "in Category"
         else:
-            trailing_string = 'in Category'
-        title = trans.app.repository_grid_filter_manager.get_grid_title(trans,
-                                                                        trailing_string=trailing_string,
-                                                                        default='Repositories')
+            trailing_string = "in Category"
+        title = trans.app.repository_grid_filter_manager.get_grid_title(
+            trans, trailing_string=trailing_string, default="Repositories"
+        )
         self.repositories_in_category_grid.title = title
         return self.repositories_in_category_grid(trans, **kwd)
 
     @web.expose
     def browse_repositories_missing_tool_test_components(self, trans, **kwd):
         _redir = self._redirect_if_necessary(trans, **kwd)
         if _redir is not None:
             return _redir
 
-        if 'message' not in kwd:
-            message = 'This list contains repositories that match the following criteria:<br>'
-            message += '<ul>'
-            message += '<li>the latest installable revision contains at least 1 tool with no defined tests <b>OR</b>:</li>'
-            message += '<li>the latest installable revision contains at least 1 tool with a test that requires a missing test data file</li>'
-            message += '</ul>'
-            kwd['message'] = message
-            kwd['status'] = 'warning'
+        if "message" not in kwd:
+            message = "This list contains repositories that match the following criteria:<br>"
+            message += "<ul>"
+            message += (
+                "<li>the latest installable revision contains at least 1 tool with no defined tests <b>OR</b>:</li>"
+            )
+            message += "<li>the latest installable revision contains at least 1 tool with a test that requires a missing test data file</li>"
+            message += "</ul>"
+            kwd["message"] = message
+            kwd["status"] = "warning"
         return self.repositories_missing_tool_test_components_grid(trans, **kwd)
 
     @web.expose
     def browse_repositories_with_invalid_tools(self, trans, **kwd):
         _redir = self._redirect_if_necessary(trans, **kwd)
         if _redir is not None:
             return _redir
 
-        if 'message' not in kwd:
-            message = 'This list contains repositories that match the following criteria:<br>'
-            message += '<ul>'
-            message += '<li>the latest metadata revision contains at least 1 invalid tool</li>'
-            message += '</ul>'
-            message += 'Click the tool config file name to see why the tool is invalid.'
-            kwd['message'] = message
-            kwd['status'] = 'warning'
+        if "message" not in kwd:
+            message = "This list contains repositories that match the following criteria:<br>"
+            message += "<ul>"
+            message += "<li>the latest metadata revision contains at least 1 invalid tool</li>"
+            message += "</ul>"
+            message += "Click the tool config file name to see why the tool is invalid."
+            kwd["message"] = message
+            kwd["status"] = "warning"
         return self.repositories_with_invalid_tools_grid(trans, **kwd)
 
     @web.expose
     def browse_repository(self, trans, id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        commit_message = escape(kwd.get('commit_message', 'Deleted selected files'))
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        commit_message = escape(kwd.get("commit_message", "Deleted selected files"))
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
         repo_path = repository.repo_path(trans.app)
         # Update repository files for browsing.
         hg_util.update_repository(repo_path)
         changeset_revision = repository.tip()
-        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                             id,
-                                                                                             changeset_revision,
-                                                                                             metadata_only=True)
+        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+            trans.app, id, changeset_revision, metadata_only=True
+        )
         repository_type_select_field = rt_util.build_repository_type_select_field(trans, repository=repository)
-        return trans.fill_template('/webapps/tool_shed/repository/browse_repository.mako',
-                                   repository=repository,
-                                   changeset_revision=changeset_revision,
-                                   metadata=metadata,
-                                   commit_message=commit_message,
-                                   repository_type_select_field=repository_type_select_field,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/browse_repository.mako",
+            repository=repository,
+            changeset_revision=changeset_revision,
+            metadata=metadata,
+            commit_message=commit_message,
+            repository_type_select_field=repository_type_select_field,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def browse_repository_dependencies(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             # The received id is a RepositoryMetadata id.
-            repository_metadata_id = kwd['id']
+            repository_metadata_id = kwd["id"]
             repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, repository_metadata_id)
             repository_id = trans.security.encode_id(repository_metadata.repository_id)
             changeset_revision = repository_metadata.changeset_revision
-            new_kwd = dict(id=repository_id,
-                           changeset_revision=changeset_revision)
+            new_kwd = dict(id=repository_id, changeset_revision=changeset_revision)
             if operation == "browse_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='browse_repository',
-                                                                **new_kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="browse_repository", **new_kwd)
+                )
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **new_kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **new_kwd)
+                )
         return self.repository_dependencies_grid(trans, **kwd)
 
     @web.expose
     def browse_tools(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             # The received id is a RepositoryMetadata id.
-            repository_metadata_id = kwd['id']
+            repository_metadata_id = kwd["id"]
             repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, repository_metadata_id)
             repository_id = trans.security.encode_id(repository_metadata.repository_id)
             changeset_revision = repository_metadata.changeset_revision
-            new_kwd = dict(id=repository_id,
-                           changeset_revision=changeset_revision)
+            new_kwd = dict(id=repository_id, changeset_revision=changeset_revision)
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **new_kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **new_kwd)
+                )
         return self.tools_grid(trans, **kwd)
 
     @web.expose
     def browse_tool_dependencies(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             # The received id is a RepositoryMetadata id.
-            repository_metadata_id = kwd['id']
+            repository_metadata_id = kwd["id"]
             repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, repository_metadata_id)
             repository_id = trans.security.encode_id(repository_metadata.repository_id)
             changeset_revision = repository_metadata.changeset_revision
-            new_kwd = dict(id=repository_id,
-                           changeset_revision=changeset_revision)
+            new_kwd = dict(id=repository_id, changeset_revision=changeset_revision)
             if operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **new_kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **new_kwd)
+                )
         return self.tool_dependencies_grid(trans, **kwd)
 
     @web.expose
     def browse_valid_categories(self, trans, **kwd):
         """Filter repositories per category by those that are valid for installing into Galaxy."""
         # The request came from Galaxy, so restrict category links to display only valid repository changeset revisions.
         galaxy_url = common_util.handle_galaxy_url(trans, **kwd)
         if galaxy_url:
-            kwd['galaxy_url'] = galaxy_url
-        if 'f-free-text-search' in kwd:
-            if kwd['f-free-text-search'] == 'All':
+            kwd["galaxy_url"] = galaxy_url
+        if "f-free-text-search" in kwd:
+            if kwd["f-free-text-search"] == "All":
                 # The user performed a search, then clicked the "x" to eliminate the search criteria.
                 new_kwd = {}
                 return self.valid_category_grid(trans, **new_kwd)
             # Since we are searching valid repositories and not categories, redirect to browse_valid_repositories().
-            if 'id' in kwd and 'f-free-text-search' in kwd and kwd['id'] == kwd['f-free-text-search']:
+            if "id" in kwd and "f-free-text-search" in kwd and kwd["id"] == kwd["f-free-text-search"]:
                 # The value of 'id' has been set to the search string, which is a repository name.
                 # We'll try to get the desired encoded repository id to pass on.
                 try:
-                    name = kwd['id']
+                    name = kwd["id"]
                     repository = repository_util.get_repository_by_name(trans.app, name)
-                    kwd['id'] = trans.security.encode_id(repository.id)
+                    kwd["id"] = trans.security.encode_id(repository.id)
                 except Exception:
                     pass
             return self.browse_valid_repositories(trans, **kwd)
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation in ["valid_repositories_by_category", "valid_repositories_by_user"]:
                 # Eliminate the current filters if any exist.
-                for k, v in list(kwd.items()):
-                    if k.startswith('f-'):
+                for k in list(kwd.keys()):
+                    if k.startswith("f-"):
                         del kwd[k]
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='browse_valid_repositories',
-                                                                **kwd))
-        title = trans.app.repository_grid_filter_manager.get_grid_title(trans,
-                                                                        trailing_string='by Category',
-                                                                        default='Categories of Valid Repositories')
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="browse_valid_repositories", **kwd)
+                )
+        title = trans.app.repository_grid_filter_manager.get_grid_title(
+            trans, trailing_string="by Category", default="Categories of Valid Repositories"
+        )
         self.valid_category_grid.title = title
         return self.valid_category_grid(trans, **kwd)
 
     @web.expose
     def browse_valid_repositories(self, trans, **kwd):
         """Filter repositories to those that are installable into Galaxy."""
         galaxy_url = common_util.handle_galaxy_url(trans, **kwd)
         if galaxy_url:
-            kwd['galaxy_url'] = galaxy_url
-        repository_id = kwd.get('id', None)
-        if 'f-free-text-search' in kwd:
-            if 'f-Category.name' in kwd:
+            kwd["galaxy_url"] = galaxy_url
+        repository_id = kwd.get("id", None)
+        if "f-free-text-search" in kwd:
+            if "f-Category.name" in kwd:
                 # The user browsed to a category and then entered a search string, so get the category associated with its value.
-                category_name = kwd['f-Category.name']
+                category_name = kwd["f-Category.name"]
                 category = suc.get_category_by_name(trans.app, category_name)
                 # Set the id value in kwd since it is required by the ValidRepositoryGrid.build_initial_query method.
-                kwd['id'] = trans.security.encode_id(category.id)
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+                kwd["id"] = trans.security.encode_id(category.id)
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "preview_tools_in_changeset":
                 repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
-                repository_metadata = metadata_util.get_latest_repository_metadata(trans.app, repository.id, downloadable=True)
+                repository_metadata = metadata_util.get_latest_repository_metadata(
+                    trans.app, repository.id, downloadable=True
+                )
                 latest_installable_changeset_revision = repository_metadata.changeset_revision
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='preview_tools_in_changeset',
-                                                                repository_id=repository_id,
-                                                                changeset_revision=latest_installable_changeset_revision))
+                return trans.response.send_redirect(
+                    web.url_for(
+                        controller="repository",
+                        action="preview_tools_in_changeset",
+                        repository_id=repository_id,
+                        changeset_revision=latest_installable_changeset_revision,
+                    )
+                )
             elif operation == "valid_repositories_by_category":
                 # Eliminate the current filters if any exist.
-                for k, v in list(kwd.items()):
-                    if k.startswith('f-'):
+                for k in list(kwd.keys()):
+                    if k.startswith("f-"):
                         del kwd[k]
-                category_id = kwd.get('id', None)
+                category_id = kwd.get("id", None)
                 category = suc.get_category(trans.app, category_id)
-                kwd['f-Category.name'] = category.name
+                kwd["f-Category.name"] = category.name
         selected_changeset_revision, repository = suc.get_repository_from_refresh_on_change(trans.app, **kwd)
         if repository:
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='preview_tools_in_changeset',
-                                                            repository_id=trans.security.encode_id(repository.id),
-                                                            changeset_revision=selected_changeset_revision))
-        url_args = dict(action='browse_valid_repositories',
-                        operation='preview_tools_in_changeset',
-                        repository_id=repository_id)
-        self.valid_repository_grid.operations = [grids.GridOperation("Preview and install",
-                                                                     url_args=url_args,
-                                                                     allow_multiple=False,
-                                                                     async_compatible=False)]
-        title = trans.app.repository_grid_filter_manager.get_grid_title(trans,
-                                                                        trailing_string='',
-                                                                        default='Valid Repositories')
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="preview_tools_in_changeset",
+                    repository_id=trans.security.encode_id(repository.id),
+                    changeset_revision=selected_changeset_revision,
+                )
+            )
+        url_args = dict(
+            action="browse_valid_repositories", operation="preview_tools_in_changeset", repository_id=repository_id
+        )
+        self.valid_repository_grid.operations = [
+            grids.GridOperation("Preview and install", url_args=url_args, allow_multiple=False, async_compatible=False)
+        ]
+        title = trans.app.repository_grid_filter_manager.get_grid_title(
+            trans, trailing_string="", default="Valid Repositories"
+        )
         self.valid_repository_grid.title = title
         return self.valid_repository_grid(trans, **kwd)
 
     @web.expose
-    def check_for_updates(self, trans, **kwd):
-        """Handle a request from a local Galaxy instance."""
-        message = escape(kwd.get('message', ''))
-        # If the request originated with the UpdateRepositoryManager, it will not include a galaxy_url.
-        galaxy_url = common_util.handle_galaxy_url(trans, **kwd)
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
-        changeset_revision = kwd.get('changeset_revision', None)
-        repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
-        repo = repository.hg_repo
-        # Default to the current changeset revision.
-        update_to_ctx = hg_util.get_changectx_for_changeset(repo, changeset_revision)
-        latest_changeset_revision = changeset_revision
-        from_update_manager = kwd.get('from_update_manager', False)
-        if from_update_manager:
-            update = 'true'
-            no_update = 'false'
-        elif galaxy_url:
-            # Start building up the url to redirect back to the calling Galaxy instance.
-            params = dict(tool_shed_url=web.url_for('/', qualified=True),
-                          name=str(repository.name),
-                          owner=str(repository.user.username),
-                          changeset_revision=changeset_revision)
-            pathspec = ['admin_toolshed', 'update_to_changeset_revision']
-        else:
-            message = 'Unable to check for updates due to an invalid Galaxy URL: <b>%s</b>.  ' % galaxy_url
-            message += 'You may need to enable third-party cookies in your browser.  '
-            return trans.show_error_message(message)
-        if changeset_revision == repository.tip():
-            # If changeset_revision is the repository tip, there are no additional updates.
-            if from_update_manager:
-                return no_update
-            # Return the same value for changeset_revision and latest_changeset_revision.
-        else:
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                              trans.security.encode_id(repository.id),
-                                                                                              changeset_revision)
-            if repository_metadata:
-                # If changeset_revision is in the repository_metadata table for this repository, there are no
-                # additional updates.
-                if from_update_manager:
-                    return no_update
-                # Return the same value for changeset_revision and latest_changeset_revision.
-            else:
-                # The changeset_revision column in the repository_metadata table has been updated with a new
-                # changeset_revision value since the repository was installed.  We need to find the changeset_revision
-                # to which we need to update.
-                update_to_changeset_hash = None
-                for changeset in repo.changelog:
-                    changeset_hash = str(repo[changeset])
-                    hg_util.get_changectx_for_changeset(repo, changeset_hash)
-                    if update_to_changeset_hash:
-                        if changeset_hash == repository.tip():
-                            update_to_ctx = hg_util.get_changectx_for_changeset(repo, changeset_hash)
-                            latest_changeset_revision = changeset_hash
-                            break
-                        else:
-                            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                                              trans.security.encode_id(repository.id),
-                                                                                                              changeset_hash)
-                            if repository_metadata:
-                                # We found a RepositoryMetadata record.
-                                update_to_ctx = hg_util.get_changectx_for_changeset(repo, changeset_hash)
-                                latest_changeset_revision = changeset_hash
-                                break
-                            else:
-                                update_to_changeset_hash = changeset_hash
-                    else:
-                        if changeset_hash == changeset_revision:
-                            # We've found the changeset in the changelog for which we need to get the next update.
-                            update_to_changeset_hash = changeset_hash
-                if from_update_manager:
-                    if latest_changeset_revision == changeset_revision:
-                        return no_update
-                    return update
-        params['latest_changeset_revision'] = str(latest_changeset_revision)
-        params['latest_ctx_rev'] = str(update_to_ctx.rev())
-        url = util.build_url(galaxy_url, pathspec=pathspec, params=params)
-        return trans.response.send_redirect(url)
-
-    @web.expose
     def contact_owner(self, trans, id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
-        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                             id,
-                                                                                             repository.tip(),
-                                                                                             metadata_only=True)
+        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+            trans.app, id, repository.tip(), metadata_only=True
+        )
         if trans.user and trans.user.email:
-            return trans.fill_template("/webapps/tool_shed/repository/contact_owner.mako",
-                                       repository=repository,
-                                       metadata=metadata,
-                                       message=message,
-                                       status=status)
+            return trans.fill_template(
+                "/webapps/tool_shed/repository/contact_owner.mako",
+                repository=repository,
+                metadata=metadata,
+                message=message,
+                status=status,
+            )
         else:
             # Do all we can to eliminate spam.
             return trans.show_error_message("You must be logged in to contact the owner of a repository.")
 
     @web.expose
     def create_galaxy_docker_image(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        repository_ids = util.listify(kwd.get('id', ''))
-        if 'operation' in kwd:
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        repository_ids = util.listify(kwd.get("id", ""))
+        if "operation" in kwd:
             if repository_ids:
-                operation = kwd['operation'].lower()
+                operation = kwd["operation"].lower()
                 if operation == "include in docker image":
                     repository_tups = []
                     for repository_id in repository_ids:
                         repository = repository_util.get_repository_by_id(trans.app, repository_id)
-                        repository_tups.append((str(repository.name),
-                                                str(repository.user.username),
-                                                str(repository.type)))
-                    return trans.fill_template("/webapps/tool_shed/repository/docker_image_repositories.mako",
-                                               id=','.join(repository_ids),
-                                               repository_tups=repository_tups,
-                                               message=message,
-                                               status=status)
+                        repository_tups.append(
+                            (str(repository.name), str(repository.user.username), str(repository.type))
+                        )
+                    return trans.fill_template(
+                        "/webapps/tool_shed/repository/docker_image_repositories.mako",
+                        id=",".join(repository_ids),
+                        repository_tups=repository_tups,
+                        message=message,
+                        status=status,
+                    )
             else:
                 # This can only occur when there is a multi-select grid with check boxes and an operation,
                 # and the user clicked the operation button without checking any of the check boxes.
-                kwd['message'] = "No items were selected."
-                kwd['status'] = 'error'
-        elif kwd.get('create_docker_image_button', False):
+                kwd["message"] = "No items were selected."
+                kwd["status"] = "error"
+        elif kwd.get("create_docker_image_button", False):
             tmp_image_dir = tempfile.mkdtemp(prefix="tmp-toolshed-cdidir")
-            docker_file_name = 'Dockerfile'
+            docker_file_name = "Dockerfile"
             docker_file_path = os.path.join(tmp_image_dir, docker_file_name)
-            tool_shed_url = tool_shed_url = web.url_for('/', qualified=True)
-            repository_string = ''
+            tool_shed_url = tool_shed_url = web.url_for("/", qualified=True)
+            repository_string = ""
             for repository_id in repository_ids:
                 repository = repository_util.get_repository_by_id(trans.app, repository_id)
                 template = basic_util.SELECTED_REPOSITORIES_TEMPLATE
-                repository_template = \
-                    string.Template(template).safe_substitute(tool_shed_url=tool_shed_url,
-                                                              repository_owner=str(repository.user.username),
-                                                              repository_name=str(repository.name))
-                repository_string = '%s\n%s' % (repository_string, repository_template)
+                repository_template = string.Template(template).safe_substitute(
+                    tool_shed_url=tool_shed_url,
+                    repository_owner=str(repository.user.username),
+                    repository_name=str(repository.name),
+                )
+                repository_string = f"{repository_string}\n{repository_template}"
             template = basic_util.DOCKER_IMAGE_TEMPLATE
-            docker_image_template = \
-                string.Template(template).safe_substitute(selected_repositories=repository_string)
+            docker_image_template = string.Template(template).safe_substitute(selected_repositories=repository_string)
             docker_image_string = docker_image_template
-            trans.response.set_content_type('application/text/plain')
-            trans.response.headers["Content-Disposition"] = 'attachment; filename="%s"' % docker_file_name
+            trans.response.set_content_type("application/text/plain")
+            trans.response.headers["Content-Disposition"] = f'attachment; filename="{docker_file_name}"'
             opened_file = open(docker_file_path, "w")
             opened_file.write(docker_image_string)
             opened_file.close()
-            opened_file = open(docker_file_path, "r")
+            opened_file = open(docker_file_path)
             # Make sure the file is removed from disk after the contents have been downloaded.
             os.unlink(docker_file_path)
             docker_file_path, docker_file_name = os.path.split(docker_file_path)
             basic_util.remove_dir(docker_file_path)
             return opened_file
         return self.docker_image_grid(trans, **kwd)
 
     @web.expose
     def create_repository(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         categories = suc.get_categories(trans)
         if not categories:
-            message = 'No categories have been configured in this instance of the Galaxy Tool Shed.  '
-            message += 'An administrator needs to create some via the Administrator control panel before creating repositories.'
-            status = 'error'
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            message=message,
-                                                            status=status))
-        name = kwd.get('name', '').strip()
-        remote_repository_url = kwd.get('remote_repository_url', '')
-        homepage_url = kwd.get('homepage_url', '')
-        description = kwd.get('description', '')
-        long_description = kwd.get('long_description', '')
-        category_ids = util.listify(kwd.get('category_id', ''))
+            message = "No categories have been configured in this instance of the Galaxy Tool Shed.  "
+            message += "An administrator needs to create some via the Administrator control panel before creating repositories."
+            status = "error"
+            return trans.response.send_redirect(
+                web.url_for(controller="repository", action="browse_repositories", message=message, status=status)
+            )
+        name = kwd.get("name", "").strip()
+        remote_repository_url = kwd.get("remote_repository_url", "")
+        homepage_url = kwd.get("homepage_url", "")
+        description = kwd.get("description", "")
+        long_description = kwd.get("long_description", "")
+        category_ids = util.listify(kwd.get("category_id", ""))
         selected_categories = [trans.security.decode_id(id) for id in category_ids]
-        repository_type = kwd.get('repository_type', rt_util.UNRESTRICTED)
-        if kwd.get('create_repository_button', False):
+        repository_type = kwd.get("repository_type", rt_util.UNRESTRICTED)
+        if kwd.get("create_repository_button", False):
             error = False
             message = repository_util.validate_repository_name(trans.app, name, trans.user)
             if message:
                 error = True
             if not description:
-                message = 'Enter a description.'
+                message = "Enter a description."
                 error = True
             if error:
-                status = 'error'
+                status = "error"
             else:
-                repository, message = repository_util.create_repository(trans.app,
-                                                                        name,
-                                                                        repository_type,
-                                                                        description,
-                                                                        long_description,
-                                                                        user_id=trans.user.id,
-                                                                        category_ids=category_ids,
-                                                                        remote_repository_url=remote_repository_url,
-                                                                        homepage_url=homepage_url)
-                trans.response.send_redirect(web.url_for(controller='repository',
-                                                         action='manage_repository',
-                                                         message=message,
-                                                         id=trans.security.encode_id(repository.id)))
+                repository, message = repository_util.create_repository(
+                    trans.app,
+                    name,
+                    repository_type,
+                    description,
+                    long_description,
+                    user_id=trans.user.id,
+                    category_ids=category_ids,
+                    remote_repository_url=remote_repository_url,
+                    homepage_url=homepage_url,
+                )
+                trans.response.send_redirect(
+                    web.url_for(
+                        controller="repository",
+                        action="manage_repository",
+                        message=message,
+                        id=trans.security.encode_id(repository.id),
+                    )
+                )
         repository_type_select_field = rt_util.build_repository_type_select_field(trans)
-        return trans.fill_template('/webapps/tool_shed/repository/create_repository.mako',
-                                   name=name,
-                                   remote_repository_url=remote_repository_url,
-                                   homepage_url=homepage_url,
-                                   description=description,
-                                   long_description=long_description,
-                                   selected_categories=selected_categories,
-                                   categories=categories,
-                                   repository_type_select_field=repository_type_select_field,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/create_repository.mako",
+            name=name,
+            remote_repository_url=remote_repository_url,
+            homepage_url=homepage_url,
+            description=description,
+            long_description=long_description,
+            selected_categories=selected_categories,
+            categories=categories,
+            repository_type_select_field=repository_type_select_field,
+            message=message,
+            status=status,
+        )
 
     @web.expose
-    @web.require_login("deprecate repository")
+    @require_login("deprecate repository")
     def deprecate(self, trans, **kwd):
         """Mark a repository in the tool shed as deprecated or not deprecated."""
         # Marking a repository in the tool shed as deprecated has no effect on any downloadable changeset
         # revisions that may be associated with the repository.  Revisions are not marked as not downlaodable
         # because those that have installed the repository must be allowed to get updates.
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        repository_id = kwd.get('id', None)
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        repository_id = kwd.get("id", None)
         repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
-        mark_deprecated = util.string_as_bool(kwd.get('mark_deprecated', False))
+        mark_deprecated = util.string_as_bool(kwd.get("mark_deprecated", False))
         repository.deprecated = mark_deprecated
         trans.sa_session.add(repository)
         trans.sa_session.flush()
         if mark_deprecated:
             # Update the repository registry.
             trans.app.repository_registry.remove_entry(repository)
-            message = 'The repository <b>%s</b> has been marked as deprecated.' % escape(repository.name)
+            message = f"The repository <b>{escape(repository.name)}</b> has been marked as deprecated."
         else:
             # Update the repository registry.
             trans.app.repository_registry.add_entry(repository)
-            message = 'The repository <b>%s</b> has been marked as not deprecated.' % escape(repository.name)
-        trans.response.send_redirect(web.url_for(controller='repository',
-                                                 action='browse_repositories',
-                                                 operation='repositories_i_own',
-                                                 message=message,
-                                                 status=status))
+            message = f"The repository <b>{escape(repository.name)}</b> has been marked as not deprecated."
+        trans.response.send_redirect(
+            web.url_for(
+                controller="repository",
+                action="browse_repositories",
+                operation="repositories_i_own",
+                message=message,
+                status=status,
+            )
+        )
 
     @web.expose
     def display_image_in_repository(self, trans, **kwd):
         """
         Open an image file that is contained in repository or that is referenced by a URL for display.  The image can be defined in
         either a README.rst file contained in the repository or the help section of a Galaxy tool config that is contained in the repository.
         The following image definitions are all supported.  The former $PATH_TO_IMAGES is no longer required, and is now ignored.
         .. image:: https://raw.github.com/galaxy/some_image.png
         .. image:: $PATH_TO_IMAGES/some_image.png
         .. image:: /static/images/some_image.gif
         .. image:: some_image.jpg
         .. image:: /deep/some_image.png
         """
-        repository_id = kwd.get('repository_id', None)
-        relative_path_to_image_file = kwd.get('image_file', None)
+        repository_id = kwd.get("repository_id", None)
+        relative_path_to_image_file = kwd.get("image_file", None)
         if repository_id and relative_path_to_image_file:
             repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
             if repository:
                 repo_files_dir = repository.repo_path(trans.app)
-                path_to_file = repository_util.get_absolute_path_to_file_in_repository(repo_files_dir, relative_path_to_image_file)
+                path_to_file = repository_util.get_absolute_path_to_file_in_repository(
+                    repo_files_dir, relative_path_to_image_file
+                )
                 if os.path.exists(path_to_file):
                     file_name = os.path.basename(relative_path_to_image_file)
                     try:
-                        extension = file_name.split('.')[-1]
+                        extension = file_name.split(".")[-1]
                     except Exception:
                         extension = None
                     if extension:
                         mimetype = trans.app.datatypes_registry.get_mimetype_by_extension(extension)
                         if mimetype:
                             trans.response.set_content_type(mimetype)
-                    return open(path_to_file, 'rb')
+                    return open(path_to_file, "rb")
         return None
 
     @web.expose
     def display_tool(self, trans, repository_id, tool_config, changeset_revision, **kwd):
-        status = kwd.get('status', 'done')
-        render_repository_actions_for = kwd.get('render_repository_actions_for', 'tool_shed')
+        status = kwd.get("status", "done")
+        render_repository_actions_for = kwd.get("render_repository_actions_for", "tool_shed")
         with ValidationContext.from_app(trans.app) as validation_context:
             tv = tool_validator.ToolValidator(validation_context)
-            repository, tool, valid, message = tv.load_tool_from_changeset_revision(repository_id,
-                                                                                    changeset_revision,
-                                                                                    tool_config)
+            repository, tool, valid, message = tv.load_tool_from_changeset_revision(
+                repository_id, changeset_revision, tool_config
+            )
         if message or not valid:
-            status = 'error'
+            status = "error"
         tool_state = tool_util.new_state(trans, tool, invalid=not valid)
-        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                             repository_id,
-                                                                                             changeset_revision,
-                                                                                             metadata_only=True)
+        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+            trans.app, repository_id, changeset_revision, metadata_only=True
+        )
         try:
-            return trans.fill_template("/webapps/tool_shed/repository/tool_form.mako",
-                                       repository=repository,
-                                       render_repository_actions_for=render_repository_actions_for,
-                                       metadata=metadata,
-                                       changeset_revision=changeset_revision,
-                                       tool=tool,
-                                       tool_state=tool_state,
-                                       message=message,
-                                       status=status)
+            return trans.fill_template(
+                "/webapps/tool_shed/repository/tool_form.mako",
+                repository=repository,
+                render_repository_actions_for=render_repository_actions_for,
+                metadata=metadata,
+                changeset_revision=changeset_revision,
+                tool=tool,
+                tool_state=tool_state,
+                message=message,
+                status=status,
+            )
         except Exception as e:
-            message = "Error displaying tool, probably due to a problem in the tool config.  The exception is: %s." % util.unicodify(e)
-        if trans.webapp.name == 'galaxy' or render_repository_actions_for == 'galaxy':
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='preview_tools_in_changeset',
-                                                            repository_id=repository_id,
-                                                            changeset_revision=changeset_revision,
-                                                            message=message,
-                                                            status='error'))
-        return trans.response.send_redirect(web.url_for(controller='repository',
-                                                        action='browse_repositories',
-                                                        operation='view_or_manage_repository',
-                                                        id=repository_id,
-                                                        changeset_revision=changeset_revision,
-                                                        message=message,
-                                                        status='error'))
+            message = f"Error displaying tool, probably due to a problem in the tool config.  The exception is: {util.unicodify(e)}."
+        if trans.webapp.name == "galaxy" or render_repository_actions_for == "galaxy":
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="preview_tools_in_changeset",
+                    repository_id=repository_id,
+                    changeset_revision=changeset_revision,
+                    message=message,
+                    status="error",
+                )
+            )
+        return trans.response.send_redirect(
+            web.url_for(
+                controller="repository",
+                action="browse_repositories",
+                operation="view_or_manage_repository",
+                id=repository_id,
+                changeset_revision=changeset_revision,
+                message=message,
+                status="error",
+            )
+        )
 
     @web.expose
     def download(self, trans, repository_id, changeset_revision, file_type, **kwd):
         """Download an archive of the repository files compressed as zip, gz or bz2."""
         # FIXME: this will currently only download the repository tip, no matter which installable changeset_revision is being viewed.
         # This should be enhanced to use the export method below, which accounts for the currently viewed changeset_revision.
         repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
@@ -924,185 +915,189 @@
         # server account's .hgrc file to include the following setting:
         # [web]
         # allow_archive = bz2, gz, zip
         file_type_str = basic_util.get_file_type_str(changeset_revision, file_type)
         repository.times_downloaded += 1
         trans.sa_session.add(repository)
         trans.sa_session.flush()
-        tool_shed_url = web.url_for('/', qualified=True)
-        pathspec = ['repos', str(repository.user.username), str(repository.name), 'archive', file_type_str]
+        tool_shed_url = web.url_for("/", qualified=True)
+        pathspec = ["repos", str(repository.user.username), str(repository.name), "archive", file_type_str]
         download_url = util.build_url(tool_shed_url, pathspec=pathspec)
         return trans.response.send_redirect(download_url)
 
     @web.expose
     def export_via_api(self, trans, **kwd):
         """Return an exported gzip compressed repository archive file opened for reading."""
-        encoded_repositories_archive_name = kwd.get('encoded_repositories_archive_name', None)
+        encoded_repositories_archive_name = kwd.get("encoded_repositories_archive_name", None)
         if encoded_repositories_archive_name:
             repositories_archive_name = encoding_util.tool_shed_decode(encoded_repositories_archive_name)
             opened_archive = open(repositories_archive_name)
             # Make sure the file is removed from disk after the contents have been downloaded.
             os.unlink(repositories_archive_name)
             return opened_archive
-        return ''
+        return ""
 
     @web.expose
     def find_tools(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         common_util.handle_galaxy_url(trans, **kwd)
-        if 'operation' in kwd:
-            item_id = kwd.get('id', '')
+        if "operation" in kwd:
+            item_id = kwd.get("id", "")
             if item_id:
-                operation = kwd['operation'].lower()
+                operation = kwd["operation"].lower()
                 is_admin = trans.user_is_admin
                 if operation == "view_or_manage_repository":
                     # The received id is a RepositoryMetadata id, so we have to get the repository id.
                     repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, item_id)
                     repository_id = trans.security.encode_id(repository_metadata.repository.id)
                     repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
-                    kwd['id'] = repository_id
-                    kwd['changeset_revision'] = repository_metadata.changeset_revision
-                    if trans.webapp.name == 'tool_shed' and (is_admin or repository.user == trans.user):
-                        a = 'manage_repository'
+                    kwd["id"] = repository_id
+                    kwd["changeset_revision"] = repository_metadata.changeset_revision
+                    if trans.webapp.name == "tool_shed" and (is_admin or repository.user == trans.user):
+                        a = "manage_repository"
                     else:
-                        a = 'view_repository'
-                    return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                    action=a,
-                                                                    **kwd))
-                if operation == "install to galaxy":
-                    # We've received a list of RepositoryMetadata ids, so we need to build a list of associated Repository ids.
-                    encoded_repository_ids = []
-                    changeset_revisions = []
-                    for repository_metadata_id in util.listify(item_id):
-                        repository_metadata = metadata_util.get_repository_metadata_by_id(trans.app, repository_metadata_id)
-                        encoded_repository_ids.append(trans.security.encode_id(repository_metadata.repository.id))
-                        changeset_revisions.append(repository_metadata.changeset_revision)
-                    new_kwd = {}
-                    new_kwd['repository_ids'] = encoded_repository_ids
-                    new_kwd['changeset_revisions'] = changeset_revisions
-                    return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                    action='install_repositories_by_revision',
-                                                                    **new_kwd))
+                        a = "view_repository"
+                    return trans.response.send_redirect(web.url_for(controller="repository", action=a, **kwd))
             else:
                 # This can only occur when there is a multi-select grid with check boxes and an operation,
                 # and the user clicked the operation button without checking any of the check boxes.
                 return trans.show_error_message("No items were selected.")
-        tool_ids = [item.lower() for item in util.listify(kwd.get('tool_id', ''))]
-        tool_names = [item.lower() for item in util.listify(kwd.get('tool_name', ''))]
-        tool_versions = [item.lower() for item in util.listify(kwd.get('tool_version', ''))]
-        exact_matches = kwd.get('exact_matches', '')
+        tool_ids = [item.lower() for item in util.listify(kwd.get("tool_id", ""))]
+        tool_names = [item.lower() for item in util.listify(kwd.get("tool_name", ""))]
+        tool_versions = [item.lower() for item in util.listify(kwd.get("tool_version", ""))]
+        exact_matches = kwd.get("exact_matches", "")
         exact_matches_checked = CheckboxField.is_checked(exact_matches)
         match_tuples = []
         ok = True
         if tool_ids or tool_names or tool_versions:
-            ok, match_tuples = search_util.search_repository_metadata(trans.app,
-                                                                      exact_matches_checked,
-                                                                      tool_ids=tool_ids,
-                                                                      tool_names=tool_names,
-                                                                      tool_versions=tool_versions)
+            ok, match_tuples = search_util.search_repository_metadata(
+                trans.app, exact_matches_checked, tool_ids=tool_ids, tool_names=tool_names, tool_versions=tool_versions
+            )
             if ok:
-                kwd['match_tuples'] = match_tuples
+                kwd["match_tuples"] = match_tuples
                 # Render the list view
-                if trans.webapp.name == 'galaxy':
+                if trans.webapp.name == "galaxy":
                     # Our initial request originated from a Galaxy instance.
-                    global_actions = [grids.GridAction("Browse valid repositories",
-                                                       dict(controller='repository', action='browse_valid_categories')),
-                                      grids.GridAction("Search for valid tools",
-                                                       dict(controller='repository', action='find_tools'))]
+                    global_actions = [
+                        grids.GridAction(
+                            "Browse valid repositories", dict(controller="repository", action="browse_valid_categories")
+                        ),
+                        grids.GridAction("Search for valid tools", dict(controller="repository", action="find_tools")),
+                    ]
                     self.install_matched_repository_grid.global_actions = global_actions
-                    install_url_args = dict(controller='repository', action='find_tools')
-                    operations = [grids.GridOperation("Install", url_args=install_url_args, allow_multiple=True, async_compatible=False)]
+                    install_url_args = dict(controller="repository", action="find_tools")
+                    operations = [
+                        grids.GridOperation(
+                            "Install", url_args=install_url_args, allow_multiple=True, async_compatible=False
+                        )
+                    ]
                     self.install_matched_repository_grid.operations = operations
                     return self.install_matched_repository_grid(trans, **kwd)
                 else:
-                    kwd['message'] = "tool id: <b>%s</b><br/>tool name: <b>%s</b><br/>tool version: <b>%s</b><br/>exact matches only: <b>%s</b>" % \
-                        (basic_util.stringify(tool_ids),
-                         escape(basic_util.stringify(tool_names)),
-                         escape(basic_util.stringify(tool_versions)),
-                         str(exact_matches_checked))
+                    kwd["message"] = (
+                        "tool id: <b>%s</b><br/>tool name: <b>%s</b><br/>tool version: <b>%s</b><br/>exact matches only: <b>%s</b>"
+                        % (
+                            basic_util.stringify(tool_ids),
+                            escape(basic_util.stringify(tool_names)),
+                            escape(basic_util.stringify(tool_versions)),
+                            str(exact_matches_checked),
+                        )
+                    )
                     self.matched_repository_grid.title = "Repositories with matching tools"
                     return self.matched_repository_grid(trans, **kwd)
             else:
                 message = "No search performed - each field must contain the same number of comma-separated items."
                 status = "error"
-        exact_matches_check_box = CheckboxField('exact_matches', value=exact_matches_checked)
-        return trans.fill_template('/webapps/tool_shed/repository/find_tools.mako',
-                                   tool_id=basic_util.stringify(tool_ids),
-                                   tool_name=basic_util.stringify(tool_names),
-                                   tool_version=basic_util.stringify(tool_versions),
-                                   exact_matches_check_box=exact_matches_check_box,
-                                   message=message,
-                                   status=status)
+        exact_matches_check_box = CheckboxField("exact_matches", value=exact_matches_checked)
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/find_tools.mako",
+            tool_id=basic_util.stringify(tool_ids),
+            tool_name=basic_util.stringify(tool_names),
+            tool_version=basic_util.stringify(tool_versions),
+            exact_matches_check_box=exact_matches_check_box,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def get_changeset_revision_and_ctx_rev(self, trans, **kwd):
         """Handle a request from a local Galaxy instance to retrieve the changeset revision hash to which an installed repository can be updated."""
+
         def has_galaxy_utilities(repository_metadata):
-            has_galaxy_utilities_dict = dict(includes_data_managers=False,
-                                             includes_datatypes=False,
-                                             includes_tools=False,
-                                             includes_tools_for_display_in_tool_panel=False,
-                                             has_repository_dependencies=False,
-                                             has_repository_dependencies_only_if_compiling_contained_td=False,
-                                             includes_tool_dependencies=False,
-                                             includes_workflows=False)
+            has_galaxy_utilities_dict = dict(
+                includes_data_managers=False,
+                includes_datatypes=False,
+                includes_tools=False,
+                includes_tools_for_display_in_tool_panel=False,
+                has_repository_dependencies=False,
+                has_repository_dependencies_only_if_compiling_contained_td=False,
+                includes_tool_dependencies=False,
+                includes_workflows=False,
+            )
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 if metadata:
-                    if 'data_manager' in metadata:
-                        has_galaxy_utilities_dict['includes_data_managers'] = True
-                    if 'datatypes' in metadata:
-                        has_galaxy_utilities_dict['includes_datatypes'] = True
-                    if 'tools' in metadata:
-                        has_galaxy_utilities_dict['includes_tools'] = True
-                    if 'tool_dependencies' in metadata:
-                        has_galaxy_utilities_dict['includes_tool_dependencies'] = True
-                    repository_dependencies_dict = metadata.get('repository_dependencies', {})
-                    repository_dependencies = repository_dependencies_dict.get('repository_dependencies', [])
-                    has_repository_dependencies, has_repository_dependencies_only_if_compiling_contained_td = \
-                        repository_util.get_repository_dependency_types(repository_dependencies)
-                    has_galaxy_utilities_dict['has_repository_dependencies'] = has_repository_dependencies
-                    has_galaxy_utilities_dict['has_repository_dependencies_only_if_compiling_contained_td'] = \
-                        has_repository_dependencies_only_if_compiling_contained_td
-                    if 'workflows' in metadata:
-                        has_galaxy_utilities_dict['includes_workflows'] = True
+                    if "data_manager" in metadata:
+                        has_galaxy_utilities_dict["includes_data_managers"] = True
+                    if "datatypes" in metadata:
+                        has_galaxy_utilities_dict["includes_datatypes"] = True
+                    if "tools" in metadata:
+                        has_galaxy_utilities_dict["includes_tools"] = True
+                    if "tool_dependencies" in metadata:
+                        has_galaxy_utilities_dict["includes_tool_dependencies"] = True
+                    repository_dependencies_dict = metadata.get("repository_dependencies", {})
+                    repository_dependencies = repository_dependencies_dict.get("repository_dependencies", [])
+                    (
+                        has_repository_dependencies,
+                        has_repository_dependencies_only_if_compiling_contained_td,
+                    ) = repository_util.get_repository_dependency_types(repository_dependencies)
+                    has_galaxy_utilities_dict["has_repository_dependencies"] = has_repository_dependencies
+                    has_galaxy_utilities_dict[
+                        "has_repository_dependencies_only_if_compiling_contained_td"
+                    ] = has_repository_dependencies_only_if_compiling_contained_td
+                    if "workflows" in metadata:
+                        has_galaxy_utilities_dict["includes_workflows"] = True
             return has_galaxy_utilities_dict
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
-        changeset_revision = kwd.get('changeset_revision', None)
+
+        name = kwd.get("name", None)
+        owner = kwd.get("owner", None)
+        changeset_revision = kwd.get("changeset_revision", None)
         repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                 trans.security.encode_id(repository.id),
-                                                                                 changeset_revision)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, trans.security.encode_id(repository.id), changeset_revision
+        )
         has_galaxy_utilities_dict = has_galaxy_utilities(repository_metadata)
-        includes_data_managers = has_galaxy_utilities_dict['includes_data_managers']
-        includes_datatypes = has_galaxy_utilities_dict['includes_datatypes']
-        includes_tools = has_galaxy_utilities_dict['includes_tools']
-        includes_tools_for_display_in_tool_panel = has_galaxy_utilities_dict['includes_tools_for_display_in_tool_panel']
-        includes_tool_dependencies = has_galaxy_utilities_dict['includes_tool_dependencies']
-        has_repository_dependencies = has_galaxy_utilities_dict['has_repository_dependencies']
-        has_repository_dependencies_only_if_compiling_contained_td = \
-            has_galaxy_utilities_dict['has_repository_dependencies_only_if_compiling_contained_td']
-        includes_workflows = has_galaxy_utilities_dict['includes_workflows']
+        includes_data_managers = has_galaxy_utilities_dict["includes_data_managers"]
+        includes_datatypes = has_galaxy_utilities_dict["includes_datatypes"]
+        includes_tools = has_galaxy_utilities_dict["includes_tools"]
+        includes_tools_for_display_in_tool_panel = has_galaxy_utilities_dict["includes_tools_for_display_in_tool_panel"]
+        includes_tool_dependencies = has_galaxy_utilities_dict["includes_tool_dependencies"]
+        has_repository_dependencies = has_galaxy_utilities_dict["has_repository_dependencies"]
+        has_repository_dependencies_only_if_compiling_contained_td = has_galaxy_utilities_dict[
+            "has_repository_dependencies_only_if_compiling_contained_td"
+        ]
+        includes_workflows = has_galaxy_utilities_dict["includes_workflows"]
         repo = repository.hg_repo
         # Default to the received changeset revision and ctx_rev.
         update_to_ctx = hg_util.get_changectx_for_changeset(repo, changeset_revision)
         ctx_rev = str(update_to_ctx.rev())
         latest_changeset_revision = changeset_revision
-        update_dict = dict(changeset_revision=changeset_revision,
-                           ctx_rev=ctx_rev,
-                           includes_data_managers=includes_data_managers,
-                           includes_datatypes=includes_datatypes,
-                           includes_tools=includes_tools,
-                           includes_tools_for_display_in_tool_panel=includes_tools_for_display_in_tool_panel,
-                           includes_tool_dependencies=includes_tool_dependencies,
-                           has_repository_dependencies=has_repository_dependencies,
-                           has_repository_dependencies_only_if_compiling_contained_td=has_repository_dependencies_only_if_compiling_contained_td,
-                           includes_workflows=includes_workflows)
+        update_dict = dict(
+            changeset_revision=changeset_revision,
+            ctx_rev=ctx_rev,
+            includes_data_managers=includes_data_managers,
+            includes_datatypes=includes_datatypes,
+            includes_tools=includes_tools,
+            includes_tools_for_display_in_tool_panel=includes_tools_for_display_in_tool_panel,
+            includes_tool_dependencies=includes_tool_dependencies,
+            has_repository_dependencies=has_repository_dependencies,
+            has_repository_dependencies_only_if_compiling_contained_td=has_repository_dependencies_only_if_compiling_contained_td,
+            includes_workflows=includes_workflows,
+        )
         if changeset_revision == repository.tip():
             # If changeset_revision is the repository tip, there are no additional updates.
             return encoding_util.tool_shed_encode(update_dict)
         else:
             if repository_metadata:
                 # If changeset_revision is in the repository_metadata table for this repository, there are no additional updates.
                 return encoding_util.tool_shed_encode(update_dict)
@@ -1112,135 +1107,141 @@
                 update_to_changeset_hash = None
                 for changeset in repo.changelog:
                     includes_tools = False
                     has_repository_dependencies = False
                     has_repository_dependencies_only_if_compiling_contained_td = False
                     changeset_hash = str(repo[changeset])
                     if update_to_changeset_hash:
-                        update_to_repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                                                    trans.security.encode_id(repository.id),
-                                                                                                                    changeset_hash)
+                        update_to_repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                            trans.app, trans.security.encode_id(repository.id), changeset_hash
+                        )
                         if update_to_repository_metadata:
                             has_galaxy_utilities_dict = has_galaxy_utilities(repository_metadata)
-                            includes_data_managers = has_galaxy_utilities_dict['includes_data_managers']
-                            includes_datatypes = has_galaxy_utilities_dict['includes_datatypes']
-                            includes_tools = has_galaxy_utilities_dict['includes_tools']
-                            includes_tools_for_display_in_tool_panel = has_galaxy_utilities_dict['includes_tools_for_display_in_tool_panel']
-                            includes_tool_dependencies = has_galaxy_utilities_dict['includes_tool_dependencies']
-                            has_repository_dependencies = has_galaxy_utilities_dict['has_repository_dependencies']
-                            has_repository_dependencies_only_if_compiling_contained_td = has_galaxy_utilities_dict['has_repository_dependencies_only_if_compiling_contained_td']
-                            includes_workflows = has_galaxy_utilities_dict['includes_workflows']
+                            includes_data_managers = has_galaxy_utilities_dict["includes_data_managers"]
+                            includes_datatypes = has_galaxy_utilities_dict["includes_datatypes"]
+                            includes_tools = has_galaxy_utilities_dict["includes_tools"]
+                            includes_tools_for_display_in_tool_panel = has_galaxy_utilities_dict[
+                                "includes_tools_for_display_in_tool_panel"
+                            ]
+                            includes_tool_dependencies = has_galaxy_utilities_dict["includes_tool_dependencies"]
+                            has_repository_dependencies = has_galaxy_utilities_dict["has_repository_dependencies"]
+                            has_repository_dependencies_only_if_compiling_contained_td = has_galaxy_utilities_dict[
+                                "has_repository_dependencies_only_if_compiling_contained_td"
+                            ]
+                            includes_workflows = has_galaxy_utilities_dict["includes_workflows"]
                             # We found a RepositoryMetadata record.
                             if changeset_hash == repository.tip():
                                 # The current ctx is the repository tip, so use it.
                                 update_to_ctx = hg_util.get_changectx_for_changeset(repo, changeset_hash)
                                 latest_changeset_revision = changeset_hash
                             else:
                                 update_to_ctx = hg_util.get_changectx_for_changeset(repo, update_to_changeset_hash)
                                 latest_changeset_revision = update_to_changeset_hash
                             break
                     elif not update_to_changeset_hash and changeset_hash == changeset_revision:
                         # We've found the changeset in the changelog for which we need to get the next update.
                         update_to_changeset_hash = changeset_hash
-                update_dict['includes_data_managers'] = includes_data_managers
-                update_dict['includes_datatypes'] = includes_datatypes
-                update_dict['includes_tools'] = includes_tools
-                update_dict['includes_tools_for_display_in_tool_panel'] = includes_tools_for_display_in_tool_panel
-                update_dict['includes_tool_dependencies'] = includes_tool_dependencies
-                update_dict['includes_workflows'] = includes_workflows
-                update_dict['has_repository_dependencies'] = has_repository_dependencies
-                update_dict['has_repository_dependencies_only_if_compiling_contained_td'] = has_repository_dependencies_only_if_compiling_contained_td
-                update_dict['changeset_revision'] = str(latest_changeset_revision)
-        update_dict['ctx_rev'] = str(update_to_ctx.rev())
+                update_dict["includes_data_managers"] = includes_data_managers
+                update_dict["includes_datatypes"] = includes_datatypes
+                update_dict["includes_tools"] = includes_tools
+                update_dict["includes_tools_for_display_in_tool_panel"] = includes_tools_for_display_in_tool_panel
+                update_dict["includes_tool_dependencies"] = includes_tool_dependencies
+                update_dict["includes_workflows"] = includes_workflows
+                update_dict["has_repository_dependencies"] = has_repository_dependencies
+                update_dict[
+                    "has_repository_dependencies_only_if_compiling_contained_td"
+                ] = has_repository_dependencies_only_if_compiling_contained_td
+                update_dict["changeset_revision"] = str(latest_changeset_revision)
+        update_dict["ctx_rev"] = str(update_to_ctx.rev())
         return encoding_util.tool_shed_encode(update_dict)
 
     @web.expose
     def get_ctx_rev(self, trans, **kwd):
         """Given a repository and changeset_revision, return the correct ctx.rev() value."""
-        repository_name = kwd['name']
-        repository_owner = kwd['owner']
-        changeset_revision = kwd['changeset_revision']
+        repository_name = kwd["name"]
+        repository_owner = kwd["owner"]
+        changeset_revision = kwd["changeset_revision"]
         repository = repository_util.get_repository_by_name_and_owner(trans.app, repository_name, repository_owner)
         repo = repository.hg_repo
         ctx = hg_util.get_changectx_for_changeset(repo, changeset_revision)
         if ctx:
             return str(ctx.rev())
-        return ''
+        return ""
 
     @web.json
     @web.do_not_cache
     def get_file_contents(self, trans, file_path, repository_id):
         is_admin = trans.user_is_admin
         return suc.get_repository_file_contents(trans.app, file_path, repository_id, is_admin)
 
     @web.json
     def get_latest_downloadable_changeset_revision(self, trans, **kwd):
         """
         Return the latest installable changeset revision for the repository associated with the received
         name and owner.  This method is called from Galaxy when attempting to install the latest revision
         of an installed repository.
         """
-        repository_name = kwd.get('name', None)
-        repository_owner = kwd.get('owner', None)
+        repository_name = kwd.get("name", None)
+        repository_owner = kwd.get("owner", None)
         if repository_name is not None and repository_owner is not None:
             repository = repository_util.get_repository_by_name_and_owner(trans.app, repository_name, repository_owner)
             if repository:
                 return metadata_util.get_latest_downloadable_changeset_revision(trans.app, repository)
         return hg_util.INITIAL_CHANGELOG_HASH
 
     @web.json
     def get_readme_files(self, trans, **kwd):
         """
         This method is called when installing or re-installing a single repository into a Galaxy instance.
         If the received changeset_revision includes one or more readme files, return them in a dictionary.
         """
-        repository_name = kwd.get('name', None)
-        repository_owner = kwd.get('owner', None)
-        changeset_revision = kwd.get('changeset_revision', None)
+        repository_name = kwd.get("name", None)
+        repository_owner = kwd.get("owner", None)
+        changeset_revision = kwd.get("changeset_revision", None)
         if repository_name is not None and repository_owner is not None and changeset_revision is not None:
             repository = repository_util.get_repository_by_name_and_owner(trans.app, repository_name, repository_owner)
             if repository:
-                repository_metadata = \
-                    metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                trans.security.encode_id(repository.id),
-                                                                                changeset_revision)
+                repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                    trans.app, trans.security.encode_id(repository.id), changeset_revision
+                )
                 if repository_metadata:
                     metadata = repository_metadata.metadata
                     if metadata:
-                        return readme_util.build_readme_files_dict(trans.app,
-                                                                   repository,
-                                                                   changeset_revision,
-                                                                   repository_metadata.metadata)
+                        return readme_util.build_readme_files_dict(
+                            trans.app, repository, changeset_revision, repository_metadata.metadata
+                        )
         return {}
 
     @web.json
     def get_repository_dependencies(self, trans, **kwd):
         """
         Return an encoded dictionary of all repositories upon which the contents of the received repository
         depends.
         """
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
-        changeset_revision = kwd.get('changeset_revision', None)
+        name = kwd.get("name", None)
+        owner = kwd.get("owner", None)
+        changeset_revision = kwd.get("changeset_revision", None)
         repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
         # get_repository_dependencies( self, app, changeset, toolshed_url )
-        dependencies = repository.get_repository_dependencies(trans.app, changeset_revision, web.url_for('/', qualified=True))
+        dependencies = repository.get_repository_dependencies(
+            trans.app, changeset_revision, web.url_for("/", qualified=True)
+        )
         if dependencies:
             return encoding_util.tool_shed_encode(dependencies)
-        return ''
+        return ""
 
     @web.expose
     def get_repository_id(self, trans, **kwd):
         """Given a repository name and owner, return the encoded repository id."""
-        repository_name = kwd['name']
-        repository_owner = kwd['owner']
+        repository_name = kwd["name"]
+        repository_owner = kwd["owner"]
         repository = repository_util.get_repository_by_name_and_owner(trans.app, repository_name, repository_owner)
         if repository:
             return trans.security.encode_id(repository.id)
-        return ''
+        return ""
 
     @web.json
     def get_repository_information(self, trans, repository_ids, changeset_revisions, **kwd):
         """
         Generate a list of dictionaries, each of which contains the information about a repository that will
         be necessary for installing it into a local Galaxy instance.
         """
@@ -1248,40 +1249,50 @@
         includes_tools_for_display_in_tool_panel = False
         has_repository_dependencies = False
         has_repository_dependencies_only_if_compiling_contained_td = False
         includes_tool_dependencies = False
         repo_info_dicts = []
         for tup in zip(util.listify(repository_ids), util.listify(changeset_revisions)):
             repository_id, changeset_revision = tup
-            repo_info_dict, cur_includes_tools, cur_includes_tool_dependencies, cur_includes_tools_for_display_in_tool_panel, \
-                cur_has_repository_dependencies, cur_has_repository_dependencies_only_if_compiling_contained_td = \
-                repository_util.get_repo_info_dict(trans.app, trans.user, repository_id, changeset_revision)
+            (
+                repo_info_dict,
+                cur_includes_tools,
+                cur_includes_tool_dependencies,
+                cur_includes_tools_for_display_in_tool_panel,
+                cur_has_repository_dependencies,
+                cur_has_repository_dependencies_only_if_compiling_contained_td,
+            ) = repository_util.get_repo_info_dict(trans.app, trans.user, repository_id, changeset_revision)
             if cur_has_repository_dependencies and not has_repository_dependencies:
                 has_repository_dependencies = True
-            if cur_has_repository_dependencies_only_if_compiling_contained_td and not has_repository_dependencies_only_if_compiling_contained_td:
+            if (
+                cur_has_repository_dependencies_only_if_compiling_contained_td
+                and not has_repository_dependencies_only_if_compiling_contained_td
+            ):
                 has_repository_dependencies_only_if_compiling_contained_td = True
             if cur_includes_tools and not includes_tools:
                 includes_tools = True
             if cur_includes_tool_dependencies and not includes_tool_dependencies:
                 includes_tool_dependencies = True
             if cur_includes_tools_for_display_in_tool_panel and not includes_tools_for_display_in_tool_panel:
                 includes_tools_for_display_in_tool_panel = True
             repo_info_dicts.append(encoding_util.tool_shed_encode(repo_info_dict))
-        return dict(includes_tools=includes_tools,
-                    includes_tools_for_display_in_tool_panel=includes_tools_for_display_in_tool_panel,
-                    has_repository_dependencies=has_repository_dependencies,
-                    has_repository_dependencies_only_if_compiling_contained_td=has_repository_dependencies_only_if_compiling_contained_td,
-                    includes_tool_dependencies=includes_tool_dependencies,
-                    repo_info_dicts=repo_info_dicts)
+        return dict(
+            includes_tools=includes_tools,
+            includes_tools_for_display_in_tool_panel=includes_tools_for_display_in_tool_panel,
+            has_repository_dependencies=has_repository_dependencies,
+            has_repository_dependencies_only_if_compiling_contained_td=has_repository_dependencies_only_if_compiling_contained_td,
+            includes_tool_dependencies=includes_tool_dependencies,
+            repo_info_dicts=repo_info_dicts,
+        )
 
     @web.expose
     def get_repository_type(self, trans, **kwd):
         """Given a repository name and owner, return the type."""
-        repository_name = kwd['name']
-        repository_owner = kwd['owner']
+        repository_name = kwd["name"]
+        repository_owner = kwd["owner"]
         repository = repository_util.get_repository_by_name_and_owner(trans.app, repository_name, repository_owner)
         return str(repository.type)
 
     @web.json
     def get_required_repo_info_dict(self, trans, encoded_str=None):
         """
         Retrieve and return a dictionary that includes a list of dictionaries that each contain all of the
@@ -1289,432 +1300,445 @@
         """
         repo_info_dict = {}
         if encoded_str:
             encoded_required_repository_str = encoding_util.tool_shed_decode(encoded_str)
             encoded_required_repository_tups = encoded_required_repository_str.split(encoding_util.encoding_sep2)
             decoded_required_repository_tups = []
             for encoded_required_repository_tup in encoded_required_repository_tups:
-                decoded_required_repository_tups.append(encoded_required_repository_tup.split(encoding_util.encoding_sep))
+                decoded_required_repository_tups.append(
+                    encoded_required_repository_tup.split(encoding_util.encoding_sep)
+                )
             encoded_repository_ids = []
             changeset_revisions = []
             for required_repository_tup in decoded_required_repository_tups:
-                tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-                    common_util.parse_repository_dependency_tuple(required_repository_tup)
+                (
+                    tool_shed,
+                    name,
+                    owner,
+                    changeset_revision,
+                    prior_installation_required,
+                    only_if_compiling_contained_td,
+                ) = common_util.parse_repository_dependency_tuple(required_repository_tup)
                 repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
                 encoded_repository_ids.append(trans.security.encode_id(repository.id))
                 changeset_revisions.append(changeset_revision)
             if encoded_repository_ids and changeset_revisions:
-                repo_info_dict = json.loads(self.get_repository_information(trans, encoded_repository_ids, changeset_revisions))
+                repo_info_dict = json.loads(
+                    self.get_repository_information(trans, encoded_repository_ids, changeset_revisions)
+                )
         return repo_info_dict
 
     @web.expose
     def get_tool_dependencies(self, trans, **kwd):
         """
         Handle a request from a Galaxy instance to get the tool_dependencies entry from the metadata
         for a specified changeset revision.
         """
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
-        changeset_revision = kwd.get('changeset_revision', None)
+        name = kwd.get("name", None)
+        owner = kwd.get("owner", None)
+        changeset_revision = kwd.get("changeset_revision", None)
         repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
         dependencies = repository.get_tool_dependencies(trans.app, changeset_revision)
         if len(dependencies) > 0:
             return encoding_util.tool_shed_encode(dependencies)
-        return ''
+        return ""
 
     @web.expose
     def get_tool_dependencies_config_contents(self, trans, **kwd):
         """
         Handle a request from a Galaxy instance to get the tool_dependencies.xml file contents for a
         specified changeset revision.
         """
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
+        name = kwd.get("name", None)
+        owner = kwd.get("owner", None)
         repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
         # TODO: We're currently returning the tool_dependencies.xml file that is available on disk.  We need
         # to enhance this process to retrieve older versions of the tool-dependencies.xml file from the repository
         # manafest.
         repo_dir = repository.repo_path(trans.app)
         # Get the tool_dependencies.xml file from disk.
         tool_dependencies_config = hg_util.get_config_from_disk(rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME, repo_dir)
         # Return the encoded contents of the tool_dependencies.xml file.
         if tool_dependencies_config:
-            tool_dependencies_config_file = open(tool_dependencies_config, 'rb')
+            tool_dependencies_config_file = open(tool_dependencies_config, "rb")
             contents = tool_dependencies_config_file.read()
             tool_dependencies_config_file.close()
             return contents
-        return ''
+        return ""
 
     @web.json
     def get_tool_dependency_definition_metadata(self, trans, **kwd):
         """
         Given a repository name and ownerof a repository whose type is
         tool_dependency_definition, return the current metadata.
         """
-        repository_name = kwd['name']
-        repository_owner = kwd['owner']
+        repository_name = kwd["name"]
+        repository_owner = kwd["owner"]
         repository = repository_util.get_repository_by_name_and_owner(trans.app, repository_name, repository_owner)
         encoded_id = trans.app.security.encode_id(repository.id)
         repository_tip = repository.tip()
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                          encoded_id,
-                                                                                          repository_tip)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, encoded_id, repository_tip
+        )
         return repository_metadata.metadata
 
     @web.expose
     def get_tool_versions(self, trans, **kwd):
         """
         For each valid /downloadable change set (up to the received changeset_revision) in the repository's
         change log, append the changeset tool_versions dictionary to the list that will be returned.
         """
-        name = kwd['name']
-        owner = kwd['owner']
-        changeset_revision = kwd['changeset_revision']
+        name = kwd["name"]
+        owner = kwd["owner"]
+        changeset_revision = kwd["changeset_revision"]
         repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
         repo = repository.hg_repo
         tool_version_dicts = []
         for changeset in repo.changelog:
             current_changeset_revision = str(repo[changeset])
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                              trans.security.encode_id(repository.id),
-                                                                                              current_changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                trans.app, trans.security.encode_id(repository.id), current_changeset_revision
+            )
             if repository_metadata and repository_metadata.tool_versions:
                 tool_version_dicts.append(repository_metadata.tool_versions)
                 if current_changeset_revision == changeset_revision:
                     break
         if tool_version_dicts:
             return json.dumps(tool_version_dicts)
-        return ''
+        return ""
 
     @web.json
     def get_updated_repository_information(self, trans, name, owner, changeset_revision, **kwd):
         """
         Generate a dictionary that contains the information about a repository that is necessary for installing
         it into a local Galaxy instance.
         """
         repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
         repository_id = trans.security.encode_id(repository.id)
         repository_clone_url = common_util.generate_clone_url_for_repository_in_tool_shed(trans.user, repository)
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, repository_id, changeset_revision)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, repository_id, changeset_revision
+        )
         if not repository_metadata:
             # The received changeset_revision is no longer associated with metadata, so get the next changeset_revision in the repository
             # changelog that is associated with metadata.
-            changeset_revision = metadata_util.get_next_downloadable_changeset_revision(trans.app, repository, after_changeset_revision=changeset_revision)
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, repository_id, changeset_revision)
+            changeset_revision = metadata_util.get_next_downloadable_changeset_revision(
+                trans.app, repository, after_changeset_revision=changeset_revision
+            )
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                trans.app, repository_id, changeset_revision
+            )
         repo_path = repository.repo_path(trans.app)
         ctx_rev = str(hg_util.changeset2rev(repo_path, changeset_revision))
-        repo_info_dict = repository_util.create_repo_info_dict(app=trans.app,
-                                                               repository_clone_url=repository_clone_url,
-                                                               changeset_revision=changeset_revision,
-                                                               ctx_rev=ctx_rev,
-                                                               repository_owner=repository.user.username,
-                                                               repository_name=repository.name,
-                                                               repository=repository,
-                                                               repository_metadata=repository_metadata,
-                                                               tool_dependencies=None,
-                                                               repository_dependencies=None)
+        repo_info_dict = repository_util.create_repo_info_dict(
+            app=trans.app,
+            repository_clone_url=repository_clone_url,
+            changeset_revision=changeset_revision,
+            ctx_rev=ctx_rev,
+            repository_owner=repository.user.username,
+            repository_name=repository.name,
+            repository=repository,
+            repository_metadata=repository_metadata,
+            tool_dependencies=None,
+            repository_dependencies=None,
+        )
         includes_data_managers = False
         includes_datatypes = False
         includes_tools = False
         includes_tools_for_display_in_tool_panel = False
         includes_workflows = False
         readme_files_dict = None
         metadata = repository_metadata.metadata
         if metadata:
-            if 'data_manager' in metadata:
+            if "data_manager" in metadata:
                 includes_data_managers = True
-            if 'datatypes' in metadata:
+            if "datatypes" in metadata:
                 includes_datatypes = True
-            if 'tools' in metadata:
+            if "tools" in metadata:
                 includes_tools = True
                 # Handle includes_tools_for_display_in_tool_panel.
-                tool_dicts = metadata['tools']
+                tool_dicts = metadata["tools"]
                 for tool_dict in tool_dicts:
-                    if tool_dict.get('includes_tools_for_display_in_tool_panel', False):
+                    if tool_dict.get("includes_tools_for_display_in_tool_panel", False):
                         includes_tools_for_display_in_tool_panel = True
                         break
-            if 'workflows' in metadata:
+            if "workflows" in metadata:
                 includes_workflows = True
             readme_files_dict = readme_util.build_readme_files_dict(trans.app, repository, changeset_revision, metadata)
         # See if the repo_info_dict was populated with repository_dependencies or tool_dependencies.
         has_repository_dependencies = False
         has_repository_dependencies_only_if_compiling_contained_td = False
         includes_tool_dependencies = False
-        for name, repo_info_tuple in repo_info_dict.items():
-            if not has_repository_dependencies or not has_repository_dependencies_only_if_compiling_contained_td or not includes_tool_dependencies:
-                description, reposectory_clone_url, changeset_revision, ctx_rev, repository_owner, repository_dependencies, tool_dependencies = \
-                    repository_util.get_repo_info_tuple_contents(repo_info_tuple)
+        for repo_info_tuple in repo_info_dict.values():
+            if (
+                not has_repository_dependencies
+                or not has_repository_dependencies_only_if_compiling_contained_td
+                or not includes_tool_dependencies
+            ):
+                (
+                    description,
+                    reposectory_clone_url,
+                    changeset_revision,
+                    ctx_rev,
+                    repository_owner,
+                    repository_dependencies,
+                    tool_dependencies,
+                ) = repository_util.get_repo_info_tuple_contents(repo_info_tuple)
                 for rd_key, rd_tups in repository_dependencies.items():
-                    if rd_key in ['root_key', 'description']:
+                    if rd_key in ["root_key", "description"]:
                         continue
-                    curr_has_repository_dependencies, curr_has_repository_dependencies_only_if_compiling_contained_td = \
-                        repository_util.get_repository_dependency_types(rd_tups)
+                    (
+                        curr_has_repository_dependencies,
+                        curr_has_repository_dependencies_only_if_compiling_contained_td,
+                    ) = repository_util.get_repository_dependency_types(rd_tups)
                     if curr_has_repository_dependencies and not has_repository_dependencies:
                         has_repository_dependencies = True
-                    if curr_has_repository_dependencies_only_if_compiling_contained_td and not has_repository_dependencies_only_if_compiling_contained_td:
+                    if (
+                        curr_has_repository_dependencies_only_if_compiling_contained_td
+                        and not has_repository_dependencies_only_if_compiling_contained_td
+                    ):
                         has_repository_dependencies_only_if_compiling_contained_td = True
                 if tool_dependencies and not includes_tool_dependencies:
                     includes_tool_dependencies = True
-        return dict(includes_data_managers=includes_data_managers,
-                    includes_datatypes=includes_datatypes,
-                    includes_tools=includes_tools,
-                    includes_tools_for_display_in_tool_panel=includes_tools_for_display_in_tool_panel,
-                    has_repository_dependencies=has_repository_dependencies,
-                    has_repository_dependencies_only_if_compiling_contained_td=has_repository_dependencies_only_if_compiling_contained_td,
-                    includes_tool_dependencies=includes_tool_dependencies,
-                    includes_workflows=includes_workflows,
-                    readme_files_dict=readme_files_dict,
-                    repo_info_dict=repo_info_dict)
+        return dict(
+            includes_data_managers=includes_data_managers,
+            includes_datatypes=includes_datatypes,
+            includes_tools=includes_tools,
+            includes_tools_for_display_in_tool_panel=includes_tools_for_display_in_tool_panel,
+            has_repository_dependencies=has_repository_dependencies,
+            has_repository_dependencies_only_if_compiling_contained_td=has_repository_dependencies_only_if_compiling_contained_td,
+            includes_tool_dependencies=includes_tool_dependencies,
+            includes_workflows=includes_workflows,
+            readme_files_dict=readme_files_dict,
+            repo_info_dict=repo_info_dict,
+        )
 
     @web.expose
     def help(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        return trans.fill_template('/webapps/tool_shed/repository/help.mako', message=message, status=status, **kwd)
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        return trans.fill_template("/webapps/tool_shed/repository/help.mako", message=message, status=status, **kwd)
 
     @web.expose
     def index(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         # See if there are any RepositoryMetadata records since menu items require them.
         repository_metadata = trans.sa_session.query(trans.model.RepositoryMetadata).first()
         current_user = trans.user
         # TODO: move the following to some in-memory register so these queries can be done once
         # at startup.  The in-memory register can then be managed during the current session.
         can_administer_repositories = False
-        has_reviewed_repositories = False
         has_deprecated_repositories = False
         if current_user:
-            # See if the current user owns any repositories that have been reviewed.
-            for repository in current_user.active_repositories:
-                if repository.reviews:
-                    has_reviewed_repositories = True
-                    break
             # See if the current user has any repositories that have been marked as deprecated.
             for repository in current_user.active_repositories:
                 if repository.deprecated:
                     has_deprecated_repositories = True
                     break
             # See if the current user can administer any repositories, but only if not an admin user.
             if not trans.user_is_admin:
                 if current_user.active_repositories:
                     can_administer_repositories = True
                 else:
-                    for repository in trans.sa_session.query(trans.model.Repository) \
-                                                      .filter(trans.model.Repository.table.c.deleted == false()):
+                    for repository in trans.sa_session.query(trans.model.Repository).filter(
+                        trans.model.Repository.table.c.deleted == false()
+                    ):
                         if trans.app.security_agent.user_can_administer_repository(current_user, repository):
                             can_administer_repositories = True
                             break
         # Route in may have been from a sharable URL, in whcih case we'll have a user_id and possibly a name
         # The received user_id will be the id of the repository owner.
-        user_id = kwd.get('user_id', None)
-        repository_id = kwd.get('repository_id', None)
-        changeset_revision = kwd.get('changeset_revision', None)
+        user_id = kwd.get("user_id", None)
+        repository_id = kwd.get("repository_id", None)
+        changeset_revision = kwd.get("changeset_revision", None)
         self.validate_changeset_revision(trans, changeset_revision, repository_id)
-        return trans.fill_template('/webapps/tool_shed/index.mako',
-                                   repository_metadata=repository_metadata,
-                                   can_administer_repositories=can_administer_repositories,
-                                   has_reviewed_repositories=has_reviewed_repositories,
-                                   has_deprecated_repositories=has_deprecated_repositories,
-                                   user_id=user_id,
-                                   repository_id=repository_id,
-                                   changeset_revision=changeset_revision,
-                                   message=message,
-                                   status=status)
-
-    @web.expose
-    def install_repositories_by_revision(self, trans, **kwd):
-        """
-        Send the list of repository_ids and changeset_revisions to Galaxy so it can begin the installation
-        process.  If the value of repository_ids is not received, then the name and owner of a single repository
-        must be received to install a single repository.
-        """
-        repository_ids = kwd.get('repository_ids', None)
-        changeset_revisions = kwd.get('changeset_revisions', None)
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
-        if not repository_ids:
-            repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
-            repository_ids = trans.security.encode_id(repository.id)
-        galaxy_url = common_util.handle_galaxy_url(trans, **kwd)
-        if galaxy_url:
-            # Redirect back to local Galaxy to perform install.
-            params = dict(tool_shed_url=web.url_for('/', qualified=True),
-                          repository_ids=','.join(util.listify(repository_ids)),
-                          changeset_revisions=','.join(util.listify(changeset_revisions)))
-            pathspec = ['admin_toolshed', 'prepare_for_install']
-            url = util.build_url(galaxy_url, pathspec=pathspec, params=params)
-            return trans.response.send_redirect(url)
-        else:
-            message = 'Repository installation is not possible due to an invalid Galaxy URL: <b>%s</b>.  ' % galaxy_url
-            message += 'You may need to enable third-party cookies in your browser.  '
-            status = 'error'
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_valid_categories',
-                                                            message=message,
-                                                            status=status))
+        return trans.fill_template(
+            "/webapps/tool_shed/index.mako",
+            repository_metadata=repository_metadata,
+            can_administer_repositories=can_administer_repositories,
+            has_deprecated_repositories=has_deprecated_repositories,
+            user_id=user_id,
+            repository_id=repository_id,
+            changeset_revision=changeset_revision,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def load_invalid_tool(self, trans, repository_id, tool_config, changeset_revision, **kwd):
-        message = escape(kwd.get('message', ''))
-        render_repository_actions_for = kwd.get('render_repository_actions_for', 'tool_shed')
+        message = escape(kwd.get("message", ""))
+        render_repository_actions_for = kwd.get("render_repository_actions_for", "tool_shed")
 
         with ValidationContext.from_app(trans.app) as validation_context:
             tv = tool_validator.ToolValidator(validation_context)
-            repository, tool, valid, error_message = tv.load_tool_from_changeset_revision(repository_id,
-                                                                                          changeset_revision,
-                                                                                          tool_config)
+            repository, tool, valid, error_message = tv.load_tool_from_changeset_revision(
+                repository_id, changeset_revision, tool_config
+            )
             tool_state = tool_util.new_state(trans, tool, invalid=True)
             invalid_file_tups = []
             if tool:
-                invalid_file_tups = tv.check_tool_input_params(repository.repo_path(trans.app),
-                                                               tool_config,
-                                                               tool,
-                                                               [])
+                invalid_file_tups = tv.check_tool_input_params(repository.repo_path(trans.app), tool_config, tool, [])
         if invalid_file_tups:
-            message = tool_util.generate_message_for_invalid_tools(trans.app,
-                                                                   invalid_file_tups,
-                                                                   repository,
-                                                                   {},
-                                                                   as_html=True,
-                                                                   displaying_invalid_tool=True)
+            message = tool_util.generate_message_for_invalid_tools(
+                trans.app, invalid_file_tups, repository, {}, as_html=True, displaying_invalid_tool=True
+            )
         elif error_message:
             message = error_message
         try:
-            return trans.fill_template("/webapps/tool_shed/repository/tool_form.mako",
-                                       repository=repository,
-                                       render_repository_actions_for=render_repository_actions_for,
-                                       changeset_revision=changeset_revision,
-                                       tool=tool,
-                                       tool_state=tool_state,
-                                       message=message,
-                                       status='error')
+            return trans.fill_template(
+                "/webapps/tool_shed/repository/tool_form.mako",
+                repository=repository,
+                render_repository_actions_for=render_repository_actions_for,
+                changeset_revision=changeset_revision,
+                tool=tool,
+                tool_state=tool_state,
+                message=message,
+                status="error",
+            )
         except Exception as e:
-            message = "Exception thrown attempting to display tool: %s." % util.unicodify(e)
-        if trans.webapp.name == 'galaxy':
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='preview_tools_in_changeset',
-                                                            repository_id=repository_id,
-                                                            changeset_revision=changeset_revision,
-                                                            message=message,
-                                                            status='error'))
-        return trans.response.send_redirect(web.url_for(controller='repository',
-                                                        action='browse_repositories',
-                                                        operation='view_or_manage_repository',
-                                                        id=repository_id,
-                                                        changeset_revision=changeset_revision,
-                                                        message=message,
-                                                        status='error'))
+            message = f"Exception thrown attempting to display tool: {util.unicodify(e)}."
+        if trans.webapp.name == "galaxy":
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="preview_tools_in_changeset",
+                    repository_id=repository_id,
+                    changeset_revision=changeset_revision,
+                    message=message,
+                    status="error",
+                )
+            )
+        return trans.response.send_redirect(
+            web.url_for(
+                controller="repository",
+                action="browse_repositories",
+                operation="view_or_manage_repository",
+                id=repository_id,
+                changeset_revision=changeset_revision,
+                message=message,
+                status="error",
+            )
+        )
 
     @web.expose
-    @web.require_login("manage email alerts")
+    @require_login("manage email alerts")
     def manage_email_alerts(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        new_repo_alert = kwd.get('new_repo_alert', '')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        new_repo_alert = kwd.get("new_repo_alert", "")
         new_repo_alert_checked = CheckboxField.is_checked(new_repo_alert)
         user = trans.user
-        if kwd.get('new_repo_alert_button', False):
+        if kwd.get("new_repo_alert_button", False):
             user.new_repo_alert = new_repo_alert_checked
             trans.sa_session.add(user)
             trans.sa_session.flush()
             if new_repo_alert_checked:
-                message = 'You will receive email alerts for all new valid tool shed repositories.'
+                message = "You will receive email alerts for all new valid tool shed repositories."
             else:
-                message = 'You will not receive any email alerts for new valid tool shed repositories.'
+                message = "You will not receive any email alerts for new valid tool shed repositories."
         checked = new_repo_alert_checked or (user and user.new_repo_alert)
-        new_repo_alert_check_box = CheckboxField('new_repo_alert', value=checked)
+        new_repo_alert_check_box = CheckboxField("new_repo_alert", value=checked)
         email_alert_repositories = []
-        for repository in trans.sa_session.query(trans.model.Repository) \
-                                          .filter(and_(trans.model.Repository.table.c.deleted == false(),
-                                                       trans.model.Repository.table.c.email_alerts != null())) \
-                                          .order_by(trans.model.Repository.table.c.name):
+        for repository in (
+            trans.sa_session.query(trans.model.Repository)
+            .filter(
+                and_(
+                    trans.model.Repository.table.c.deleted == false(),
+                    trans.model.Repository.table.c.email_alerts != null(),
+                )
+            )
+            .order_by(trans.model.Repository.table.c.name)
+        ):
             if user.email in repository.email_alerts:
                 email_alert_repositories.append(repository)
-        return trans.fill_template("/webapps/tool_shed/user/manage_email_alerts.mako",
-                                   new_repo_alert_check_box=new_repo_alert_check_box,
-                                   email_alert_repositories=email_alert_repositories,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/user/manage_email_alerts.mako",
+            new_repo_alert_check_box=new_repo_alert_check_box,
+            email_alert_repositories=email_alert_repositories,
+            message=message,
+            status=status,
+        )
 
     @web.expose
-    @web.require_login("manage repository")
+    @require_login("manage repository")
     def manage_repository(self, trans, id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
-        repository_type = kwd.get('repository_type', str(repository.type))
+        repository_type = kwd.get("repository_type", str(repository.type))
         repo = repository.hg_repo
-        repo_name = kwd.get('repo_name', repository.name)
-        changeset_revision = kwd.get('changeset_revision', repository.tip())
-        repository.share_url = repository_util.generate_sharable_link_for_repository_in_tool_shed(repository, changeset_revision=changeset_revision)
+        repo_name = kwd.get("repo_name", repository.name)
+        changeset_revision = kwd.get("changeset_revision", repository.tip())
+        repository.share_url = repository_util.generate_sharable_link_for_repository_in_tool_shed(
+            repository, changeset_revision=changeset_revision
+        )
         repository.clone_url = common_util.generate_clone_url_for_repository_in_tool_shed(trans.user, repository)
-        remote_repository_url = kwd.get('remote_repository_url', repository.remote_repository_url)
-        homepage_url = kwd.get('homepage_url', repository.homepage_url)
-        description = kwd.get('description', repository.description)
-        long_description = kwd.get('long_description', repository.long_description)
+        remote_repository_url = kwd.get("remote_repository_url", repository.remote_repository_url)
+        homepage_url = kwd.get("homepage_url", repository.homepage_url)
+        description = kwd.get("description", repository.description)
+        long_description = kwd.get("long_description", repository.long_description)
         avg_rating, num_ratings = self.get_ave_item_rating_data(trans.sa_session, repository, webapp_model=trans.model)
-        display_reviews = util.string_as_bool(kwd.get('display_reviews', False))
-        alerts = kwd.get('alerts', '')
+        alerts = kwd.get("alerts", "")
         alerts_checked = CheckboxField.is_checked(alerts)
-        category_ids = util.listify(kwd.get('category_id', ''))
+        category_ids = util.listify(kwd.get("category_id", ""))
         if repository.email_alerts:
             email_alerts = json.loads(repository.email_alerts)
         else:
             email_alerts = []
-        allow_push = kwd.get('allow_push', '')
+        allow_push = kwd.get("allow_push", "")
         error = False
         user = trans.user
-        if kwd.get('edit_repository_button', False):
+        if kwd.get("edit_repository_button", False):
             update_kwds = dict(
                 name=repo_name,
                 description=description,
                 long_description=long_description,
                 remote_repository_url=remote_repository_url,
                 homepage_url=homepage_url,
                 type=repository_type,
             )
 
             repository, message = repository_util.update_repository(app=trans.app, trans=trans, id=id, **update_kwds)
             if repository is None:
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_repository',
-                                                                id=id,
-                                                                message=message,
-                                                                status='error'))
+                return trans.response.send_redirect(
+                    web.url_for(
+                        controller="repository", action="view_repository", id=id, message=message, status="error"
+                    )
+                )
 
-        elif kwd.get('manage_categories_button', False):
+        elif kwd.get("manage_categories_button", False):
             flush_needed = False
             # Delete all currently existing categories.
             for rca in repository.categories:
                 trans.sa_session.delete(rca)
                 trans.sa_session.flush()
             if category_ids:
                 # Create category associations
                 for category_id in category_ids:
                     category = trans.sa_session.query(trans.model.Category).get(trans.security.decode_id(category_id))
                     rca = trans.app.model.RepositoryCategoryAssociation(repository, category)
                     trans.sa_session.add(rca)
                     trans.sa_session.flush()
             message = "The repository information has been updated."
-        elif kwd.get('user_access_button', False):
-            if allow_push not in ['none']:
-                remove_auth = kwd.get('remove_auth', '')
+        elif kwd.get("user_access_button", False):
+            if allow_push not in ["none"]:
+                remove_auth = kwd.get("remove_auth", "")
                 if remove_auth:
-                    usernames = ''
+                    usernames = ""
                 else:
                     user_ids = util.listify(allow_push)
                     usernames = []
                     for user_id in user_ids:
                         user = trans.sa_session.query(trans.model.User).get(trans.security.decode_id(user_id))
                         usernames.append(user.username)
-                    usernames = ','.join(usernames)
+                    usernames = ",".join(usernames)
                 repository.set_allow_push(usernames, remove_auth=remove_auth)
             message = "The repository information has been updated."
-        elif kwd.get('receive_email_alerts_button', False):
+        elif kwd.get("receive_email_alerts_button", False):
             flush_needed = False
             if alerts_checked:
                 if user.email not in email_alerts:
                     email_alerts.append(user.email)
                     repository.email_alerts = json.dumps(email_alerts)
                     flush_needed = True
             else:
@@ -1723,298 +1747,305 @@
                     repository.email_alerts = json.dumps(email_alerts)
                     flush_needed = True
             if flush_needed:
                 trans.sa_session.add(repository)
                 trans.sa_session.flush()
             message = "The repository information has been updated."
         if error:
-            status = 'error'
-        allow_push_select_field = SelectField(name='allow_push',
-                                              multiple=True)
+            status = "error"
+        allow_push_select_field = SelectField(name="allow_push", multiple=True)
         current_allow_push = repository.allow_push()
         if current_allow_push:
-            current_allow_push_list = current_allow_push.split(',')
+            current_allow_push_list = current_allow_push.split(",")
         else:
             current_allow_push_list = []
         options = []
         for user in trans.sa_session.query(trans.model.User):
             if user.username not in current_allow_push_list:
                 options.append(user)
         for obj in options:
-            label = getattr(obj, 'username')
+            label = obj.username
             allow_push_select_field.add_option(label, trans.security.encode_id(obj.id))
         checked = alerts_checked or user.email in email_alerts
-        alerts_check_box = CheckboxField('alerts', value=checked)
-        changeset_revision_select_field = grids_util.build_changeset_revision_select_field(trans,
-                                                                                           repository,
-                                                                                           selected_value=changeset_revision,
-                                                                                           add_id_to_name=False,
-                                                                                           downloadable=False)
+        alerts_check_box = CheckboxField("alerts", value=checked)
+        changeset_revision_select_field = grids_util.build_changeset_revision_select_field(
+            trans, repository, selected_value=changeset_revision, add_id_to_name=False, downloadable=False
+        )
         revision_label = hg_util.get_revision_label(trans.app, repository, repository.tip(), include_date=False)
         repository_metadata = None
         metadata = None
         is_malicious = False
         repository_dependencies = None
         if changeset_revision != hg_util.INITIAL_CHANGELOG_HASH:
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, id, changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                trans.app, id, changeset_revision
+            )
             if repository_metadata:
-                revision_label = hg_util.get_revision_label(trans.app, repository, changeset_revision, include_date=False)
+                revision_label = hg_util.get_revision_label(
+                    trans.app, repository, changeset_revision, include_date=False
+                )
                 metadata = repository_metadata.metadata
                 is_malicious = repository_metadata.malicious
             else:
                 # There is no repository_metadata defined for the changeset_revision, so see if it was defined in a previous
                 # changeset in the changelog.
-                previous_changeset_revision = \
-                    metadata_util.get_previous_metadata_changeset_revision(trans.app, repository, changeset_revision, downloadable=False)
+                previous_changeset_revision = metadata_util.get_previous_metadata_changeset_revision(
+                    trans.app, repository, changeset_revision, downloadable=False
+                )
                 if previous_changeset_revision != hg_util.INITIAL_CHANGELOG_HASH:
-                    repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, id, previous_changeset_revision)
+                    repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                        trans.app, id, previous_changeset_revision
+                    )
                     if repository_metadata:
-                        revision_label = hg_util.get_revision_label(trans.app, repository, previous_changeset_revision, include_date=False)
+                        revision_label = hg_util.get_revision_label(
+                            trans.app, repository, previous_changeset_revision, include_date=False
+                        )
                         metadata = repository_metadata.metadata
                         is_malicious = repository_metadata.malicious
                         changeset_revision = previous_changeset_revision
             if repository_metadata:
                 metadata = repository_metadata.metadata
                 # Get a dictionary of all repositories upon which the contents of the current repository_metadata record depend.
-                toolshed_base_url = str(web.url_for('/', qualified=True)).rstrip('/')
+                toolshed_base_url = str(web.url_for("/", qualified=True)).rstrip("/")
                 rb = relation_builder.RelationBuilder(trans.app, repository, repository_metadata, toolshed_base_url)
                 repository_dependencies = rb.get_repository_dependencies_for_changeset_revision()
                 if str(repository.type) != rt_util.REPOSITORY_SUITE_DEFINITION:
                     # Handle messaging for resetting repository type to the optimal value.
-                    change_repository_type_message = rt_util.generate_message_for_repository_type_change(trans.app,
-                                                                                                         repository)
+                    change_repository_type_message = rt_util.generate_message_for_repository_type_change(
+                        trans.app, repository
+                    )
                     if change_repository_type_message:
                         message += change_repository_type_message
-                        status = 'warning'
+                        status = "warning"
                 elif str(repository.type) != rt_util.TOOL_DEPENDENCY_DEFINITION:
                     # Handle messaging for resetting repository type to the optimal value.
-                    change_repository_type_message = rt_util.generate_message_for_repository_type_change(trans.app,
-                                                                                                         repository)
+                    change_repository_type_message = rt_util.generate_message_for_repository_type_change(
+                        trans.app, repository
+                    )
                     if change_repository_type_message:
                         message += change_repository_type_message
-                        status = 'warning'
+                        status = "warning"
                     else:
                         # Handle messaging for orphan tool dependency definitions.
                         dd = dependency_display.DependencyDisplayer(trans.app)
                         orphan_message = dd.generate_message_for_orphan_tool_dependencies(repository, metadata)
                         if orphan_message:
                             message += orphan_message
-                            status = 'warning'
+                            status = "warning"
         if is_malicious:
             if trans.app.security_agent.can_push(trans.app, trans.user, repository):
                 message += malicious_error_can_push
             else:
                 message += malicious_error
-            status = 'error'
+            status = "error"
         repository_type_select_field = rt_util.build_repository_type_select_field(trans, repository=repository)
-        malicious_check_box = CheckboxField('malicious', value=is_malicious)
+        malicious_check_box = CheckboxField("malicious", value=is_malicious)
         categories = suc.get_categories(trans.app)
         selected_categories = [_rca.category_id for _rca in repository.categories]
         tsucm = ToolShedUtilityContainerManager(trans.app)
-        containers_dict = tsucm.build_repository_containers(repository,
-                                                            changeset_revision,
-                                                            repository_dependencies,
-                                                            repository_metadata)
+        containers_dict = tsucm.build_repository_containers(
+            repository, changeset_revision, repository_dependencies, repository_metadata
+        )
         heads = hg_util.get_repository_heads(repo)
-        deprecated_repository_dependency_tups = \
-            metadata_util.get_repository_dependency_tups_from_repository_metadata(trans.app,
-                                                                                  repository_metadata,
-                                                                                  deprecated_only=True)
-        return trans.fill_template('/webapps/tool_shed/repository/manage_repository.mako',
-                                   repo_name=repo_name,
-                                   remote_repository_url=remote_repository_url,
-                                   homepage_url=homepage_url,
-                                   description=description,
-                                   long_description=long_description,
-                                   current_allow_push_list=current_allow_push_list,
-                                   allow_push_select_field=allow_push_select_field,
-                                   deprecated_repository_dependency_tups=deprecated_repository_dependency_tups,
-                                   repo=repo,
-                                   heads=heads,
-                                   repository=repository,
-                                   containers_dict=containers_dict,
-                                   repository_metadata=repository_metadata,
-                                   changeset_revision=changeset_revision,
-                                   changeset_revision_select_field=changeset_revision_select_field,
-                                   revision_label=revision_label,
-                                   selected_categories=selected_categories,
-                                   categories=categories,
-                                   metadata=metadata,
-                                   avg_rating=avg_rating,
-                                   display_reviews=display_reviews,
-                                   num_ratings=num_ratings,
-                                   alerts_check_box=alerts_check_box,
-                                   malicious_check_box=malicious_check_box,
-                                   repository_type_select_field=repository_type_select_field,
-                                   message=message,
-                                   status=status)
+        deprecated_repository_dependency_tups = metadata_util.get_repository_dependency_tups_from_repository_metadata(
+            trans.app, repository_metadata, deprecated_only=True
+        )
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/manage_repository.mako",
+            repo_name=repo_name,
+            remote_repository_url=remote_repository_url,
+            homepage_url=homepage_url,
+            description=description,
+            long_description=long_description,
+            current_allow_push_list=current_allow_push_list,
+            allow_push_select_field=allow_push_select_field,
+            deprecated_repository_dependency_tups=deprecated_repository_dependency_tups,
+            repo=repo,
+            heads=heads,
+            repository=repository,
+            containers_dict=containers_dict,
+            repository_metadata=repository_metadata,
+            changeset_revision=changeset_revision,
+            changeset_revision_select_field=changeset_revision_select_field,
+            revision_label=revision_label,
+            selected_categories=selected_categories,
+            categories=categories,
+            metadata=metadata,
+            avg_rating=avg_rating,
+            num_ratings=num_ratings,
+            alerts_check_box=alerts_check_box,
+            malicious_check_box=malicious_check_box,
+            repository_type_select_field=repository_type_select_field,
+            message=message,
+            status=status,
+        )
 
     @web.expose
-    @web.require_login("manage repository administrators")
+    @require_login("manage repository administrators")
     def manage_repository_admins(self, trans, id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
-        changeset_revision = kwd.get('changeset_revision', repository.tip())
+        changeset_revision = kwd.get("changeset_revision", repository.tip())
         metadata = None
         if changeset_revision != hg_util.INITIAL_CHANGELOG_HASH:
-            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, id, changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                trans.app, id, changeset_revision
+            )
             if repository_metadata:
                 metadata = repository_metadata.metadata
             else:
                 # There is no repository_metadata defined for the changeset_revision, so see if it was defined
                 # in a previous changeset in the changelog.
-                previous_changeset_revision = \
-                    metadata_util.get_previous_metadata_changeset_revision(trans.app, repository, changeset_revision, downloadable=False)
+                previous_changeset_revision = metadata_util.get_previous_metadata_changeset_revision(
+                    trans.app, repository, changeset_revision, downloadable=False
+                )
                 if previous_changeset_revision != hg_util.INITIAL_CHANGELOG_HASH:
-                    repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app,
-                                                                                                      id,
-                                                                                                      previous_changeset_revision)
+                    repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+                        trans.app, id, previous_changeset_revision
+                    )
                     if repository_metadata:
                         metadata = repository_metadata.metadata
         role = repository.admin_role
-        associations_dict = repository_util.handle_role_associations(trans.app,
-                                                                     role,
-                                                                     repository,
-                                                                     **kwd)
-        in_users = associations_dict.get('in_users', [])
-        out_users = associations_dict.get('out_users', [])
-        in_groups = associations_dict.get('in_groups', [])
-        out_groups = associations_dict.get('out_groups', [])
-        message = associations_dict.get('message', '')
-        status = associations_dict.get('status', 'done')
-        return trans.fill_template('/webapps/tool_shed/role/role.mako',
-                                   in_admin_controller=False,
-                                   repository=repository,
-                                   metadata=metadata,
-                                   changeset_revision=changeset_revision,
-                                   role=role,
-                                   in_users=in_users,
-                                   out_users=out_users,
-                                   in_groups=in_groups,
-                                   out_groups=out_groups,
-                                   message=message,
-                                   status=status)
-
-    @web.expose
-    @web.require_login("review repository revision")
-    def manage_repository_reviews_of_revision(self, trans, **kwd):
-        return trans.response.send_redirect(web.url_for(controller='repository_review',
-                                                        action='manage_repository_reviews_of_revision',
-                                                        **kwd))
+        associations_dict = repository_util.handle_role_associations(trans.app, role, repository, **kwd)
+        in_users = associations_dict.get("in_users", [])
+        out_users = associations_dict.get("out_users", [])
+        in_groups = associations_dict.get("in_groups", [])
+        out_groups = associations_dict.get("out_groups", [])
+        message = associations_dict.get("message", "")
+        status = associations_dict.get("status", "done")
+        return trans.fill_template(
+            "/webapps/tool_shed/role/role.mako",
+            in_admin_controller=False,
+            repository=repository,
+            metadata=metadata,
+            changeset_revision=changeset_revision,
+            role=role,
+            in_users=in_users,
+            out_users=out_users,
+            in_groups=in_groups,
+            out_groups=out_groups,
+            message=message,
+            status=status,
+        )
 
     @web.expose
-    @web.require_login("multi select email alerts")
+    @require_login("multi select email alerts")
     def multi_select_email_alerts(self, trans, **kwd):
-        if 'operation' in kwd:
-            operation = kwd['operation'].lower()
+        if "operation" in kwd:
+            operation = kwd["operation"].lower()
             if operation == "receive email alerts":
                 if trans.user:
-                    if kwd['id']:
-                        kwd['caller'] = 'multi_select_email_alerts'
-                        return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                        action='set_email_alerts',
-                                                                        **kwd))
+                    if kwd["id"]:
+                        kwd["caller"] = "multi_select_email_alerts"
+                        return trans.response.send_redirect(
+                            web.url_for(controller="repository", action="set_email_alerts", **kwd)
+                        )
                 else:
-                    kwd['message'] = 'You must be logged in to set email alerts.'
-                    kwd['status'] = 'error'
-                    del kwd['operation']
+                    kwd["message"] = "You must be logged in to set email alerts."
+                    kwd["status"] = "error"
+                    del kwd["operation"]
             elif operation == "view_or_manage_repository":
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='view_or_manage_repository',
-                                                                **kwd))
+                return trans.response.send_redirect(
+                    web.url_for(controller="repository", action="view_or_manage_repository", **kwd)
+                )
         self.email_alerts_repository_grid.title = "Set email alerts for repository changes"
         return self.email_alerts_repository_grid(trans, **kwd)
 
     @web.expose
     def next_installable_changeset_revision(self, trans, **kwd):
         """
         Handle a request from a Galaxy instance where the changeset_revision defined for a repository
         in a dependency definition file is older than the changeset_revision associated with the installed
         repository.
         """
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
-        changeset_revision = kwd.get('changeset_revision', None)
+        name = kwd.get("name", None)
+        owner = kwd.get("owner", None)
+        changeset_revision = kwd.get("changeset_revision", None)
         repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
         # Get the next installable changeset_revision beyond the received changeset_revision.
-        next_changeset_revision = metadata_util.get_next_downloadable_changeset_revision(trans.app, repository, changeset_revision)
+        next_changeset_revision = metadata_util.get_next_downloadable_changeset_revision(
+            trans.app, repository, changeset_revision
+        )
         if next_changeset_revision and next_changeset_revision != changeset_revision:
             return next_changeset_revision
-        return ''
+        return ""
 
     @web.json
     @web.do_not_cache
     def open_folder(self, trans, folder_path, repository_id):
         is_admin = trans.user_is_admin
         return suc.open_repository_files_folder(trans.app, folder_path, repository_id, is_admin)
 
     @web.expose
     def preview_tools_in_changeset(self, trans, repository_id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
-        changeset_revision = kwd.get('changeset_revision', repository.tip())
+        changeset_revision = kwd.get("changeset_revision", repository.tip())
         self.validate_changeset_revision(trans, changeset_revision, repository_id)
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, repository_id, changeset_revision)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, repository_id, changeset_revision
+        )
         if repository_metadata:
-            repository_metadata_id = trans.security.encode_id(repository_metadata.id),
+            repository_metadata_id = (trans.security.encode_id(repository_metadata.id),)
             metadata = repository_metadata.metadata
             # Get a dictionary of all repositories upon which the contents of the current repository_metadata record depend.
-            toolshed_base_url = str(web.url_for('/', qualified=True)).rstrip('/')
+            toolshed_base_url = str(web.url_for("/", qualified=True)).rstrip("/")
             rb = relation_builder.RelationBuilder(trans.app, repository, repository_metadata, toolshed_base_url)
             repository_dependencies = rb.get_repository_dependencies_for_changeset_revision()
             if metadata:
-                if 'repository_dependencies' in metadata and not repository_dependencies:
+                if "repository_dependencies" in metadata and not repository_dependencies:
                     # See if we have an invalid repository dependency definition or if the repository dependency is required
                     # only for compiling the repository's tool dependency.
                     invalid = False
-                    repository_dependencies_dict = metadata['repository_dependencies']
-                    rd_tups = repository_dependencies_dict.get('repository_dependencies', [])
+                    repository_dependencies_dict = metadata["repository_dependencies"]
+                    rd_tups = repository_dependencies_dict.get("repository_dependencies", [])
                     for rd_tup in rd_tups:
-                        rdtool_shed, \
-                            rd_name, \
-                            rd_owner, \
-                            rd_changeset_revision, \
-                            rd_prior_installation_required, \
-                            rd_only_if_compiling_contained_td = \
-                            common_util.parse_repository_dependency_tuple(rd_tup)
+                        (
+                            rdtool_shed,
+                            rd_name,
+                            rd_owner,
+                            rd_changeset_revision,
+                            rd_prior_installation_required,
+                            rd_only_if_compiling_contained_td,
+                        ) = common_util.parse_repository_dependency_tuple(rd_tup)
                         if not util.asbool(rd_only_if_compiling_contained_td):
                             invalid = True
                             break
                     if invalid:
                         dd = dependency_display.DependencyDisplayer(trans.app)
-                        message = dd.generate_message_for_invalid_repository_dependencies(metadata,
-                                                                                          error_from_tuple=False)
-                        status = 'error'
+                        message = dd.generate_message_for_invalid_repository_dependencies(
+                            metadata, error_from_tuple=False
+                        )
+                        status = "error"
         else:
             repository_metadata_id = None
             metadata = None
             repository_dependencies = None
         revision_label = hg_util.get_revision_label(trans.app, repository, changeset_revision, include_date=True)
-        changeset_revision_select_field = grids_util.build_changeset_revision_select_field(trans,
-                                                                                           repository,
-                                                                                           selected_value=changeset_revision,
-                                                                                           add_id_to_name=False,
-                                                                                           downloadable=False)
+        changeset_revision_select_field = grids_util.build_changeset_revision_select_field(
+            trans, repository, selected_value=changeset_revision, add_id_to_name=False, downloadable=False
+        )
         tsucm = ToolShedUtilityContainerManager(trans.app)
-        containers_dict = tsucm.build_repository_containers(repository,
-                                                            changeset_revision,
-                                                            repository_dependencies,
-                                                            repository_metadata)
-        return trans.fill_template('/webapps/tool_shed/repository/preview_tools_in_changeset.mako',
-                                   repository=repository,
-                                   containers_dict=containers_dict,
-                                   repository_metadata_id=repository_metadata_id,
-                                   changeset_revision=changeset_revision,
-                                   revision_label=revision_label,
-                                   changeset_revision_select_field=changeset_revision_select_field,
-                                   metadata=metadata,
-                                   message=message,
-                                   status=status)
+        containers_dict = tsucm.build_repository_containers(
+            repository, changeset_revision, repository_dependencies, repository_metadata
+        )
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/preview_tools_in_changeset.mako",
+            repository=repository,
+            containers_dict=containers_dict,
+            repository_metadata_id=repository_metadata_id,
+            changeset_revision=changeset_revision,
+            revision_label=revision_label,
+            changeset_revision_select_field=changeset_revision_select_field,
+            metadata=metadata,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def previous_changeset_revisions(self, trans, from_tip=False, **kwd):
         """
         Handle a request from a local Galaxy instance.  This method will handle two scenarios: (1) the
         repository was previously installed using an older changeset_revsion, but later the repository
         was updated in the tool shed and the Galaxy admin is trying to install the latest changeset
@@ -2022,255 +2053,181 @@
         the admin is attempting to get updates for an installed repository that has a repository dependency
         and both the repository and its dependency have available updates.  In this case, the from_tip
         parameter will be True because the repository dependency definition may define a changeset hash
         for the dependency that is newer than the installed changeset revision of the dependency (this is
         due to the behavior of "Tool dependency definition" repositories, whose metadata is always the tip),
         so the complete list of changset hashes in the changelog must be returned.
         """
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
+        name = kwd.get("name", None)
+        owner = kwd.get("owner", None)
         if name is not None and owner is not None:
             repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
             from_tip = util.string_as_bool(from_tip)
             if from_tip:
                 changeset_revision = repository.tip()
             else:
-                changeset_revision = kwd.get('changeset_revision', None)
+                changeset_revision = kwd.get("changeset_revision", None)
             if changeset_revision is not None:
                 repo = repository.hg_repo
                 # Get the lower bound changeset revision.
-                lower_bound_changeset_revision = \
-                    metadata_util.get_previous_metadata_changeset_revision(trans.app, repository, changeset_revision, downloadable=True)
+                lower_bound_changeset_revision = metadata_util.get_previous_metadata_changeset_revision(
+                    trans.app, repository, changeset_revision, downloadable=True
+                )
                 # Build the list of changeset revision hashes.
                 changeset_hashes = []
-                for changeset in hg_util.reversed_lower_upper_bounded_changelog(repo,
-                                                                                lower_bound_changeset_revision,
-                                                                                changeset_revision):
+                for changeset in hg_util.reversed_lower_upper_bounded_changelog(
+                    repo, lower_bound_changeset_revision, changeset_revision
+                ):
                     changeset_hashes.append(str(repo[changeset]))
                 if changeset_hashes:
-                    changeset_hashes_str = ','.join(changeset_hashes)
+                    changeset_hashes_str = ",".join(changeset_hashes)
                     return changeset_hashes_str
-        return ''
+        return ""
 
     @web.expose
-    @web.require_login("rate repositories")
+    @require_login("rate repositories")
     def rate_repository(self, trans, **kwd):
-        """ Rate a repository and return updated rating data. """
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        id = kwd.get('id', None)
+        """Rate a repository and return updated rating data."""
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        id = kwd.get("id", None)
         if not id:
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            message='Select a repository to rate',
-                                                            status='error'))
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="browse_repositories",
+                    message="Select a repository to rate",
+                    status="error",
+                )
+            )
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
         changeset_revision = repository.tip()
         if repository.user == trans.user:
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='browse_repositories',
-                                                            message="You are not allowed to rate your own repository",
-                                                            status='error'))
-        if kwd.get('rate_button', False):
-            rating = int(kwd.get('rating', '0'))
-            comment = kwd.get('comment', '')
+            return trans.response.send_redirect(
+                web.url_for(
+                    controller="repository",
+                    action="browse_repositories",
+                    message="You are not allowed to rate your own repository",
+                    status="error",
+                )
+            )
+        if kwd.get("rate_button", False):
+            rating = int(kwd.get("rating", "0"))
+            comment = kwd.get("comment", "")
             rating = self.rate_item(trans, trans.user, repository, rating, comment)
         avg_rating, num_ratings = self.get_ave_item_rating_data(trans.sa_session, repository, webapp_model=trans.model)
-        display_reviews = util.string_as_bool(kwd.get('display_reviews', False))
         rra = self.get_user_item_rating(trans.sa_session, trans.user, repository, webapp_model=trans.model)
-        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                             id,
-                                                                                             changeset_revision,
-                                                                                             metadata_only=True)
+        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+            trans.app, id, changeset_revision, metadata_only=True
+        )
         repository_type_select_field = rt_util.build_repository_type_select_field(trans, repository=repository)
         revision_label = hg_util.get_revision_label(trans.app, repository, changeset_revision, include_date=True)
-        return trans.fill_template('/webapps/tool_shed/repository/rate_repository.mako',
-                                   repository=repository,
-                                   metadata=metadata,
-                                   revision_label=revision_label,
-                                   avg_rating=avg_rating,
-                                   display_reviews=display_reviews,
-                                   num_ratings=num_ratings,
-                                   rra=rra,
-                                   repository_type_select_field=repository_type_select_field,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/rate_repository.mako",
+            repository=repository,
+            metadata=metadata,
+            revision_label=revision_label,
+            avg_rating=avg_rating,
+            num_ratings=num_ratings,
+            rra=rra,
+            repository_type_select_field=repository_type_select_field,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def reset_all_metadata(self, trans, id, **kwd):
         """Reset all metadata on the complete changelog for a single repository in the tool shed."""
         # This method is called only from the ~/templates/webapps/tool_shed/repository/manage_repository.mako template.
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
-        rmm = repository_metadata_manager.RepositoryMetadataManager(app=trans.app,
-                                                                    user=trans.user,
-                                                                    repository=repository,
-                                                                    resetting_all_metadata_on_repository=True)
+        rmm = repository_metadata_manager.RepositoryMetadataManager(
+            app=trans.app, user=trans.user, repository=repository, resetting_all_metadata_on_repository=True
+        )
         rmm.reset_all_metadata_on_repository_in_tool_shed()
         rmm_metadata_dict = rmm.get_metadata_dict()
         rmm_invalid_file_tups = rmm.get_invalid_file_tups()
         if rmm_invalid_file_tups:
-            message = tool_util.generate_message_for_invalid_tools(trans.app,
-                                                                   rmm_invalid_file_tups,
-                                                                   repository,
-                                                                   rmm_metadata_dict)
-            status = 'error'
+            message = tool_util.generate_message_for_invalid_tools(
+                trans.app, rmm_invalid_file_tups, repository, rmm_metadata_dict
+            )
+            status = "error"
         else:
             message = "All repository metadata has been reset.  "
-            status = 'done'
-        return trans.response.send_redirect(web.url_for(controller='repository',
-                                                        action='manage_repository',
-                                                        id=id,
-                                                        message=message,
-                                                        status=status))
+            status = "done"
+        return trans.response.send_redirect(
+            web.url_for(controller="repository", action="manage_repository", id=id, message=message, status=status)
+        )
 
     @web.expose
     def reset_metadata_on_my_writable_repositories_in_tool_shed(self, trans, **kwd):
-        rmm = repository_metadata_manager.RepositoryMetadataManager(trans.app, trans.user, resetting_all_metadata_on_repository=True)
-        if 'reset_metadata_on_selected_repositories_button' in kwd:
+        rmm = repository_metadata_manager.RepositoryMetadataManager(
+            trans.app, trans.user, resetting_all_metadata_on_repository=True
+        )
+        if "reset_metadata_on_selected_repositories_button" in kwd:
             message, status = rmm.reset_metadata_on_selected_repositories(**kwd)
         else:
-            message = escape(kwd.get('message', ''))
-            status = kwd.get('status', 'done')
-        repositories_select_field = rmm.build_repository_ids_select_field(name='repository_ids',
-                                                                          multiple=True,
-                                                                          display='checkboxes',
-                                                                          my_writable=True)
-        return trans.fill_template('/webapps/tool_shed/common/reset_metadata_on_selected_repositories.mako',
-                                   repositories_select_field=repositories_select_field,
-                                   message=message,
-                                   status=status)
-
-    @web.expose
-    def select_files_to_delete(self, trans, id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        commit_message = escape(kwd.get('commit_message', 'Deleted selected files'))
-        repository = repository_util.get_repository_in_tool_shed(trans.app, id)
-        repo_dir = repository.repo_path(trans.app)
-        repo = repository.hg_repo
-        selected_files_to_delete = kwd.get('selected_files_to_delete', '')
-        if kwd.get('select_files_to_delete_button', False):
-            if selected_files_to_delete:
-                selected_files_to_delete = selected_files_to_delete.split(',')
-                # Get the current repository tip.
-                tip = repository.tip()
-                for selected_file in selected_files_to_delete:
-                    try:
-                        hg_util.remove_file(repo_dir, selected_file, force=True)
-                    except Exception as e:
-                        log.debug("Error removing the following file using the mercurial API:\n %s", selected_file)
-                        log.debug("The error was: %s", util.unicodify(e))
-                        log.debug("Attempting to remove the file using a different approach.")
-                        relative_selected_file = selected_file.split('repo_%d' % repository.id)[1].lstrip('/')
-                        repo.dirstate.remove(relative_selected_file)
-                        repo.dirstate.write()
-                        absolute_selected_file = os.path.abspath(selected_file)
-                        if os.path.isdir(absolute_selected_file):
-                            try:
-                                os.rmdir(absolute_selected_file)
-                            except OSError:
-                                # The directory is not empty
-                                pass
-                        elif os.path.isfile(absolute_selected_file):
-                            os.remove(absolute_selected_file)
-                            dir = os.path.split(absolute_selected_file)[0]
-                            try:
-                                os.rmdir(dir)
-                            except OSError:
-                                # The directory is not empty
-                                pass
-                # Commit the change set.
-                if not commit_message:
-                    commit_message = 'Deleted selected files'
-                hg_util.commit_changeset(repo_dir,
-                                         full_path_to_changeset=repo_dir,
-                                         username=trans.user.username,
-                                         message=commit_message)
-                suc.handle_email_alerts(trans.app, trans.request.host, repository)
-                # Update the repository files for browsing.
-                hg_util.update_repository(repo_dir)
-                # Get the new repository tip.
-                if tip == repository.tip():
-                    message += 'No changes to repository.  '
-                else:
-                    rmm = repository_metadata_manager.RepositoryMetadataManager(app=trans.app,
-                                                                                user=trans.user,
-                                                                                repository=repository)
-                    status, error_message = rmm.set_repository_metadata_due_to_new_tip(trans.request.host, **kwd)
-                    if error_message:
-                        message = error_message
-                    else:
-                        message += 'The selected files were deleted from the repository.  '
-            else:
-                message = "Select at least 1 file to delete from the repository before clicking <b>Delete selected files</b>."
-                status = "error"
-        repository_type_select_field = rt_util.build_repository_type_select_field(trans, repository=repository)
-        changeset_revision = repository.tip()
-        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                             id,
-                                                                                             changeset_revision,
-                                                                                             metadata_only=True)
-        return trans.fill_template('/webapps/tool_shed/repository/browse_repository.mako',
-                                   repo=repo,
-                                   repository=repository,
-                                   changeset_revision=changeset_revision,
-                                   metadata=metadata,
-                                   commit_message=commit_message,
-                                   repository_type_select_field=repository_type_select_field,
-                                   message=message,
-                                   status=status)
+            message = escape(kwd.get("message", ""))
+            status = kwd.get("status", "done")
+        repositories_select_field = rmm.build_repository_ids_select_field(
+            name="repository_ids", multiple=True, display="checkboxes", my_writable=True
+        )
+        return trans.fill_template(
+            "/webapps/tool_shed/common/reset_metadata_on_selected_repositories.mako",
+            repositories_select_field=repositories_select_field,
+            message=message,
+            status=status,
+        )
 
     @web.expose
-    def send_to_owner(self, trans, id, message=''):
+    def send_to_owner(self, trans, id, message=""):
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
         if not message:
-            message = 'Enter a message'
-            status = 'error'
+            message = "Enter a message"
+            status = "error"
         elif trans.user and trans.user.email:
             smtp_server = trans.app.config.smtp_server
             from_address = trans.app.config.email_from
             if smtp_server is None or from_address is None:
                 return trans.show_error_message("Mail is not configured for this Galaxy tool shed instance")
             to_address = repository.user.email
             # Get the name of the server hosting the tool shed instance.
             host = trans.request.host
             # Build the email message
-            body = string.Template(suc.contact_owner_template) \
-                .safe_substitute(username=trans.user.username,
-                                 repository_name=repository.name,
-                                 email=trans.user.email,
-                                 message=message,
-                                 host=host)
-            subject = "Regarding your tool shed repository named %s" % repository.name
+            body = string.Template(suc.contact_owner_template).safe_substitute(
+                username=trans.user.username,
+                repository_name=repository.name,
+                email=trans.user.email,
+                message=message,
+                host=host,
+            )
+            subject = f"Regarding your tool shed repository named {repository.name}"
             # Send it
             try:
                 util.send_mail(from_address, to_address, subject, body, trans.app.config)
                 message = "Your message has been sent"
                 status = "done"
             except Exception as e:
-                message = "An error occurred sending your message by email: %s" % util.unicodify(e)
+                message = f"An error occurred sending your message by email: {util.unicodify(e)}"
                 status = "error"
         else:
             # Do all we can to eliminate spam.
             return trans.show_error_message("You must be logged in to contact the owner of a repository.")
-        return trans.response.send_redirect(web.url_for(controller='repository',
-                                                        action='contact_owner',
-                                                        id=id,
-                                                        message=message,
-                                                        status=status))
+        return trans.response.send_redirect(
+            web.url_for(controller="repository", action="contact_owner", id=id, message=message, status=status)
+        )
 
     @web.expose
-    @web.require_login("set email alerts")
+    @require_login("set email alerts")
     def set_email_alerts(self, trans, **kwd):
         """Set email alerts for selected repositories."""
         # This method is called from multiple grids, so the caller must be passed.
-        caller = kwd['caller']
+        caller = kwd["caller"]
         user = trans.user
         if user:
-            repository_ids = util.listify(kwd.get('id', ''))
+            repository_ids = util.listify(kwd.get("id", ""))
             total_alerts_added = 0
             total_alerts_removed = 0
             flush_needed = False
             for repository_id in repository_ids:
                 repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
                 if repository.email_alerts:
                     email_alerts = json.loads(repository.email_alerts)
@@ -2286,298 +2243,320 @@
                     email_alerts.append(user.email)
                     repository.email_alerts = json.dumps(email_alerts)
                     trans.sa_session.add(repository)
                     flush_needed = True
                     total_alerts_added += 1
             if flush_needed:
                 trans.sa_session.flush()
-            message = 'Total alerts added: %d, total alerts removed: %d' % (total_alerts_added, total_alerts_removed)
-            kwd['message'] = message
-            kwd['status'] = 'done'
-        del kwd['operation']
-        return trans.response.send_redirect(web.url_for(controller='repository',
-                                                        action=caller,
-                                                        **kwd))
+            message = "Total alerts added: %d, total alerts removed: %d" % (total_alerts_added, total_alerts_removed)
+            kwd["message"] = message
+            kwd["status"] = "done"
+        del kwd["operation"]
+        return trans.response.send_redirect(web.url_for(controller="repository", action=caller, **kwd))
 
     @web.expose
-    @web.require_login("set repository as malicious")
+    @require_login("set repository as malicious")
     def set_malicious(self, trans, id, ctx_str, **kwd):
-        malicious = kwd.get('malicious', '')
-        if kwd.get('malicious_button', False):
+        malicious = kwd.get("malicious", "")
+        if kwd.get("malicious_button", False):
             repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, id, ctx_str)
             malicious_checked = CheckboxField.is_checked(malicious)
             repository_metadata.malicious = malicious_checked
             trans.sa_session.add(repository_metadata)
             trans.sa_session.flush()
             if malicious_checked:
                 message = "The repository tip has been defined as malicious."
             else:
                 message = "The repository tip has been defined as <b>not</b> malicious."
-            status = 'done'
-        return trans.response.send_redirect(web.url_for(controller='repository',
-                                                        action='manage_repository',
-                                                        id=id,
-                                                        changeset_revision=ctx_str,
-                                                        malicious=malicious,
-                                                        message=message,
-                                                        status=status))
+            status = "done"
+        return trans.response.send_redirect(
+            web.url_for(
+                controller="repository",
+                action="manage_repository",
+                id=id,
+                changeset_revision=ctx_str,
+                malicious=malicious,
+                message=message,
+                status=status,
+            )
+        )
 
     @web.expose
     def sharable_owner(self, trans, owner):
         """Support for sharable URL for each repository owner's tools, e.g. http://example.org/view/owner."""
         try:
             user = common_util.get_user_by_username(trans, owner)
         except Exception:
             user = None
         if user:
             user_id = trans.security.encode_id(user.id)
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='index',
-                                                            user_id=user_id))
+            return trans.response.send_redirect(web.url_for(controller="repository", action="index", user_id=user_id))
         else:
-            return trans.show_error_message("The tool shed <b>%s</b> contains no repositories owned by <b>%s</b>." %
-                                            (web.url_for('/', qualified=True).rstrip('/'), str(owner)))
+            return trans.show_error_message(
+                "The tool shed <b>%s</b> contains no repositories owned by <b>%s</b>."
+                % (web.url_for("/", qualified=True).rstrip("/"), str(owner))
+            )
 
     @web.expose
     def sharable_repository(self, trans, owner, name):
         """Support for sharable URL for a specified repository, e.g. http://example.org/view/owner/name."""
         try:
             repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
         except Exception:
             repository = None
         if repository:
             repository_id = trans.security.encode_id(repository.id)
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='index',
-                                                            repository_id=repository_id))
+            return trans.response.send_redirect(
+                web.url_for(controller="repository", action="index", repository_id=repository_id)
+            )
         else:
             # If the owner is valid, then show all of their repositories.
             try:
                 user = common_util.get_user_by_username(trans, owner)
             except Exception:
                 user = None
             if user:
                 user_id = trans.security.encode_id(user.id)
-                message = "This list of repositories owned by <b>%s</b>, does not include one named <b>%s</b>." % (str(owner), str(name))
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='index',
-                                                                user_id=user_id,
-                                                                message=message,
-                                                                status='error'))
-            else:
-                return trans.show_error_message("The tool shed <b>%s</b> contains no repositories named <b>%s</b> with owner <b>%s</b>." %
-                                                (web.url_for('/', qualified=True).rstrip('/'), str(name), str(owner)))
+                message = f"This list of repositories owned by <b>{str(owner)}</b>, does not include one named <b>{str(name)}</b>."
+                return trans.response.send_redirect(
+                    web.url_for(
+                        controller="repository", action="index", user_id=user_id, message=message, status="error"
+                    )
+                )
+            else:
+                return trans.show_error_message(
+                    "The tool shed <b>%s</b> contains no repositories named <b>%s</b> with owner <b>%s</b>."
+                    % (web.url_for("/", qualified=True).rstrip("/"), str(name), str(owner))
+                )
 
     @web.expose
     def sharable_repository_revision(self, trans, owner, name, changeset_revision):
         """Support for sharable URL for a specified repository revision, e.g. http://example.org/view/owner/name/changeset_revision."""
         try:
             repository = repository_util.get_repository_by_name_and_owner(trans.app, name, owner)
         except Exception:
             repository = None
         if repository:
             repository_id = trans.security.encode_id(repository.id)
-            repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                                            repository_id,
-                                                                                                            changeset_revision)
+            repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+                trans.app, repository_id, changeset_revision
+            )
             if not repository_metadata:
                 # Get updates to the received changeset_revision if any exist.
-                upper_bound_changeset_revision = metadata_util.get_next_downloadable_changeset_revision(trans.app, repository, changeset_revision)
+                upper_bound_changeset_revision = metadata_util.get_next_downloadable_changeset_revision(
+                    trans.app, repository, changeset_revision
+                )
                 if upper_bound_changeset_revision and upper_bound_changeset_revision != changeset_revision:
                     changeset_revision = upper_bound_changeset_revision
-                    repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                                                    repository_id,
-                                                                                                                    changeset_revision)
+                    repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+                        trans.app, repository_id, changeset_revision
+                    )
             if repository_metadata:
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='index',
-                                                                repository_id=repository_id,
-                                                                changeset_revision=changeset_revision))
-            else:
-                message = "The change log for the repository named <b>%s</b> owned by <b>%s</b> does not include revision <b>%s</b>." % \
-                    (escape(str(name)), escape(str(owner)), escape(str(changeset_revision)))
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='index',
-                                                                repository_id=repository_id,
-                                                                message=message,
-                                                                status='error'))
+                return trans.response.send_redirect(
+                    web.url_for(
+                        controller="repository",
+                        action="index",
+                        repository_id=repository_id,
+                        changeset_revision=changeset_revision,
+                    )
+                )
+            else:
+                message = (
+                    "The change log for the repository named <b>%s</b> owned by <b>%s</b> does not include revision <b>%s</b>."
+                    % (escape(str(name)), escape(str(owner)), escape(str(changeset_revision)))
+                )
+                return trans.response.send_redirect(
+                    web.url_for(
+                        controller="repository",
+                        action="index",
+                        repository_id=repository_id,
+                        message=message,
+                        status="error",
+                    )
+                )
         else:
             # See if the owner is valid.
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='sharable_owner',
-                                                            owner=owner))
+            return trans.response.send_redirect(
+                web.url_for(controller="repository", action="sharable_owner", owner=owner)
+            )
 
     @web.expose
     def updated_changeset_revisions(self, trans, **kwd):
         """
         Handle a request from a local Galaxy instance to retrieve the list of changeset revisions to which an
         installed repository can be updated.  This method will return a string of comma-separated changeset revision
         hashes for all available updates to the received changeset revision.  Among other things , this method
         handles the scenario where an installed tool shed repository's tool_dependency definition file defines a
         changeset revision for a complex repository dependency that is outdated.  In other words, a defined changeset
         revision is older than the current changeset revision for the required repository, making it impossible to
         discover the repository without knowledge of revisions to which it could have been updated.
         """
-        name = kwd.get('name', None)
-        owner = kwd.get('owner', None)
-        changeset_revision = kwd.get('changeset_revision', None)
+        name = kwd.get("name", None)
+        owner = kwd.get("owner", None)
+        changeset_revision = kwd.get("changeset_revision", None)
         if name and owner and changeset_revision:
             return metadata_util.get_updated_changeset_revisions(trans.app, name, owner, changeset_revision)
-        return ''
+        return ""
 
     @web.expose
     def view_changelog(self, trans, id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
         repo = repository.hg_repo
         changesets = []
         for changeset in repo.changelog:
             ctx = repo[changeset]
             if metadata_util.get_repository_metadata_by_changeset_revision(trans.app, id, str(ctx)):
                 has_metadata = True
             else:
                 has_metadata = False
-            change_dict = {'ctx': ctx,
-                           'rev': str(ctx.rev()),
-                           'date': date,
-                           'display_date': hg_util.get_readable_ctx_date(ctx),
-                           'description': ctx.description(),
-                           'files': ctx.files(),
-                           'user': ctx.user(),
-                           'parent': ctx.parents()[0],
-                           'has_metadata': has_metadata}
+            change_dict = {
+                "ctx": ctx,
+                "rev": str(ctx.rev()),
+                "date": date,
+                "display_date": hg_util.get_readable_ctx_date(ctx),
+                "description": ctx.description(),
+                "files": ctx.files(),
+                "user": ctx.user(),
+                "parent": ctx.parents()[0],
+                "has_metadata": has_metadata,
+            }
             # Make sure we'll view latest changeset first.
             changesets.insert(0, change_dict)
-        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                             id,
-                                                                                             repository.tip(),
-                                                                                             metadata_only=True)
-        return trans.fill_template('/webapps/tool_shed/repository/view_changelog.mako',
-                                   repository=repository,
-                                   metadata=metadata,
-                                   changesets=changesets,
-                                   message=message,
-                                   status=status)
+        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+            trans.app, id, repository.tip(), metadata_only=True
+        )
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/view_changelog.mako",
+            repository=repository,
+            metadata=metadata,
+            changesets=changesets,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def view_changeset(self, trans, id, ctx_str, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
         repo = repository.hg_repo
         ctx = hg_util.get_changectx_for_changeset(repo, ctx_str)
         if ctx is None:
-            message = "Repository does not include changeset revision '%s'." % str(ctx_str)
-            status = 'error'
-            return trans.response.send_redirect(web.url_for(controller='repository',
-                                                            action='view_changelog',
-                                                            id=id,
-                                                            message=message,
-                                                            status=status))
+            message = f"Repository does not include changeset revision '{str(ctx_str)}'."
+            status = "error"
+            return trans.response.send_redirect(
+                web.url_for(controller="repository", action="view_changelog", id=id, message=message, status=status)
+            )
         ctx_parent = ctx.parents()[0]
         if ctx.children():
             ctx_child = ctx.children()[0]
         else:
             ctx_child = None
         diffs = []
         options_dict = get_mercurial_default_options_dict(b"diff")
         # Not quite sure if the following settings make any difference, but with a combination of them and the size check on each
         # diff, we don't run out of memory when viewing the changelog of the cisortho2 repository on the test tool shed.
         options_dict = {util.unicodify(k): util.unicodify(v) for k, v in options_dict.items()}
-        options_dict['maxfile'] = basic_util.MAXDIFFSIZE
-        options_dict['maxtotal'] = basic_util.MAXDIFFSIZE
+        options_dict["maxfile"] = basic_util.MAXDIFFSIZE
+        options_dict["maxtotal"] = basic_util.MAXDIFFSIZE
         diffopts = mdiff.diffopts(**options_dict)
         for diff in patch.diff(repo, node1=ctx_parent.node(), node2=ctx.node(), opts=diffopts):
             if len(diff) > basic_util.MAXDIFFSIZE:
                 diff = util.shrink_string_by_size(diff, basic_util.MAXDIFFSIZE)
             diffs.append(basic_util.to_html_string(diff))
-        modified, added, removed, deleted, unknown, ignored, clean = repo.status(node1=ctx_parent.node(), node2=ctx.node())
+        modified, added, removed, deleted, unknown, ignored, clean = repo.status(
+            node1=ctx_parent.node(), node2=ctx.node()
+        )
         anchors = modified + added + removed + deleted + unknown + ignored + clean
         anchors = util.unicodify(anchors)
-        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(trans.app,
-                                                                                             id,
-                                                                                             ctx_str,
-                                                                                             metadata_only=True)
+        metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(
+            trans.app, id, ctx_str, metadata_only=True
+        )
         # For rendering the prev button.
         if ctx_parent:
             ctx_parent_date = hg_util.get_readable_ctx_date(ctx_parent)
             ctx_parent_rev = ctx_parent.rev()
             if ctx_parent_rev < 0:
                 prev = None
             else:
-                prev = "<b>%s:%s</b> <i>(%s)</i>" % (ctx_parent_rev, ctx_parent, ctx_parent_date)
+                prev = f"<b>{ctx_parent_rev}:{ctx_parent}</b> <i>({ctx_parent_date})</i>"
         else:
             prev = None
         if ctx_child:
             ctx_child_date = hg_util.get_readable_ctx_date(ctx_child)
             ctx_child_rev = ctx_child.rev()
-            next = "<b>%s:%s</b> <i>(%s)</i>" % (ctx_child_rev, ctx_child, ctx_child_date)
+            next = f"<b>{ctx_child_rev}:{ctx_child}</b> <i>({ctx_child_date})</i>"
         else:
             next = None
-        return trans.fill_template('/webapps/tool_shed/repository/view_changeset.mako',
-                                   repository=repository,
-                                   metadata=metadata,
-                                   prev=prev,
-                                   next=next,
-                                   ctx=ctx,
-                                   ctx_parent=ctx_parent,
-                                   ctx_child=ctx_child,
-                                   anchors=anchors,
-                                   modified=modified,
-                                   added=added,
-                                   removed=removed,
-                                   deleted=deleted,
-                                   unknown=unknown,
-                                   ignored=ignored,
-                                   clean=clean,
-                                   diffs=diffs,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/view_changeset.mako",
+            repository=repository,
+            metadata=metadata,
+            prev=prev,
+            next=next,
+            ctx=ctx,
+            ctx_parent=ctx_parent,
+            ctx_child=ctx_child,
+            anchors=anchors,
+            modified=modified,
+            added=added,
+            removed=removed,
+            deleted=deleted,
+            unknown=unknown,
+            ignored=ignored,
+            clean=clean,
+            diffs=diffs,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def view_or_manage_repository(self, trans, **kwd):
-        repository_id = kwd.get('id', None)
+        repository_id = kwd.get("id", None)
         if repository_id:
             repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
             user = trans.user
             if repository:
-                if user is not None and (trans.user_is_admin or
-                                         trans.app.security_agent.user_can_administer_repository(user, repository)):
-                    return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                    action='manage_repository',
-                                                                    **kwd))
+                if user is not None and (
+                    trans.user_is_admin or trans.app.security_agent.user_can_administer_repository(user, repository)
+                ):
+                    return trans.response.send_redirect(
+                        web.url_for(controller="repository", action="manage_repository", **kwd)
+                    )
                 else:
-                    return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                    action='view_repository',
-                                                                    **kwd))
-            return trans.show_error_message("Invalid repository id '%s' received." % repository_id)
+                    return trans.response.send_redirect(
+                        web.url_for(controller="repository", action="view_repository", **kwd)
+                    )
+            return trans.show_error_message(f"Invalid repository id '{repository_id}' received.")
         return trans.show_error_message("The repository id was not received.")
 
     @web.expose
     def view_repository(self, trans, id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
         repository = repository_util.get_repository_in_tool_shed(trans.app, id)
         repo = repository.hg_repo
         avg_rating, num_ratings = self.get_ave_item_rating_data(trans.sa_session, repository, webapp_model=trans.model)
-        changeset_revision = kwd.get('changeset_revision', repository.tip())
+        changeset_revision = kwd.get("changeset_revision", repository.tip())
         self.validate_changeset_revision(trans, changeset_revision, id)
-        repository.share_url = repository_util.generate_sharable_link_for_repository_in_tool_shed(repository, changeset_revision=changeset_revision)
+        repository.share_url = repository_util.generate_sharable_link_for_repository_in_tool_shed(
+            repository, changeset_revision=changeset_revision
+        )
         repository.clone_url = common_util.generate_clone_url_for_repository_in_tool_shed(trans.user, repository)
-        display_reviews = kwd.get('display_reviews', False)
-        alerts = kwd.get('alerts', '')
+        alerts = kwd.get("alerts", "")
         alerts_checked = CheckboxField.is_checked(alerts)
         if repository.email_alerts:
             email_alerts = json.loads(repository.email_alerts)
         else:
             email_alerts = []
         repository_dependencies = None
         user = trans.user
-        if user and kwd.get('receive_email_alerts_button', False):
+        if user and kwd.get("receive_email_alerts_button", False):
             flush_needed = False
             if alerts_checked:
                 if user.email not in email_alerts:
                     email_alerts.append(user.email)
                     repository.email_alerts = json.dumps(email_alerts)
                     flush_needed = True
             else:
@@ -2585,153 +2564,156 @@
                     email_alerts.remove(user.email)
                     repository.email_alerts = json.dumps(email_alerts)
                     flush_needed = True
             if flush_needed:
                 trans.sa_session.add(repository)
                 trans.sa_session.flush()
         checked = alerts_checked or (user and user.email in email_alerts)
-        alerts_check_box = CheckboxField('alerts', value=checked)
-        changeset_revision_select_field = grids_util.build_changeset_revision_select_field(trans,
-                                                                                           repository,
-                                                                                           selected_value=changeset_revision,
-                                                                                           add_id_to_name=False,
-                                                                                           downloadable=False)
+        alerts_check_box = CheckboxField("alerts", value=checked)
+        changeset_revision_select_field = grids_util.build_changeset_revision_select_field(
+            trans, repository, selected_value=changeset_revision, add_id_to_name=False, downloadable=False
+        )
         revision_label = hg_util.get_revision_label(trans.app, repository, changeset_revision, include_date=False)
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, id, changeset_revision)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, id, changeset_revision
+        )
         if repository_metadata:
             metadata = repository_metadata.metadata
             # Get a dictionary of all repositories upon which the contents of the current repository_metadata record depend.
-            toolshed_base_url = str(web.url_for('/', qualified=True)).rstrip('/')
+            toolshed_base_url = str(web.url_for("/", qualified=True)).rstrip("/")
             rb = relation_builder.RelationBuilder(trans.app, repository, repository_metadata, toolshed_base_url)
             repository_dependencies = rb.get_repository_dependencies_for_changeset_revision()
             if str(repository.type) != rt_util.TOOL_DEPENDENCY_DEFINITION:
                 # Handle messaging for orphan tool dependency definitions.
                 dd = dependency_display.DependencyDisplayer(trans.app)
                 orphan_message = dd.generate_message_for_orphan_tool_dependencies(repository, metadata)
                 if orphan_message:
                     message += orphan_message
-                    status = 'warning'
+                    status = "warning"
         else:
             metadata = None
         is_malicious = metadata_util.is_malicious(trans.app, id, repository.tip())
         if is_malicious:
             if trans.app.security_agent.can_push(trans.app, trans.user, repository):
                 message += malicious_error_can_push
             else:
                 message += malicious_error
-            status = 'error'
+            status = "error"
         tsucm = ToolShedUtilityContainerManager(trans.app)
-        containers_dict = tsucm.build_repository_containers(repository,
-                                                            changeset_revision,
-                                                            repository_dependencies,
-                                                            repository_metadata)
+        containers_dict = tsucm.build_repository_containers(
+            repository, changeset_revision, repository_dependencies, repository_metadata
+        )
         repository_type_select_field = rt_util.build_repository_type_select_field(trans, repository=repository)
         heads = hg_util.get_repository_heads(repo)
-        return trans.fill_template('/webapps/tool_shed/repository/view_repository.mako',
-                                   repo=repo,
-                                   heads=heads,
-                                   repository=repository,
-                                   repository_metadata=repository_metadata,
-                                   metadata=metadata,
-                                   containers_dict=containers_dict,
-                                   avg_rating=avg_rating,
-                                   display_reviews=display_reviews,
-                                   num_ratings=num_ratings,
-                                   alerts_check_box=alerts_check_box,
-                                   changeset_revision=changeset_revision,
-                                   changeset_revision_select_field=changeset_revision_select_field,
-                                   revision_label=revision_label,
-                                   repository_type_select_field=repository_type_select_field,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/view_repository.mako",
+            repo=repo,
+            heads=heads,
+            repository=repository,
+            repository_metadata=repository_metadata,
+            metadata=metadata,
+            containers_dict=containers_dict,
+            avg_rating=avg_rating,
+            num_ratings=num_ratings,
+            alerts_check_box=alerts_check_box,
+            changeset_revision=changeset_revision,
+            changeset_revision_select_field=changeset_revision_select_field,
+            revision_label=revision_label,
+            repository_type_select_field=repository_type_select_field,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def view_tool_metadata(self, trans, repository_id, changeset_revision, tool_id, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        render_repository_actions_for = kwd.get('render_repository_actions_for', 'tool_shed')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        render_repository_actions_for = kwd.get("render_repository_actions_for", "tool_shed")
         repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
         repo_files_dir = repository.repo_path(trans.app)
         repo = repository.hg_repo
         tool_metadata_dict = {}
         tool_lineage = []
         tool = None
         guid = None
         self.validate_changeset_revision(trans, changeset_revision, repository_id)
         revision_label = hg_util.get_revision_label(trans.app, repository, changeset_revision, include_date=False)
-        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(trans.app, repository_id, changeset_revision)
+        repository_metadata = metadata_util.get_repository_metadata_by_changeset_revision(
+            trans.app, repository_id, changeset_revision
+        )
         if repository_metadata:
             repository_metadata_id = trans.security.encode_id(repository_metadata.id)
             metadata = repository_metadata.metadata
             if metadata:
-                if 'tools' in metadata:
+                if "tools" in metadata:
                     with ValidationContext.from_app(trans.app) as validation_context:
                         tv = tool_validator.ToolValidator(validation_context)
-                        for tool_metadata_dict in metadata['tools']:
-                            if tool_metadata_dict['id'] == tool_id:
+                        for tool_metadata_dict in metadata["tools"]:
+                            if tool_metadata_dict["id"] == tool_id:
                                 work_dir = tempfile.mkdtemp()
-                                relative_path_to_tool_config = tool_metadata_dict['tool_config']
-                                guid = tool_metadata_dict['guid']
+                                relative_path_to_tool_config = tool_metadata_dict["tool_config"]
+                                guid = tool_metadata_dict["guid"]
                                 full_path_to_tool_config = os.path.abspath(relative_path_to_tool_config)
                                 full_path_to_dir, tool_config_filename = os.path.split(full_path_to_tool_config)
-                                can_use_disk_file = tv.can_use_tool_config_disk_file(repository,
-                                                                                     repo,
-                                                                                     full_path_to_tool_config,
-                                                                                     changeset_revision)
+                                can_use_disk_file = tv.can_use_tool_config_disk_file(
+                                    repository, repo, full_path_to_tool_config, changeset_revision
+                                )
                                 if can_use_disk_file:
-                                    tool, valid, message, sample_files = \
-                                        tv.handle_sample_files_and_load_tool_from_disk(repo_files_dir,
-                                                                                       repository_id,
-                                                                                       full_path_to_tool_config,
-                                                                                       work_dir)
+                                    tool, valid, message, sample_files = tv.handle_sample_files_and_load_tool_from_disk(
+                                        repo_files_dir, repository_id, full_path_to_tool_config, work_dir
+                                    )
                                     if message:
-                                        status = 'error'
+                                        status = "error"
                                 else:
-                                    tool, valid, message, sample_files = \
-                                        tv.handle_sample_files_and_load_tool_from_tmp_config(repo,
-                                                                                             repository_id,
-                                                                                             changeset_revision,
-                                                                                             tool_config_filename,
-                                                                                             work_dir)
+                                    (
+                                        tool,
+                                        valid,
+                                        message,
+                                        sample_files,
+                                    ) = tv.handle_sample_files_and_load_tool_from_tmp_config(
+                                        repo, repository_id, changeset_revision, tool_config_filename, work_dir
+                                    )
                                     if message:
-                                        status = 'error'
+                                        status = "error"
                                 basic_util.remove_dir(work_dir)
                                 break
                         if guid:
                             tvm = tool_version_manager.ToolVersionManager(trans.app)
-                            tool_lineage = tvm.get_version_lineage_for_tool(repository_id,
-                                                                            repository_metadata,
-                                                                            guid)
+                            tool_lineage = tvm.get_version_lineage_for_tool(repository_id, repository_metadata, guid)
         else:
             repository_metadata_id = None
             metadata = None
-        changeset_revision_select_field = grids_util.build_changeset_revision_select_field(trans,
-                                                                                           repository,
-                                                                                           selected_value=changeset_revision,
-                                                                                           add_id_to_name=False,
-                                                                                           downloadable=False)
-        return trans.fill_template("/webapps/tool_shed/repository/view_tool_metadata.mako",
-                                   render_repository_actions_for=render_repository_actions_for,
-                                   repository=repository,
-                                   repository_metadata_id=repository_metadata_id,
-                                   metadata=metadata,
-                                   tool=tool,
-                                   tool_metadata_dict=tool_metadata_dict,
-                                   tool_lineage=tool_lineage,
-                                   changeset_revision=changeset_revision,
-                                   revision_label=revision_label,
-                                   changeset_revision_select_field=changeset_revision_select_field,
-                                   message=message,
-                                   status=status)
+        changeset_revision_select_field = grids_util.build_changeset_revision_select_field(
+            trans, repository, selected_value=changeset_revision, add_id_to_name=False, downloadable=False
+        )
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/view_tool_metadata.mako",
+            render_repository_actions_for=render_repository_actions_for,
+            repository=repository,
+            repository_metadata_id=repository_metadata_id,
+            metadata=metadata,
+            tool=tool,
+            tool_metadata_dict=tool_metadata_dict,
+            tool_lineage=tool_lineage,
+            changeset_revision=changeset_revision,
+            revision_label=revision_label,
+            changeset_revision_select_field=changeset_revision_select_field,
+            message=message,
+            status=status,
+        )
 
     def validate_changeset_revision(self, trans, changeset_revision, repository_id):
         """In case changeset revision is invalid send them to the repository page"""
         if changeset_revision:
             repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
             repo = repository.hg_repo
             if not hg_util.get_changectx_for_changeset(repo, changeset_revision):
-                message = 'Invalid changeset revision'
-                return trans.response.send_redirect(web.url_for(controller='repository',
-                                                                action='index',
-                                                                repository_id=repository_id,
-                                                                message=message,
-                                                                status='error'))
+                message = "Invalid changeset revision"
+                return trans.response.send_redirect(
+                    web.url_for(
+                        controller="repository",
+                        action="index",
+                        repository_id=repository_id,
+                        message=message,
+                        status="error",
+                    )
+                )
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/upload.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/upload.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,94 +4,94 @@
 import tarfile
 import tempfile
 
 import requests
 
 from galaxy import (
     util,
-    web
+    web,
 )
+from galaxy.tool_shed.util import dependency_display
 from galaxy.util import checkers
 from galaxy.webapps.base.controller import BaseUIController
 from tool_shed.dependencies import attribute_handlers
-from tool_shed.galaxy_install import dependency_display
 from tool_shed.metadata import repository_metadata_manager
 from tool_shed.repository_types import util as rt_util
 from tool_shed.tools.data_table_manager import ShedToolDataTableManager
 from tool_shed.util import (
     basic_util,
     commit_util,
     hg_util,
     repository_content_util,
     repository_util,
     shed_util_common as suc,
-    xml_util
+    xml_util,
 )
 from tool_shed.util.web_util import escape
+from tool_shed.webapp.framework.decorators import require_login
 
 log = logging.getLogger(__name__)
 
 
 class UploadController(BaseUIController):
-
     @web.expose
-    @web.require_login('upload', use_panels=True)
+    @require_login("upload", use_panels=True)
     def upload(self, trans, **kwd):
-        message = escape(kwd.get('message', ''))
-        status = kwd.get('status', 'done')
-        commit_message = escape(kwd.get('commit_message', 'Uploaded'))
-        repository_id = kwd.get('repository_id', '')
+        message = escape(kwd.get("message", ""))
+        status = kwd.get("status", "done")
+        commit_message = escape(kwd.get("commit_message", "Uploaded"))
+        repository_id = kwd.get("repository_id", "")
         repository = repository_util.get_repository_in_tool_shed(trans.app, repository_id)
         repo_dir = repository.repo_path(trans.app)
-        uncompress_file = util.string_as_bool(kwd.get('uncompress_file', 'true'))
-        remove_repo_files_not_in_tar = util.string_as_bool(kwd.get('remove_repo_files_not_in_tar', 'true'))
+        uncompress_file = util.string_as_bool(kwd.get("uncompress_file", "true"))
+        remove_repo_files_not_in_tar = util.string_as_bool(kwd.get("remove_repo_files_not_in_tar", "true"))
         uploaded_file = None
         upload_point = commit_util.get_upload_point(repository, **kwd)
         tip = repository.tip()
-        file_data = kwd.get('file_data', '')
-        url = kwd.get('url', '')
+        file_data = kwd.get("file_data", "")
+        url = kwd.get("url", "")
         # Part of the upload process is sending email notification to those that have registered to
         # receive them.  One scenario occurs when the first change set is produced for the repository.
         # See the suc.handle_email_alerts() method for the definition of the scenarios.
         new_repo_alert = repository.is_new()
         uploaded_directory = None
-        if kwd.get('upload_button', False):
-            if file_data == '' and url == '':
-                message = 'No files were entered on the upload form.'
-                status = 'error'
+        if kwd.get("upload_button", False):
+            if file_data == "" and url == "":
+                message = "No files were entered on the upload form."
+                status = "error"
                 uploaded_file = None
-            elif url and url.startswith('hg'):
+            elif url and url.startswith("hg"):
                 # Use mercurial clone to fetch repository, contents will then be copied over.
                 uploaded_directory = tempfile.mkdtemp()
-                repo_url = 'http%s' % url[len('hg'):]
+                repo_url = f"http{url[len('hg'):]}"
                 cloned_ok, error_message = hg_util.clone_repository(repo_url, uploaded_directory)
                 if not cloned_ok:
-                    message = 'Error uploading via mercurial clone: %s' % error_message
-                    status = 'error'
+                    message = f"Error uploading via mercurial clone: {error_message}"
+                    status = "error"
                     basic_util.remove_dir(uploaded_directory)
                     uploaded_directory = None
             elif url:
                 valid_url = True
                 try:
-                    stream = requests.get(url, stream=True)
+                    stream = requests.get(url, stream=True, timeout=util.DEFAULT_SOCKET_TIMEOUT)
                 except Exception as e:
                     valid_url = False
-                    message = 'Error uploading file via http: %s' % util.unicodify(e)
-                    status = 'error'
+                    message = f"Error uploading file via http: {util.unicodify(e)}"
+                    status = "error"
                     uploaded_file = None
                 if valid_url:
-                    fd, uploaded_file_name = tempfile.mkstemp()
-                    uploaded_file = open(uploaded_file_name, 'wb')
-                    for chunk in stream.iter_content(chunk_size=util.CHUNK_SIZE):
-                        if chunk:
-                            uploaded_file.write(chunk)
-                    uploaded_file.flush()
-                    uploaded_file_filename = url.split('/')[-1]
+                    with tempfile.NamedTemporaryFile(mode="wb", delete=False) as uploaded_file:
+                        uploaded_file_name = uploaded_file.name
+                        for chunk in stream.iter_content(chunk_size=util.CHUNK_SIZE):
+                            if chunk:
+                                uploaded_file.write(chunk)
+                        uploaded_file.flush()
+                    uploaded_file_filename = url.split("/")[-1]
                     isempty = os.path.getsize(os.path.abspath(uploaded_file_name)) == 0
-            elif file_data not in ('', None):
+            elif file_data not in ("", None):
                 uploaded_file = file_data.file
                 uploaded_file_name = uploaded_file.name
                 uploaded_file_filename = os.path.split(file_data.filename)[-1]
                 isempty = os.path.getsize(os.path.abspath(uploaded_file_name)) == 0
             if uploaded_file or uploaded_directory:
                 rdah = attribute_handlers.RepositoryDependencyAttributeHandler(trans.app, unpopulate=False)
                 tdah = attribute_handlers.ToolDependencyAttributeHandler(trans.app, unpopulate=False)
@@ -108,93 +108,109 @@
                         tar = None
                         istar = False
                     else:
                         # Determine what we have - a single file or an archive
                         try:
                             if (isgzip or isbz2) and uncompress_file:
                                 # Open for reading with transparent compression.
-                                tar = tarfile.open(uploaded_file_name, 'r:*')
+                                tar = tarfile.open(uploaded_file_name, "r:*")
                             else:
                                 tar = tarfile.open(uploaded_file_name)
                             istar = True
                         except tarfile.ReadError:
                             tar = None
                             istar = False
                 else:
                     # Uploaded directory
                     istar = False
                 if istar:
-                    ok, message, files_to_remove, content_alert_str, undesirable_dirs_removed, undesirable_files_removed = \
-                        repository_content_util.upload_tar(
-                            trans,
-                            rdah,
-                            tdah,
-                            repository,
-                            tar,
-                            uploaded_file,
-                            upload_point,
-                            remove_repo_files_not_in_tar,
-                            commit_message,
-                            new_repo_alert
-                        )
+                    (
+                        ok,
+                        message,
+                        files_to_remove,
+                        content_alert_str,
+                        undesirable_dirs_removed,
+                        undesirable_files_removed,
+                    ) = repository_content_util.upload_tar(
+                        trans,
+                        rdah,
+                        tdah,
+                        repository,
+                        tar,
+                        uploaded_file,
+                        upload_point,
+                        remove_repo_files_not_in_tar,
+                        commit_message,
+                        new_repo_alert,
+                    )
                 elif uploaded_directory:
-                    ok, message, files_to_remove, content_alert_str, undesirable_dirs_removed, undesirable_files_removed = \
-                        self.upload_directory(trans,
-                                              rdah,
-                                              tdah,
-                                              repository,
-                                              uploaded_directory,
-                                              upload_point,
-                                              remove_repo_files_not_in_tar,
-                                              commit_message,
-                                              new_repo_alert)
+                    (
+                        ok,
+                        message,
+                        files_to_remove,
+                        content_alert_str,
+                        undesirable_dirs_removed,
+                        undesirable_files_removed,
+                    ) = self.upload_directory(
+                        trans,
+                        rdah,
+                        tdah,
+                        repository,
+                        uploaded_directory,
+                        upload_point,
+                        remove_repo_files_not_in_tar,
+                        commit_message,
+                        new_repo_alert,
+                    )
                 else:
                     if (isgzip or isbz2) and uncompress_file:
-                        uploaded_file_filename = commit_util.uncompress(repository,
-                                                                        uploaded_file_name,
-                                                                        uploaded_file_filename,
-                                                                        isgzip=isgzip,
-                                                                        isbz2=isbz2)
-                    if repository.type == rt_util.REPOSITORY_SUITE_DEFINITION and \
-                            uploaded_file_filename != rt_util.REPOSITORY_DEPENDENCY_DEFINITION_FILENAME:
+                        uploaded_file_filename = commit_util.uncompress(
+                            repository, uploaded_file_name, uploaded_file_filename, isgzip=isgzip, isbz2=isbz2
+                        )
+                    if (
+                        repository.type == rt_util.REPOSITORY_SUITE_DEFINITION
+                        and uploaded_file_filename != rt_util.REPOSITORY_DEPENDENCY_DEFINITION_FILENAME
+                    ):
                         ok = False
-                        message = 'Repositories of type <b>Repository suite definition</b> can only contain a single file named '
-                        message += '<b>repository_dependencies.xml</b>.'
-                    elif repository.type == rt_util.TOOL_DEPENDENCY_DEFINITION and \
-                            uploaded_file_filename != rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME:
+                        message = "Repositories of type <b>Repository suite definition</b> can only contain a single file named "
+                        message += "<b>repository_dependencies.xml</b>."
+                    elif (
+                        repository.type == rt_util.TOOL_DEPENDENCY_DEFINITION
+                        and uploaded_file_filename != rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME
+                    ):
                         ok = False
-                        message = 'Repositories of type <b>Tool dependency definition</b> can only contain a single file named '
-                        message += '<b>tool_dependencies.xml</b>.'
+                        message = "Repositories of type <b>Tool dependency definition</b> can only contain a single file named "
+                        message += "<b>tool_dependencies.xml</b>."
                     if ok:
                         if upload_point is not None:
                             full_path = os.path.abspath(os.path.join(repo_dir, upload_point, uploaded_file_filename))
                         else:
                             full_path = os.path.abspath(os.path.join(repo_dir, uploaded_file_filename))
                         # Move some version of the uploaded file to the load_point within the repository hierarchy.
                         if uploaded_file_filename in [rt_util.REPOSITORY_DEPENDENCY_DEFINITION_FILENAME]:
                             # Inspect the contents of the file to see if toolshed or changeset_revision attributes
                             # are missing and if so, set them appropriately.
                             altered, root_elem, error_message = rdah.handle_tag_attributes(uploaded_file_name)
                             if error_message:
                                 ok = False
                                 message = error_message
-                                status = 'error'
+                                status = "error"
                             elif altered:
                                 tmp_filename = xml_util.create_and_write_tmp_file(root_elem)
                                 shutil.move(tmp_filename, full_path)
                             else:
                                 shutil.move(uploaded_file_name, full_path)
                         elif uploaded_file_filename in [rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME]:
                             # Inspect the contents of the file to see if changeset_revision values are
                             # missing and if so, set them appropriately.
                             altered, root_elem, error_message = tdah.handle_tag_attributes(uploaded_file_name)
                             if error_message:
                                 ok = False
                                 message = error_message
-                                status = 'error'
+                                status = "error"
                             if ok:
                                 if altered:
                                     tmp_filename = xml_util.create_and_write_tmp_file(root_elem)
                                     shutil.move(tmp_filename, full_path)
                                 else:
                                     shutil.move(uploaded_file_name, full_path)
                         else:
@@ -202,200 +218,230 @@
                         if ok:
                             # See if any admin users have chosen to receive email alerts when a repository is updated.
                             # If so, check every uploaded file to ensure content is appropriate.
                             check_contents = commit_util.check_file_contents_for_email_alerts(trans.app)
                             if check_contents and os.path.isfile(full_path):
                                 content_alert_str = commit_util.check_file_content_for_html_and_images(full_path)
                             else:
-                                content_alert_str = ''
+                                content_alert_str = ""
                             hg_util.add_changeset(repo_dir, full_path)
-                            hg_util.commit_changeset(repo_dir,
-                                                     full_path_to_changeset=full_path,
-                                                     username=trans.user.username,
-                                                     message=commit_message)
-                            if full_path.endswith('tool_data_table_conf.xml.sample'):
+                            hg_util.commit_changeset(
+                                repo_dir,
+                                full_path_to_changeset=full_path,
+                                username=trans.user.username,
+                                message=commit_message,
+                            )
+                            if full_path.endswith("tool_data_table_conf.xml.sample"):
                                 # Handle the special case where a tool_data_table_conf.xml.sample file is being uploaded
                                 # by parsing the file and adding new entries to the in-memory trans.app.tool_data_tables
                                 # dictionary.
-                                error, error_message = stdtm.handle_sample_tool_data_table_conf_file(full_path, persist=False)
+                                error, error_message = stdtm.handle_sample_tool_data_table_conf_file(
+                                    full_path, persist=False
+                                )
                                 if error:
-                                    message = '%s<br/>%s' % (message, error_message)
+                                    message = f"{message}<br/>{error_message}"
                             # See if the content of the change set was valid.
                             admin_only = len(repository.downloadable_revisions) != 1
-                            suc.handle_email_alerts(trans.app,
-                                                    trans.request.host,
-                                                    repository,
-                                                    content_alert_str=content_alert_str,
-                                                    new_repo_alert=new_repo_alert,
-                                                    admin_only=admin_only)
+                            suc.handle_email_alerts(
+                                trans.app,
+                                trans.request.host,
+                                repository,
+                                content_alert_str=content_alert_str,
+                                new_repo_alert=new_repo_alert,
+                                admin_only=admin_only,
+                            )
                 if ok:
                     # Update the repository files for browsing.
                     hg_util.update_repository(repo_dir)
                     # Get the new repository tip.
                     if tip == repository.tip():
-                        message = 'No changes to repository.  '
-                        status = 'warning'
+                        message = "No changes to repository.  "
+                        status = "warning"
                     else:
                         if (isgzip or isbz2) and uncompress_file:
-                            uncompress_str = ' uncompressed and '
+                            uncompress_str = " uncompressed and "
                         else:
-                            uncompress_str = ' '
+                            uncompress_str = " "
                         if uploaded_directory:
                             source_type = "repository"
                             source = url
                         else:
                             source_type = "file"
                             source = uploaded_file_filename
-                        message = "The %s <b>%s</b> has been successfully%suploaded to the repository.  " % \
-                            (source_type, escape(source), uncompress_str)
+                        message = f"The {source_type} <b>{escape(source)}</b> has been successfully{uncompress_str}uploaded to the repository.  "
                         if istar and (undesirable_dirs_removed or undesirable_files_removed):
                             items_removed = undesirable_dirs_removed + undesirable_files_removed
-                            message += "  %d undesirable items (.hg .svn .git directories, .DS_Store, hgrc files, etc) " % items_removed
+                            message += (
+                                "  %d undesirable items (.hg .svn .git directories, .DS_Store, hgrc files, etc) "
+                                % items_removed
+                            )
                             message += "were removed from the archive.  "
                         if istar and remove_repo_files_not_in_tar and files_to_remove:
                             if upload_point is not None:
-                                message += "  %d files were removed from the repository relative to the selected upload point '%s'.  " % \
-                                    (len(files_to_remove), upload_point)
+                                message += (
+                                    "  %d files were removed from the repository relative to the selected upload point '%s'.  "
+                                    % (len(files_to_remove), upload_point)
+                                )
                             else:
                                 message += "  %d files were removed from the repository root.  " % len(files_to_remove)
-                        rmm = repository_metadata_manager.RepositoryMetadataManager(app=trans.app,
-                                                                                    user=trans.user,
-                                                                                    repository=repository)
-                        status, error_message = \
-                            rmm.set_repository_metadata_due_to_new_tip(trans.request.host,
-                                                                       content_alert_str=content_alert_str,
-                                                                       **kwd)
+                        rmm = repository_metadata_manager.RepositoryMetadataManager(
+                            app=trans.app, user=trans.user, repository=repository
+                        )
+                        status, error_message = rmm.set_repository_metadata_due_to_new_tip(
+                            trans.request.host, content_alert_str=content_alert_str, **kwd
+                        )
                         if error_message:
                             message = error_message
-                        kwd['message'] = message
+                        kwd["message"] = message
                     if repository.metadata_revisions:
                         # A repository's metadata revisions are order descending by update_time, so the zeroth revision
                         # will be the tip just after an upload.
                         metadata_dict = repository.metadata_revisions[0].metadata
                     else:
                         metadata_dict = {}
                     dd = dependency_display.DependencyDisplayer(trans.app)
-                    if str(repository.type) not in [rt_util.REPOSITORY_SUITE_DEFINITION,
-                                                    rt_util.TOOL_DEPENDENCY_DEFINITION]:
-                        change_repository_type_message = rt_util.generate_message_for_repository_type_change(trans.app,
-                                                                                                             repository)
+                    if str(repository.type) not in [
+                        rt_util.REPOSITORY_SUITE_DEFINITION,
+                        rt_util.TOOL_DEPENDENCY_DEFINITION,
+                    ]:
+                        change_repository_type_message = rt_util.generate_message_for_repository_type_change(
+                            trans.app, repository
+                        )
                         if change_repository_type_message:
                             message += change_repository_type_message
-                            status = 'warning'
+                            status = "warning"
                         else:
                             # Provide a warning message if a tool_dependencies.xml file is provided, but tool dependencies
                             # weren't loaded due to a requirement tag mismatch or some other problem.  Tool dependency
                             # definitions can define orphan tool dependencies (no relationship to any tools contained in the
                             # repository), so warning messages are important because orphans are always valid.  The repository
                             # owner must be warned in case they did not intend to define an orphan dependency, but simply
                             # provided incorrect information (tool shed, name owner, changeset_revision) for the definition.
                             orphan_message = dd.generate_message_for_orphan_tool_dependencies(repository, metadata_dict)
                             if orphan_message:
                                 message += orphan_message
-                                status = 'warning'
+                                status = "warning"
                     # Handle messaging for invalid tool dependencies.
                     invalid_tool_dependencies_message = dd.generate_message_for_invalid_tool_dependencies(metadata_dict)
                     if invalid_tool_dependencies_message:
                         message += invalid_tool_dependencies_message
-                        status = 'error'
+                        status = "error"
                     # Handle messaging for invalid repository dependencies.
-                    invalid_repository_dependencies_message = \
-                        dd.generate_message_for_invalid_repository_dependencies(metadata_dict,
-                                                                                error_from_tuple=True)
+                    invalid_repository_dependencies_message = dd.generate_message_for_invalid_repository_dependencies(
+                        metadata_dict, error_from_tuple=True
+                    )
                     if invalid_repository_dependencies_message:
                         message += invalid_repository_dependencies_message
-                        status = 'error'
+                        status = "error"
                     # Reset the tool_data_tables by loading the empty tool_data_table_conf.xml file.
                     stdtm.reset_tool_data_tables()
                     if uploaded_directory:
                         basic_util.remove_dir(uploaded_directory)
-                    trans.response.send_redirect(web.url_for(controller='repository',
-                                                             action='browse_repository',
-                                                             id=repository_id,
-                                                             commit_message='Deleted selected files',
-                                                             message=message,
-                                                             status=status))
+                    trans.response.send_redirect(
+                        web.url_for(
+                            controller="repository",
+                            action="browse_repository",
+                            id=repository_id,
+                            commit_message="Deleted selected files",
+                            message=message,
+                            status=status,
+                        )
+                    )
                 else:
                     if uploaded_directory:
                         basic_util.remove_dir(uploaded_directory)
-                    status = 'error'
+                    status = "error"
                 # Reset the tool_data_tables by loading the empty tool_data_table_conf.xml file.
                 stdtm.reset_tool_data_tables()
-        return trans.fill_template('/webapps/tool_shed/repository/upload.mako',
-                                   repository=repository,
-                                   changeset_revision=tip,
-                                   url=url,
-                                   commit_message=commit_message,
-                                   uncompress_file=uncompress_file,
-                                   remove_repo_files_not_in_tar=remove_repo_files_not_in_tar,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/repository/upload.mako",
+            repository=repository,
+            changeset_revision=tip,
+            url=url,
+            commit_message=commit_message,
+            uncompress_file=uncompress_file,
+            remove_repo_files_not_in_tar=remove_repo_files_not_in_tar,
+            message=message,
+            status=status,
+        )
 
-    def upload_directory(self, trans, rdah, tdah, repository, uploaded_directory, upload_point, remove_repo_files_not_in_tar,
-                         commit_message, new_repo_alert):
+    def upload_directory(
+        self,
+        trans,
+        rdah,
+        tdah,
+        repository,
+        uploaded_directory,
+        upload_point,
+        remove_repo_files_not_in_tar,
+        commit_message,
+        new_repo_alert,
+    ):
         repo_dir = repository.repo_path(trans.app)
         undesirable_dirs_removed = 0
         undesirable_files_removed = 0
         if upload_point is not None:
             full_path = os.path.abspath(os.path.join(repo_dir, upload_point))
         else:
             full_path = os.path.abspath(repo_dir)
         filenames_in_archive = []
-        for root, dirs, files in os.walk(uploaded_directory):
+        for root, _dirs, files in os.walk(uploaded_directory):
             for uploaded_file in files:
                 relative_path = os.path.normpath(os.path.join(os.path.relpath(root, uploaded_directory), uploaded_file))
                 if repository.type == rt_util.REPOSITORY_SUITE_DEFINITION:
                     ok = os.path.basename(uploaded_file) == rt_util.REPOSITORY_DEPENDENCY_DEFINITION_FILENAME
                 elif repository.type == rt_util.TOOL_DEPENDENCY_DEFINITION:
                     ok = os.path.basename(uploaded_file) == rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME
                 else:
                     ok = os.path.basename(uploaded_file) not in commit_util.UNDESIRABLE_FILES
                 if ok:
-                    for file_path_item in relative_path.split('/'):
+                    for file_path_item in relative_path.split("/"):
                         if file_path_item in commit_util.UNDESIRABLE_DIRS:
                             undesirable_dirs_removed += 1
                             ok = False
                             break
                 else:
                     undesirable_files_removed += 1
                 if ok:
                     uploaded_file_name = os.path.abspath(os.path.join(root, uploaded_file))
                     if os.path.split(uploaded_file_name)[-1] == rt_util.REPOSITORY_DEPENDENCY_DEFINITION_FILENAME:
                         # Inspect the contents of the file to see if toolshed or changeset_revision
                         # attributes are missing and if so, set them appropriately.
                         altered, root_elem, error_message = rdah.handle_tag_attributes(uploaded_file_name)
                         if error_message:
-                            return False, error_message, [], '', [], []
+                            return False, error_message, [], "", [], []
                         elif altered:
                             tmp_filename = xml_util.create_and_write_tmp_file(root_elem)
                             shutil.move(tmp_filename, uploaded_file_name)
                     elif os.path.split(uploaded_file_name)[-1] == rt_util.TOOL_DEPENDENCY_DEFINITION_FILENAME:
                         # Inspect the contents of the file to see if toolshed or changeset_revision
                         # attributes are missing and if so, set them appropriately.
                         altered, root_elem, error_message = tdah.handle_tag_attributes(uploaded_file_name)
                         if error_message:
-                            return False, error_message, [], '', [], []
+                            return False, error_message, [], "", [], []
                         if altered:
                             tmp_filename = xml_util.create_and_write_tmp_file(root_elem)
                             shutil.move(tmp_filename, uploaded_file_name)
                     repo_path = os.path.join(full_path, relative_path)
                     repo_basedir = os.path.normpath(os.path.join(repo_path, os.path.pardir))
                     if not os.path.exists(repo_basedir):
                         os.makedirs(repo_basedir)
                     if os.path.exists(repo_path):
                         if os.path.isdir(repo_path):
                             shutil.rmtree(repo_path)
                         else:
                             os.remove(repo_path)
                     shutil.move(os.path.join(uploaded_directory, relative_path), repo_path)
                     filenames_in_archive.append(relative_path)
-        return commit_util.handle_directory_changes(trans.app,
-                                                    trans.request.host,
-                                                    trans.user.username,
-                                                    repository,
-                                                    full_path,
-                                                    filenames_in_archive,
-                                                    remove_repo_files_not_in_tar,
-                                                    new_repo_alert,
-                                                    commit_message,
-                                                    undesirable_dirs_removed,
-                                                    undesirable_files_removed)
+        return commit_util.handle_directory_changes(
+            trans.app,
+            trans.request.host,
+            trans.user.username,
+            repository,
+            full_path,
+            filenames_in_archive,
+            remove_repo_files_not_in_tar,
+            new_repo_alert,
+            commit_message,
+            undesirable_dirs_removed,
+            undesirable_files_removed,
+        )
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/controllers/user.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/controllers/user.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,24 +2,26 @@
 import socket
 
 from markupsafe import escape
 from sqlalchemy import func
 
 from galaxy import (
     util,
-    web
+    web,
 )
+from galaxy.managers.api_keys import ApiKeyManager
 from galaxy.security.validate_user_input import (
     validate_email,
     validate_password,
-    validate_publicname
+    validate_publicname,
 )
 from galaxy.web import url_for
 from galaxy.web.form_builder import CheckboxField
 from galaxy.webapps.galaxy.controllers.user import User as BaseUser
+from tool_shed.webapp.framework.decorators import require_login
 
 log = logging.getLogger(__name__)
 
 REQUIRE_LOGIN_TEMPLATE = """
 <p>
     This %s has been configured such that only users who are logged in may use it.%s
 </p>
@@ -36,360 +38,390 @@
 
 If you're having trouble using the link when clicking it from email client, you
 can also copy and paste it into your browser.
 """
 
 
 class User(BaseUser):
-
     @web.expose
-    def index(self, trans, cntrller='user', **kwd):
-        return trans.fill_template('/webapps/tool_shed/user/index.mako', cntrller=cntrller)
+    def index(self, trans, cntrller="user", **kwd):
+        return trans.fill_template("/webapps/tool_shed/user/index.mako", cntrller=cntrller)
 
     @web.expose
-    def login(self, trans, refresh_frames=[], **kwd):
-        '''Handle Galaxy Log in'''
-        referer = trans.request.referer or ''
-        redirect = self.__get_redirect_url(kwd.get('redirect', referer).strip())
-        redirect_url = ''  # always start with redirect_url being empty
-        use_panels = util.string_as_bool(kwd.get('use_panels', False))
-        message = kwd.get('message', '')
-        status = kwd.get('status', 'done')
-        header = ''
+    def login(self, trans, refresh_frames=None, **kwd):
+        """Handle Galaxy Log in"""
+        refresh_frames = refresh_frames or []
+        referer = trans.request.referer or ""
+        redirect = self.__get_redirect_url(kwd.get("redirect", referer).strip())
+        redirect_url = ""  # always start with redirect_url being empty
+        use_panels = util.string_as_bool(kwd.get("use_panels", False))
+        message = kwd.get("message", "")
+        status = kwd.get("status", "done")
+        header = ""
         user = trans.user
         success = False
-        login = kwd.get('login', '')
+        login = kwd.get("login", "")
         if user:
             # Already logged in.
             redirect_url = redirect
-            message = 'You are already logged in.'
-            status = 'info'
-        elif kwd.get('login_button', False):
+            message = "You are already logged in."
+            status = "info"
+        elif kwd.get("login_button", False):
             response = self.__validate_login(trans, **kwd)
             if trans.response.status == 400:
                 trans.response.status = 200
                 message = response.get("err_msg")
                 status = "error"
             elif response.get("expired_user"):
-                change_password_url = url_for(controller='user', action='change_password', id=response.get("expired_user"))
-                message = "%s<br>Click <a href='%s'>here</a> to change your password." % (response.get("message"), change_password_url)
+                change_password_url = url_for(
+                    controller="user", action="change_password", id=response.get("expired_user")
+                )
+                message = f"{response.get('message')}<br>Click <a href='{change_password_url}'>here</a> to change your password."
                 status = "warning"
             else:
                 success = True
             if success:
                 redirect_url = redirect
         if not success and not user and trans.app.config.require_login:
             if trans.app.config.allow_user_creation:
-                create_account_str = "  If you don't already have an account, <a href='%s'>you may create one</a>." % \
-                    web.url_for(controller='user', action='create', cntrller='user')
+                create_account_str = (
+                    "  If you don't already have an account, <a href='%s'>you may create one</a>."
+                    % web.url_for(controller="user", action="create", cntrller="user")
+                )
                 header = REQUIRE_LOGIN_TEMPLATE % ("Galaxy tool shed", create_account_str)
             else:
                 header = REQUIRE_LOGIN_TEMPLATE % ("Galaxy tool shed", "")
-        return trans.fill_template('/webapps/tool_shed/user/login.mako',
-                                   login=login,
-                                   header=header,
-                                   use_panels=use_panels,
-                                   redirect_url=redirect_url,
-                                   redirect=redirect,
-                                   refresh_frames=refresh_frames,
-                                   message=message,
-                                   status=status,
-                                   form_input_auto_focus=True,
-                                   active_view="user")
+        return trans.fill_template(
+            "/webapps/tool_shed/user/login.mako",
+            login=login,
+            header=header,
+            use_panels=use_panels,
+            redirect_url=redirect_url,
+            redirect=redirect,
+            refresh_frames=refresh_frames,
+            message=message,
+            status=status,
+            form_input_auto_focus=True,
+            active_view="user",
+        )
 
     @web.expose
-    def create(self, trans, cntrller='user', redirect_url='', refresh_frames=[], **kwd):
+    def create(self, trans, cntrller="user", redirect_url="", refresh_frames=None, **kwd):
+        refresh_frames = refresh_frames or []
         params = util.Params(kwd)
         # If the honeypot field is not empty we are dealing with a bot.
-        honeypot_field = params.get('bear_field', '')
-        if honeypot_field != '':
-            return trans.show_error_message("You've been flagged as a possible bot. If you are not, please try registering again and fill the form out carefully. <a target=\"_top\" href=\"%s\">Go to the home page</a>.") % url_for('/')
-
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
-        use_panels = util.string_as_bool(kwd.get('use_panels', True))
-        email = util.restore_text(params.get('email', ''))
+        honeypot_field = params.get("bear_field", "")
+        if honeypot_field != "":
+            return trans.show_error_message(
+                'You\'ve been flagged as a possible bot. If you are not, please try registering again and fill the form out carefully. <a target="_top" href="%s">Go to the home page</a>.'
+            ) % url_for("/")
+
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
+        use_panels = util.string_as_bool(kwd.get("use_panels", True))
+        email = util.restore_text(params.get("email", ""))
         # Do not sanitize passwords, so take from kwd
         # instead of params ( which were sanitized )
-        password = kwd.get('password', '')
-        confirm = kwd.get('confirm', '')
-        username = util.restore_text(params.get('username', ''))
-        subscribe = params.get('subscribe', '')
+        password = kwd.get("password", "")
+        confirm = kwd.get("confirm", "")
+        username = util.restore_text(params.get("username", ""))
+        subscribe = params.get("subscribe", "")
         subscribe_checked = CheckboxField.is_checked(subscribe)
-        referer = trans.request.referer or ''
-        redirect = kwd.get('redirect', referer).strip()
+        referer = trans.request.referer or ""
+        redirect = kwd.get("redirect", referer).strip()
         is_admin = trans.user_is_admin
         success = False
         show_user_prepopulate_form = False
         if not trans.app.config.allow_user_creation and not trans.user_is_admin:
-            message = 'User registration is disabled.  Please contact your local Galaxy administrator for an account.'
+            message = "User registration is disabled.  Please contact your local Galaxy administrator for an account."
             if trans.app.config.error_email_to is not None:
-                message += ' Contact: %s' % trans.app.config.error_email_to
-            status = 'error'
+                message += f" Contact: {trans.app.config.error_email_to}"
+            status = "error"
         else:
             # check user is allowed to register
-            message, status = trans.app.auth_manager.check_registration_allowed(email, username, password)
+            message, status = trans.app.auth_manager.check_registration_allowed(
+                email, username, password, trans.request
+            )
             if not message:
                 # Create the user, save all the user info and login to Galaxy
-                if params.get('create_user_button', False):
+                if params.get("create_user_button", False):
                     # Check email and password validity
                     message = self.__validate(trans, email, password, confirm, username)
                     if not message:
                         # All the values are valid
-                        message, status, user, success = self.__register(trans, subscribe_checked=subscribe_checked, **kwd)
+                        message, status, user, success = self.__register(
+                            trans, subscribe_checked=subscribe_checked, **kwd
+                        )
                         if success and not is_admin:
                             # The handle_user_login() method has a call to the history_set_default_permissions() method
                             # (needed when logging in with a history), user needs to have default permissions set before logging in
                             trans.handle_user_login(user)
                             trans.log_event("User created a new account")
                             trans.log_event("User logged in")
                     else:
-                        status = 'error'
+                        status = "error"
         registration_warning_message = trans.app.config.registration_warning_message
         if success:
             if is_admin:
-                redirect_url = web.url_for('/admin/users?status=success&message=Created new user account.')
+                redirect_url = web.url_for("/admin/users?status=success&message=Created new user account.")
             else:
-                redirect_url = web.url_for('/')
-        return trans.fill_template('/webapps/tool_shed/user/register.mako',
-                                   cntrller=cntrller,
-                                   email=email,
-                                   username=username,
-                                   subscribe_checked=subscribe_checked,
-                                   show_user_prepopulate_form=show_user_prepopulate_form,
-                                   use_panels=use_panels,
-                                   redirect=redirect,
-                                   redirect_url=redirect_url,
-                                   refresh_frames=refresh_frames,
-                                   registration_warning_message=registration_warning_message,
-                                   message=message,
-                                   status=status)
+                redirect_url = web.url_for("/")
+        return trans.fill_template(
+            "/webapps/tool_shed/user/register.mako",
+            cntrller=cntrller,
+            email=email,
+            username=username,
+            subscribe_checked=subscribe_checked,
+            show_user_prepopulate_form=show_user_prepopulate_form,
+            use_panels=use_panels,
+            redirect=redirect,
+            redirect_url=redirect_url,
+            refresh_frames=refresh_frames,
+            registration_warning_message=registration_warning_message,
+            message=message,
+            status=status,
+        )
 
     def __register(self, trans, email=None, username=None, password=None, subscribe_checked=False, **kwd):
         """Registers a new user."""
         email = util.restore_text(email)
         username = util.restore_text(username)
         status = None
         message = None
         is_admin = trans.user_is_admin
         user = self.user_manager.create(email=email, username=username, password=password)
         if subscribe_checked:
             # subscribe user to email list
             if trans.app.config.smtp_server is None:
                 status = "error"
-                message = "Now logged in as " + user.email + ". However, subscribing to the mailing list has failed because mail is not configured for this Galaxy instance. <br>Please contact your local Galaxy administrator."
+                message = f"Now logged in as {user.email}. However, subscribing to the mailing list has failed because mail is not configured for this Galaxy instance. <br>Please contact your local Galaxy administrator."
             else:
-                body = 'Join Mailing list.\n'
+                body = "Join Mailing list.\n"
                 to = trans.app.config.mailing_join_addr
                 frm = email
-                subject = 'Join Mailing List'
+                subject = "Join Mailing List"
                 try:
                     util.send_mail(frm, to, subject, body, trans.app.config)
                 except Exception:
-                    log.exception('Subscribing to the mailing list has failed.')
+                    log.exception("Subscribing to the mailing list has failed.")
                     status = "warning"
-                    message = "Now logged in as " + user.email + ". However, subscribing to the mailing list has failed."
+                    message = f"Now logged in as {user.email}. However, subscribing to the mailing list has failed."
         if status != "error":
             if not is_admin:
                 # The handle_user_login() method has a call to the history_set_default_permissions() method
                 # (needed when logging in with a history), user needs to have default permissions set before logging in
                 trans.handle_user_login(user)
                 trans.log_event("User created a new account")
                 trans.log_event("User logged in")
             if trans.app.config.user_activation_on:
                 is_activation_sent = self.user_manager.send_activation_email(trans, email, username)
                 if is_activation_sent:
-                    message = 'Now logged in as %s.<br>Verification email has been sent to your email address. Please verify it by clicking the activation link in the email.<br>Please check your spam/trash folder in case you cannot find the message.<br><a target="_top" href="%s">Return to the home page.</a>' % (escape(user.email), url_for('/'))
+                    message = f"Now logged in as {escape(user.email)}.<br>Verification email has been sent to your email address. Please verify it by clicking the activation link in the email.<br>Please check your spam/trash folder in case you cannot find the message.<br><a target=\"_top\" href=\"{url_for('/')}\">Return to the home page.</a>"
                 else:
                     status = "error"
-                    message = 'Unable to send activation email, please contact your local Galaxy administrator.'
+                    message = "Unable to send activation email, please contact your local Galaxy administrator."
                     if trans.app.config.error_email_to is not None:
-                        message += ' Contact: %s' % trans.app.config.error_email_to
+                        message += f" Contact: {trans.app.config.error_email_to}"
         else:
             # User activation is OFF, proceed without sending the activation email.
-            message = 'Now logged in as %s.<br><a target="_top" href="%s">Return to the home page.</a>' % (escape(user.email), url_for('/'))
+            message = f"Now logged in as {escape(user.email)}.<br><a target=\"_top\" href=\"{url_for('/')}\">Return to the home page.</a>"
         return message, status, user, status is None
 
     @web.expose
     def reset_password(self, trans, email=None, **kwd):
         """Reset the user's password. Send an email with token that allows a password change."""
         if trans.app.config.smtp_server is None:
-            return trans.show_error_message("Mail is not configured for this Galaxy instance "
-                                            "and password reset information cannot be sent. "
-                                            "Please contact your local Galaxy administrator.")
+            return trans.show_error_message(
+                "Mail is not configured for this Galaxy instance "
+                "and password reset information cannot be sent. "
+                "Please contact your local Galaxy administrator."
+            )
         message = None
-        status = 'done'
-        if kwd.get('reset_password_button', False):
+        status = "done"
+        if kwd.get("reset_password_button", False):
             message = validate_email(trans, email, check_dup=False)
             if not message:
                 # Default to a non-userinfo-leaking response message
-                message = ("Your reset request for %s has been received.  "
-                           "Please check your email account for more instructions.  "
-                           "If you do not receive an email shortly, please contact an administrator." % (escape(email)))
-                reset_user = trans.sa_session.query(trans.app.model.User).filter(trans.app.model.User.table.c.email == email).first()
+                message = (
+                    "Your reset request for %s has been received.  "
+                    "Please check your email account for more instructions.  "
+                    "If you do not receive an email shortly, please contact an administrator." % (escape(email))
+                )
+                reset_user = (
+                    trans.sa_session.query(trans.app.model.User)
+                    .filter(trans.app.model.User.table.c.email == email)
+                    .first()
+                )
                 if not reset_user:
                     # Perform a case-insensitive check only if the user wasn't found
-                    reset_user = trans.sa_session.query(trans.app.model.User).filter(func.lower(trans.app.model.User.table.c.email) == func.lower(email)).first()
+                    reset_user = (
+                        trans.sa_session.query(trans.app.model.User)
+                        .filter(func.lower(trans.app.model.User.table.c.email) == func.lower(email))
+                        .first()
+                    )
                 if reset_user:
                     prt = trans.app.model.PasswordResetToken(reset_user)
                     trans.sa_session.add(prt)
                     trans.sa_session.flush()
-                    host = trans.request.host.split(':')[0]
-                    if host in ['localhost', '127.0.0.1', '0.0.0.0']:
+                    host = trans.request.host.split(":")[0]
+                    if host in ["localhost", "127.0.0.1", "0.0.0.0"]:
                         host = socket.getfqdn()
-                    reset_url = url_for(controller='user',
-                                        action="change_password",
-                                        token=prt.token, qualified=True)
-                    body = PASSWORD_RESET_TEMPLATE % (host, prt.expiration_time.strftime(trans.app.config.pretty_datetime_format),
-                                                      reset_url)
-                    frm = trans.app.config.email_from or 'galaxy-no-reply@' + host
-                    subject = 'Galaxy Password Reset'
+                    reset_url = url_for(controller="user", action="change_password", token=prt.token, qualified=True)
+                    body = PASSWORD_RESET_TEMPLATE % (
+                        host,
+                        prt.expiration_time.strftime(trans.app.config.pretty_datetime_format),
+                        reset_url,
+                    )
+                    frm = trans.app.config.email_from or f"galaxy-no-reply@{host}"
+                    subject = "Galaxy Password Reset"
                     try:
                         util.send_mail(frm, email, subject, body, trans.app.config)
                         trans.sa_session.add(reset_user)
                         trans.sa_session.flush()
-                        trans.log_event("User reset password: %s" % email)
+                        trans.log_event(f"User reset password: {email}")
                     except Exception:
-                        log.exception('Unable to reset password.')
-        return trans.fill_template('/webapps/tool_shed/user/reset_password.mako',
-                                   message=message,
-                                   status=status)
+                        log.exception("Unable to reset password.")
+        return trans.fill_template("/webapps/tool_shed/user/reset_password.mako", message=message, status=status)
 
     @web.expose
     def manage_user_info(self, trans, cntrller, **kwd):
-        '''Manage a user's login, password, public username, type, addresses, etc.'''
+        """Manage a user's login, password, public username, type, addresses, etc."""
         params = util.Params(kwd)
-        user_id = params.get('id', None)
+        user_id = params.get("id", None)
         if user_id:
             user = trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(user_id))
         else:
             user = trans.user
         if not user:
-            raise AssertionError("The user id (%s) is not valid" % str(user_id))
-        email = util.restore_text(params.get('email', user.email))
-        username = util.restore_text(params.get('username', ''))
+            raise AssertionError(f"The user id ({str(user_id)}) is not valid")
+        email = util.restore_text(params.get("email", user.email))
+        username = util.restore_text(params.get("username", ""))
         if not username:
             username = user.username
-        message = escape(util.restore_text(params.get('message', '')))
-        status = params.get('status', 'done')
-        return trans.fill_template('/webapps/tool_shed/user/manage_info.mako',
-                                   cntrller=cntrller,
-                                   user=user,
-                                   email=email,
-                                   username=username,
-                                   message=message,
-                                   status=status)
+        message = escape(util.restore_text(params.get("message", "")))
+        status = params.get("status", "done")
+        return trans.fill_template(
+            "/webapps/tool_shed/user/manage_info.mako",
+            cntrller=cntrller,
+            user=user,
+            email=email,
+            username=username,
+            message=message,
+            status=status,
+        )
 
     @web.expose
-    @web.require_login()
+    @require_login()
     def api_keys(self, trans, cntrller, **kwd):
         params = util.Params(kwd)
-        message = escape(util.restore_text(params.get('message', '')))
-        status = params.get('status', 'done')
-        if params.get('new_api_key_button', False):
-            self.create_api_key(trans, trans.user)
+        message = escape(util.restore_text(params.get("message", "")))
+        status = params.get("status", "done")
+        if params.get("new_api_key_button", False):
+            ApiKeyManager(trans.app).create_api_key(trans.user)
             message = "Generated a new web API key"
             status = "done"
-        return trans.fill_template('/webapps/tool_shed/user/api_keys.mako',
-                                   cntrller=cntrller,
-                                   user=trans.user,
-                                   message=message,
-                                   status=status)
+        return trans.fill_template(
+            "/webapps/tool_shed/user/api_keys.mako", cntrller=cntrller, user=trans.user, message=message, status=status
+        )
 
     # For REMOTE_USER, we need the ability to just edit the username
     @web.expose
-    @web.require_login("to manage the public name")
+    @require_login("to manage the public name")
     def edit_username(self, trans, cntrller, **kwd):
         params = util.Params(kwd)
-        is_admin = cntrller == 'admin' and trans.user_is_admin
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
-        user_id = params.get('user_id', None)
+        is_admin = cntrller == "admin" and trans.user_is_admin
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
+        user_id = params.get("user_id", None)
         if user_id and is_admin:
             user = trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(user_id))
         else:
             user = trans.user
-        if user and params.get('change_username_button', False):
-            username = kwd.get('username', '')
+        if user and params.get("change_username_button", False):
+            username = kwd.get("username", "")
             if username:
                 message = validate_publicname(trans, username, user)
             if message:
-                status = 'error'
+                status = "error"
             else:
                 user.username = username
                 trans.sa_session.add(user)
                 trans.sa_session.flush()
-                message = 'The username has been updated with the changes.'
-        return trans.fill_template('/webapps/tool_shed/user/username.mako',
-                                   cntrller=cntrller,
-                                   user=user,
-                                   username=user.username,
-                                   message=message,
-                                   status=status)
+                message = "The username has been updated with the changes."
+        return trans.fill_template(
+            "/webapps/tool_shed/user/username.mako",
+            cntrller=cntrller,
+            user=user,
+            username=user.username,
+            message=message,
+            status=status,
+        )
 
     @web.expose
     def edit_info(self, trans, cntrller, **kwd):
         """
         Edit user information = username, email or password.
         """
         params = util.Params(kwd)
-        is_admin = cntrller == 'admin' and trans.user_is_admin
-        message = util.restore_text(params.get('message', ''))
-        status = params.get('status', 'done')
-        user_id = params.get('user_id', None)
+        is_admin = cntrller == "admin" and trans.user_is_admin
+        message = util.restore_text(params.get("message", ""))
+        status = params.get("status", "done")
+        user_id = params.get("user_id", None)
         if user_id and is_admin:
             user = trans.sa_session.query(trans.app.model.User).get(trans.security.decode_id(user_id))
         elif user_id and (not trans.user or trans.user.id != trans.security.decode_id(user_id)):
-            message = 'Invalid user id'
-            status = 'error'
+            message = "Invalid user id"
+            status = "error"
             user = None
         else:
             user = trans.user
-        if user and params.get('login_info_button', False):
+        if user and params.get("login_info_button", False):
             # Editing email and username
-            email = util.restore_text(params.get('email', ''))
-            username = util.restore_text(params.get('username', '')).lower()
+            email = util.restore_text(params.get("email", ""))
+            username = util.restore_text(params.get("username", "")).lower()
 
             # Validate the new values for email and username
             message = validate_email(trans, email, user)
             if not message and username:
                 message = validate_publicname(trans, username, user)
             if message:
-                status = 'error'
+                status = "error"
             else:
-                if (user.email != email):
+                if user.email != email:
                     # The user's private role name must match the user's login ( email )
                     private_role = trans.app.security_agent.get_private_user_role(user)
                     private_role.name = email
-                    private_role.description = 'Private role for ' + email
+                    private_role.description = f"Private role for {email}"
                     # Change the email itself
                     user.email = email
                     trans.sa_session.add_all((user, private_role))
                     trans.sa_session.flush()
-                    if trans.webapp.name == 'galaxy' and trans.app.config.user_activation_on:
+                    if trans.webapp.name == "galaxy" and trans.app.config.user_activation_on:
                         user.active = False
                         trans.sa_session.add(user)
                         trans.sa_session.flush()
                         is_activation_sent = self.user_manager.send_activation_email(trans, user.email, user.username)
                         if is_activation_sent:
-                            message = 'The login information has been updated with the changes.<br>Verification email has been sent to your new email address. Please verify it by clicking the activation link in the email.<br>Please check your spam/trash folder in case you cannot find the message.'
+                            message = "The login information has been updated with the changes.<br>Verification email has been sent to your new email address. Please verify it by clicking the activation link in the email.<br>Please check your spam/trash folder in case you cannot find the message."
                         else:
-                            message = 'Unable to send activation email, please contact your local Galaxy administrator.'
+                            message = "Unable to send activation email, please contact your local Galaxy administrator."
                             if trans.app.config.error_email_to is not None:
-                                message += ' Contact: %s' % trans.app.config.error_email_to
-                if (user.username != username):
+                                message += f" Contact: {trans.app.config.error_email_to}"
+                if user.username != username:
                     user.username = username
                     trans.sa_session.add(user)
                     trans.sa_session.flush()
-                message = 'The login information has been updated with the changes.'
-        elif user and params.get('edit_user_info_button', False):
+                message = "The login information has been updated with the changes."
+        elif user and params.get("edit_user_info_button", False):
             # Edit user information - webapp MUST BE 'galaxy'
-            user_type_fd_id = params.get('user_type_fd_id', 'none')
-            if user_type_fd_id not in ['none']:
-                user_type_form_definition = trans.sa_session.query(trans.app.model.FormDefinition).get(trans.security.decode_id(user_type_fd_id))
+            user_type_fd_id = params.get("user_type_fd_id", "none")
+            if user_type_fd_id not in ["none"]:
+                user_type_form_definition = trans.sa_session.query(trans.app.model.FormDefinition).get(
+                    trans.security.decode_id(user_type_fd_id)
+                )
             elif user.values:
                 user_type_form_definition = user.values.form_definition
             else:
                 # User was created before any of the user_info forms were created
                 user_type_form_definition = None
             if user_type_form_definition:
                 values = self.get_form_values(trans, user, user_type_form_definition, **kwd)
@@ -406,57 +438,66 @@
                 trans.sa_session.add(form_values)
                 user.values = form_values
                 flush_needed = True
             if flush_needed:
                 trans.sa_session.add(user)
                 trans.sa_session.flush()
             message = "The user information has been updated with the changes."
-        if user and trans.webapp.name == 'galaxy' and is_admin:
-            kwd['user_id'] = trans.security.encode_id(user.id)
-        kwd['id'] = user_id
+        if user and trans.webapp.name == "galaxy" and is_admin:
+            kwd["user_id"] = trans.security.encode_id(user.id)
+        kwd["id"] = user_id
         if message:
-            kwd['message'] = util.sanitize_text(message)
+            kwd["message"] = util.sanitize_text(message)
         if status:
-            kwd['status'] = status
-        return trans.response.send_redirect(web.url_for(controller='user',
-                                                        action='manage_user_info',
-                                                        cntrller=cntrller,
-                                                        **kwd))
+            kwd["status"] = status
+        return trans.response.send_redirect(
+            web.url_for(controller="user", action="manage_user_info", cntrller=cntrller, **kwd)
+        )
 
     @web.expose
     def change_password(self, trans, token=None, id=None, **kwd):
         """
         Provides a form with which one can change their password.  If token is
         provided, don't require current password.
         """
-        if kwd.get('change_password_button', False):
-            password = kwd.get('password', '')
-            confirm = kwd.get('confirm', '')
-            current = kwd.get('current', '')
-            user, message = self.user_manager.change_password(trans, password=password,
-                current=current, token=token, confirm=confirm, id=id)
+        if kwd.get("change_password_button", False):
+            password = kwd.get("password", "")
+            confirm = kwd.get("confirm", "")
+            current = kwd.get("current", "")
+            user, message = self.user_manager.change_password(
+                trans, password=password, current=current, token=token, confirm=confirm, id=id
+            )
             if not user:
                 return trans.show_error_message(message)
-            return trans.show_ok_message('The password has been changed and any other existing Galaxy sessions have been logged out (but jobs in histories in those sessions will not be interrupted).')
-        return trans.fill_template('/webapps/tool_shed/user/change_password.mako', token=token, id=id)
+            return trans.show_ok_message(
+                "The password has been changed and any other existing Galaxy sessions have been logged out (but jobs in histories in those sessions will not be interrupted)."
+            )
+        return trans.fill_template("/webapps/tool_shed/user/change_password.mako", token=token, id=id)
 
     @web.expose
     def logout(self, trans, logout_all=False, **kwd):
         trans.handle_user_logout(logout_all=logout_all)
-        message = 'You have been logged out.<br>To log in again <a target="_top" href="%s">go to the home page</a>.' % \
-            (url_for('/'))
+        message = 'You have been logged out.<br>To log in again <a target="_top" href="%s">go to the home page</a>.' % (
+            url_for("/")
+        )
         if trans.app.config.use_remote_user and trans.app.config.remote_user_logout_href:
             trans.response.send_redirect(trans.app.config.remote_user_logout_href)
         else:
-            return trans.fill_template('/webapps/tool_shed/user/logout.mako',
-                                       refresh_frames=['masthead'],
-                                       message=message,
-                                       status='done',
-                                       active_view="user")
+            return trans.fill_template(
+                "/webapps/tool_shed/user/logout.mako",
+                refresh_frames=["masthead"],
+                message=message,
+                status="done",
+                active_view="user",
+            )
 
     def __validate(self, trans, email, password, confirm, username):
-        if username in ['repos']:
-            return "The term '%s' is a reserved word in the Tool Shed, so it cannot be used as a public user name." % username
-        message = "\n".join((validate_email(trans, email),
-                             validate_password(trans, password, confirm),
-                             validate_publicname(trans, username))).rstrip()
+        if username in ["repos"]:
+            return f"The term '{username}' is a reserved word in the Tool Shed, so it cannot be used as a public user name."
+        message = "\n".join(
+            (
+                validate_email(trans, email),
+                validate_password(trans, password, confirm),
+                validate_publicname(trans, username),
+            )
+        ).rstrip()
         return message
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/framework/middleware/remoteuser.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/framework/middleware/remoteuser.py`

 * *Files 6% similar despite different names*

```diff
@@ -33,33 +33,33 @@
             <p>%s</p>
         </div>
     </body>
 </html>
 """
 
 
-class RemoteUser(object):
+class RemoteUser:
     def __init__(self, app, maildomain=None, display_servers=None, admin_users=None, remote_user_secret_header=None):
         self.app = app
         self.maildomain = maildomain
         self.display_servers = display_servers or []
         self.admin_users = admin_users or []
         self.config_secret_header = remote_user_secret_header
 
     def __call__(self, environ, start_response):
-        environ['webapp'] = 'tool_shed'
+        environ["webapp"] = "tool_shed"
         # Allow display servers
-        if self.display_servers and 'REMOTE_ADDR' in environ:
+        if self.display_servers and "REMOTE_ADDR" in environ:
             try:
-                host = socket.gethostbyaddr(environ['REMOTE_ADDR'])[0]
-            except(socket.error, socket.herror, socket.gaierror, socket.timeout):
+                host = socket.gethostbyaddr(environ["REMOTE_ADDR"])[0]
+            except (OSError, socket.herror, socket.gaierror, socket.timeout):
                 # in the event of a lookup failure, deny access
                 host = None
             if host in self.display_servers:
-                environ['HTTP_REMOTE_USER'] = 'remote_display_server@%s' % (self.maildomain or 'example.org')
+                environ["HTTP_REMOTE_USER"] = "remote_display_server@%s" % (self.maildomain or "example.org")
                 return self.app(environ, start_response)
 
         # If the secret header is enabled, we expect upstream to send along some key
         # in HTTP_GX_SECRET, so we'll need to compare that here to the correct value
         #
         # This is not an ideal location for this function.  The reason being
         # that because this check is done BEFORE the REMOTE_USER check,  it is
@@ -67,15 +67,15 @@
         # credentials. However, that's why it's not "ideal", but it is "good
         # enough". The only users able to exploit this are ones with access to
         # the local system (unless Galaxy is listening on 0.0.0.0....). It
         # seems improbable that an attacker with access to the server hosting
         # Galaxy would not have access to Galaxy itself, and be attempting to
         # attack the system
         if self.config_secret_header is not None:
-            if not safe_str_cmp(environ.get('HTTP_GX_SECRET'), self.config_secret_header):
+            if not safe_str_cmp(environ.get("HTTP_GX_SECRET"), self.config_secret_header):
                 title = "Access to Galaxy is denied"
                 message = """
                 Galaxy is configured to authenticate users via an external
                 method (such as HTTP authentication in Apache), but an
                 incorrect shared secret key was provided by the
                 upstream (proxy) server.</p>
                 <p>Please contact your local Galaxy administrator.  The
@@ -83,44 +83,46 @@
                 <code>GX_SECRET</code> header must be set before you may
                 access Galaxy.
                 """
                 return self.error(start_response, title, message)
 
         # Apache sets REMOTE_USER to the string '(null)' when using the Rewrite* method for passing REMOTE_USER and a user is
         # un-authenticated.  Any other possible values need to go here as well.
-        path_info = environ.get('PATH_INFO', '')
-        if 'HTTP_REMOTE_USER' in environ and environ['HTTP_REMOTE_USER'] != '(null)':
-            if not environ['HTTP_REMOTE_USER'].count('@'):
+        path_info = environ.get("PATH_INFO", "")
+        if "HTTP_REMOTE_USER" in environ and environ["HTTP_REMOTE_USER"] != "(null)":
+            if not environ["HTTP_REMOTE_USER"].count("@"):
                 if self.maildomain is not None:
-                    environ['HTTP_REMOTE_USER'] += '@' + self.maildomain
+                    environ["HTTP_REMOTE_USER"] += f"@{self.maildomain}"
                 else:
                     title = "Access to this Galaxy tool shed is denied"
                     message = """
                         This Galaxy tool shed is configured to authenticate users via an external
                         method (such as HTTP authentication in Apache), but only a username (not
                         an email address) was provided by the upstream (proxy) server.  Since tool
                         shed usernames are email addresses, a default mail domain must be set.</[>
                         <p>The variable <code>remote_user_maildomain</code> must be set before you
                         can access this tool shed.  Contact your local tool shed administrator.
                     """
                     return self.error(start_response, title, message)
             return self.app(environ, start_response)
-        elif path_info.startswith('/api/'):
+        elif path_info.startswith("/api/"):
             # The API handles its own authentication via keys
             return self.app(environ, start_response)
-        elif path_info.startswith('/user/api_keys'):
+        elif path_info.startswith("/user/api_keys"):
             # api_keys can be managed when remote_user is in use.
             pass
         else:
             title = "Access to this Galaxy tool shed is denied"
             message = """
                 This Galaxy tool shed is configured to authenticate users via an external
                 method (such as HTTP authentication in Apache), but a username was not
                 provided by the upstream (proxy) server.  This is generally due to a
                 misconfiguration in the upstream server.</p>
                 <p>Contact your local Galaxy tool shed administrator.
             """
             return self.error(start_response, title, message)
 
-    def error(self, start_response, title="Access denied", message="Contact your local Galaxy tool shed administrator."):
-        start_response('403 Forbidden', [('Content-type', 'text/html')])
+    def error(
+        self, start_response, title="Access denied", message="Contact your local Galaxy tool shed administrator."
+    ):
+        start_response("403 Forbidden", [("Content-type", "text/html")])
         return [errorpage % (title, message)]
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/check.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/check.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,41 +1,50 @@
 import logging
 import os.path
 import sys
 
-from migrate.versioning import repository, schema
-from sqlalchemy import create_engine, MetaData, Table
+from migrate.versioning import (
+    repository,
+    schema,
+)
+from sqlalchemy import (
+    create_engine,
+    MetaData,
+    Table,
+)
 from sqlalchemy.exc import NoSuchTableError
-from sqlalchemy_utils import (
+
+from galaxy.model.database_utils import (
     create_database,
     database_exists,
 )
 
 log = logging.getLogger(__name__)
 
 # path relative to galaxy
-migrate_repository_directory = os.path.dirname(__file__).replace(os.getcwd() + os.path.sep, '', 1)
+migrate_repository_directory = os.path.dirname(__file__).replace(os.getcwd() + os.path.sep, "", 1)
 migrate_repository = repository.Repository(migrate_repository_directory)
 
 
-def create_or_verify_database(url, engine_options={}):
+def create_or_verify_database(url, engine_options=None):
     """
-    Check that the database is use-able, possibly creating it if empty (this is
+    Check that the database is useable, possibly creating it if empty (this is
     the only time we automatically create tables, otherwise we force the
     user to do it using the management script so they can create backups).
 
     1) Empty database --> initialize with latest version and return
     2) Database older than migration support --> fail and require manual update
     3) Database at state where migrate support introduced --> add version control information but make no changes (might still require manual update)
-    4) Database versioned but out of date --> fail with informative message, user must run "sh manage_db.sh upgrade"
+    4) Database versioned but out of date --> fail with informative message, user must run "sh manage_toolshed_db.sh upgrade"
 
     """
+    engine_options = engine_options or {}
     # Create engine and metadata
     if not database_exists(url):
-        message = "Creating database for URI [%s]" % url
+        message = f"Creating database for URI [{url}]"
         log.info(message)
         create_database(url)
 
     engine = create_engine(url, **engine_options)
     meta = MetaData(bind=engine)
     # Try to load dataset table
     try:
@@ -63,39 +72,43 @@
             Table("metadata_file", meta, autoload=True)
             schema.ControlledSchema.create(engine, migrate_repository, version=2)
         except NoSuchTableError:
             schema.ControlledSchema.create(engine, migrate_repository, version=1)
     # Verify that the code and the DB are in sync
     db_schema = schema.ControlledSchema(engine, migrate_repository)
     if migrate_repository.versions.latest != db_schema.version:
-        exception_msg = "Your database has version '%d' but this code expects version '%d'.  " % (db_schema.version, migrate_repository.versions.latest)
+        exception_msg = "Your database has version '%d' but this code expects version '%d'.  " % (
+            db_schema.version,
+            migrate_repository.versions.latest,
+        )
         exception_msg += "Back up your database and then migrate the schema by running the following from your Galaxy installation directory:"
-        exception_msg += "\n\nsh manage_db.sh upgrade tool_shed\n"
+        exception_msg += "\n\nsh manage_toolshed_db.sh upgrade\n"
         raise Exception(exception_msg)
     else:
         log.info("At database version %d" % db_schema.version)
 
 
 def migrate_to_current_version(engine, schema):
     # Changes to get to current version
     changeset = schema.changeset(None)
     for ver, change in changeset:
         nextver = ver + changeset.step
-        log.info('Migrating %s -> %s... ' % (ver, nextver))
+        log.info(f"Migrating {ver} -> {nextver}... ")
         old_stdout = sys.stdout
 
-        class FakeStdout(object):
+        class FakeStdout:
             def __init__(self):
                 self.buffer = []
 
             def write(self, s):
                 self.buffer.append(s)
 
             def flush(self):
                 pass
+
         sys.stdout = FakeStdout()
         try:
             schema.runchange(ver, change, changeset.step)
         finally:
             for message in "".join(sys.stdout.buffer).split("\n"):
                 log.info(message)
             sys.stdout = old_stdout
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/migrate.cfg` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/migrate.cfg`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0002_add_tool_suite_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0002_add_tool_suite_column.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,16 +1,20 @@
 """
 Migration script to add the suite column to the tool table.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, MetaData, Table
+from sqlalchemy import (
+    Boolean,
+    Column,
+    MetaData,
+    Table,
+)
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
@@ -24,22 +28,22 @@
     print(__doc__)
     metadata.reflect()
     # Create and initialize imported column in job table.
     Tool_table = Table("tool", metadata, autoload=True)
     c = Column("suite", Boolean, default=False, index=True)
     try:
         # Create
-        c.create(Tool_table, index_name='ix_tool_suite')
+        c.create(Tool_table, index_name="ix_tool_suite")
         assert c is Tool_table.c.suite
         # Initialize.
-        if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+        if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
             default_false = "0"
-        elif migrate_engine.name in ['postgresql', 'postgres']:
+        elif migrate_engine.name in ["postgresql", "postgres"]:
             default_false = "false"
-        migrate_engine.execute("UPDATE tool SET suite=%s" % default_false)
+        migrate_engine.execute(f"UPDATE tool SET suite={default_false}")
     except Exception:
         log.exception("Adding suite column to the tool table failed.")
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0003_review_and_review_association_tables.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0003_review_and_review_association_tables.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,37 +1,47 @@
 """
 Adds the tool_rating_association table, enabling tools to be rated along with review comments.
 """
-from __future__ import print_function
 
 import datetime
 import logging
 import sys
 
-from sqlalchemy import Column, DateTime, ForeignKey, Integer, MetaData, Table, TEXT
+from sqlalchemy import (
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    Table,
+    TEXT,
+)
 
 now = datetime.datetime.utcnow
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
 metadata = MetaData()
 
-ToolRatingAssociation_table = Table("tool_rating_association", metadata,
-                                    Column("id", Integer, primary_key=True),
-                                    Column("create_time", DateTime, default=now),
-                                    Column("update_time", DateTime, default=now, onupdate=now),
-                                    Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
-                                    Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
-                                    Column("rating", Integer, index=True),
-                                    Column("comment", TEXT))
+ToolRatingAssociation_table = Table(
+    "tool_rating_association",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("create_time", DateTime, default=now),
+    Column("update_time", DateTime, default=now, onupdate=now),
+    Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
+    Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+    Column("rating", Integer, index=True),
+    Column("comment", TEXT),
+)
 
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     # Load existing tables
     metadata.reflect()
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0004_repository_tables.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0004_repository_tables.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,17 +1,25 @@
 """
 Adds the repository, repository_rating_association and repository_category_association tables.
 """
-from __future__ import print_function
 
 import datetime
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, DateTime, ForeignKey, Integer, MetaData, Table, TEXT
+from sqlalchemy import (
+    Boolean,
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    Table,
+    TEXT,
+)
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import TrimmedString
 
 now = datetime.datetime.utcnow
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
@@ -19,37 +27,46 @@
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
 metadata = MetaData()
 
-Repository_table = Table("repository", metadata,
-                         Column("id", Integer, primary_key=True),
-                         Column("create_time", DateTime, default=now),
-                         Column("update_time", DateTime, default=now, onupdate=now),
-                         Column("name", TrimmedString(255), index=True),
-                         Column("description", TEXT),
-                         Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
-                         Column("private", Boolean, default=False),
-                         Column("deleted", Boolean, index=True, default=False))
-
-RepositoryRatingAssociation_table = Table("repository_rating_association", metadata,
-                                          Column("id", Integer, primary_key=True),
-                                          Column("create_time", DateTime, default=now),
-                                          Column("update_time", DateTime, default=now, onupdate=now),
-                                          Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
-                                          Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
-                                          Column("rating", Integer, index=True),
-                                          Column("comment", TEXT))
-
-RepositoryCategoryAssociation_table = Table("repository_category_association", metadata,
-                                            Column("id", Integer, primary_key=True),
-                                            Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
-                                            Column("category_id", Integer, ForeignKey("category.id"), index=True))
+Repository_table = Table(
+    "repository",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("create_time", DateTime, default=now),
+    Column("update_time", DateTime, default=now, onupdate=now),
+    Column("name", TrimmedString(255), index=True),
+    Column("description", TEXT),
+    Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+    Column("private", Boolean, default=False),
+    Column("deleted", Boolean, index=True, default=False),
+)
+
+RepositoryRatingAssociation_table = Table(
+    "repository_rating_association",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("create_time", DateTime, default=now),
+    Column("update_time", DateTime, default=now, onupdate=now),
+    Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
+    Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+    Column("rating", Integer, index=True),
+    Column("comment", TEXT),
+)
+
+RepositoryCategoryAssociation_table = Table(
+    "repository_category_association",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
+    Column("category_id", Integer, ForeignKey("category.id"), index=True),
+)
 
 
 def upgrade(migrate_engine):
     print(__doc__)
     # Load existing tables
     metadata.bind = migrate_engine
     metadata.reflect()
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0005_drop_tool_related_tables.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0005_drop_tool_related_tables.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,27 @@
 """
 Drops the tool, tool_category_association, event, tool_event_association, tool_rating_association,
 tool_tag_association and tool_annotation_association tables since they are no longer used in the
 next-gen tool shed.
 """
-from __future__ import print_function
 
 import datetime
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, DateTime, ForeignKey, Integer, MetaData, Table, TEXT
+from sqlalchemy import (
+    Boolean,
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    Table,
+    TEXT,
+)
 from sqlalchemy.exc import NoSuchTableError
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import TrimmedString
 
 now = datetime.datetime.utcnow
 log = logging.getLogger(__name__)
@@ -99,70 +107,91 @@
 
 def downgrade(migrate_engine):
     # Load existing tables
     metadata.bind = migrate_engine
     metadata.reflect()
     # We've lost all of our data, so downgrading is useless. However, we'll
     # at least re-create the dropped tables.
-    Event_table = Table('event', metadata,
-                        Column("id", Integer, primary_key=True),
-                        Column("create_time", DateTime, default=now),
-                        Column("update_time", DateTime, default=now, onupdate=now),
-                        Column("state", TrimmedString(255), index=True),
-                        Column("comment", TEXT))
-
-    Tool_table = Table("tool", metadata,
-                       Column("id", Integer, primary_key=True),
-                       Column("guid", TrimmedString(255), index=True, unique=True),
-                       Column("tool_id", TrimmedString(255), index=True),
-                       Column("create_time", DateTime, default=now),
-                       Column("update_time", DateTime, default=now, onupdate=now),
-                       Column("newer_version_id", Integer, ForeignKey("tool.id"), nullable=True),
-                       Column("name", TrimmedString(255), index=True),
-                       Column("description", TEXT),
-                       Column("user_description", TEXT),
-                       Column("version", TrimmedString(255)),
-                       Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
-                       Column("external_filename", TEXT),
-                       Column("deleted", Boolean, index=True, default=False),
-                       Column("suite", Boolean, default=False, index=True))
-
-    ToolCategoryAssociation_table = Table("tool_category_association", metadata,
-                                          Column("id", Integer, primary_key=True),
-                                          Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
-                                          Column("category_id", Integer, ForeignKey("category.id"), index=True))
-
-    ToolEventAssociation_table = Table("tool_event_association", metadata,
-                                       Column("id", Integer, primary_key=True),
-                                       Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
-                                       Column("event_id", Integer, ForeignKey("event.id"), index=True))
-
-    ToolRatingAssociation_table = Table("tool_rating_association", metadata,
-                                        Column("id", Integer, primary_key=True),
-                                        Column("create_time", DateTime, default=now),
-                                        Column("update_time", DateTime, default=now, onupdate=now),
-                                        Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
-                                        Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
-                                        Column("rating", Integer, index=True),
-                                        Column("comment", TEXT))
-
-    ToolTagAssociation_table = Table("tool_tag_association", metadata,
-                                     Column("id", Integer, primary_key=True),
-                                     Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
-                                     Column("tag_id", Integer, ForeignKey("tag.id"), index=True),
-                                     Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
-                                     Column("user_tname", TrimmedString(255), index=True),
-                                     Column("value", TrimmedString(255), index=True),
-                                     Column("user_value", TrimmedString(255), index=True))
-
-    ToolAnnotationAssociation_table = Table("tool_annotation_association", metadata,
-                                            Column("id", Integer, primary_key=True),
-                                            Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
-                                            Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
-                                            Column("annotation", TEXT, index=True))
+    Event_table = Table(
+        "event",
+        metadata,
+        Column("id", Integer, primary_key=True),
+        Column("create_time", DateTime, default=now),
+        Column("update_time", DateTime, default=now, onupdate=now),
+        Column("state", TrimmedString(255), index=True),
+        Column("comment", TEXT),
+    )
+
+    Tool_table = Table(
+        "tool",
+        metadata,
+        Column("id", Integer, primary_key=True),
+        Column("guid", TrimmedString(255), index=True, unique=True),
+        Column("tool_id", TrimmedString(255), index=True),
+        Column("create_time", DateTime, default=now),
+        Column("update_time", DateTime, default=now, onupdate=now),
+        Column("newer_version_id", Integer, ForeignKey("tool.id"), nullable=True),
+        Column("name", TrimmedString(255), index=True),
+        Column("description", TEXT),
+        Column("user_description", TEXT),
+        Column("version", TrimmedString(255)),
+        Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+        Column("external_filename", TEXT),
+        Column("deleted", Boolean, index=True, default=False),
+        Column("suite", Boolean, default=False, index=True),
+    )
+
+    ToolCategoryAssociation_table = Table(
+        "tool_category_association",
+        metadata,
+        Column("id", Integer, primary_key=True),
+        Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
+        Column("category_id", Integer, ForeignKey("category.id"), index=True),
+    )
+
+    ToolEventAssociation_table = Table(
+        "tool_event_association",
+        metadata,
+        Column("id", Integer, primary_key=True),
+        Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
+        Column("event_id", Integer, ForeignKey("event.id"), index=True),
+    )
+
+    ToolRatingAssociation_table = Table(
+        "tool_rating_association",
+        metadata,
+        Column("id", Integer, primary_key=True),
+        Column("create_time", DateTime, default=now),
+        Column("update_time", DateTime, default=now, onupdate=now),
+        Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
+        Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+        Column("rating", Integer, index=True),
+        Column("comment", TEXT),
+    )
+
+    ToolTagAssociation_table = Table(
+        "tool_tag_association",
+        metadata,
+        Column("id", Integer, primary_key=True),
+        Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
+        Column("tag_id", Integer, ForeignKey("tag.id"), index=True),
+        Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+        Column("user_tname", TrimmedString(255), index=True),
+        Column("value", TrimmedString(255), index=True),
+        Column("user_value", TrimmedString(255), index=True),
+    )
+
+    ToolAnnotationAssociation_table = Table(
+        "tool_annotation_association",
+        metadata,
+        Column("id", Integer, primary_key=True),
+        Column("tool_id", Integer, ForeignKey("tool.id"), index=True),
+        Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+        Column("annotation", TEXT, index=True),
+    )
 
     # Create the event table
     try:
         Event_table.create()
     except Exception:
         log.exception("Creating event table failed.")
     # Create the tool table
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0006_add_email_alerts_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0006_add_email_alerts_column.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 """
 Migration script to add the email_alerts column to the repository table.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Column, MetaData, Table
+from sqlalchemy import (
+    Column,
+    MetaData,
+    Table,
+)
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import JSONType
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0007_add_long_description_times_downloaded_columns.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0007_add_long_description_times_downloaded_columns.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,21 @@
 """
 Migration script to add the long_description and times_downloaded columns to the repository table.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Column, Integer, MetaData, Table, TEXT
+from sqlalchemy import (
+    Column,
+    Integer,
+    MetaData,
+    Table,
+    TEXT,
+)
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0008_add_repository_metadata_table.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0009_add_malicious_column.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,53 +1,55 @@
 """
-Migration script to add the repository_metadata table.
+Migration script to add the malicious column to the repository_metadata table.
 """
-from __future__ import print_function
 
-import datetime
 import logging
 import sys
 
-from sqlalchemy import Column, DateTime, ForeignKey, Integer, MetaData, Table
+from sqlalchemy import (
+    Boolean,
+    Column,
+    MetaData,
+    Table,
+)
 
-# Need our custom types, but don't import anything else from model
-from galaxy.model.custom_types import JSONType, TrimmedString
-
-now = datetime.datetime.utcnow
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
 metadata = MetaData()
 
-RepositoryMetadata_table = Table("repository_metadata", metadata,
-                                 Column("id", Integer, primary_key=True),
-                                 Column("create_time", DateTime, default=now),
-                                 Column("update_time", DateTime, default=now, onupdate=now),
-                                 Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
-                                 Column("changeset_revision", TrimmedString(255), index=True),
-                                 Column("metadata", JSONType, nullable=True))
-
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
-    # Create repository_metadata table.
+    # Create and initialize imported column in job table.
+    Repository_metadata_table = Table("repository_metadata", metadata, autoload=True)
+    c = Column("malicious", Boolean, default=False, index=True)
     try:
-        RepositoryMetadata_table.create()
+        # Create
+        c.create(Repository_metadata_table, index_name="ix_repository_metadata_malicious")
+        assert c is Repository_metadata_table.c.malicious
+        # Initialize.
+        if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
+            default_false = "0"
+        elif migrate_engine.name in ["postgresql", "postgres"]:
+            default_false = "false"
+        migrate_engine.execute(f"UPDATE repository_metadata SET malicious={default_false}")
     except Exception:
-        log.exception("Creating repository_metadata table failed.")
+        log.exception("Adding malicious column to the repository_metadata table failed.")
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
-    # Drop repository_metadata table.
+    # Drop malicious column from repository_metadata table.
+    Repository_metadata_table = Table("repository_metadata", metadata, autoload=True)
     try:
-        RepositoryMetadata_table.drop()
+        Repository_metadata_table.c.malicious.drop()
     except Exception:
-        log.exception("Dropping repository_metadata table failed.")
+        log.exception("Dropping column malicious from the repository_metadata table failed.")
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0009_add_malicious_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0008_add_repository_metadata_table.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,51 +1,65 @@
 """
-Migration script to add the malicious column to the repository_metadata table.
+Migration script to add the repository_metadata table.
 """
-from __future__ import print_function
 
+import datetime
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, MetaData, Table
+from sqlalchemy import (
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    Table,
+)
+
+# Need our custom types, but don't import anything else from model
+from galaxy.model.custom_types import (
+    JSONType,
+    TrimmedString,
+)
 
+now = datetime.datetime.utcnow
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
 metadata = MetaData()
 
+RepositoryMetadata_table = Table(
+    "repository_metadata",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("create_time", DateTime, default=now),
+    Column("update_time", DateTime, default=now, onupdate=now),
+    Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
+    Column("changeset_revision", TrimmedString(255), index=True),
+    Column("metadata", JSONType, nullable=True),
+)
+
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
-    # Create and initialize imported column in job table.
-    Repository_metadata_table = Table("repository_metadata", metadata, autoload=True)
-    c = Column("malicious", Boolean, default=False, index=True)
+    # Create repository_metadata table.
     try:
-        # Create
-        c.create(Repository_metadata_table, index_name="ix_repository_metadata_malicious")
-        assert c is Repository_metadata_table.c.malicious
-        # Initialize.
-        if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
-            default_false = "0"
-        elif migrate_engine.name in ['postgresql', 'postgres']:
-            default_false = "false"
-        migrate_engine.execute("UPDATE repository_metadata SET malicious=%s" % default_false)
+        RepositoryMetadata_table.create()
     except Exception:
-        log.exception("Adding malicious column to the repository_metadata table failed.")
+        log.exception("Creating repository_metadata table failed.")
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
-    # Drop malicious column from repository_metadata table.
-    Repository_metadata_table = Table("repository_metadata", metadata, autoload=True)
+    # Drop repository_metadata table.
     try:
-        Repository_metadata_table.c.malicious.drop()
+        RepositoryMetadata_table.drop()
     except Exception:
-        log.exception("Dropping column malicious from the repository_metadata table failed.")
+        log.exception("Dropping repository_metadata table failed.")
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0010_add_new_repo_alert_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0010_add_new_repo_alert_column.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,16 +1,20 @@
 """
 Migration script to add the new_repo_alert column to the galaxy_user table.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, MetaData, Table
+from sqlalchemy import (
+    Boolean,
+    Column,
+    MetaData,
+    Table,
+)
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
@@ -27,21 +31,21 @@
     User_table = Table("galaxy_user", metadata, autoload=True)
     c = Column("new_repo_alert", Boolean, default=False, index=True)
     try:
         # Create
         c.create(User_table, index_name="ix_galaxy_user_new_repo_alert")
         assert c is User_table.c.new_repo_alert
         # Initialize.
-        if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+        if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
             default_false = "0"
-        elif migrate_engine.name in ['postgresql', 'postgres']:
+        elif migrate_engine.name in ["postgresql", "postgres"]:
             default_false = "false"
         else:
             log.debug("unknown migrate_engine dialect")
-        migrate_engine.execute("UPDATE galaxy_user SET new_repo_alert=%s" % default_false)
+        migrate_engine.execute(f"UPDATE galaxy_user SET new_repo_alert={default_false}")
     except Exception:
         log.exception("Adding new_repo_alert column to the galaxy_user table failed.")
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0011_add_tool_versions_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0011_add_tool_versions_column.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 """
 Migration script to add the tool_versions column to the repository_metadata table.
 """
-from __future__ import print_function
 
 import datetime
 import logging
 import sys
 
-from sqlalchemy import Column, MetaData, Table
+from sqlalchemy import (
+    Column,
+    MetaData,
+    Table,
+)
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import JSONType
 
 now = datetime.datetime.utcnow
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0012_add_downloadable_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0015_add_api_keys_table.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,51 +1,60 @@
 """
-Migration script to add the downloadable column to the repository_metadata table.
+Migration script to add the api_keys table.
 """
-from __future__ import print_function
 
+import datetime
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, MetaData, Table
+from sqlalchemy import (
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    Table,
+)
+
+# Need our custom types, but don't import anything else from model
+from galaxy.model.custom_types import TrimmedString
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
+now = datetime.datetime.utcnow
+
 metadata = MetaData()
 
+APIKeys_table = Table(
+    "api_keys",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("create_time", DateTime, default=now),
+    Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+    Column("key", TrimmedString(32), index=True, unique=True),
+)
+
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
-    # Create and initialize imported column in job table.
-    RepositoryMetadata_table = Table("repository_metadata", metadata, autoload=True)
-    c = Column("downloadable", Boolean, default=True)
     try:
-        # Create
-        c.create(RepositoryMetadata_table)
-        assert c is RepositoryMetadata_table.c.downloadable
-        # Initialize.
-        if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
-            default_true = "1"
-        elif migrate_engine.name in ['postgresql', 'postgres']:
-            default_true = "true"
-        migrate_engine.execute("UPDATE repository_metadata SET downloadable=%s" % default_true)
+        APIKeys_table.create()
     except Exception:
-        log.exception("Adding downloadable column to the repository_metadata table failed.")
+        log.exception("Creating api_keys table failed.")
 
 
 def downgrade(migrate_engine):
+    # Load existing tables
     metadata.bind = migrate_engine
     metadata.reflect()
-    # Drop downloadable column from repository_metadata table.
-    RepositoryMetadata_table = Table("repository_metadata", metadata, autoload=True)
     try:
-        RepositoryMetadata_table.c.downloadable.drop()
+        APIKeys_table.drop()
     except Exception:
-        log.exception("Dropping column downloadable from the repository_metadata table failed.")
+        log.exception("Dropping api_keys table failed.")
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0013_add_review_tables.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0013_add_review_tables.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,91 +1,108 @@
 """
 Migration script to add the repository_review, component_review and component tables and the Repository Reviewer group and role.
 """
-from __future__ import print_function
 
 import datetime
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, DateTime, ForeignKey, Integer, MetaData, Table, TEXT
+from sqlalchemy import (
+    Boolean,
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    Table,
+    TEXT,
+)
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import TrimmedString
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
 metadata = MetaData()
 
-IUC = 'Intergalactic Utilities Commission'
+IUC = "Intergalactic Utilities Commission"
 NOW = datetime.datetime.utcnow
-REVIEWER = 'Repository Reviewer'
-ROLE_TYPE = 'system'
+REVIEWER = "Repository Reviewer"
+ROLE_TYPE = "system"
 
 
-def nextval(migrate_engine, table, col='id'):
-    if migrate_engine.name in ['postgresql', 'postgres']:
-        return "nextval('%s_%s_seq')" % (table, col)
-    elif migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+def nextval(migrate_engine, table, col="id"):
+    if migrate_engine.name in ["postgresql", "postgres"]:
+        return f"nextval('{table}_{col}_seq')"
+    elif migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
         return "null"
     else:
-        raise Exception('Unable to convert data for unknown database type: %s' % migrate_engine.name)
+        raise Exception(f"Unable to convert data for unknown database type: {migrate_engine.name}")
 
 
 def localtimestamp(migrate_engine):
-    if migrate_engine.name in ['postgresql', 'postgres'] or migrate_engine.name == 'mysql':
+    if migrate_engine.name in ["postgresql", "postgres"] or migrate_engine.name == "mysql":
         return "LOCALTIMESTAMP"
-    elif migrate_engine.name == 'sqlite':
+    elif migrate_engine.name == "sqlite":
         return "current_date || ' ' || current_time"
     else:
-        raise Exception('Unable to convert data for unknown database type: %s' % migrate_engine.name)
+        raise Exception(f"Unable to convert data for unknown database type: {migrate_engine.name}")
 
 
 def boolean_false(migrate_engine):
-    if migrate_engine.name in ['postgresql', 'postgres'] or migrate_engine.name == 'mysql':
+    if migrate_engine.name in ["postgresql", "postgres"] or migrate_engine.name == "mysql":
         return False
-    elif migrate_engine.name == 'sqlite':
+    elif migrate_engine.name == "sqlite":
         return 0
     else:
-        raise Exception('Unable to convert data for unknown database type: %s' % migrate_engine.name)
+        raise Exception(f"Unable to convert data for unknown database type: {migrate_engine.name}")
 
 
-RepositoryReview_table = Table("repository_review", metadata,
-                               Column("id", Integer, primary_key=True),
-                               Column("create_time", DateTime, default=NOW),
-                               Column("update_time", DateTime, default=NOW, onupdate=NOW),
-                               Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
-                               Column("changeset_revision", TrimmedString(255), index=True),
-                               Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True, nullable=False),
-                               Column("approved", TrimmedString(255)),
-                               Column("rating", Integer, index=True),
-                               Column("deleted", Boolean, index=True, default=False))
-
-ComponentReview_table = Table("component_review", metadata,
-                              Column("id", Integer, primary_key=True),
-                              Column("create_time", DateTime, default=NOW),
-                              Column("update_time", DateTime, default=NOW, onupdate=NOW),
-                              Column("repository_review_id", Integer, ForeignKey("repository_review.id"), index=True),
-                              Column("component_id", Integer, ForeignKey("component.id"), index=True),
-                              Column("comment", TEXT),
-                              Column("private", Boolean, default=False),
-                              Column("approved", TrimmedString(255)),
-                              Column("rating", Integer),
-                              Column("deleted", Boolean, index=True, default=False))
-
-Component_table = Table("component", metadata,
-                        Column("id", Integer, primary_key=True),
-                        Column("name", TrimmedString(255)),
-                        Column("description", TEXT))
+RepositoryReview_table = Table(
+    "repository_review",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("create_time", DateTime, default=NOW),
+    Column("update_time", DateTime, default=NOW, onupdate=NOW),
+    Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
+    Column("changeset_revision", TrimmedString(255), index=True),
+    Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True, nullable=False),
+    Column("approved", TrimmedString(255)),
+    Column("rating", Integer, index=True),
+    Column("deleted", Boolean, index=True, default=False),
+)
+
+ComponentReview_table = Table(
+    "component_review",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("create_time", DateTime, default=NOW),
+    Column("update_time", DateTime, default=NOW, onupdate=NOW),
+    Column("repository_review_id", Integer, ForeignKey("repository_review.id"), index=True),
+    Column("component_id", Integer, ForeignKey("component.id"), index=True),
+    Column("comment", TEXT),
+    Column("private", Boolean, default=False),
+    Column("approved", TrimmedString(255)),
+    Column("rating", Integer),
+    Column("deleted", Boolean, index=True, default=False),
+)
+
+Component_table = Table(
+    "component",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("name", TrimmedString(255)),
+    Column("description", TEXT),
+)
 
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
     # Create new review tables.
@@ -98,71 +115,73 @@
     except Exception:
         log.exception("Creating repository_review table failed.")
     try:
         ComponentReview_table.create()
     except Exception:
         log.exception("Creating component_review table failed.")
     # Insert default Component values.
-    names = ['Data types', 'Functional tests', 'README', 'Tool dependencies', 'Tools', 'Workflows']
-    descriptions = ['Proprietary datatypes defined in a file named datatypes_conf.xml included in the repository',
-                    'Functional tests defined in each tool config included in the repository along with test data files',
-                    'An appropriately named file included in the repository that contains installation information or 3rd-party tool dependency licensing information',
-                    'Tool dependencies defined in a file named tool_dependencies.xml included in the repository for contained tools',
-                    'Galaxy tools included in the repository',
-                    'Exported Galaxy workflows included in the repository']
+    names = ["Data types", "Functional tests", "README", "Tool dependencies", "Tools", "Workflows"]
+    descriptions = [
+        "Proprietary datatypes defined in a file named datatypes_conf.xml included in the repository",
+        "Functional tests defined in each tool config included in the repository along with test data files",
+        "An appropriately named file included in the repository that contains installation information or 3rd-party tool dependency licensing information",
+        "Tool dependencies defined in a file named tool_dependencies.xml included in the repository for contained tools",
+        "Galaxy tools included in the repository",
+        "Exported Galaxy workflows included in the repository",
+    ]
     for tup in zip(names, descriptions):
         name, description = tup
         cmd = "INSERT INTO component VALUES ("
-        cmd += "%s, " % nextval(migrate_engine, 'component')
-        cmd += "'%s', " % name
-        cmd += "'%s' " % description
+        cmd += f"{nextval(migrate_engine, 'component')}, "
+        cmd += f"'{name}', "
+        cmd += f"'{description}' "
         cmd += ");"
         migrate_engine.execute(cmd)
     # Insert a REVIEWER role into the role table.
     cmd = "INSERT INTO role VALUES ("
-    cmd += "%s, " % nextval(migrate_engine, 'role')
-    cmd += "%s, " % localtimestamp(migrate_engine)
-    cmd += "%s, " % localtimestamp(migrate_engine)
-    cmd += "'%s', " % REVIEWER
+    cmd += f"{nextval(migrate_engine, 'role')}, "
+    cmd += f"{localtimestamp(migrate_engine)}, "
+    cmd += f"{localtimestamp(migrate_engine)}, "
+    cmd += f"'{REVIEWER}', "
     cmd += "'A user or group member with this role can review repositories.', "
-    cmd += "'%s', " % ROLE_TYPE
-    cmd += "%s" % boolean_false(migrate_engine)
+    cmd += f"'{ROLE_TYPE}', "
+    cmd += f"{boolean_false(migrate_engine)}"
     cmd += ");"
     migrate_engine.execute(cmd)
     # Get the id of the REVIEWER role.
-    cmd = "SELECT id FROM role WHERE name = '%s' and type = '%s';" % (REVIEWER, ROLE_TYPE)
+    cmd = f"SELECT id FROM role WHERE name = '{REVIEWER}' and type = '{ROLE_TYPE}';"
     row = migrate_engine.execute(cmd).fetchone()
     if row:
         role_id = row[0]
     else:
         role_id = None
     # Insert an IUC group into the galaxy_group table.
     cmd = "INSERT INTO galaxy_group VALUES ("
-    cmd += "%s, " % nextval(migrate_engine, 'galaxy_group')
-    cmd += "%s, " % localtimestamp(migrate_engine)
-    cmd += "%s, " % localtimestamp(migrate_engine)
-    cmd += "'%s', " % IUC
-    cmd += "%s" % boolean_false(migrate_engine)
+    cmd += f"{nextval(migrate_engine, 'galaxy_group')}, "
+    cmd += f"{localtimestamp(migrate_engine)}, "
+    cmd += f"{localtimestamp(migrate_engine)}, "
+    cmd += f"'{IUC}', "
+    cmd += f"{boolean_false(migrate_engine)}"
     cmd += ");"
     migrate_engine.execute(cmd)
     # Get the id of the IUC group.
-    cmd = "SELECT id FROM galaxy_group WHERE name = '%s';" % (IUC)
+    cmd = f"SELECT id FROM galaxy_group WHERE name = '{IUC}';"
     row = migrate_engine.execute(cmd).fetchone()
     if row:
         group_id = row[0]
     else:
         group_id = None
     if group_id and role_id:
         # Insert a group_role_association for the IUC group and the REVIEWER role.
         cmd = "INSERT INTO group_role_association VALUES ("
-        cmd += "%s, " % nextval(migrate_engine, 'group_role_association')
+        cmd += f"{nextval(migrate_engine, 'group_role_association')}, "
         cmd += "%d, " % int(group_id)
         cmd += "%d, " % int(role_id)
-        cmd += "%s, " % localtimestamp(migrate_engine)
-        cmd += "%s " % localtimestamp(migrate_engine)
+        cmd += f"{localtimestamp(migrate_engine)}, "
+        cmd += f"{localtimestamp(migrate_engine)} "
         cmd += ");"
         migrate_engine.execute(cmd)
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
@@ -176,22 +195,22 @@
     except Exception:
         log.exception("Dropping repository_review table failed.")
     try:
         Component_table.drop()
     except Exception:
         log.exception("Dropping component table failed.")
     # Get the id of the REVIEWER group.
-    cmd = "SELECT id FROM galaxy_group WHERE name = '%s';" % (IUC)
+    cmd = f"SELECT id FROM galaxy_group WHERE name = '{IUC}';"
     row = migrate_engine.execute(cmd).fetchone()
     if row:
         group_id = row[0]
     else:
         group_id = None
     # Get the id of the REVIEWER role.
-    cmd = "SELECT id FROM role WHERE name = '%s' and type = '%s';" % (REVIEWER, ROLE_TYPE)
+    cmd = f"SELECT id FROM role WHERE name = '{REVIEWER}' and type = '{ROLE_TYPE}';"
     row = migrate_engine.execute(cmd).fetchone()
     if row:
         role_id = row[0]
     else:
         role_id = None
     # See if we have at least 1 user
     cmd = "SELECT * FROM galaxy_user;"
@@ -202,15 +221,18 @@
             cmd = "DELETE FROM user_role_association WHERE role_id = %d;" % int(role_id)
             migrate_engine.execute(cmd)
         if group_id:
             # Delete all UserGroupAssociations for members of the IUC group.
             cmd = "DELETE FROM user_group_association WHERE group_id = %d;" % int(group_id)
             migrate_engine.execute(cmd)
             # Delete all GroupRoleAssociations for the IUC group and the REVIEWER role.
-            cmd = "DELETE FROM group_role_association WHERE group_id = %d and role_id = %d;" % (int(group_id), int(role_id))
+            cmd = "DELETE FROM group_role_association WHERE group_id = %d and role_id = %d;" % (
+                int(group_id),
+                int(role_id),
+            )
             migrate_engine.execute(cmd)
             # Delete the IUC group from the galaxy_group table.
             cmd = "DELETE FROM galaxy_group WHERE id = %d;" % int(group_id)
             migrate_engine.execute(cmd)
         # Delete the REVIEWER role from the role table.
         cmd = "DELETE FROM role WHERE id = %d;" % int(role_id)
         migrate_engine.execute(cmd)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0014_add_deprecated_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0014_add_deprecated_column.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,20 @@
 """
 Migration script to add the deprecated column to the repository table.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, MetaData, Table
+from sqlalchemy import (
+    Boolean,
+    Column,
+    MetaData,
+    Table,
+)
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
@@ -27,19 +31,19 @@
     Repository_table = Table("repository", metadata, autoload=True)
     c = Column("deprecated", Boolean, default=False)
     try:
         # Create
         c.create(Repository_table)
         assert c is Repository_table.c.deprecated
         # Initialize.
-        if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+        if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
             default_false = "0"
-        elif migrate_engine.name in ['postgresql', 'postgres']:
+        elif migrate_engine.name in ["postgresql", "postgres"]:
             default_false = "false"
-        migrate_engine.execute("UPDATE repository SET deprecated=%s" % default_false)
+        migrate_engine.execute(f"UPDATE repository SET deprecated={default_false}")
     except Exception:
         log.exception("Adding deprecated column to the repository table failed.")
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0015_add_api_keys_table.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0020_add_repository_type_column.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,51 +1,51 @@
-"""
-Migration script to add the api_keys table.
-"""
-from __future__ import print_function
+"""Migration script to add the type column to the repository table."""
 
-import datetime
 import logging
 import sys
 
-from sqlalchemy import Column, DateTime, ForeignKey, Integer, MetaData, Table
+from sqlalchemy import (
+    Column,
+    MetaData,
+    Table,
+)
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import TrimmedString
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
-now = datetime.datetime.utcnow
-
 metadata = MetaData()
 
-APIKeys_table = Table("api_keys", metadata,
-                      Column("id", Integer, primary_key=True),
-                      Column("create_time", DateTime, default=now),
-                      Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
-                      Column("key", TrimmedString(32), index=True, unique=True))
-
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
+    Repository_table = Table("repository", metadata, autoload=True)
+    c = Column("type", TrimmedString(255), index=True)
     try:
-        APIKeys_table.create()
+        # Create
+        c.create(Repository_table, index_name="ix_repository_type")
+        assert c is Repository_table.c.type
     except Exception:
-        log.exception("Creating api_keys table failed.")
+        log.exception("Adding type column to the repository table failed.")
+    # Update the type column to have the default unrestricted value.
+    cmd = "UPDATE repository SET type = 'unrestricted'"
+    migrate_engine.execute(cmd)
 
 
 def downgrade(migrate_engine):
-    # Load existing tables
     metadata.bind = migrate_engine
     metadata.reflect()
+    # Drop type column from repository table.
+    Repository_table = Table("repository", metadata, autoload=True)
     try:
-        APIKeys_table.drop()
+        Repository_table.c.type.drop()
     except Exception:
-        log.exception("Dropping api_keys table failed.")
+        log.exception("Dropping column type from the repository table failed.")
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0016_add_do_not_test_tools_functionally_correct_errors_columns.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0016_add_do_not_test_tools_functionally_correct_errors_columns.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,21 @@
 """
 Migration script to add the tool_test_errors, do_not_test, tools_functionally_correct, and time_last_tested columns to the repository_metadata table.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, DateTime, MetaData, Table
+from sqlalchemy import (
+    Boolean,
+    Column,
+    DateTime,
+    MetaData,
+    Table,
+)
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import JSONType
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
@@ -30,32 +35,32 @@
     RepositoryMetadata_table = Table("repository_metadata", metadata, autoload=True)
     c = Column("tools_functionally_correct", Boolean, default=False, index=True)
     try:
         # Create tools_functionally_correct column
         c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_tfc")
         assert c is RepositoryMetadata_table.c.tools_functionally_correct
         # Initialize.
-        if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+        if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
             default_false = "0"
-        elif migrate_engine.name in ['postgresql', 'postgres']:
+        elif migrate_engine.name in ["postgresql", "postgres"]:
             default_false = "false"
-        migrate_engine.execute("UPDATE repository_metadata SET tools_functionally_correct=%s" % default_false)
+        migrate_engine.execute(f"UPDATE repository_metadata SET tools_functionally_correct={default_false}")
     except Exception:
         log.exception("Adding tools_functionally_correct column to the repository_metadata table failed.")
     c = Column("do_not_test", Boolean, default=False, index=True)
     try:
         # Create do_not_test column
         c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_dnt")
         assert c is RepositoryMetadata_table.c.do_not_test
         # Initialize.
-        if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+        if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
             default_false = "0"
-        elif migrate_engine.name in ['postgresql', 'postgres']:
+        elif migrate_engine.name in ["postgresql", "postgres"]:
             default_false = "false"
-        migrate_engine.execute("UPDATE repository_metadata SET do_not_test=%s" % default_false)
+        migrate_engine.execute(f"UPDATE repository_metadata SET do_not_test={default_false}")
     except Exception:
         log.exception("Adding do_not_test column to the repository_metadata table failed.")
     c = Column("time_last_tested", DateTime, default=None, nullable=True)
     try:
         # Create time_last_tested column
         c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_tlt")
         assert c is RepositoryMetadata_table.c.time_last_tested
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0017_add_galaxy_utility_columns_to_repository_metadata_table.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0017_add_galaxy_utility_columns_to_repository_metadata_table.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,17 +1,21 @@
 """
 Migration script to add the includes_datatypes, has_repository_dependencies, includes_tools, includes_tool_dependencies and includes_workflows
 columns to the repository_metadata table.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, MetaData, Table
+from sqlalchemy import (
+    Boolean,
+    Column,
+    MetaData,
+    Table,
+)
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
@@ -21,63 +25,63 @@
 
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
     # Initialize.
-    if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+    if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
         default_false = "0"
-    elif migrate_engine.name in ['postgres', 'postgresql']:
+    elif migrate_engine.name in ["postgres", "postgresql"]:
         default_false = "false"
     # Create and initialize tools_functionally_correct, do_not_test, time_last_tested, and tool_test_errors columns in repository_metadata table.
     RepositoryMetadata_table = Table("repository_metadata", metadata, autoload=True)
 
     # Create tools_functionally_correct column
     c = Column("includes_datatypes", Boolean, default=False, index=True)
     try:
         c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_inc_datatypes")
         assert c is RepositoryMetadata_table.c.includes_datatypes
-        migrate_engine.execute("UPDATE repository_metadata SET includes_datatypes=%s" % default_false)
+        migrate_engine.execute(f"UPDATE repository_metadata SET includes_datatypes={default_false}")
     except Exception:
         log.exception("Adding includes_datatypes column to the repository_metadata table failed.")
 
     # Create includes_datatypes column
     c = Column("has_repository_dependencies", Boolean, default=False, index=True)
     try:
         c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_has_repo_deps")
         assert c is RepositoryMetadata_table.c.has_repository_dependencies
-        migrate_engine.execute("UPDATE repository_metadata SET has_repository_dependencies=%s" % default_false)
+        migrate_engine.execute(f"UPDATE repository_metadata SET has_repository_dependencies={default_false}")
     except Exception:
         log.exception("Adding has_repository_dependencies column to the repository_metadata table failed.")
 
     # Create includes_tools column
     c = Column("includes_tools", Boolean, default=False, index=True)
     try:
         c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_inc_tools")
         assert c is RepositoryMetadata_table.c.includes_tools
-        migrate_engine.execute("UPDATE repository_metadata SET includes_tools=%s" % default_false)
+        migrate_engine.execute(f"UPDATE repository_metadata SET includes_tools={default_false}")
     except Exception:
         log.exception("Adding includes_tools column to the repository_metadata table failed.")
 
     # Create includes_tool_dependencies column
     c = Column("includes_tool_dependencies", Boolean, default=False, index=True)
     try:
         c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_inc_tool_deps")
         assert c is RepositoryMetadata_table.c.includes_tool_dependencies
-        migrate_engine.execute("UPDATE repository_metadata SET includes_tool_dependencies=%s" % default_false)
+        migrate_engine.execute(f"UPDATE repository_metadata SET includes_tool_dependencies={default_false}")
     except Exception:
         log.exception("Adding includes_tool_dependencies column to the repository_metadata table failed.")
 
     # Create includes_workflows column
     c = Column("includes_workflows", Boolean, default=False, index=True)
     try:
         c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_inc_workflows")
         assert c is RepositoryMetadata_table.c.includes_workflows
-        migrate_engine.execute("UPDATE repository_metadata SET includes_workflows=%s" % default_false)
+        migrate_engine.execute(f"UPDATE repository_metadata SET includes_workflows={default_false}")
     except Exception:
         log.exception("Adding includes_workflows column to the repository_metadata table failed.")
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0018_add_repository_metadata_flag_columns.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0018_add_repository_metadata_flag_columns.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,21 @@
 """
 Migration script to alter the repository_metadata table by dropping the tool_test_errors column and adding columns
 tool_test_results, missing_test_components.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, MetaData, Table
+from sqlalchemy import (
+    Boolean,
+    Column,
+    MetaData,
+    Table,
+)
 from sqlalchemy.exc import NoSuchTableError
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import JSONType
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
@@ -25,17 +29,17 @@
 
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
     # Initialize.
-    if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+    if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
         default_false = "0"
-    elif migrate_engine.name in ['postgresql', 'postgres']:
+    elif migrate_engine.name in ["postgresql", "postgres"]:
         default_false = "false"
 
     try:
         RepositoryMetadata_table = Table("repository_metadata", metadata, autoload=True)
     except NoSuchTableError:
         RepositoryMetadata_table = None
         log.debug("Failed loading table repository_metadata.")
@@ -58,15 +62,15 @@
             log.exception("Adding tool_test_results column to the repository_metadata table failed.")
 
         # Create the missing_test_components column.
         c = Column("missing_test_components", Boolean, default=False, index=True)
         try:
             c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_mtc")
             assert c is RepositoryMetadata_table.c.missing_test_components
-            migrate_engine.execute("UPDATE repository_metadata SET missing_test_components=%s" % default_false)
+            migrate_engine.execute(f"UPDATE repository_metadata SET missing_test_components={default_false}")
         except Exception:
             log.exception("Adding missing_test_components column to the repository_metadata table failed.")
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0019_add_skip_tool_test_table_and_test_install_error_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0019_add_skip_tool_test_table_and_test_install_error_column.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,25 @@
 """
 Migration script to add the skip_tool_test table and add the test_install_error column to the repository_metadata table.
 """
-from __future__ import print_function
 
 import datetime
 import logging
 import sys
 
-from sqlalchemy import Boolean, Column, DateTime, ForeignKey, Integer, MetaData, Table, TEXT
+from sqlalchemy import (
+    Boolean,
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    Table,
+    TEXT,
+)
 from sqlalchemy.exc import NoSuchTableError
 
 # Need our custom types, but don't import anything else from model
 from galaxy.model.custom_types import TrimmedString
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
@@ -21,46 +29,49 @@
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
 now = datetime.datetime.utcnow
 
 metadata = MetaData()
 
-SkipToolTest_table = Table("skip_tool_test", metadata,
-                           Column("id", Integer, primary_key=True),
-                           Column("create_time", DateTime, default=now),
-                           Column("update_time", DateTime, default=now, onupdate=now),
-                           Column("repository_metadata_id", Integer, ForeignKey("repository_metadata.id"), index=True),
-                           Column("initial_changeset_revision", TrimmedString(255), index=True),
-                           Column("comment", TEXT))
+SkipToolTest_table = Table(
+    "skip_tool_test",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("create_time", DateTime, default=now),
+    Column("update_time", DateTime, default=now, onupdate=now),
+    Column("repository_metadata_id", Integer, ForeignKey("repository_metadata.id"), index=True),
+    Column("initial_changeset_revision", TrimmedString(255), index=True),
+    Column("comment", TEXT),
+)
 
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
     # Initialize.
-    if migrate_engine.name == 'mysql' or migrate_engine.name == 'sqlite':
+    if migrate_engine.name == "mysql" or migrate_engine.name == "sqlite":
         default_false = "0"
-    elif migrate_engine.name in ['postgresql', 'postgres']:
+    elif migrate_engine.name in ["postgresql", "postgres"]:
         default_false = "false"
 
     try:
         RepositoryMetadata_table = Table("repository_metadata", metadata, autoload=True)
     except NoSuchTableError:
         RepositoryMetadata_table = None
         log.debug("Failed loading table repository_metadata.")
 
     if RepositoryMetadata_table is not None:
         # Create the test_install_error column.
         c = Column("test_install_error", Boolean, default=False, index=True)
         try:
             c.create(RepositoryMetadata_table, index_name="ix_repository_metadata_ttie")
             assert c is RepositoryMetadata_table.c.test_install_error
-            migrate_engine.execute("UPDATE repository_metadata SET test_install_error=%s" % default_false)
+            migrate_engine.execute(f"UPDATE repository_metadata SET test_install_error={default_false}")
         except Exception:
             log.exception("Adding test_install_error column to the repository_metadata table failed.")
 
     # Create skip_tool_test table.
     try:
         SkipToolTest_table.create()
     except Exception:
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0020_add_repository_type_column.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0026_add_numeric_revision_column.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,17 +1,18 @@
-"""Migration script to add the type column to the repository table."""
-from __future__ import print_function
+"""Migration script to add the numeric_revision column to the repository metadata table."""
 
 import logging
 import sys
 
-from sqlalchemy import Column, MetaData, Table
-
-# Need our custom types, but don't import anything else from model
-from galaxy.model.custom_types import TrimmedString
+from sqlalchemy import (
+    Column,
+    Integer,
+    MetaData,
+    Table,
+)
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
@@ -20,29 +21,29 @@
 metadata = MetaData()
 
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
-    Repository_table = Table("repository", metadata, autoload=True)
-    c = Column("type", TrimmedString(255), index=True)
+    RepositoryMetadata_table = Table("repository_metadata", metadata, autoload=True)
+    c = Column("numeric_revision", Integer, index=True)
     try:
         # Create
-        c.create(Repository_table, index_name="ix_repository_type")
-        assert c is Repository_table.c.type
+        c.create(RepositoryMetadata_table, index_name="ix_numeric_revision")
+        assert c is RepositoryMetadata_table.c.numeric_revision
     except Exception:
-        log.exception("Adding type column to the repository table failed.")
-    # Update the type column to have the default unrestricted value.
-    cmd = "UPDATE repository SET type = 'unrestricted'"
+        log.exception("Adding numeric_revision column to the repository table failed.")
+    # Update the numeric_revision column to have the default undefined value.
+    cmd = "UPDATE repository_metadata SET numeric_revision = -1"
     migrate_engine.execute(cmd)
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
-    # Drop type column from repository table.
-    Repository_table = Table("repository", metadata, autoload=True)
+    # Drop numeric_revision column from repository table.
+    RepositoryMetadata_table = Table("repository_metadata", metadata, autoload=True)
     try:
-        Repository_table.c.type.drop()
+        RepositoryMetadata_table.c.numeric_revision.drop()
     except Exception:
-        log.exception("Dropping column type from the repository table failed.")
+        log.exception("Dropping column numeric_revision from the repository table failed.")
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0021_change_repository_type_value.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0021_change_repository_type_value.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 """Migration script to change repository.type column value from generic to unrestricted."""
-from __future__ import print_function
 
 import logging
 import sys
 
 from sqlalchemy import MetaData
 
 log = logging.getLogger(__name__)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0022_add_repository_admin_roles.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0022_add_repository_admin_roles.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,67 +1,76 @@
 """
 Migration script to create the repository_role_association table, insert name-spaced
 repository administrative roles into the role table and associate each repository and
 owner with the appropriate name-spaced role.
 """
-from __future__ import print_function
 
 import datetime
 import logging
 import sys
 
-from sqlalchemy import Column, DateTime, ForeignKey, Integer, MetaData, Table
+from sqlalchemy import (
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    Table,
+)
 from sqlalchemy.exc import NoSuchTableError
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
 formatter = logging.Formatter(format)
 handler.setFormatter(formatter)
 log.addHandler(handler)
 
 metadata = MetaData()
 
 NOW = datetime.datetime.utcnow
-ROLE_TYPE = 'system'
+ROLE_TYPE = "system"
 
-RepositoryRoleAssociation_table = Table("repository_role_association", metadata,
-                                        Column("id", Integer, primary_key=True),
-                                        Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
-                                        Column("role_id", Integer, ForeignKey("role.id"), index=True),
-                                        Column("create_time", DateTime, default=NOW),
-                                        Column("update_time", DateTime, default=NOW, onupdate=NOW))
-
-
-def nextval(migrate_engine, table, col='id'):
-    if migrate_engine.name in ['postgresql', 'postgres']:
-        return "nextval('%s_%s_seq')" % (table, col)
-    elif migrate_engine.name in ['mysql', 'sqlite']:
+RepositoryRoleAssociation_table = Table(
+    "repository_role_association",
+    metadata,
+    Column("id", Integer, primary_key=True),
+    Column("repository_id", Integer, ForeignKey("repository.id"), index=True),
+    Column("role_id", Integer, ForeignKey("role.id"), index=True),
+    Column("create_time", DateTime, default=NOW),
+    Column("update_time", DateTime, default=NOW, onupdate=NOW),
+)
+
+
+def nextval(migrate_engine, table, col="id"):
+    if migrate_engine.name in ["postgresql", "postgres"]:
+        return f"nextval('{table}_{col}_seq')"
+    elif migrate_engine.name in ["mysql", "sqlite"]:
         return "null"
     else:
-        raise Exception('Unable to convert data for unknown database type: %s' % migrate_engine.name)
+        raise Exception(f"Unable to convert data for unknown database type: {migrate_engine.name}")
 
 
 def localtimestamp(migrate_engine):
-    if migrate_engine.name in ['postgresql', 'postgres', 'mysql']:
+    if migrate_engine.name in ["postgresql", "postgres", "mysql"]:
         return "LOCALTIMESTAMP"
-    elif migrate_engine.name == 'sqlite':
+    elif migrate_engine.name == "sqlite":
         return "current_date || ' ' || current_time"
     else:
-        raise Exception('Unable to convert data for unknown database type: %s' % migrate_engine.name)
+        raise Exception(f"Unable to convert data for unknown database type: {migrate_engine.name}")
 
 
 def boolean_false(migrate_engine):
-    if migrate_engine.name in ['postgresql', 'postgres', 'mysql']:
+    if migrate_engine.name in ["postgresql", "postgres", "mysql"]:
         return False
-    elif migrate_engine.name == 'sqlite':
+    elif migrate_engine.name == "sqlite":
         return 0
     else:
-        raise Exception('Unable to convert data for unknown database type: %s' % migrate_engine.name)
+        raise Exception(f"Unable to convert data for unknown database type: {migrate_engine.name}")
 
 
 def upgrade(migrate_engine):
     print(__doc__)
     metadata.bind = migrate_engine
     metadata.reflect()
     # Create the new repository_role_association table.
@@ -69,79 +78,79 @@
         RepositoryRoleAssociation_table.create()
     except Exception:
         log.exception("Creating repository_role_association table failed.")
     # Select the list of repositories and associated public user names for their owners.
     user_ids = []
     repository_ids = []
     role_names = []
-    cmd = 'SELECT repository.id, repository.name, repository.user_id, galaxy_user.username FROM repository, galaxy_user WHERE repository.user_id = galaxy_user.id;'
+    cmd = "SELECT repository.id, repository.name, repository.user_id, galaxy_user.username FROM repository, galaxy_user WHERE repository.user_id = galaxy_user.id;"
     for row in migrate_engine.execute(cmd):
         repository_id = row[0]
         name = row[1]
         user_id = row[2]
         username = row[3]
         repository_ids.append(int(repository_id))
-        role_names.append('%s_%s_admin' % (str(name), str(username)))
+        role_names.append(f"{str(name)}_{str(username)}_admin")
         user_ids.append(int(user_id))
     # Insert a new record into the role table for each new role.
     for tup in zip(repository_ids, user_ids, role_names):
         repository_id, user_id, role_name = tup
         cmd = "INSERT INTO role VALUES ("
-        cmd += "%s, " % nextval(migrate_engine, 'role')
-        cmd += "%s, " % localtimestamp(migrate_engine)
-        cmd += "%s, " % localtimestamp(migrate_engine)
-        cmd += "'%s', " % role_name
+        cmd += f"{nextval(migrate_engine, 'role')}, "
+        cmd += f"{localtimestamp(migrate_engine)}, "
+        cmd += f"{localtimestamp(migrate_engine)}, "
+        cmd += f"'{role_name}', "
         cmd += "'A user or group member with this role can administer this repository.', "
-        cmd += "'%s', " % ROLE_TYPE
-        cmd += "%s" % boolean_false(migrate_engine)
+        cmd += f"'{ROLE_TYPE}', "
+        cmd += f"{boolean_false(migrate_engine)}"
         cmd += ");"
         migrate_engine.execute(cmd)
         # Get the id of the new role.
-        cmd = "SELECT id FROM role WHERE name = '%s' and type = '%s';" % (role_name, ROLE_TYPE)
+        cmd = f"SELECT id FROM role WHERE name = '{role_name}' and type = '{ROLE_TYPE}';"
         row = migrate_engine.execute(cmd).fetchone()
         if row:
             role_id = row[0]
         else:
             role_id = None
         if role_id:
             # Create a repository_role_association record to associate the repository with the new role.
             cmd = "INSERT INTO repository_role_association VALUES ("
-            cmd += "%s, " % nextval(migrate_engine, 'repository_role_association')
+            cmd += f"{nextval(migrate_engine, 'repository_role_association')}, "
             cmd += "%d, " % int(repository_id)
             cmd += "%d, " % int(role_id)
-            cmd += "%s, " % localtimestamp(migrate_engine)
-            cmd += "%s " % localtimestamp(migrate_engine)
+            cmd += f"{localtimestamp(migrate_engine)}, "
+            cmd += f"{localtimestamp(migrate_engine)} "
             cmd += ");"
             migrate_engine.execute(cmd)
             # Create a user_role_association record to associate the repository owner with the new role.
             cmd = "INSERT INTO user_role_association VALUES ("
-            cmd += "%s, " % nextval(migrate_engine, 'user_role_association')
+            cmd += f"{nextval(migrate_engine, 'user_role_association')}, "
             cmd += "%d, " % int(user_id)
             cmd += "%d, " % int(role_id)
-            cmd += "%s, " % localtimestamp(migrate_engine)
-            cmd += "%s " % localtimestamp(migrate_engine)
+            cmd += f"{localtimestamp(migrate_engine)}, "
+            cmd += f"{localtimestamp(migrate_engine)} "
             cmd += ");"
             migrate_engine.execute(cmd)
 
 
 def downgrade(migrate_engine):
     metadata.bind = migrate_engine
     metadata.reflect()
     # Determine the list of roles to delete by first selecting the list of repositories and associated
     # public user names for their owners.
     role_names = []
-    cmd = 'SELECT name, username FROM repository, galaxy_user WHERE repository.user_id = galaxy_user.id;'
+    cmd = "SELECT name, username FROM repository, galaxy_user WHERE repository.user_id = galaxy_user.id;"
     for row in migrate_engine.execute(cmd):
         name = row[0]
         username = row[1]
-        role_names.append('%s_%s_admin' % (str(name), str(username)))
+        role_names.append(f"{str(name)}_{str(username)}_admin")
     # Delete each role as well as all users associated with each role.
     for role_name in role_names:
         # Select the id of the record associated with the current role_name from the role table.
-        cmd = "SELECT id, name FROM role WHERE name = '%s';" % role_name
+        cmd = f"SELECT id, name FROM role WHERE name = '{role_name}';"
         row = migrate_engine.execute(cmd).fetchone()
         if row:
             role_id = row[0]
         else:
             role_id = None
         if role_id:
             # Delete all user_role_association records for the current role.
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0023_add_repository_url_and_hompeage_url.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0023_add_repository_url_and_hompeage_url.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 """
 Migration script to add the remote_repository_url and homepage_url
 columns to the repository table.
 """
-from __future__ import print_function
 
 import logging
 import sys
 
-from sqlalchemy import Column, MetaData, Table
+from sqlalchemy import (
+    Column,
+    MetaData,
+    Table,
+)
 
 from galaxy.model.custom_types import TrimmedString
 
 log = logging.getLogger(__name__)
 log.setLevel(logging.DEBUG)
 handler = logging.StreamHandler(sys.stdout)
 format = "%(name)s %(levelname)s %(asctime)s %(message)s"
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0024_password_reset.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0024_password_reset.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,25 +1,35 @@
 """
 Migration script for the password reset table
 """
-from __future__ import print_function
 
 import datetime
 import logging
 
-from sqlalchemy import Column, DateTime, ForeignKey, Integer, MetaData, String, Table
+from sqlalchemy import (
+    Column,
+    DateTime,
+    ForeignKey,
+    Integer,
+    MetaData,
+    String,
+    Table,
+)
 
 now = datetime.datetime.utcnow
 log = logging.getLogger(__name__)
 metadata = MetaData()
 
-PasswordResetToken_table = Table("password_reset_token", metadata,
-                                 Column("token", String(32), primary_key=True, unique=True, index=True),
-                                 Column("expiration_time", DateTime),
-                                 Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True))
+PasswordResetToken_table = Table(
+    "password_reset_token",
+    metadata,
+    Column("token", String(32), primary_key=True, unique=True, index=True),
+    Column("expiration_time", DateTime),
+    Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+)
 
 
 def upgrade(migrate_engine):
     metadata.bind = migrate_engine
     print(__doc__)
     metadata.reflect()
     try:
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/model/migrate/versions/0025_session_timeout.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/model/migrate/versions/0025_session_timeout.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,20 @@
 """
 Migration script to add session update time (used for timeouts)
 """
-from __future__ import print_function
 
 import datetime
 import logging
 
-from sqlalchemy import Column, DateTime, MetaData, Table
+from sqlalchemy import (
+    Column,
+    DateTime,
+    MetaData,
+    Table,
+)
 
 now = datetime.datetime.utcnow
 log = logging.getLogger(__name__)
 metadata = MetaData()
 
 
 def upgrade(migrate_engine):
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/search/repo_search.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/search/repo_search.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,29 @@
 """Module for searching the toolshed repositories"""
 import logging
-import re
-import sys
 
 import whoosh.index
 from whoosh import scoring
-from whoosh.fields import KEYWORD, NUMERIC, Schema, STORED, TEXT
+from whoosh.fields import (
+    KEYWORD,
+    NUMERIC,
+    Schema,
+    STORED,
+    TEXT,
+)
 from whoosh.qparser import MultifieldParser
-from whoosh.query import And, Every, Term
+from whoosh.query import (
+    And,
+    Every,
+    Term,
+)
 
 from galaxy import exceptions
 from galaxy.exceptions import ObjectNotFound
-
-if sys.version_info > (3,):
-    long = int
+from galaxy.util.search import parse_filters
 
 log = logging.getLogger(__name__)
 
 schema = Schema(
     id=NUMERIC(stored=True),
     name=TEXT(field_boost=1.7, stored=True),
     description=TEXT(field_boost=1.5, stored=True),
@@ -26,130 +32,141 @@
     remote_repository_url=TEXT(stored=True),
     repo_owner_username=TEXT(stored=True),
     categories=KEYWORD(stored=True, commas=True, scorable=True),
     times_downloaded=STORED,
     approved=STORED,
     last_updated=STORED,
     repo_lineage=STORED,
-    full_last_updated=STORED)
+    full_last_updated=STORED,
+)
 
 
 class RepoWeighting(scoring.BM25F):
     """
     Affect the BM25G scoring model through the final method.
     source: https://groups.google.com/forum/#!msg/whoosh/1AKNbW8R_l8/XySW0OecH6gJ
     """
+
     use_final = True
 
     def final(self, searcher, docnum, score):
         # Arbitrary for now
         reasonable_hits = 100.0
 
         stored_times_downloaded = searcher.stored_fields(docnum)["times_downloaded"]
-        if not isinstance(stored_times_downloaded, (int, long)):
+        if not isinstance(stored_times_downloaded, int):
             times_downloaded = int(stored_times_downloaded)
         else:
             times_downloaded = stored_times_downloaded
         # Add 1 to prevent 0 being divided
         if times_downloaded == 0:
             times_downloaded = 1
-        popularity_modifier = (times_downloaded / reasonable_hits)
+        popularity_modifier = times_downloaded / reasonable_hits
 
-        cert_modifier = 2 if searcher.stored_fields(docnum)["approved"] == 'yes' else 1
+        cert_modifier = 2 if searcher.stored_fields(docnum)["approved"] == "yes" else 1
 
         # Adjust the computed score for this document by the popularity
         # and by the certification level.
         final_score = score * popularity_modifier * cert_modifier
         return final_score
 
 
-class RepoSearch(object):
-
+class RepoSearch:
     def search(self, trans, search_term, page, page_size, boosts):
         """
         Perform the search on the given search_term
 
         :param search_term: unicode encoded string with the search term(s)
         :param boosts: namedtuple containing custom boosts for searchfields, see api/repositories.py
         :param page_size: integer defining a length of one page
         :param page: integer with the number of page requested
 
         :returns results: dictionary containing hits themselves and the hits summary
         """
-        log.debug('raw search query: #' + str(search_term))
+        log.debug(f"raw search query: #{str(search_term)}")
         lower_search_term = search_term.lower()
         allow_query, search_term_without_filters = self._parse_reserved_filters(lower_search_term)
-        log.debug('term without filters: #' + str(search_term_without_filters))
+        log.debug(f"term without filters: #{str(search_term_without_filters)}")
 
         whoosh_index_dir = trans.app.config.whoosh_index_dir
         index_exists = whoosh.index.exists_in(whoosh_index_dir)
         if index_exists:
             index = whoosh.index.open_dir(whoosh_index_dir)
             try:
                 # Some literature about BM25F:
                 # http://trec.nist.gov/pubs/trec13/papers/microsoft-cambridge.web.hard.pdf
                 # http://en.wikipedia.org/wiki/Okapi_BM25
                 # __Basically__ the higher number the bigger weight.
-                repo_weighting = RepoWeighting(field_B={'name_B' : boosts.repo_name_boost,
-                                                        'description_B' : boosts.repo_description_boost,
-                                                        'long_description_B' : boosts.repo_long_description_boost,
-                                                        'homepage_url_B' : boosts.repo_homepage_url_boost,
-                                                        'remote_repository_url_B' : boosts.repo_remote_repository_url_boost,
-                                                        'repo_owner_username_B' : boosts.repo_owner_username_boost,
-                                                        'categories_B' : boosts.categories_boost})
+                repo_weighting = RepoWeighting(
+                    field_B={
+                        "name_B": boosts.repo_name_boost,
+                        "description_B": boosts.repo_description_boost,
+                        "long_description_B": boosts.repo_long_description_boost,
+                        "homepage_url_B": boosts.repo_homepage_url_boost,
+                        "remote_repository_url_B": boosts.repo_remote_repository_url_boost,
+                        "repo_owner_username_B": boosts.repo_owner_username_boost,
+                        "categories_B": boosts.categories_boost,
+                    }
+                )
                 searcher = index.searcher(weighting=repo_weighting)
-                parser = MultifieldParser([
-                    'name',
-                    'description',
-                    'long_description',
-                    'homepage_url',
-                    'remote_repository_url',
-                    'repo_owner_username',
-                    'categories'], schema=schema)
+                parser = MultifieldParser(
+                    [
+                        "name",
+                        "description",
+                        "long_description",
+                        "homepage_url",
+                        "remote_repository_url",
+                        "repo_owner_username",
+                        "categories",
+                    ],
+                    schema=schema,
+                )
 
                 # If user query has just filters prevent wildcard search.
                 if len(search_term_without_filters) < 1:
-                    user_query = Every('name')
-                    sortedby = 'name'
+                    user_query = Every("name")
+                    sortedby = "name"
                 else:
-                    user_query = parser.parse('*' + search_term_without_filters + '*')
-                    sortedby = ''
+                    user_query = parser.parse(f"*{search_term_without_filters}*")
+                    sortedby = ""
                 try:
-                    hits = searcher.search_page(user_query, page, pagelen=page_size, filter=allow_query, terms=True, sortedby=sortedby)
-                    log.debug('total hits: ' + str(len(hits)))
-                    log.debug('scored hits: ' + str(hits.scored_length()))
+                    hits = searcher.search_page(
+                        user_query, page, pagelen=page_size, filter=allow_query, terms=True, sortedby=sortedby
+                    )
+                    log.debug(f"total hits: {str(len(hits))}")
+                    log.debug(f"scored hits: {str(hits.scored_length())}")
                 except ValueError:
-                    raise ObjectNotFound('The requested page does not exist.')
+                    raise ObjectNotFound("The requested page does not exist.")
                 results = {}
-                results['total_results'] = str(len(hits))
-                results['page'] = str(page)
-                results['page_size'] = str(page_size)
-                results['hits'] = []
+                results["total_results"] = str(len(hits))
+                results["page"] = str(page)
+                results["page_size"] = str(page_size)
+                results["hits"] = []
                 for hit in hits:
-                    log.debug('matched terms: ' + str(hit.matched_terms()))
+                    log.debug(f"matched terms: {str(hit.matched_terms())}")
                     hit_dict = {}
-                    hit_dict['id'] = trans.security.encode_id(hit.get('id'))
-                    hit_dict['repo_owner_username'] = hit.get('repo_owner_username')
-                    hit_dict['name'] = hit.get('name')
-                    hit_dict['long_description'] = hit.get('long_description')
-                    hit_dict['remote_repository_url'] = hit.get('remote_repository_url')
-                    hit_dict['homepage_url'] = hit.get('homepage_url')
-                    hit_dict['description'] = hit.get('description')
-                    hit_dict['last_updated'] = hit.get('last_updated')
-                    hit_dict['full_last_updated'] = hit.get('full_last_updated')
-                    hit_dict['repo_lineage'] = hit.get('repo_lineage')
-                    hit_dict['categories'] = hit.get('categories')
-                    hit_dict['approved'] = hit.get('approved')
-                    hit_dict['times_downloaded'] = hit.get('times_downloaded')
-                    results['hits'].append({'repository': hit_dict, 'score': hit.score})
+                    hit_dict["id"] = trans.security.encode_id(hit.get("id"))
+                    hit_dict["repo_owner_username"] = hit.get("repo_owner_username")
+                    hit_dict["name"] = hit.get("name")
+                    hit_dict["long_description"] = hit.get("long_description")
+                    hit_dict["remote_repository_url"] = hit.get("remote_repository_url")
+                    hit_dict["homepage_url"] = hit.get("homepage_url")
+                    hit_dict["description"] = hit.get("description")
+                    hit_dict["last_updated"] = hit.get("last_updated")
+                    hit_dict["full_last_updated"] = hit.get("full_last_updated")
+                    hit_dict["repo_lineage"] = hit.get("repo_lineage")
+                    hit_dict["categories"] = hit.get("categories")
+                    hit_dict["approved"] = hit.get("approved")
+                    hit_dict["times_downloaded"] = hit.get("times_downloaded")
+                    results["hits"].append({"repository": hit_dict, "score": hit.score})
                 return results
             finally:
                 searcher.close()
         else:
-            raise exceptions.InternalServerError('The search index file is missing.')
+            raise exceptions.InternalServerError("The search index file is missing.")
 
     def _parse_reserved_filters(self, search_term):
         """
         Support github-like filters for narrowing the results.
         Order of chunks does not matter, only recognized
         filter names are allowed.
 
@@ -180,23 +197,18 @@
         >>> rs._parse_reserved_filters("owner:greg")
         (And([Term('repo_owner_username', 'greg')]), '')
         >>> rs._parse_reserved_filters("owner:greg category:assembly abyss")
         (And([Term('repo_owner_username', 'greg'), Term('categories', 'assembly')]), 'abyss')
         >>> rs._parse_reserved_filters("meaningoflife:42")
         (None, 'meaningoflife:42')
         """
-        allow_terms = []
-        search_term_without_filters = None
-        search_space = search_term.replace('"', "'")
-        reserved = re.compile(r"(category|c|owner|o):(\w+|\'.*?\')")
-        while True:
-            match = reserved.search(search_space)
-            if match is None:
-                search_term_without_filters = ' '.join(search_space.split())
-                break
-            if match.groups()[0] in ["category", "c"]:
-                allow_terms.append(Term('categories', match.groups()[1].strip().replace("'", "")))
-            elif match.groups()[0] in ["owner", "o"]:
-                allow_terms.append(Term('repo_owner_username', match.groups()[1].strip().replace("'", "")))
-            search_space = search_space[0:match.start()] + search_space[match.end():]
-        allow_query = And(allow_terms) if allow_terms else None
+        filters = {
+            "category": "categories",
+            "c": "categories",
+            "owner": "repo_owner_username",
+            "o": "repo_owner_username",
+        }
+        allow_query, search_term_without_filters = parse_filters(search_term, filters)
+        allow_query = (
+            And([Term(t, v) for (t, v, _) in allow_query] if len(allow_query) > 0 else None) if allow_query else None
+        )
         return allow_query, search_term_without_filters
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/search/tool_search.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/search/tool_search.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,90 +1,91 @@
 """Module for searching the toolshed tools within all repositories"""
 import logging
 import os
 
 import whoosh.index
 from whoosh import scoring
 from whoosh.fields import (
+    ID,
     Schema,
-    STORED,
-    TEXT
+    TEXT,
 )
 from whoosh.qparser import MultifieldParser
 
 from galaxy import exceptions
 from galaxy.exceptions import ObjectNotFound
+from galaxy.util import unicodify
 
 log = logging.getLogger(__name__)
 
 schema = Schema(
     name=TEXT(stored=True),
     description=TEXT(stored=True),
     owner=TEXT(stored=True),
     id=TEXT(stored=True),
     help=TEXT(stored=True),
     version=TEXT(stored=True),
     repo_name=TEXT(stored=True),
     repo_owner_username=TEXT(stored=True),
-    repo_id=STORED)
-
+    repo_id=ID(stored=True),
+)
 
-class ToolSearch(object):
 
+class ToolSearch:
     def search(self, trans, search_term, page, page_size, boosts):
         """
         Perform the search on the given search_term
 
         :param search_term: unicode encoded string with the search term(s)
 
         :returns results: dictionary containing number of hits, hits themselves and matched terms for each
         """
-        tool_index_dir = os.path.join(trans.app.config.whoosh_index_dir, 'tools')
+        tool_index_dir = os.path.join(trans.app.config.whoosh_index_dir, "tools")
         index_exists = whoosh.index.exists_in(tool_index_dir)
         if index_exists:
             index = whoosh.index.open_dir(tool_index_dir)
             try:
                 # Some literature about BM25F:
                 # http://trec.nist.gov/pubs/trec13/papers/microsoft-cambridge.web.hard.pdf
                 # http://en.wikipedia.org/wiki/Okapi_BM25
                 # __Basically__ the higher number the bigger weight.
-                tool_weighting = scoring.BM25F(field_B={
-                                               'name_B' : boosts.tool_name_boost,
-                                               'description_B' : boosts.tool_description_boost,
-                                               'help_B' : boosts.tool_help_boost,
-                                               'repo_owner_username_B' : boosts.tool_repo_owner_username_boost})
+                tool_weighting = scoring.BM25F(
+                    field_B={
+                        "name_B": boosts.tool_name_boost,
+                        "description_B": boosts.tool_description_boost,
+                        "help_B": boosts.tool_help_boost,
+                        "repo_owner_username_B": boosts.tool_repo_owner_username_boost,
+                    }
+                )
                 searcher = index.searcher(weighting=tool_weighting)
 
-                parser = MultifieldParser([
-                    'name',
-                    'description',
-                    'help',
-                    'repo_owner_username'], schema=schema)
+                parser = MultifieldParser(["name", "description", "help", "repo_owner_username"], schema=schema)
 
-                user_query = parser.parse('*' + search_term + '*')
+                user_query = parser.parse(f"*{search_term}*")
 
                 try:
                     hits = searcher.search_page(user_query, page, pagelen=page_size, terms=True)
                 except ValueError:
-                    raise ObjectNotFound('The requested page does not exist.')
+                    raise ObjectNotFound("The requested page does not exist.")
 
-                log.debug('searching tools for: #' + str(search_term))
-                log.debug('total hits: ' + str(len(hits)))
-                log.debug('scored hits: ' + str(hits.scored_length()))
+                log.debug(f"searching tools for: #{str(search_term)}")
+                log.debug(f"total hits: {str(len(hits))}")
+                log.debug(f"scored hits: {str(hits.scored_length())}")
                 results = {}
-                results['total_results'] = str(len(hits))
-                results['page'] = str(page)
-                results['page_size'] = str(page_size)
-                results['hits'] = []
+                results["total_results"] = str(len(hits))
+                results["page"] = str(page)
+                results["page_size"] = str(page_size)
+                results["hits"] = []
                 for hit in hits:
                     hit_dict = {}
-                    hit_dict['id'] = hit.get('id')
-                    hit_dict['repo_owner_username'] = hit.get('repo_owner_username')
-                    hit_dict['repo_name'] = hit.get('repo_name')
-                    hit_dict['name'] = hit.get('name')
-                    hit_dict['description'] = hit.get('description')
-                    results['hits'].append({'tool': hit_dict, 'matched_terms': hit.matched_terms(), 'score': hit.score})
+                    hit_dict["id"] = hit.get("id")
+                    hit_dict["repo_owner_username"] = hit.get("repo_owner_username")
+                    hit_dict["repo_name"] = hit.get("repo_name")
+                    hit_dict["name"] = hit.get("name")
+                    hit_dict["description"] = hit.get("description")
+                    matched_terms = {k: unicodify(v) for k, v in hit.matched_terms()}
+                    results["hits"].append({"tool": hit_dict, "matched_terms": matched_terms, "score": hit.score})
                 return results
             finally:
                 searcher.close()
         else:
-            raise exceptions.InternalServerError('The search index file is missing.')
+            raise exceptions.InternalServerError("The search index file is missing.")
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/security/__init__.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/security/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,42 +1,49 @@
 """Tool Shed Security"""
 import logging
 
-from sqlalchemy import and_, false
+from sqlalchemy import (
+    and_,
+    false,
+)
 
 from galaxy.util import listify
 from galaxy.util.bunch import Bunch
 
 log = logging.getLogger(__name__)
 
 
-class Action(object):
-
+class Action:
     def __init__(self, action, description, model):
         self.action = action
         self.description = description
         self.model = model
 
 
-class RBACAgent(object):
+class RBACAgent:
     """Handle Galaxy Tool Shed security"""
+
     permitted_actions = Bunch()
 
     def associate_components(self, **kwd):
-        raise Exception('No valid method of associating provided components: %s' % kwd)
+        raise Exception(f"No valid method of associating provided components: {kwd}")
 
     def associate_user_role(self, user, role):
-        raise Exception('No valid method of associating a user with a role')
+        raise Exception("No valid method of associating a user with a role")
 
     def convert_permitted_action_strings(self, permitted_action_strings):
         """
         When getting permitted actions from an untrusted source like a
         form, ensure that they match our actual permitted actions.
         """
-        return [x for x in [self.permitted_actions.get(action_string) for action_string in permitted_action_strings] if x is not None]
+        return [
+            x
+            for x in [self.permitted_actions.get(action_string) for action_string in permitted_action_strings]
+            if x is not None
+        ]
 
     def create_user_role(self, user, app):
         raise Exception("Unimplemented Method")
 
     def create_private_user_role(self, user):
         raise Exception("Unimplemented Method")
 
@@ -48,22 +55,21 @@
         return default
 
     def get_actions(self):
         """Get all permitted actions as a list of Action objects"""
         return list(self.permitted_actions.__dict__.values())
 
     def get_item_actions(self, action, item):
-        raise Exception('No valid method of retrieving action (%s) for item %s.' % (action, item))
+        raise Exception(f"No valid method of retrieving action ({action}) for item {item}.")
 
     def get_private_user_role(self, user):
         raise Exception("Unimplemented Method")
 
 
 class CommunityRBACAgent(RBACAgent):
-
     def __init__(self, model, permitted_actions=None):
         self.model = model
         if permitted_actions:
             self.permitted_actions = permitted_actions
 
     @property
     def sa_session(self):
@@ -73,34 +79,34 @@
     def allow_action(self, roles, action, item):
         """
         Method for checking a permission for the current user ( based on roles ) to perform a
         specific action on an item
         """
         item_actions = self.get_item_actions(action, item)
         if not item_actions:
-            return action.model == 'restrict'
+            return action.model == "restrict"
         ret_val = False
         for item_action in item_actions:
             if item_action.role in roles:
                 ret_val = True
                 break
         return ret_val
 
     def associate_components(self, **kwd):
-        if 'user' in kwd:
-            if 'group' in kwd:
-                return self.associate_user_group(kwd['user'], kwd['group'])
-            elif 'role' in kwd:
-                return self.associate_user_role(kwd['user'], kwd['role'])
-        elif 'role' in kwd:
-            if 'group' in kwd:
-                return self.associate_group_role(kwd['group'], kwd['role'])
-        elif 'repository' in kwd:
-            return self.associate_repository_category(kwd['repository'], kwd['category'])
-        raise Exception('No valid method of associating provided components: %s' % kwd)
+        if "user" in kwd:
+            if "group" in kwd:
+                return self.associate_user_group(kwd["user"], kwd["group"])
+            elif "role" in kwd:
+                return self.associate_user_role(kwd["user"], kwd["role"])
+        elif "role" in kwd:
+            if "group" in kwd:
+                return self.associate_group_role(kwd["group"], kwd["role"])
+        elif "repository" in kwd:
+            return self.associate_repository_category(kwd["repository"], kwd["category"])
+        raise Exception(f"No valid method of associating provided components: {kwd}")
 
     def associate_group_role(self, group, role):
         assoc = self.model.GroupRoleAssociation(group, role)
         self.sa_session.add(assoc)
         self.sa_session.flush()
         return assoc
 
@@ -120,46 +126,48 @@
         assoc = self.model.RepositoryCategoryAssociation(repository, category)
         self.sa_session.add(assoc)
         self.sa_session.flush()
         return assoc
 
     def create_private_user_role(self, user):
         # Create private role
-        role = self.model.Role(name=user.email, description='Private Role for ' + user.email, type=self.model.Role.types.PRIVATE)
+        role = self.model.Role(
+            name=user.email, description=f"Private Role for {user.email}", type=self.model.Role.types.PRIVATE
+        )
         self.sa_session.add(role)
         self.sa_session.flush()
         # Add user to role
         self.associate_components(role=role, user=user)
         return role
 
     def create_user_role(self, user, app):
         self.get_private_user_role(user, auto_create=True)
 
     def get_item_actions(self, action, item):
         # item must be one of: Dataset, Library, LibraryFolder, LibraryDataset, LibraryDatasetDatasetAssociation
         return [permission for permission in item.actions if permission.action == action.action]
 
     def get_private_user_role(self, user, auto_create=False):
-        role = self.sa_session.query(self.model.Role) \
-                              .filter(and_(self.model.Role.table.c.name == user.email,
-                                           self.model.Role.table.c.type == self.model.Role.types.PRIVATE)) \
-                              .first()
+        role = (
+            self.sa_session.query(self.model.Role)
+            .filter(
+                and_(
+                    self.model.Role.table.c.name == user.email,
+                    self.model.Role.table.c.type == self.model.Role.types.PRIVATE,
+                )
+            )
+            .first()
+        )
         if not role:
             if auto_create:
                 return self.create_private_user_role(user)
             else:
                 return None
         return role
 
-    def get_repository_reviewer_role(self):
-        return self.sa_session.query(self.model.Role) \
-                              .filter(and_(self.model.Role.table.c.name == 'Repository Reviewer',
-                                           self.model.Role.table.c.type == self.model.Role.types.SYSTEM)) \
-                              .first()
-
     def set_entity_group_associations(self, groups=None, users=None, roles=None, delete_existing_assocs=True):
         if groups is None:
             groups = []
         if users is None:
             users = []
         if roles is None:
             roles = []
@@ -169,15 +177,17 @@
                     self.sa_session.delete(a)
                     self.sa_session.flush()
             for role in roles:
                 self.associate_components(group=group, role=role)
             for user in users:
                 self.associate_components(group=group, user=user)
 
-    def set_entity_role_associations(self, roles=None, users=None, groups=None, repositories=None, delete_existing_assocs=True):
+    def set_entity_role_associations(
+        self, roles=None, users=None, groups=None, repositories=None, delete_existing_assocs=True
+    ):
         if roles is None:
             roles = []
         if users is None:
             users = []
         if groups is None:
             groups = []
         if repositories is None:
@@ -230,56 +240,42 @@
                             role_member = ura.user
                             if role_member.id == user.id:
                                 return True
                         # The user is not directly associated with the role, so see if they are a member
                         # of a group that is associated with the role.
                         for gra in role.groups:
                             group = gra.group
-                            for uga in group.members:
+                            for uga in group.users:
                                 member = uga.user
                                 if member.id == user.id:
                                     return True
         return False
 
     def user_can_import_repository_archive(self, user, archive_owner):
         # This method should be called only if the current user is not an admin.
         if user.username == archive_owner:
             return True
         # A member of the IUC is authorized to create new repositories that are owned by another user.
-        iuc_group = self.sa_session.query(self.model.Group) \
-                                   .filter(and_(self.model.Group.table.c.name == 'Intergalactic Utilities Commission',
-                                                self.model.Group.table.c.deleted == false())) \
-                                   .first()
+        iuc_group = (
+            self.sa_session.query(self.model.Group)
+            .filter(
+                and_(
+                    self.model.Group.table.c.name == "Intergalactic Utilities Commission",
+                    self.model.Group.table.c.deleted == false(),
+                )
+            )
+            .first()
+        )
         if iuc_group is not None:
             for uga in iuc_group.users:
                 if uga.user.id == user.id:
                     return True
         return False
 
-    def user_can_review_repositories(self, user):
-        if user:
-            roles = user.all_roles()
-            if roles:
-                repository_reviewer_role = self.get_repository_reviewer_role()
-                if repository_reviewer_role:
-                    return repository_reviewer_role in roles
-        return False
-
-    def user_can_browse_component_review(self, app, repository, component_review, user):
-        if component_review and user:
-            if self.can_push(app, user, repository):
-                # A user with write permission on the repository can access private/public component reviews.
-                return True
-            else:
-                if self.user_can_review_repositories(user):
-                    # Reviewers can access private/public component reviews.
-                    return True
-        return False
-
 
 def get_permitted_actions(filter=None):
-    '''Utility method to return a subset of RBACAgent's permitted actions'''
+    """Utility method to return a subset of RBACAgent's permitted actions"""
     if filter is None:
         return RBACAgent.permitted_actions
     tmp_bunch = Bunch()
     [tmp_bunch.__dict__.__setitem__(k, v) for k, v in RBACAgent.permitted_actions.items() if k.startswith(filter)]
     return tmp_bunch
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/browse_repository.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/browse_repository.mako`

 * *Files 24% similar despite different names*

```diff
@@ -1,35 +1,64 @@
 <%inherit file="/base.mako"/>
 <%namespace file="/message.mako" import="render_msg" />
-<%namespace file="/admin/tool_shed_repository/common.mako" import="*" />
-<%namespace file="/admin/tool_shed_repository/repository_actions_menu.mako" import="*" />
+<%namespace file="/webapps/tool_shed/common/repository_actions_menu.mako" import="render_tool_shed_repository_actions" />
+<%namespace file="/webapps/tool_shed/common/common.mako" import="*" />
+<%namespace file="/webapps/tool_shed/repository/common.mako" import="*" />
+
+<%!
+   def inherit(context):
+       if context.get('use_panels'):
+           return '/webapps/tool_shed/base_panels.mako'
+       else:
+           return '/base.mako'
+%>
+<%inherit file="${inherit(context)}"/>
 
 <%def name="stylesheets()">
     ${parent.stylesheets()}
     ${h.css( "dynatree_skin/ui.dynatree" )}
 </%def>
 
 <%def name="javascripts()">
     ${parent.javascripts()}
-    ${browse_files(repository.name, repository.repo_files_directory(trans.app))}
+    ${common_javascripts(repository)}
 </%def>
 
-${render_galaxy_repository_actions( repository )}
+<%
+    is_new = repository.is_new()
+    can_push = trans.app.security_agent.can_push( trans.app, trans.user, repository )
+    can_download = not is_new and ( not is_malicious or can_push )
+    can_browse_contents = not is_new
+%>
+
+${render_tool_shed_repository_actions( repository, metadata=metadata, changeset_revision=changeset_revision )}
 
 %if message:
     ${render_msg( message, status )}
 %endif
 
-<div class="card">
-    <div class="card-header">Browse ${repository.name|h} revision ${repository.changeset_revision} files</div>
-    <div class="card-body">
-        <div class="form-row" >
-            <label>Contents:</label>
-            <div id="tree" >
-                Loading...
+%if can_browse_contents:
+    <div class="toolForm">
+        <div class="toolFormTitle">Repository '${repository.name | h}' revision ${repository.tip() | h} (repository tip)</div>
+        %if can_download:
+            <div class="form-row">
+                <label>Clone this repository:</label>
+                ${render_clone_str( repository )}
+            </div>
+        %endif
+        <form name="repository_type">
+            ${render_repository_type_select_field( repository_type_select_field, render_help=False )}
+        </form>
+        <div class="toolFormBody">
+            <div class="form-row" >
+                <label>Contents:</label>
+                <div id="tree" >
+                    Loading...
+                </div>
+            </div>
+            <div class="form-row">
+                <div id="file_contents" class="toolParamHelp" style="clear: both;background-color:#FAFAFA;"></div>
             </div>
-        </div>
-        <div class="form-row">
-            <div id="file_contents" class="toolParamHelp" style="clear: both;background-color:#FAFAFA;"></div>
         </div>
     </div>
-</div>
+    <p/>
+%endif
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/browse_tool_dependency.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/docker_image_repositories.mako`

 * *Files 24% similar despite different names*

```diff
@@ -1,65 +1,64 @@
-<%inherit file="/base.mako"/>
 <%namespace file="/message.mako" import="render_msg" />
-<%namespace file="/admin/tool_shed_repository/common.mako" import="*" />
-<%namespace file="/admin/tool_shed_repository/repository_actions_menu.mako" import="*" />
+
+<%!
+   def inherit(context):
+       if context.get('use_panels'):
+           return '/webapps/tool_shed/base_panels.mako'
+       else:
+           return '/base.mako'
+%>
+
+<%inherit file="${inherit(context)}"/>
 
 <%def name="stylesheets()">
     ${parent.stylesheets()}
-    ${h.css( "dynatree_skin/ui.dynatree" )}
 </%def>
 
 <%def name="javascripts()">
     ${parent.javascripts()}
-    ${browse_files(tool_dependency.name, tool_dependency.installation_directory( trans.app ))}
 </%def>
 
-<% tool_dependency_ids = [ trans.security.encode_id( td.id ) for td in repository.tool_dependencies ] %>
-
-${render_galaxy_repository_actions( repository )}
-
 %if message:
     ${render_msg( message, status )}
 %endif
 
-<div class="card">
-    <div class="card-header">Browse tool dependency ${tool_dependency.name|h} installation directory</div>
-    <div class="card-body">
-        <div class="form-row" >
-            <label>Tool shed repository:</label>
-            ${repository.name|h}
-            <div style="clear: both"></div>
-        </div>
-        <div class="form-row" >
-            <label>Tool shed repository changeset revision:</label>
-            ${repository.changeset_revision|h}
-            <div style="clear: both"></div>
-        </div>
-        <div class="form-row" >
-            <label>Tool dependency status:</label>
-            ${tool_dependency.status|h}
-            <div style="clear: both"></div>
-        </div>
-        %if tool_dependency.in_error_state:
-            <div class="form-row" >
-                <label>Tool dependency installation error:</label>
-                ${tool_dependency.error_message|h}
-                <div style="clear: both"></div>
+<div class="toolForm">
+    <div class="toolFormBody">
+        <div class="form-row">
+            <div class="warningmessage">
+                Click the <b>Create Docker Image</b> button below to create a Docker Image that will install the following repositories.
             </div>
-        %endif
-        <div class="form-row" >
-            <label>Tool dependency installation directory:</label>
-            ${tool_dependency.installation_directory( trans.app )|h}
             <div style="clear: both"></div>
         </div>
-        <div class="form-row" >
-            <label>Contents:</label>
-            <div id="tree" >
-                Loading...
+    </div>
+</div>
+<div class="toolForm">
+    <div class="toolFormTitle">Repositories for inclusion in Docker Image</div>
+        <form id="docker_image_form" name="docker_image_form" action="${h.url_for( controller='repository', action='create_galaxy_docker_image' )}" enctype="multipart/form-data" method="post">
+            <div class="form-row">
+                <input type="hidden" name="id" value="${id}" />
+            </div>
+            <div class="form-row">
+                <table class="grid">
+                    <tr>
+                        <th bgcolor="#D8D8D8">Name</th>
+                        <th bgcolor="#D8D8D8">Owner</th>
+                        <th bgcolor="#D8D8D8">Type</th>
+                    </tr>
+                    %for repository_tup in repository_tups:
+                        <% name, owner, type = repository_tup %>
+                        <tr>
+                            <td>${ name | h }</td>
+                            <td>${ owner | h }</td>
+                            <td>${ type | h }</td>
+                        </tr>
+                    %endfor
+                </table>
             </div>
             <div style="clear: both"></div>
-        </div>
-        <div class="form-row">
-            <div id="file_contents" class="toolParamHelp" style="clear: both;background-color:#FAFAFA;"></div>
-        </div>
+            <div class="form-row">
+                <input type="submit" class="primary-button" name="create_docker_image_button" value="Create Docker Image">
+            </div>
+        </form>
     </div>
 </div>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/select_tool_panel_section.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/view_repository.mako`

 * *Files 27% similar despite different names*

```diff
@@ -1,662 +1,593 @@
 00000000: 3c25 696e 6865 7269 7420 6669 6c65 3d22  <%inherit file="
 00000010: 2f62 6173 652e 6d61 6b6f 222f 3e0a 3c25  /base.mako"/>.<%
 00000020: 6e61 6d65 7370 6163 6520 6669 6c65 3d22  namespace file="
 00000030: 2f6d 6573 7361 6765 2e6d 616b 6f22 2069  /message.mako" i
 00000040: 6d70 6f72 743d 2272 656e 6465 725f 6d73  mport="render_ms
 00000050: 6722 202f 3e0a 3c25 6e61 6d65 7370 6163  g" />.<%namespac
-00000060: 6520 6669 6c65 3d22 2f61 646d 696e 2f74  e file="/admin/t
-00000070: 6f6f 6c5f 7368 6564 5f72 6570 6f73 6974  ool_shed_reposit
-00000080: 6f72 792f 636f 6d6d 6f6e 2e6d 616b 6f22  ory/common.mako"
-00000090: 2069 6d70 6f72 743d 2272 656e 6465 725f   import="render_
-000000a0: 6465 7065 6e64 656e 6369 6573 5f73 6563  dependencies_sec
-000000b0: 7469 6f6e 2220 2f3e 0a3c 256e 616d 6573  tion" />.<%names
-000000c0: 7061 6365 2066 696c 653d 222f 6164 6d69  pace file="/admi
-000000d0: 6e2f 746f 6f6c 5f73 6865 645f 7265 706f  n/tool_shed_repo
-000000e0: 7369 746f 7279 2f63 6f6d 6d6f 6e2e 6d61  sitory/common.ma
-000000f0: 6b6f 2220 696d 706f 7274 3d22 7265 6e64  ko" import="rend
-00000100: 6572 5f72 6561 646d 655f 7365 6374 696f  er_readme_sectio
-00000110: 6e22 202f 3e0a 3c25 6e61 6d65 7370 6163  n" />.<%namespac
-00000120: 6520 6669 6c65 3d22 2f77 6562 6170 7073  e file="/webapps
-00000130: 2f74 6f6f 6c5f 7368 6564 2f72 6570 6f73  /tool_shed/repos
-00000140: 6974 6f72 792f 636f 6d6d 6f6e 2e6d 616b  itory/common.mak
-00000150: 6f22 2069 6d70 6f72 743d 222a 2220 2f3e  o" import="*" />
-00000160: 0a3c 256e 616d 6573 7061 6365 2066 696c  .<%namespace fil
-00000170: 653d 222f 7765 6261 7070 732f 746f 6f6c  e="/webapps/tool
-00000180: 5f73 6865 642f 636f 6d6d 6f6e 2f63 6f6d  _shed/common/com
-00000190: 6d6f 6e2e 6d61 6b6f 2220 696d 706f 7274  mon.mako" import
-000001a0: 3d22 2a22 202f 3e0a 0a3c 2564 6566 206e  ="*" />..<%def n
-000001b0: 616d 653d 2273 7479 6c65 7368 6565 7473  ame="stylesheets
-000001c0: 2829 223e 0a20 2020 2024 7b70 6172 656e  ()">.    ${paren
-000001d0: 742e 7374 796c 6573 6865 6574 7328 297d  t.stylesheets()}
-000001e0: 0a20 2020 2024 7b68 2e63 7373 2820 226c  .    ${h.css( "l
-000001f0: 6962 7261 7279 2220 297d 0a3c 2f25 6465  ibrary" )}.</%de
-00000200: 663e 0a0a 3c25 6465 6620 6e61 6d65 3d22  f>..<%def name="
-00000210: 6a61 7661 7363 7269 7074 7328 2922 3e0a  javascripts()">.
-00000220: 2020 2020 247b 7061 7265 6e74 2e6a 6176      ${parent.jav
-00000230: 6173 6372 6970 7473 2829 7d0a 2020 2020  ascripts()}.    
-00000240: 247b 636f 6e74 6169 6e65 725f 6a61 7661  ${container_java
-00000250: 7363 7269 7074 7328 297d 0a3c 2f25 6465  scripts()}.</%de
-00000260: 663e 0a0a 3c25 0a20 2020 2023 2048 616e  f>..<%.    # Han
-00000270: 646c 6520 7468 6520 6361 7365 2077 6865  dle the case whe
-00000280: 7265 2061 6e20 756e 696e 7374 616c 6c65  re an uninstalle
-00000290: 6420 7265 706f 7369 746f 7279 2065 6e63  d repository enc
-000002a0: 6f75 6e74 6572 6564 2065 7272 6f72 7320  ountered errors 
-000002b0: 6475 7269 6e67 2074 6865 2070 726f 6365  during the proce
-000002c0: 7373 206f 6620 6265 696e 6720 7265 696e  ss of being rein
-000002d0: 7374 616c 6c65 642e 2020 496e 0a20 2020  stalled.  In.   
-000002e0: 2023 2074 6869 7320 6361 7365 2c20 7468   # this case, th
-000002f0: 6520 7265 706f 7369 746f 7279 206d 6574  e repository met
-00000300: 6164 6174 6120 6973 2061 6e20 656d 7074  adata is an empt
-00000310: 7920 6469 6374 696f 6e61 7279 2c20 6275  y dictionary, bu
-00000320: 7420 6f6e 6520 6f72 2062 6f74 6820 6f66  t one or both of
-00000330: 2068 6173 5f72 6570 6f73 6974 6f72 795f   has_repository_
-00000340: 6465 7065 6e64 656e 6369 6573 0a20 2020  dependencies.   
-00000350: 2023 2061 6e64 2069 6e63 6c75 6465 735f   # and includes_
-00000360: 746f 6f6c 5f64 6570 656e 6465 6e63 6965  tool_dependencie
-00000370: 7320 6d61 7920 6265 2054 7275 652e 2020  s may be True.  
-00000380: 4966 2065 6974 6865 7220 6f66 2074 6865  If either of the
-00000390: 7365 2061 7265 2054 7275 6520 6275 7420  se are True but 
-000003a0: 7765 2068 6176 6520 6e6f 206d 6574 6164  we have no metad
-000003b0: 6174 612c 2077 6520 6361 6e6e 6f74 2069  ata, we cannot i
-000003c0: 6e73 7461 6c6c 0a20 2020 2023 2072 6570  nstall.    # rep
-000003d0: 6f73 6974 6f72 7920 6465 7065 6e64 656e  ository dependen
-000003e0: 6369 6573 206f 6e20 7468 6973 2070 6173  cies on this pas
-000003f0: 732e 0a20 2020 2069 6620 6861 735f 7265  s..    if has_re
-00000400: 706f 7369 746f 7279 5f64 6570 656e 6465  pository_depende
-00000410: 6e63 6965 733a 0a20 2020 2020 2020 2072  ncies:.        r
-00000420: 6570 6f73 6974 6f72 795f 6465 7065 6e64  epository_depend
-00000430: 656e 6369 6573 203d 2063 6f6e 7461 696e  encies = contain
-00000440: 6572 735f 6469 6374 5b20 2772 6570 6f73  ers_dict[ 'repos
-00000450: 6974 6f72 795f 6465 7065 6e64 656e 6369  itory_dependenci
-00000460: 6573 2720 5d0a 2020 2020 2020 2020 6d69  es' ].        mi
-00000470: 7373 696e 675f 7265 706f 7369 746f 7279  ssing_repository
-00000480: 5f64 6570 656e 6465 6e63 6965 7320 3d20  _dependencies = 
-00000490: 636f 6e74 6169 6e65 7273 5f64 6963 745b  containers_dict[
-000004a0: 2027 6d69 7373 696e 675f 7265 706f 7369   'missing_reposi
-000004b0: 746f 7279 5f64 6570 656e 6465 6e63 6965  tory_dependencie
-000004c0: 7327 205d 0a20 2020 2020 2020 2069 6620  s' ].        if 
-000004d0: 7265 706f 7369 746f 7279 5f64 6570 656e  repository_depen
-000004e0: 6465 6e63 6965 7320 6f72 206d 6973 7369  dencies or missi
-000004f0: 6e67 5f72 6570 6f73 6974 6f72 795f 6465  ng_repository_de
-00000500: 7065 6e64 656e 6369 6573 3a0a 2020 2020  pendencies:.    
-00000510: 2020 2020 2020 2020 6361 6e5f 6469 7370          can_disp
-00000520: 6c61 795f 7265 706f 7369 746f 7279 5f64  lay_repository_d
-00000530: 6570 656e 6465 6e63 6965 7320 3d20 5472  ependencies = Tr
-00000540: 7565 0a20 2020 2020 2020 2065 6c73 653a  ue.        else:
-00000550: 0a20 2020 2020 2020 2020 2020 2063 616e  .            can
-00000560: 5f64 6973 706c 6179 5f72 6570 6f73 6974  _display_reposit
-00000570: 6f72 795f 6465 7065 6e64 656e 6369 6573  ory_dependencies
-00000580: 203d 2046 616c 7365 0a20 2020 2065 6c73   = False.    els
-00000590: 653a 0a20 2020 2020 2020 2063 616e 5f64  e:.        can_d
-000005a0: 6973 706c 6179 5f72 6570 6f73 6974 6f72  isplay_repositor
-000005b0: 795f 6465 7065 6e64 656e 6369 6573 203d  y_dependencies =
-000005c0: 2046 616c 7365 0a20 2020 2069 6620 696e   False.    if in
-000005d0: 636c 7564 6573 5f74 6f6f 6c5f 6465 7065  cludes_tool_depe
-000005e0: 6e64 656e 6369 6573 3a0a 2020 2020 2020  ndencies:.      
-000005f0: 2020 746f 6f6c 5f64 6570 656e 6465 6e63    tool_dependenc
-00000600: 6965 7320 3d20 636f 6e74 6169 6e65 7273  ies = containers
-00000610: 5f64 6963 745b 2027 746f 6f6c 5f64 6570  _dict[ 'tool_dep
-00000620: 656e 6465 6e63 6965 7327 205d 0a20 2020  endencies' ].   
-00000630: 2020 2020 206d 6973 7369 6e67 5f74 6f6f       missing_too
-00000640: 6c5f 6465 7065 6e64 656e 6369 6573 203d  l_dependencies =
-00000650: 2063 6f6e 7461 696e 6572 735f 6469 6374   containers_dict
-00000660: 5b20 276d 6973 7369 6e67 5f74 6f6f 6c5f  [ 'missing_tool_
-00000670: 6465 7065 6e64 656e 6369 6573 2720 5d0a  dependencies' ].
-00000680: 2020 2020 2020 2020 6966 2074 6f6f 6c5f          if tool_
-00000690: 6465 7065 6e64 656e 6369 6573 206f 7220  dependencies or 
-000006a0: 6d69 7373 696e 675f 746f 6f6c 5f64 6570  missing_tool_dep
-000006b0: 656e 6465 6e63 6965 733a 0a20 2020 2020  endencies:.     
-000006c0: 2020 2020 2020 2063 616e 5f64 6973 706c         can_displ
-000006d0: 6179 5f74 6f6f 6c5f 6465 7065 6e64 656e  ay_tool_dependen
-000006e0: 6369 6573 203d 2054 7275 650a 2020 2020  cies = True.    
-000006f0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-00000700: 2020 2020 2020 6361 6e5f 6469 7370 6c61        can_displa
-00000710: 795f 746f 6f6c 5f64 6570 656e 6465 6e63  y_tool_dependenc
-00000720: 6965 7320 3d20 4661 6c73 650a 2020 2020  ies = False.    
-00000730: 656c 7365 3a0a 2020 2020 2020 2020 6361  else:.        ca
-00000740: 6e5f 6469 7370 6c61 795f 746f 6f6c 5f64  n_display_tool_d
-00000750: 6570 656e 6465 6e63 6965 7320 3d20 4661  ependencies = Fa
-00000760: 6c73 650a 2020 2020 6361 6e5f 6469 7370  lse.    can_disp
-00000770: 6c61 795f 7265 736f 6c76 6572 5f69 6e73  lay_resolver_ins
-00000780: 7461 6c6c 6174 696f 6e20 3d20 696e 7374  tallation = inst
-00000790: 616c 6c5f 7265 736f 6c76 6572 5f64 6570  all_resolver_dep
-000007a0: 656e 6465 6e63 6965 735f 6368 6563 6b5f  endencies_check_
-000007b0: 626f 7820 6973 206e 6f74 204e 6f6e 650a  box is not None.
-000007c0: 253e 0a0a 2569 6620 6d65 7373 6167 653a  %>..%if message:
-000007d0: 0a20 2020 2024 7b72 656e 6465 725f 6d73  .    ${render_ms
-000007e0: 6728 206d 6573 7361 6765 2c20 7374 6174  g( message, stat
-000007f0: 7573 2029 7d0a 2565 6e64 6966 0a0a 3c64  us )}.%endif..<d
-00000800: 6976 2063 6c61 7373 3d22 7761 726e 696e  iv class="warnin
-00000810: 676d 6573 7361 6765 223e 0a20 2020 203c  gmessage">.    <
-00000820: 703e 0a20 2020 2020 2020 2054 6865 2047  p>.        The G
-00000830: 616c 6178 7920 6465 7665 6c6f 706d 656e  alaxy developmen
-00000840: 7420 7465 616d 2064 6f65 7320 6e6f 7420  t team does not 
-00000850: 6d61 696e 7461 696e 2074 6865 2063 6f6e  maintain the con
-00000860: 7465 6e74 7320 6f66 206d 616e 7920 4761  tents of many Ga
-00000870: 6c61 7879 2054 6f6f 6c20 5368 6564 2072  laxy Tool Shed r
-00000880: 6570 6f73 6974 6f72 6965 732e 2020 536f  epositories.  So
-00000890: 6d65 0a20 2020 2020 2020 2072 6570 6f73  me.        repos
-000008a0: 6974 6f72 7920 746f 6f6c 7320 6d61 7920  itory tools may 
-000008b0: 696e 636c 7564 6520 636f 6465 2074 6861  include code tha
-000008c0: 7420 7072 6f64 7563 6573 206d 616c 6963  t produces malic
-000008d0: 696f 7573 2062 6568 6176 696f 722c 2073  ious behavior, s
-000008e0: 6f20 6265 2061 7761 7265 206f 6620 7768  o be aware of wh
-000008f0: 6174 2079 6f75 2061 7265 2069 6e73 7461  at you are insta
-00000900: 6c6c 696e 672e 0a20 2020 203c 2f70 3e0a  lling..    </p>.
-00000910: 2020 2020 3c70 3e0a 2020 2020 2020 2020      <p>.        
-00000920: 4966 2079 6f75 2064 6973 636f 7665 7220  If you discover 
-00000930: 6120 7265 706f 7369 746f 7279 2074 6861  a repository tha
-00000940: 7420 6361 7573 6573 2070 726f 626c 656d  t causes problem
-00000950: 7320 6166 7465 7220 696e 7374 616c 6c61  s after installa
-00000960: 7469 6f6e 2c20 636f 6e74 6163 7420 3c61  tion, contact <a
-00000970: 2068 7265 663d 2268 7474 7073 3a2f 2f67   href="https://g
-00000980: 616c 6178 7970 726f 6a65 6374 2e6f 7267  alaxyproject.org
-00000990: 2f73 7570 706f 7274 2220 7461 7267 6574  /support" target
-000009a0: 3d22 5f62 6c61 6e6b 223e 4761 6c61 7879  ="_blank">Galaxy
-000009b0: 2073 7570 706f 7274 3c2f 613e 2c0a 2020   support</a>,.  
-000009c0: 2020 2020 2020 7365 6e64 696e 6720 616c        sending al
-000009d0: 6c20 6e65 6365 7373 6172 7920 696e 666f  l necessary info
-000009e0: 726d 6174 696f 6e2c 2061 6e64 2061 7070  rmation, and app
-000009f0: 726f 7072 6961 7465 2061 6374 696f 6e20  ropriate action 
-00000a00: 7769 6c6c 2062 6520 7461 6b65 6e2e 0a20  will be taken.. 
-00000a10: 2020 203c 2f70 3e0a 2020 2020 3c70 3e0a     </p>.    <p>.
-00000a20: 2020 2020 2020 2020 3c61 2068 7265 663d          <a href=
-00000a30: 2268 7474 7073 3a2f 2f67 616c 6178 7970  "https://galaxyp
-00000a40: 726f 6a65 6374 2e6f 7267 2f74 6f6f 6c73  roject.org/tools
-00000a50: 6865 642f 7265 706f 7369 746f 7279 2d66  hed/repository-f
-00000a60: 6561 7475 7265 732f 2363 6f6e 7461 6374  eatures/#contact
-00000a70: 2d72 6570 6f73 6974 6f72 792d 6f77 6e65  -repository-owne
-00000a80: 7222 2074 6172 6765 743d 225f 626c 616e  r" target="_blan
-00000a90: 6b22 3e43 6f6e 7461 6374 2074 6865 2072  k">Contact the r
-00000aa0: 6570 6f73 6974 6f72 7920 6f77 6e65 723c  epository owner<
-00000ab0: 2f61 3e20 666f 720a 2020 2020 2020 2020  /a> for.        
-00000ac0: 6765 6e65 7261 6c20 7175 6573 7469 6f6e  general question
-00000ad0: 7320 6f72 2063 6f6e 6365 726e 732e 0a20  s or concerns.. 
-00000ae0: 2020 203c 2f70 3e0a 3c2f 6469 763e 0a3c     </p>.</div>.<
-00000af0: 6469 7620 636c 6173 733d 2263 6172 6422  div class="card"
-00000b00: 3e0a 2020 2020 3c64 6976 2063 6c61 7373  >.    <div class
-00000b10: 3d22 6361 7264 2d62 6f64 7922 3e0a 2020  ="card-body">.  
-00000b20: 2020 2020 2020 3c66 6f72 6d20 6e61 6d65        <form name
-00000b30: 3d22 7365 6c65 6374 5f74 6f6f 6c5f 7061  ="select_tool_pa
-00000b40: 6e65 6c5f 7365 6374 696f 6e22 2069 643d  nel_section" id=
-00000b50: 2273 656c 6563 745f 746f 6f6c 5f70 616e  "select_tool_pan
-00000b60: 656c 5f73 6563 7469 6f6e 2220 6163 7469  el_section" acti
-00000b70: 6f6e 3d22 247b 682e 7572 6c5f 666f 7228  on="${h.url_for(
-00000b80: 2063 6f6e 7472 6f6c 6c65 723d 2761 646d   controller='adm
-00000b90: 696e 5f74 6f6f 6c73 6865 6427 2c20 6163  in_toolshed', ac
-00000ba0: 7469 6f6e 3d27 7072 6570 6172 655f 666f  tion='prepare_fo
-00000bb0: 725f 696e 7374 616c 6c27 2029 7d22 206d  r_install' )}" m
-00000bc0: 6574 686f 643d 2270 6f73 7422 203e 0a20  ethod="post" >. 
-00000bd0: 2020 2020 2020 2020 2020 203c 6469 7620             <div 
-00000be0: 636c 6173 733d 2266 6f72 6d2d 726f 7722  class="form-row"
-00000bf0: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
-00000c00: 2020 3c69 6e70 7574 2074 7970 653d 2268    <input type="h
-00000c10: 6964 6465 6e22 206e 616d 653d 2269 6e63  idden" name="inc
-00000c20: 6c75 6465 735f 746f 6f6c 7322 2076 616c  ludes_tools" val
-00000c30: 7565 3d22 247b 696e 636c 7564 6573 5f74  ue="${includes_t
-00000c40: 6f6f 6c73 7d22 202f 3e0a 2020 2020 2020  ools}" />.      
-00000c50: 2020 2020 2020 2020 2020 3c69 6e70 7574            <input
-00000c60: 2074 7970 653d 2268 6964 6465 6e22 206e   type="hidden" n
-00000c70: 616d 653d 2269 6e63 6c75 6465 735f 746f  ame="includes_to
-00000c80: 6f6c 5f64 6570 656e 6465 6e63 6965 7322  ol_dependencies"
-00000c90: 2076 616c 7565 3d22 247b 696e 636c 7564   value="${includ
-00000ca0: 6573 5f74 6f6f 6c5f 6465 7065 6e64 656e  es_tool_dependen
-00000cb0: 6369 6573 7d22 202f 3e0a 2020 2020 2020  cies}" />.      
-00000cc0: 2020 2020 2020 2020 2020 3c69 6e70 7574            <input
-00000cd0: 2074 7970 653d 2268 6964 6465 6e22 206e   type="hidden" n
-00000ce0: 616d 653d 2272 6571 7569 7265 6d65 6e74  ame="requirement
-00000cf0: 735f 7374 6174 7573 2220 7661 6c75 653d  s_status" value=
-00000d00: 2224 7b72 6571 7569 7265 6d65 6e74 735f  "${requirements_
-00000d10: 7374 6174 7573 7d22 202f 3e0a 2020 2020  status}" />.    
-00000d20: 2020 2020 2020 2020 2020 2020 3c69 6e70              <inp
-00000d30: 7574 2074 7970 653d 2268 6964 6465 6e22  ut type="hidden"
-00000d40: 206e 616d 653d 2269 6e63 6c75 6465 735f   name="includes_
-00000d50: 746f 6f6c 735f 666f 725f 6469 7370 6c61  tools_for_displa
-00000d60: 795f 696e 5f74 6f6f 6c5f 7061 6e65 6c22  y_in_tool_panel"
-00000d70: 2076 616c 7565 3d22 247b 696e 636c 7564   value="${includ
-00000d80: 6573 5f74 6f6f 6c73 5f66 6f72 5f64 6973  es_tools_for_dis
-00000d90: 706c 6179 5f69 6e5f 746f 6f6c 5f70 616e  play_in_tool_pan
-00000da0: 656c 7d22 202f 3e0a 2020 2020 2020 2020  el}" />.        
-00000db0: 2020 2020 2020 2020 3c69 6e70 7574 2074          <input t
-00000dc0: 7970 653d 2268 6964 6465 6e22 206e 616d  ype="hidden" nam
-00000dd0: 653d 2274 6f6f 6c5f 7368 6564 5f75 726c  e="tool_shed_url
-00000de0: 2220 7661 6c75 653d 2224 7b74 6f6f 6c5f  " value="${tool_
-00000df0: 7368 6564 5f75 726c 7d22 202f 3e0a 2020  shed_url}" />.  
-00000e00: 2020 2020 2020 2020 2020 2020 2020 3c69                <i
-00000e10: 6e70 7574 2074 7970 653d 2268 6964 6465  nput type="hidde
-00000e20: 6e22 206e 616d 653d 2265 6e63 6f64 6564  n" name="encoded
-00000e30: 5f72 6570 6f5f 696e 666f 5f64 6963 7473  _repo_info_dicts
-00000e40: 2220 7661 6c75 653d 2224 7b65 6e63 6f64  " value="${encod
-00000e50: 6564 5f72 6570 6f5f 696e 666f 5f64 6963  ed_repo_info_dic
-00000e60: 7473 7d22 202f 3e0a 2020 2020 2020 2020  ts}" />.        
-00000e70: 2020 2020 2020 2020 3c69 6e70 7574 2074          <input t
-00000e80: 7970 653d 2268 6964 6465 6e22 206e 616d  ype="hidden" nam
-00000e90: 653d 2275 7064 6174 696e 6722 2076 616c  e="updating" val
-00000ea0: 7565 3d22 247b 7570 6461 7469 6e67 7d22  ue="${updating}"
-00000eb0: 202f 3e0a 2020 2020 2020 2020 2020 2020   />.            
-00000ec0: 2020 2020 3c69 6e70 7574 2074 7970 653d      <input type=
-00000ed0: 2268 6964 6465 6e22 206e 616d 653d 2275  "hidden" name="u
-00000ee0: 7064 6174 696e 675f 7265 706f 7369 746f  pdating_reposito
-00000ef0: 7279 5f69 6422 2076 616c 7565 3d22 247b  ry_id" value="${
-00000f00: 7570 6461 7469 6e67 5f72 6570 6f73 6974  updating_reposit
-00000f10: 6f72 795f 6964 7d22 202f 3e0a 2020 2020  ory_id}" />.    
-00000f20: 2020 2020 2020 2020 2020 2020 3c69 6e70              <inp
-00000f30: 7574 2074 7970 653d 2268 6964 6465 6e22  ut type="hidden"
-00000f40: 206e 616d 653d 2275 7064 6174 696e 675f   name="updating_
-00000f50: 746f 5f63 7478 5f72 6576 2220 7661 6c75  to_ctx_rev" valu
-00000f60: 653d 2224 7b75 7064 6174 696e 675f 746f  e="${updating_to
-00000f70: 5f63 7478 5f72 6576 7d22 202f 3e0a 2020  _ctx_rev}" />.  
-00000f80: 2020 2020 2020 2020 2020 2020 2020 3c69                <i
-00000f90: 6e70 7574 2074 7970 653d 2268 6964 6465  nput type="hidde
-00000fa0: 6e22 206e 616d 653d 2275 7064 6174 696e  n" name="updatin
-00000fb0: 675f 746f 5f63 6861 6e67 6573 6574 5f72  g_to_changeset_r
-00000fc0: 6576 6973 696f 6e22 2076 616c 7565 3d22  evision" value="
-00000fd0: 247b 7570 6461 7469 6e67 5f74 6f5f 6368  ${updating_to_ch
-00000fe0: 616e 6765 7365 745f 7265 7669 7369 6f6e  angeset_revision
-00000ff0: 7d22 202f 3e0a 2020 2020 2020 2020 2020  }" />.          
-00001000: 2020 2020 2020 3c69 6e70 7574 2074 7970        <input typ
-00001010: 653d 2268 6964 6465 6e22 206e 616d 653d  e="hidden" name=
-00001020: 2265 6e63 6f64 6564 5f75 7064 6174 6564  "encoded_updated
-00001030: 5f6d 6574 6164 6174 6122 2076 616c 7565  _metadata" value
-00001040: 3d22 247b 656e 636f 6465 645f 7570 6461  ="${encoded_upda
-00001050: 7465 645f 6d65 7461 6461 7461 7d22 202f  ted_metadata}" /
-00001060: 3e0a 2020 2020 2020 2020 2020 2020 3c2f  >.            </
-00001070: 6469 763e 0a20 2020 2020 2020 2020 2020  div>.           
-00001080: 203c 6469 7620 7374 796c 653d 2263 6c65   <div style="cle
-00001090: 6172 3a20 626f 7468 223e 3c2f 6469 763e  ar: both"></div>
-000010a0: 0a20 2020 2020 2020 2020 2020 203c 2520  .            <% 
-000010b0: 7265 6164 6d65 5f66 696c 6573 5f64 6963  readme_files_dic
-000010c0: 7420 3d20 636f 6e74 6169 6e65 7273 5f64  t = containers_d
-000010d0: 6963 742e 6765 7428 2027 7265 6164 6d65  ict.get( 'readme
-000010e0: 5f66 696c 6573 272c 204e 6f6e 6520 2920  _files', None ) 
-000010f0: 253e 0a20 2020 2020 2020 2020 2020 2025  %>.            %
-00001100: 6966 2072 6561 646d 655f 6669 6c65 735f  if readme_files_
-00001110: 6469 6374 3a0a 2020 2020 2020 2020 2020  dict:.          
-00001120: 2020 2020 2020 3c64 6976 2063 6c61 7373        <div class
-00001130: 3d22 666f 726d 2d72 6f77 223e 0a20 2020  ="form-row">.   
-00001140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001150: 203c 7461 626c 6520 636c 6173 733d 2263   <table class="c
-00001160: 6f6c 6f72 6564 2220 7769 6474 683d 2231  olored" width="1
-00001170: 3030 2522 3e0a 2020 2020 2020 2020 2020  00%">.          
-00001180: 2020 2020 2020 2020 2020 2020 2020 3c74                <t
-00001190: 6820 6267 636f 6c6f 723d 2223 4542 4439  h bgcolor="#EBD9
-000011a0: 4232 223e 5265 706f 7369 746f 7279 2052  B2">Repository R
-000011b0: 4541 444d 4520 6669 6c65 202d 206d 6179  EADME file - may
-000011c0: 2063 6f6e 7461 696e 2069 6d70 6f72 7461   contain importa
-000011d0: 6e74 2069 6e73 7461 6c6c 6174 696f 6e20  nt installation 
-000011e0: 6f72 206c 6963 656e 7365 2069 6e66 6f72  or license infor
-000011f0: 6d61 7469 6f6e 3c2f 7468 3e0a 2020 2020  mation</th>.    
-00001200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001210: 3c2f 7461 626c 653e 0a20 2020 2020 2020  </table>.       
-00001220: 2020 2020 2020 2020 203c 2f64 6976 3e0a           </div>.
-00001230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001240: 247b 7265 6e64 6572 5f72 6561 646d 655f  ${render_readme_
-00001250: 7365 6374 696f 6e28 2063 6f6e 7461 696e  section( contain
-00001260: 6572 735f 6469 6374 2029 7d0a 2020 2020  ers_dict )}.    
-00001270: 2020 2020 2020 2020 2020 2020 3c64 6976              <div
-00001280: 2073 7479 6c65 3d22 636c 6561 723a 2062   style="clear: b
-00001290: 6f74 6822 3e3c 2f64 6976 3e0a 2020 2020  oth"></div>.    
-000012a0: 2020 2020 2020 2020 2565 6e64 6966 0a20          %endif. 
-000012b0: 2020 2020 2020 2020 2020 203c 250a 2020             <%.  
-000012c0: 2020 2020 2020 2020 2020 2020 2020 6966                if
-000012d0: 2072 6571 7569 7265 6d65 6e74 735f 7374   requirements_st
-000012e0: 6174 7573 2061 6e64 2069 6e73 7461 6c6c  atus and install
-000012f0: 5f72 6573 6f6c 7665 725f 6465 7065 6e64  _resolver_depend
-00001300: 656e 6369 6573 5f63 6865 636b 5f62 6f78  encies_check_box
-00001310: 206f 7220 696e 636c 7564 6573 5f74 6f6f   or includes_too
-00001320: 6c5f 6465 7065 6e64 656e 6369 6573 3a0a  l_dependencies:.
-00001330: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001340: 2020 2020 6469 7370 6c61 795f 6465 7065      display_depe
-00001350: 6e64 656e 6379 5f63 6f6e 6669 726d 6174  ndency_confirmat
-00001360: 696f 6e20 3d20 5472 7565 0a20 2020 2020  ion = True.     
-00001370: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
-00001380: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00001390: 2020 2020 2064 6973 706c 6179 5f64 6570       display_dep
-000013a0: 656e 6465 6e63 795f 636f 6e66 6972 6d61  endency_confirma
-000013b0: 7469 6f6e 203d 2046 616c 7365 0a20 2020  tion = False.   
-000013c0: 2020 2020 2020 2020 2025 3e0a 2020 2020           %>.    
-000013d0: 2020 2020 2020 2020 2569 6620 7265 7175          %if requ
-000013e0: 6972 656d 656e 7473 5f73 7461 7475 733a  irements_status:
-000013f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00001400: 2025 6966 206e 6f74 2069 6e73 7461 6c6c   %if not install
-00001410: 5f72 6573 6f6c 7665 725f 6465 7065 6e64  _resolver_depend
-00001420: 656e 6369 6573 5f63 6865 636b 5f62 6f78  encies_check_box
-00001430: 2061 6e64 206e 6f74 2069 6e63 6c75 6465   and not include
-00001440: 735f 746f 6f6c 5f64 6570 656e 6465 6e63  s_tool_dependenc
-00001450: 6965 733a 0a20 2020 2020 2020 2020 2020  ies:.           
-00001460: 2020 2020 203c 6469 7620 636c 6173 733d       <div class=
-00001470: 2266 6f72 6d2d 726f 7722 3e0a 2020 2020  "form-row">.    
-00001480: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001490: 3c74 6162 6c65 2063 6c61 7373 3d22 636f  <table class="co
-000014a0: 6c6f 7265 6422 2077 6964 7468 3d22 3130  lored" width="10
-000014b0: 3025 223e 0a20 2020 2020 2020 2020 2020  0%">.           
-000014c0: 2020 2020 2020 2020 2020 2020 203c 6865               <he
-000014d0: 6164 3e0a 2020 2020 2020 2020 2020 2020  ad>.            
-000014e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000014f0: 3c74 683e 0a20 2020 2020 2020 2020 2020  <th>.           
-00001500: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001510: 2020 2020 203c 696d 6720 7372 633d 2224       <img src="$
-00001520: 7b68 2e75 726c 5f66 6f72 2827 2f73 7461  {h.url_for('/sta
-00001530: 7469 6327 297d 2f69 6d61 6765 732f 6963  tic')}/images/ic
-00001540: 6f6e 5f65 7272 6f72 5f73 6d6c 2e67 6966  on_error_sml.gif
-00001550: 2220 7469 746c 653d 2743 616e 6e6f 7420  " title='Cannot 
-00001560: 696e 7374 616c 6c20 6465 7065 6e64 656e  install dependen
-00001570: 6369 6573 272f 3e0a 2020 2020 2020 2020  cies'/>.        
-00001580: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001590: 2020 2020 2020 2020 5468 6973 2072 6570          This rep
-000015a0: 6f73 6974 6f72 7920 7265 7175 6972 6573  ository requires
-000015b0: 2064 6570 656e 6465 6e63 6965 7320 7468   dependencies th
-000015c0: 6174 2063 616e 6e6f 7420 6265 2069 6e73  at cannot be ins
-000015d0: 7461 6c6c 6564 2074 6872 6f75 6768 2074  talled through t
-000015e0: 6865 2054 6f6f 6c20 5368 6564 0a20 2020  he Tool Shed.   
-000015f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001600: 2020 2020 2020 2020 203c 2f74 683e 0a20           </th>. 
-00001610: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001620: 2020 2020 2020 203c 2f68 6561 643e 0a20         </head>. 
-00001630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001640: 2020 203c 2f74 6162 6c65 3e0a 2020 2020     </table>.    
-00001650: 2020 2020 2020 2020 2020 2020 3c2f 6469              </di
-00001660: 763e 0a20 2020 2020 2020 2020 2020 2020  v>.             
-00001670: 2020 203c 6469 7620 636c 6173 733d 2266     <div class="f
-00001680: 6f72 6d2d 726f 7722 3e0a 2020 2020 2020  orm-row">.      
-00001690: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-000016a0: 703e 5468 6973 2072 6570 6f73 6974 6f72  p>This repositor
-000016b0: 7920 6465 6669 6e65 7320 746f 6f6c 2072  y defines tool r
-000016c0: 6571 7569 7265 6d65 6e74 7320 7468 6174  equirements that
-000016d0: 2063 616e 6e6f 7420 6265 2069 6e73 7461   cannot be insta
-000016e0: 6c6c 6564 2074 6872 6f75 6768 2074 6865  lled through the
-000016f0: 2054 6f6f 6c20 5368 6564 2e3c 2f70 3e0a   Tool Shed.</p>.
-00001700: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001710: 2020 2020 203c 703e 506c 6561 7365 2061       <p>Please a
-00001720: 6374 6976 6174 6520 436f 6e64 6120 6465  ctivate Conda de
-00001730: 7065 6e64 656e 6379 2072 6573 6f6c 7574  pendency resolut
-00001740: 696f 6e2c 2061 6374 6976 6174 6520 446f  ion, activate Do
-00001750: 636b 6572 2064 6570 656e 6465 6e63 7920  cker dependency 
-00001760: 7265 736f 6c75 7469 6f6e 2c20 7365 7475  resolution, setu
-00001770: 7020 456e 7669 726f 6e6d 656e 7420 4d6f  p Environment Mo
-00001780: 6475 6c65 730a 6f72 206d 616e 7561 6c6c  dules.or manuall
-00001790: 7920 7361 7469 7366 7920 7468 6520 6465  y satisfy the de
-000017a0: 7065 6e64 656e 6369 6573 206c 6973 7465  pendencies liste
-000017b0: 6420 6265 6c6f 772e 3c2f 703e 0a20 2020  d below.</p>.   
-000017c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000017d0: 2020 3c70 3e46 6f72 2064 6574 6169 6c73    <p>For details
-000017e0: 2073 6565 203c 6120 7461 7267 6574 3d22   see <a target="
-000017f0: 5f62 6c61 6e6b 2220 6872 6566 3d22 6874  _blank" href="ht
-00001800: 7470 733a 2f2f 646f 6373 2e67 616c 6178  tps://docs.galax
-00001810: 7970 726f 6a65 6374 2e6f 7267 2f65 6e2f  yproject.org/en/
-00001820: 6d61 7374 6572 2f61 646d 696e 2f64 6570  master/admin/dep
-00001830: 656e 6465 6e63 795f 7265 736f 6c76 6572  endency_resolver
-00001840: 732e 6874 6d6c 223e 7468 6520 6465 7065  s.html">the depe
-00001850: 6e64 656e 6379 2072 6573 6f6c 7665 7220  ndency resolver 
-00001860: 646f 6375 6d65 6e74 6174 696f 6e2e 3c2f  documentation.</
-00001870: 613e 3c2f 703e 0a20 2020 2020 2020 2020  a></p>.         
-00001880: 2020 2020 2020 203c 2f64 6976 3e0a 2020         </div>.  
-00001890: 2020 2020 2020 2020 2020 2020 2020 2565                %e
-000018a0: 6e64 6966 0a20 2020 2020 2020 2020 2020  ndif.           
-000018b0: 2020 2020 203c 6469 7620 636c 6173 733d       <div class=
-000018c0: 2266 6f72 6d2d 726f 7722 3e0a 2020 2020  "form-row">.    
-000018d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000018e0: 3c74 6162 6c65 2063 6c61 7373 3d22 636f  <table class="co
-000018f0: 6c6f 7265 6422 2077 6964 7468 3d22 3130  lored" width="10
-00001900: 3025 223e 0a20 2020 2020 2020 2020 2020  0%">.           
-00001910: 2020 2020 2020 2020 2020 2020 203c 7468               <th
-00001920: 2062 6763 6f6c 6f72 3d22 2345 4244 3942   bgcolor="#EBD9B
-00001930: 3222 3e54 6865 2066 6f6c 6c6f 7769 6e67  2">The following
-00001940: 2074 6f6f 6c20 6465 7065 6e64 656e 6369   tool dependenci
-00001950: 6573 2061 7265 2072 6571 7569 7265 6420  es are required 
-00001960: 6279 2074 6865 2063 7572 7265 6e74 2072  by the current r
-00001970: 6570 6f73 6974 6f72 793c 2f74 683e 0a20  epository</th>. 
-00001980: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001990: 2020 203c 2f74 6162 6c65 3e0a 2020 2020     </table>.    
-000019a0: 2020 2020 2020 2020 2020 2020 3c2f 6469              </di
-000019b0: 763e 0a20 2020 2020 2020 2020 2020 2020  v>.             
-000019c0: 2020 203c 6469 7620 636c 6173 733d 2266     <div class="f
-000019d0: 6f72 6d2d 726f 7722 3e0a 2020 2020 2020  orm-row">.      
-000019e0: 2020 2020 2020 2020 2020 2020 2020 247b                ${
-000019f0: 7265 6e64 6572 5f74 6f6f 6c5f 6465 7065  render_tool_depe
-00001a00: 6e64 656e 6379 5f72 6573 6f6c 7665 7228  ndency_resolver(
-00001a10: 2072 6571 7569 7265 6d65 6e74 735f 7374   requirements_st
-00001a20: 6174 7573 2c20 7072 6570 6172 655f 666f  atus, prepare_fo
-00001a30: 725f 696e 7374 616c 6c3d 5472 7565 2029  r_install=True )
-00001a40: 7d0a 2020 2020 2020 2020 2020 2020 2020  }.              
-00001a50: 2020 3c2f 6469 763e 0a20 2020 2020 2020    </div>.       
-00001a60: 2020 2020 2020 2020 203c 6469 7620 7374           <div st
-00001a70: 796c 653d 2263 6c65 6172 3a20 626f 7468  yle="clear: both
-00001a80: 223e 3c2f 6469 763e 0a20 2020 2020 2020  "></div>.       
-00001a90: 2020 2020 2025 656e 6469 660a 2020 2020       %endif.    
-00001aa0: 2020 2020 2020 2020 2569 6620 6361 6e5f          %if can_
-00001ab0: 6469 7370 6c61 795f 7265 706f 7369 746f  display_reposito
-00001ac0: 7279 5f64 6570 656e 6465 6e63 6965 7320  ry_dependencies 
-00001ad0: 6f72 2064 6973 706c 6179 5f64 6570 656e  or display_depen
-00001ae0: 6465 6e63 795f 636f 6e66 6972 6d61 7469  dency_confirmati
-00001af0: 6f6e 3a0a 2020 2020 2020 2020 2020 2020  on:.            
-00001b00: 2020 2020 3c64 6976 2063 6c61 7373 3d22      <div class="
-00001b10: 666f 726d 2d72 6f77 223e 0a20 2020 2020  form-row">.     
-00001b20: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00001b30: 7461 626c 6520 636c 6173 733d 2263 6f6c  table class="col
-00001b40: 6f72 6564 2220 7769 6474 683d 2231 3030  ored" width="100
-00001b50: 2522 3e0a 2020 2020 2020 2020 2020 2020  %">.            
-00001b60: 2020 2020 2020 2020 2020 2020 3c74 6820              <th 
-00001b70: 6267 636f 6c6f 723d 2223 4542 4439 4232  bgcolor="#EBD9B2
-00001b80: 223e 436f 6e66 6972 6d20 6465 7065 6e64  ">Confirm depend
-00001b90: 656e 6379 2069 6e73 7461 6c6c 6174 696f  ency installatio
-00001ba0: 6e3c 2f74 683e 0a20 2020 2020 2020 2020  n</th>.         
-00001bb0: 2020 2020 2020 2020 2020 203c 2f74 6162             </tab
-00001bc0: 6c65 3e0a 2020 2020 2020 2020 2020 2020  le>.            
-00001bd0: 2020 2020 3c2f 6469 763e 0a20 2020 2020      </div>.     
-00001be0: 2020 2020 2020 2020 2020 2024 7b72 656e             ${ren
-00001bf0: 6465 725f 6465 7065 6e64 656e 6369 6573  der_dependencies
-00001c00: 5f73 6563 7469 6f6e 2820 696e 7374 616c  _section( instal
-00001c10: 6c5f 7265 736f 6c76 6572 5f64 6570 656e  l_resolver_depen
-00001c20: 6465 6e63 6965 735f 6368 6563 6b5f 626f  dencies_check_bo
-00001c30: 782c 2069 6e73 7461 6c6c 5f72 6570 6f73  x, install_repos
-00001c40: 6974 6f72 795f 6465 7065 6e64 656e 6369  itory_dependenci
-00001c50: 6573 5f63 6865 636b 5f62 6f78 2c20 696e  es_check_box, in
-00001c60: 7374 616c 6c5f 746f 6f6c 5f64 6570 656e  stall_tool_depen
-00001c70: 6465 6e63 6965 735f 6368 6563 6b5f 626f  dencies_check_bo
-00001c80: 782c 2063 6f6e 7461 696e 6572 735f 6469  x, containers_di
-00001c90: 6374 2c20 7265 7669 7369 6f6e 5f6c 6162  ct, revision_lab
-00001ca0: 656c 3d4e 6f6e 652c 2065 7870 6f72 743d  el=None, export=
-00001cb0: 4661 6c73 652c 2072 6571 7569 7265 6d65  False, requireme
-00001cc0: 6e74 735f 7374 6174 7573 3d72 6571 7569  nts_status=requi
-00001cd0: 7265 6d65 6e74 735f 7374 6174 7573 2029  rements_status )
-00001ce0: 7d0a 2020 2020 2020 2020 2020 2020 2020  }.              
-00001cf0: 2020 3c64 6976 2073 7479 6c65 3d22 636c    <div style="cl
-00001d00: 6561 723a 2062 6f74 6822 3e3c 2f64 6976  ear: both"></div
-00001d10: 3e0a 2020 2020 2020 2020 2020 2020 2565  >.            %e
-00001d20: 6e64 6966 0a20 2020 2020 2020 2020 2020  ndif.           
-00001d30: 2025 6966 2073 6865 645f 746f 6f6c 5f63   %if shed_tool_c
-00001d40: 6f6e 665f 7365 6c65 6374 5f66 6965 6c64  onf_select_field
-00001d50: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00001d60: 2020 2569 6620 696e 636c 7564 6573 5f74    %if includes_t
-00001d70: 6f6f 6c73 5f66 6f72 5f64 6973 706c 6179  ools_for_display
-00001d80: 5f69 6e5f 746f 6f6c 5f70 616e 656c 3a0a  _in_tool_panel:.
-00001d90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001da0: 2020 2020 3c64 6976 2063 6c61 7373 3d22      <div class="
-00001db0: 666f 726d 2d72 6f77 223e 0a20 2020 2020  form-row">.     
-00001dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001dd0: 2020 203c 7461 626c 6520 636c 6173 733d     <table class=
-00001de0: 2263 6f6c 6f72 6564 2220 7769 6474 683d  "colored" width=
-00001df0: 2231 3030 2522 3e0a 2020 2020 2020 2020  "100%">.        
-00001e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001e10: 2020 2020 3c74 6820 6267 636f 6c6f 723d      <th bgcolor=
-00001e20: 2223 4542 4439 4232 223e 4368 6f6f 7365  "#EBD9B2">Choose
-00001e30: 2074 6865 2074 6f6f 6c20 7061 6e65 6c20   the tool panel 
-00001e40: 7365 6374 696f 6e20 746f 2063 6f6e 7461  section to conta
-00001e50: 696e 2074 6865 2069 6e73 7461 6c6c 6564  in the installed
-00001e60: 2074 6f6f 6c73 2028 6f70 7469 6f6e 616c   tools (optional
-00001e70: 293c 2f74 683e 0a20 2020 2020 2020 2020  )</th>.         
-00001e80: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00001e90: 2f74 6162 6c65 3e0a 2020 2020 2020 2020  /table>.        
-00001ea0: 2020 2020 2020 2020 2020 2020 3c2f 6469              </di
-00001eb0: 763e 0a20 2020 2020 2020 2020 2020 2020  v>.             
-00001ec0: 2020 203c 6469 7620 636c 6173 733d 2264     <div class="d
-00001ed0: 6574 6169 6c2d 7365 6374 696f 6e22 3e0a  etail-section">.
-00001ee0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001ef0: 2565 6e64 6966 0a20 2020 2020 2020 2020  %endif.         
-00001f00: 2020 2020 2020 203c 250a 2020 2020 2020         <%.      
-00001f10: 2020 2020 2020 2020 2020 2020 2020 6966                if
-00001f20: 206c 656e 2820 7368 6564 5f74 6f6f 6c5f   len( shed_tool_
-00001f30: 636f 6e66 5f73 656c 6563 745f 6669 656c  conf_select_fiel
-00001f40: 642e 6f70 7469 6f6e 7320 2920 3d3d 2031  d.options ) == 1
-00001f50: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00001f60: 2020 2020 2020 2020 2020 7365 6c65 6374            select
-00001f70: 5f68 656c 7020 3d20 2259 6f75 7220 4761  _help = "Your Ga
-00001f80: 6c61 7879 2069 6e73 7461 6e63 6520 6973  laxy instance is
-00001f90: 2063 6f6e 6669 6775 7265 6420 7769 7468   configured with
-00001fa0: 2031 2073 6865 642d 7265 6c61 7465 6420   1 shed-related 
-00001fb0: 746f 6f6c 2063 6f6e 6669 6775 7261 7469  tool configurati
-00001fc0: 6f6e 2066 696c 652c 2073 6f20 7265 706f  on file, so repo
-00001fd0: 7369 746f 7269 6573 2077 696c 6c20 6265  sitories will be
-00001fe0: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-00001ff0: 2020 2020 2020 2020 2020 2073 656c 6563             selec
-00002000: 745f 6865 6c70 202b 3d20 2269 6e73 7461  t_help += "insta
-00002010: 6c6c 6564 2075 7369 6e67 2069 7473 203c  lled using its <
-00002020: 623e 746f 6f6c 5f70 6174 683c 2f62 3e20  b>tool_path</b> 
-00002030: 7365 7474 696e 672e 220a 2020 2020 2020  setting.".      
-00002040: 2020 2020 2020 2020 2020 2020 2020 656c                el
-00002050: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-00002060: 2020 2020 2020 2020 2020 2020 7365 6c65              sele
-00002070: 6374 5f68 656c 7020 3d20 2259 6f75 7220  ct_help = "Your 
-00002080: 4761 6c61 7879 2069 6e73 7461 6e63 6520  Galaxy instance 
-00002090: 6973 2063 6f6e 6669 6775 7265 6420 7769  is configured wi
-000020a0: 7468 2025 6420 7368 6564 2d72 656c 6174  th %d shed-relat
-000020b0: 6564 2074 6f6f 6c20 636f 6e66 6967 7572  ed tool configur
-000020c0: 6174 696f 6e20 6669 6c65 732c 2022 2025  ation files, " %
-000020d0: 206c 656e 2820 7368 6564 5f74 6f6f 6c5f   len( shed_tool_
-000020e0: 636f 6e66 5f73 656c 6563 745f 6669 656c  conf_select_fiel
-000020f0: 642e 6f70 7469 6f6e 7320 290a 2020 2020  d.options ).    
-00002100: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002110: 2020 2020 7365 6c65 6374 5f68 656c 7020      select_help 
-00002120: 2b3d 2022 736f 2073 656c 6563 7420 7468  += "so select th
-00002130: 6520 6669 6c65 2077 686f 7365 203c 623e  e file whose <b>
-00002140: 746f 6f6c 5f70 6174 683c 2f62 3e20 7365  tool_path</b> se
-00002150: 7474 696e 6720 796f 7520 7761 6e74 2075  tting you want u
-00002160: 7365 6420 666f 7220 696e 7374 616c 6c69  sed for installi
-00002170: 6e67 2072 6570 6f73 6974 6f72 6965 732e  ng repositories.
-00002180: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-00002190: 2020 253e 0a20 2020 2020 2020 2020 2020    %>.           
-000021a0: 2020 2020 203c 6469 7620 636c 6173 733d       <div class=
-000021b0: 2266 6f72 6d2d 726f 7722 3e0a 2020 2020  "form-row">.    
-000021c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000021d0: 3c6c 6162 656c 3e53 6865 6420 746f 6f6c  <label>Shed tool
-000021e0: 2063 6f6e 6669 6775 7261 7469 6f6e 2066   configuration f
-000021f0: 696c 653a 3c2f 6c61 6265 6c3e 0a20 2020  ile:</label>.   
-00002200: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002210: 2024 7b72 656e 6465 725f 7365 6c65 6374   ${render_select
-00002220: 2873 6865 645f 746f 6f6c 5f63 6f6e 665f  (shed_tool_conf_
-00002230: 7365 6c65 6374 5f66 6965 6c64 297d 0a20  select_field)}. 
-00002240: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002250: 2020 203c 6469 7620 636c 6173 733d 2274     <div class="t
-00002260: 6f6f 6c50 6172 616d 4865 6c70 2220 7374  oolParamHelp" st
-00002270: 796c 653d 2263 6c65 6172 3a20 626f 7468  yle="clear: both
-00002280: 3b22 3e0a 2020 2020 2020 2020 2020 2020  ;">.            
-00002290: 2020 2020 2020 2020 2020 2020 247b 7365              ${se
-000022a0: 6c65 6374 5f68 656c 707c 687d 0a20 2020  lect_help|h}.   
-000022b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000022c0: 203c 2f64 6976 3e0a 2020 2020 2020 2020   </div>.        
-000022d0: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
-000022e0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-000022f0: 6469 7620 7374 796c 653d 2263 6c65 6172  div style="clear
-00002300: 3a20 626f 7468 223e 3c2f 6469 763e 0a20  : both"></div>. 
-00002310: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00002320: 2f64 6976 3e0a 2020 2020 2020 2020 2020  /div>.          
-00002330: 2020 2565 6c73 653a 0a20 2020 2020 2020    %else:.       
-00002340: 2020 2020 2020 2020 203c 696e 7075 7420           <input 
-00002350: 7479 7065 3d22 6869 6464 656e 2220 6e61  type="hidden" na
-00002360: 6d65 3d22 7368 6564 5f74 6f6f 6c5f 636f  me="shed_tool_co
-00002370: 6e66 2220 7661 6c75 653d 2224 7b73 6865  nf" value="${she
-00002380: 645f 746f 6f6c 5f63 6f6e 667c 687d 222f  d_tool_conf|h}"/
-00002390: 3e0a 2020 2020 2020 2020 2020 2020 2565  >.            %e
-000023a0: 6e64 6966 0a20 2020 2020 2020 2020 2020  ndif.           
-000023b0: 2025 6966 2069 6e63 6c75 6465 735f 746f   %if includes_to
-000023c0: 6f6c 735f 666f 725f 6469 7370 6c61 795f  ols_for_display_
-000023d0: 696e 5f74 6f6f 6c5f 7061 6e65 6c3a 0a20  in_tool_panel:. 
-000023e0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-000023f0: 6469 7620 636c 6173 733d 2266 6f72 6d2d  div class="form-
-00002400: 726f 7722 3e0a 2020 2020 2020 2020 2020  row">.          
-00002410: 2020 2020 2020 2020 2020 3c6c 6162 656c            <label
-00002420: 3e41 6464 206e 6577 2074 6f6f 6c20 7061  >Add new tool pa
-00002430: 6e65 6c20 7365 6374 696f 6e3a 3c2f 6c61  nel section:</la
-00002440: 6265 6c3e 0a20 2020 2020 2020 2020 2020  bel>.           
-00002450: 2020 2020 2020 2020 203c 696e 7075 7420           <input 
-00002460: 6e61 6d65 3d22 6e65 775f 746f 6f6c 5f70  name="new_tool_p
-00002470: 616e 656c 5f73 6563 7469 6f6e 5f6c 6162  anel_section_lab
-00002480: 656c 2220 7479 7065 3d22 7465 7874 6669  el" type="textfi
-00002490: 656c 6422 2076 616c 7565 3d22 247b 6e65  eld" value="${ne
-000024a0: 775f 746f 6f6c 5f70 616e 656c 5f73 6563  w_tool_panel_sec
-000024b0: 7469 6f6e 5f6c 6162 656c 7c68 7d22 2073  tion_label|h}" s
-000024c0: 697a 653d 2234 3022 2f3e 0a20 2020 2020  ize="40"/>.     
-000024d0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-000024e0: 6469 7620 636c 6173 733d 2274 6f6f 6c50  div class="toolP
-000024f0: 6172 616d 4865 6c70 2220 7374 796c 653d  aramHelp" style=
-00002500: 2263 6c65 6172 3a20 626f 7468 3b22 3e0a  "clear: both;">.
-00002510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002520: 2020 2020 2020 2020 4164 6420 6120 6e65          Add a ne
-00002530: 7720 746f 6f6c 2070 616e 656c 2073 6563  w tool panel sec
-00002540: 7469 6f6e 2074 6f20 636f 6e74 6169 6e20  tion to contain 
-00002550: 7468 6520 696e 7374 616c 6c65 6420 746f  the installed to
-00002560: 6f6c 7320 286f 7074 696f 6e61 6c29 2e0a  ols (optional)..
-00002570: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002580: 2020 2020 3c2f 6469 763e 0a20 2020 2020      </div>.     
-00002590: 2020 2020 2020 2020 2020 203c 2f64 6976             </div
-000025a0: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
-000025b0: 2020 3c64 6976 2063 6c61 7373 3d22 666f    <div class="fo
-000025c0: 726d 2d72 6f77 223e 0a20 2020 2020 2020  rm-row">.       
-000025d0: 2020 2020 2020 2020 2020 2020 203c 6c61               <la
-000025e0: 6265 6c3e 5365 6c65 6374 2065 7869 7374  bel>Select exist
-000025f0: 696e 6720 746f 6f6c 2070 616e 656c 2073  ing tool panel s
-00002600: 6563 7469 6f6e 3a3c 2f6c 6162 656c 3e0a  ection:</label>.
-00002610: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002620: 2020 2020 247b 7265 6e64 6572 5f73 656c      ${render_sel
-00002630: 6563 7428 746f 6f6c 5f70 616e 656c 5f73  ect(tool_panel_s
-00002640: 6563 7469 6f6e 5f73 656c 6563 745f 6669  ection_select_fi
-00002650: 656c 6429 7d0a 2020 2020 2020 2020 2020  eld)}.          
-00002660: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
-00002670: 6c61 7373 3d22 746f 6f6c 5061 7261 6d48  lass="toolParamH
-00002680: 656c 7022 2073 7479 6c65 3d22 636c 6561  elp" style="clea
-00002690: 723a 2062 6f74 683b 223e 0a20 2020 2020  r: both;">.     
-000026a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000026b0: 2020 2043 686f 6f73 6520 616e 2065 7869     Choose an exi
-000026c0: 7374 696e 6720 7365 6374 696f 6e20 696e  sting section in
-000026d0: 2079 6f75 7220 746f 6f6c 2070 616e 656c   your tool panel
-000026e0: 2074 6f20 636f 6e74 6169 6e20 7468 6520   to contain the 
-000026f0: 696e 7374 616c 6c65 6420 746f 6f6c 7320  installed tools 
-00002700: 286f 7074 696f 6e61 6c29 2e0a 2020 2020  (optional)..    
-00002710: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002720: 3c2f 6469 763e 0a20 2020 2020 2020 2020  </div>.         
-00002730: 2020 2020 2020 203c 2f64 6976 3e0a 2020         </div>.  
-00002740: 2020 2020 2020 2020 2020 2565 6e64 6966            %endif
-00002750: 0a20 2020 2020 2020 2020 2020 203c 6469  .            <di
-00002760: 7620 636c 6173 733d 2266 6f72 6d2d 726f  v class="form-ro
-00002770: 7722 3e0a 2020 2020 2020 2020 2020 2020  w">.            
-00002780: 2020 2020 3c69 6e70 7574 2074 7970 653d      <input type=
-00002790: 2273 7562 6d69 7422 206e 616d 653d 2273  "submit" name="s
-000027a0: 656c 6563 745f 746f 6f6c 5f70 616e 656c  elect_tool_panel
-000027b0: 5f73 6563 7469 6f6e 5f62 7574 746f 6e22  _section_button"
-000027c0: 2076 616c 7565 3d22 496e 7374 616c 6c22   value="Install"
-000027d0: 2f3e 0a20 2020 2020 2020 2020 2020 2020  />.             
-000027e0: 2020 203c 6469 7620 636c 6173 733d 2274     <div class="t
-000027f0: 6f6f 6c50 6172 616d 4865 6c70 2220 7374  oolParamHelp" st
-00002800: 796c 653d 2263 6c65 6172 3a20 626f 7468  yle="clear: both
-00002810: 3b22 3e0a 2020 2020 2020 2020 2020 2020  ;">.            
-00002820: 2020 2020 2020 2020 2569 6620 696e 636c          %if incl
-00002830: 7564 6573 5f74 6f6f 6c73 5f66 6f72 5f64  udes_tools_for_d
-00002840: 6973 706c 6179 5f69 6e5f 746f 6f6c 5f70  isplay_in_tool_p
-00002850: 616e 656c 3a0a 2020 2020 2020 2020 2020  anel:.          
-00002860: 2020 2020 2020 2020 2020 2020 2020 436c                Cl
-00002870: 6963 6b69 6e67 203c 623e 496e 7374 616c  icking <b>Instal
-00002880: 6c3c 2f62 3e20 7769 7468 6f75 7420 7365  l</b> without se
-00002890: 6c65 6374 696e 6720 6120 746f 6f6c 2070  lecting a tool p
-000028a0: 616e 656c 2073 6563 7469 6f6e 2077 696c  anel section wil
-000028b0: 6c20 6c6f 6164 2074 6865 2069 6e73 7461  l load the insta
-000028c0: 6c6c 6564 2074 6f6f 6c73 2069 6e74 6f20  lled tools into 
-000028d0: 7468 6520 746f 6f6c 2070 616e 656c 206f  the tool panel o
-000028e0: 7574 7369 6465 206f 6620 616e 7920 7365  utside of any se
-000028f0: 6374 696f 6e73 2e0a 2020 2020 2020 2020  ctions..        
-00002900: 2020 2020 2020 2020 2020 2020 2565 6e64              %end
-00002910: 6966 0a20 2020 2020 2020 2020 2020 2020  if.             
-00002920: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
-00002930: 2020 2020 2020 3c2f 6469 763e 0a20 2020        </div>.   
-00002940: 2020 2020 203c 2f66 6f72 6d3e 0a20 2020       </form>.   
-00002950: 203c 2f64 6976 3e0a 3c2f 6469 763e 0a     </div>.</div>.
+00000060: 6520 6669 6c65 3d22 2f77 6562 6170 7073  e file="/webapps
+00000070: 2f74 6f6f 6c5f 7368 6564 2f63 6f6d 6d6f  /tool_shed/commo
+00000080: 6e2f 636f 6d6d 6f6e 2e6d 616b 6f22 2069  n/common.mako" i
+00000090: 6d70 6f72 743d 222a 2220 2f3e 0a3c 256e  mport="*" />.<%n
+000000a0: 616d 6573 7061 6365 2066 696c 653d 222f  amespace file="/
+000000b0: 7765 6261 7070 732f 746f 6f6c 5f73 6865  webapps/tool_she
+000000c0: 642f 7265 706f 7369 746f 7279 2f63 6f6d  d/repository/com
+000000d0: 6d6f 6e2e 6d61 6b6f 2220 696d 706f 7274  mon.mako" import
+000000e0: 3d22 2a22 202f 3e0a 3c25 6e61 6d65 7370  ="*" />.<%namesp
+000000f0: 6163 6520 6669 6c65 3d22 2f77 6562 6170  ace file="/webap
+00000100: 7073 2f74 6f6f 6c5f 7368 6564 2f63 6f6d  ps/tool_shed/com
+00000110: 6d6f 6e2f 7265 706f 7369 746f 7279 5f61  mon/repository_a
+00000120: 6374 696f 6e73 5f6d 656e 752e 6d61 6b6f  ctions_menu.mako
+00000130: 2220 696d 706f 7274 3d22 2a22 202f 3e0a  " import="*" />.
+00000140: 0a3c 250a 2020 2020 6672 6f6d 2067 616c  .<%.    from gal
+00000150: 6178 792e 7765 622e 6672 616d 6577 6f72  axy.web.framewor
+00000160: 6b2e 6865 6c70 6572 7320 696d 706f 7274  k.helpers import
+00000170: 2074 696d 655f 6167 6f0a 2020 2020 6672   time_ago.    fr
+00000180: 6f6d 2074 6f6f 6c5f 7368 6564 2e75 7469  om tool_shed.uti
+00000190: 6c2e 6261 7369 635f 7574 696c 2069 6d70  l.basic_util imp
+000001a0: 6f72 7420 746f 5f68 746d 6c5f 7374 7269  ort to_html_stri
+000001b0: 6e67 0a0a 2020 2020 6973 5f6e 6577 203d  ng..    is_new =
+000001c0: 2072 6570 6f73 6974 6f72 792e 6973 5f6e   repository.is_n
+000001d0: 6577 2829 0a20 2020 2069 735f 6465 7072  ew().    is_depr
+000001e0: 6563 6174 6564 203d 2072 6570 6f73 6974  ecated = reposit
+000001f0: 6f72 792e 6465 7072 6563 6174 6564 0a0a  ory.deprecated..
+00000200: 2020 2020 6361 6e5f 6272 6f77 7365 5f63      can_browse_c
+00000210: 6f6e 7465 6e74 7320 3d20 7472 616e 732e  ontents = trans.
+00000220: 7765 6261 7070 2e6e 616d 6520 3d3d 2027  webapp.name == '
+00000230: 746f 6f6c 5f73 6865 6427 2061 6e64 206e  tool_shed' and n
+00000240: 6f74 2069 735f 6e65 770a 2020 2020 6361  ot is_new.    ca
+00000250: 6e5f 7075 7368 203d 206e 6f74 2069 735f  n_push = not is_
+00000260: 6465 7072 6563 6174 6564 2061 6e64 2074  deprecated and t
+00000270: 7261 6e73 2e61 7070 2e73 6563 7572 6974  rans.app.securit
+00000280: 795f 6167 656e 742e 6361 6e5f 7075 7368  y_agent.can_push
+00000290: 2820 7472 616e 732e 6170 702c 2074 7261  ( trans.app, tra
+000002a0: 6e73 2e75 7365 722c 2072 6570 6f73 6974  ns.user, reposit
+000002b0: 6f72 7920 290a 2020 2020 6361 6e5f 646f  ory ).    can_do
+000002c0: 776e 6c6f 6164 203d 206e 6f74 2069 735f  wnload = not is_
+000002d0: 6465 7072 6563 6174 6564 2061 6e64 206e  deprecated and n
+000002e0: 6f74 2069 735f 6e65 7720 616e 6420 2820  ot is_new and ( 
+000002f0: 6e6f 7420 6973 5f6d 616c 6963 696f 7573  not is_malicious
+00000300: 206f 7220 6361 6e5f 7075 7368 2029 0a20   or can_push ). 
+00000310: 2020 2063 616e 5f76 6965 775f 6368 616e     can_view_chan
+00000320: 6765 5f6c 6f67 203d 2074 7261 6e73 2e77  ge_log = trans.w
+00000330: 6562 6170 702e 6e61 6d65 203d 3d20 2774  ebapp.name == 't
+00000340: 6f6f 6c5f 7368 6564 2720 616e 6420 6e6f  ool_shed' and no
+00000350: 7420 6973 5f6e 6577 0a20 2020 2063 6861  t is_new.    cha
+00000360: 6e67 6573 6574 5f72 6576 6973 696f 6e5f  ngeset_revision_
+00000370: 6973 5f72 6570 6f73 6974 6f72 795f 7469  is_repository_ti
+00000380: 7020 3d20 6368 616e 6765 7365 745f 7265  p = changeset_re
+00000390: 7669 7369 6f6e 203d 3d20 7265 706f 7369  vision == reposi
+000003a0: 746f 7279 2e74 6970 2829 0a0a 2020 2020  tory.tip()..    
+000003b0: 6966 2063 6861 6e67 6573 6574 5f72 6576  if changeset_rev
+000003c0: 6973 696f 6e5f 6973 5f72 6570 6f73 6974  ision_is_reposit
+000003d0: 6f72 795f 7469 703a 0a20 2020 2020 2020  ory_tip:.       
+000003e0: 2074 6970 5f73 7472 203d 2027 7265 706f   tip_str = 'repo
+000003f0: 7369 746f 7279 2074 6970 270a 2020 2020  sitory tip'.    
+00000400: 2020 2020 7368 6172 6162 6c65 5f6c 696e      sharable_lin
+00000410: 6b5f 6c61 6265 6c20 3d20 274c 696e 6b20  k_label = 'Link 
+00000420: 746f 2074 6869 7320 7265 706f 7369 746f  to this reposito
+00000430: 7279 3a27 0a20 2020 2020 2020 2073 6861  ry:'.        sha
+00000440: 7261 626c 655f 6c69 6e6b 5f63 6861 6e67  rable_link_chang
+00000450: 6573 6574 5f72 6576 6973 696f 6e20 3d20  eset_revision = 
+00000460: 4e6f 6e65 0a20 2020 2065 6c73 653a 0a20  None.    else:. 
+00000470: 2020 2020 2020 2074 6970 5f73 7472 203d         tip_str =
+00000480: 2027 270a 2020 2020 2020 2020 7368 6172   ''.        shar
+00000490: 6162 6c65 5f6c 696e 6b5f 6c61 6265 6c20  able_link_label 
+000004a0: 3d20 274c 696e 6b20 746f 2074 6869 7320  = 'Link to this 
+000004b0: 7265 706f 7369 746f 7279 2072 6576 6973  repository revis
+000004c0: 696f 6e3a 270a 2020 2020 2020 2020 7368  ion:'.        sh
+000004d0: 6172 6162 6c65 5f6c 696e 6b5f 6368 616e  arable_link_chan
+000004e0: 6765 7365 745f 7265 7669 7369 6f6e 203d  geset_revision =
+000004f0: 2063 6861 6e67 6573 6574 5f72 6576 6973   changeset_revis
+00000500: 696f 6e0a 0a20 2020 2069 6620 6865 6164  ion..    if head
+00000510: 733a 0a20 2020 2020 2020 206d 756c 7469  s:.        multi
+00000520: 706c 655f 6865 6164 7320 3d20 6c65 6e28  ple_heads = len(
+00000530: 2068 6561 6473 2029 203e 2031 0a20 2020   heads ) > 1.   
+00000540: 2065 6c73 653a 0a20 2020 2020 2020 206d   else:.        m
+00000550: 756c 7469 706c 655f 6865 6164 7320 3d20  ultiple_heads = 
+00000560: 4661 6c73 650a 0a20 2020 2069 6620 7265  False..    if re
+00000570: 706f 7369 746f 7279 5f6d 6574 6164 6174  pository_metadat
+00000580: 6120 6973 204e 6f6e 653a 0a20 2020 2020  a is None:.     
+00000590: 2020 2072 6576 6973 696f 6e5f 696e 7374     revision_inst
+000005a0: 616c 6c61 626c 6520 3d20 4661 6c73 650a  allable = False.
+000005b0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+000005c0: 2020 6966 2072 6570 6f73 6974 6f72 795f    if repository_
+000005d0: 6d65 7461 6461 7461 2e64 6f77 6e6c 6f61  metadata.downloa
+000005e0: 6461 626c 6520 6973 204e 6f6e 653a 0a20  dable is None:. 
+000005f0: 2020 2020 2020 2020 2020 2072 6576 6973             revis
+00000600: 696f 6e5f 696e 7374 616c 6c61 626c 6520  ion_installable 
+00000610: 3d20 2775 6e6b 6e6f 776e 270a 2020 2020  = 'unknown'.    
+00000620: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00000630: 2020 2020 2020 7265 7669 7369 6f6e 5f69        revision_i
+00000640: 6e73 7461 6c6c 6162 6c65 203d 2072 6570  nstallable = rep
+00000650: 6f73 6974 6f72 795f 6d65 7461 6461 7461  ository_metadata
+00000660: 2e64 6f77 6e6c 6f61 6461 626c 650a 253e  .downloadable.%>
+00000670: 0a0a 3c25 210a 2020 2064 6566 2069 6e68  ..<%!.   def inh
+00000680: 6572 6974 2863 6f6e 7465 7874 293a 0a20  erit(context):. 
+00000690: 2020 2020 2020 6966 2063 6f6e 7465 7874        if context
+000006a0: 2e67 6574 2827 7573 655f 7061 6e65 6c73  .get('use_panels
+000006b0: 2729 3a0a 2020 2020 2020 2020 2020 2072  '):.           r
+000006c0: 6574 7572 6e20 272f 7765 6261 7070 732f  eturn '/webapps/
+000006d0: 746f 6f6c 5f73 6865 642f 6261 7365 5f70  tool_shed/base_p
+000006e0: 616e 656c 732e 6d61 6b6f 270a 2020 2020  anels.mako'.    
+000006f0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
+00000700: 2020 2020 7265 7475 726e 2027 2f62 6173      return '/bas
+00000710: 652e 6d61 6b6f 270a 253e 0a3c 2569 6e68  e.mako'.%>.<%inh
+00000720: 6572 6974 2066 696c 653d 2224 7b69 6e68  erit file="${inh
+00000730: 6572 6974 2863 6f6e 7465 7874 297d 222f  erit(context)}"/
+00000740: 3e0a 0a3c 2564 6566 206e 616d 653d 2273  >..<%def name="s
+00000750: 7479 6c65 7368 6565 7473 2829 223e 0a20  tylesheets()">. 
+00000760: 2020 2024 7b70 6172 656e 742e 7374 796c     ${parent.styl
+00000770: 6573 6865 6574 7328 297d 0a20 2020 2024  esheets()}.    $
+00000780: 7b68 2e63 7373 2827 6261 7365 272c 276c  {h.css('base','l
+00000790: 6962 7261 7279 2729 7d0a 3c2f 2564 6566  ibrary')}.</%def
+000007a0: 3e0a 0a3c 2564 6566 206e 616d 653d 226a  >..<%def name="j
+000007b0: 6176 6173 6372 6970 7473 2829 223e 0a20  avascripts()">. 
+000007c0: 2020 2024 7b70 6172 656e 742e 6a61 7661     ${parent.java
+000007d0: 7363 7269 7074 7328 297d 0a20 2020 2024  scripts()}.    $
+000007e0: 7b63 6f6e 7461 696e 6572 5f6a 6176 6173  {container_javas
+000007f0: 6372 6970 7473 2829 7d0a 3c2f 2564 6566  cripts()}.</%def
+00000800: 3e0a 0a25 6966 2074 7261 6e73 2e77 6562  >..%if trans.web
+00000810: 6170 702e 6e61 6d65 203d 3d20 2774 6f6f  app.name == 'too
+00000820: 6c5f 7368 6564 273a 0a20 2020 2024 7b72  l_shed':.    ${r
+00000830: 656e 6465 725f 746f 6f6c 5f73 6865 645f  ender_tool_shed_
+00000840: 7265 706f 7369 746f 7279 5f61 6374 696f  repository_actio
+00000850: 6e73 2820 7265 706f 7369 746f 7279 3d72  ns( repository=r
+00000860: 6570 6f73 6974 6f72 792c 206d 6574 6164  epository, metad
+00000870: 6174 613d 6d65 7461 6461 7461 2c20 6368  ata=metadata, ch
+00000880: 616e 6765 7365 745f 7265 7669 7369 6f6e  angeset_revision
+00000890: 3d63 6861 6e67 6573 6574 5f72 6576 6973  =changeset_revis
+000008a0: 696f 6e20 297d 0a25 656c 7365 3a0a 2020  ion )}.%else:.  
+000008b0: 2020 247b 7265 6e64 6572 5f67 616c 6178    ${render_galax
+000008c0: 795f 7265 706f 7369 746f 7279 5f61 6374  y_repository_act
+000008d0: 696f 6e73 2820 7265 706f 7369 746f 7279  ions( repository
+000008e0: 3d72 6570 6f73 6974 6f72 7920 297d 0a25  =repository )}.%
+000008f0: 656e 6469 660a 0a25 6966 206d 6573 7361  endif..%if messa
+00000900: 6765 3a0a 2020 2020 247b 7265 6e64 6572  ge:.    ${render
+00000910: 5f6d 7367 2820 6d65 7373 6167 652c 2073  _msg( message, s
+00000920: 7461 7475 7320 297d 0a25 656e 6469 660a  tatus )}.%endif.
+00000930: 0a25 6966 2072 6570 6f73 6974 6f72 792e  .%if repository.
+00000940: 6465 7072 6563 6174 6564 3a0a 2020 2020  deprecated:.    
+00000950: 3c64 6976 2063 6c61 7373 3d22 7761 726e  <div class="warn
+00000960: 696e 676d 6573 7361 6765 223e 0a20 2020  ingmessage">.   
+00000970: 2020 2020 2054 6869 7320 7265 706f 7369       This reposi
+00000980: 746f 7279 2068 6173 2062 6565 6e20 6d61  tory has been ma
+00000990: 726b 6564 2061 7320 6465 7072 6563 6174  rked as deprecat
+000009a0: 6564 2c20 736f 2073 6f6d 6520 746f 6f6c  ed, so some tool
+000009b0: 2073 6865 6420 6665 6174 7572 6573 206d   shed features m
+000009c0: 6179 2062 6520 7265 7374 7269 6374 6564  ay be restricted
+000009d0: 2e0a 2020 2020 3c2f 6469 763e 0a25 656e  ..    </div>.%en
+000009e0: 6469 663a 0a25 6966 206d 756c 7469 706c  dif:.%if multipl
+000009f0: 655f 6865 6164 733a 0a20 2020 2024 7b72  e_heads:.    ${r
+00000a00: 656e 6465 725f 6d75 6c74 6970 6c65 5f68  ender_multiple_h
+00000a10: 6561 6473 5f6d 6573 7361 6765 2820 6865  eads_message( he
+00000a20: 6164 7320 297d 0a25 656e 6469 660a 2569  ads )}.%endif.%i
+00000a30: 6620 6465 7072 6563 6174 6564 5f72 6570  f deprecated_rep
+00000a40: 6f73 6974 6f72 795f 6465 7065 6e64 656e  ository_dependen
+00000a50: 6379 5f74 7570 733a 0a20 2020 2024 7b72  cy_tups:.    ${r
+00000a60: 656e 6465 725f 6465 7072 6563 6174 6564  ender_deprecated
+00000a70: 5f72 6570 6f73 6974 6f72 795f 6465 7065  _repository_depe
+00000a80: 6e64 656e 6369 6573 5f6d 6573 7361 6765  ndencies_message
+00000a90: 2820 6465 7072 6563 6174 6564 5f72 6570  ( deprecated_rep
+00000aa0: 6f73 6974 6f72 795f 6465 7065 6e64 656e  ository_dependen
+00000ab0: 6379 5f74 7570 7320 297d 0a25 656e 6469  cy_tups )}.%endi
+00000ac0: 660a 0a25 6966 206c 656e 2820 6368 616e  f..%if len( chan
+00000ad0: 6765 7365 745f 7265 7669 7369 6f6e 5f73  geset_revision_s
+00000ae0: 656c 6563 745f 6669 656c 642e 6f70 7469  elect_field.opti
+00000af0: 6f6e 7320 2920 3e20 313a 0a20 2020 203c  ons ) > 1:.    <
+00000b00: 6469 7620 636c 6173 733d 2274 6f6f 6c46  div class="toolF
+00000b10: 6f72 6d22 3e0a 2020 2020 2020 2020 3c64  orm">.        <d
+00000b20: 6976 2063 6c61 7373 3d22 746f 6f6c 466f  iv class="toolFo
+00000b30: 726d 5469 746c 6522 3e52 6570 6f73 6974  rmTitle">Reposit
+00000b40: 6f72 7920 7265 7669 7369 6f6e 3c2f 6469  ory revision</di
+00000b50: 763e 0a20 2020 2020 2020 203c 6469 7620  v>.        <div 
+00000b60: 636c 6173 733d 2274 6f6f 6c46 6f72 6d42  class="toolFormB
+00000b70: 6f64 7922 3e0a 2020 2020 2020 2020 2020  ody">.          
+00000b80: 2020 3c66 6f72 6d20 6e61 6d65 3d22 6368    <form name="ch
+00000b90: 616e 6765 5f72 6576 6973 696f 6e22 2069  ange_revision" i
+00000ba0: 643d 2263 6861 6e67 655f 7265 7669 7369  d="change_revisi
+00000bb0: 6f6e 2220 6163 7469 6f6e 3d22 247b 682e  on" action="${h.
+00000bc0: 7572 6c5f 666f 7228 2063 6f6e 7472 6f6c  url_for( control
+00000bd0: 6c65 723d 2772 6570 6f73 6974 6f72 7927  ler='repository'
+00000be0: 2c20 6163 7469 6f6e 3d27 7669 6577 5f72  , action='view_r
+00000bf0: 6570 6f73 6974 6f72 7927 2c20 6964 3d74  epository', id=t
+00000c00: 7261 6e73 2e73 6563 7572 6974 792e 656e  rans.security.en
+00000c10: 636f 6465 5f69 6428 2072 6570 6f73 6974  code_id( reposit
+00000c20: 6f72 792e 6964 2029 2029 7d22 206d 6574  ory.id ) )}" met
+00000c30: 686f 643d 2270 6f73 7422 203e 0a20 2020  hod="post" >.   
+00000c40: 2020 2020 2020 2020 2020 2020 203c 6469               <di
+00000c50: 7620 636c 6173 733d 2266 6f72 6d2d 726f  v class="form-ro
+00000c60: 7722 3e0a 2020 2020 2020 2020 2020 2020  w">.            
+00000c70: 2020 2020 2020 2020 247b 7265 6e64 6572          ${render
+00000c80: 5f73 656c 6563 7428 6368 616e 6765 7365  _select(changese
+00000c90: 745f 7265 7669 7369 6f6e 5f73 656c 6563  t_revision_selec
+00000ca0: 745f 6669 656c 6429 7d20 3c69 3e24 7b74  t_field)} <i>${t
+00000cb0: 6970 5f73 7472 7d3c 2f69 3e0a 2020 2020  ip_str}</i>.    
+00000cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000cd0: 3c64 6976 2063 6c61 7373 3d22 746f 6f6c  <div class="tool
+00000ce0: 5061 7261 6d48 656c 7022 2073 7479 6c65  ParamHelp" style
+00000cf0: 3d22 636c 6561 723a 2062 6f74 683b 223e  ="clear: both;">
+00000d00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000d10: 2020 2020 2020 2020 2053 656c 6563 7420           Select 
+00000d20: 6120 7265 7669 7369 6f6e 2074 6f20 696e  a revision to in
+00000d30: 7370 6563 7420 616e 6420 646f 776e 6c6f  spect and downlo
+00000d40: 6164 2076 6572 7369 6f6e 7320 6f66 2047  ad versions of G
+00000d50: 616c 6178 7920 7574 696c 6974 6965 7320  alaxy utilities 
+00000d60: 6672 6f6d 2074 6869 7320 7265 706f 7369  from this reposi
+00000d70: 746f 7279 2e0a 2020 2020 2020 2020 2020  tory..          
+00000d80: 2020 2020 2020 2020 2020 3c2f 6469 763e            </div>
+00000d90: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000da0: 203c 2f64 6976 3e0a 2020 2020 2020 2020   </div>.        
+00000db0: 2020 2020 3c2f 666f 726d 3e0a 2020 2020      </form>.    
+00000dc0: 2020 2020 3c2f 6469 763e 0a20 2020 203c      </div>.    <
+00000dd0: 2f64 6976 3e0a 2020 2020 3c70 2f3e 0a25  /div>.    <p/>.%
+00000de0: 656e 6469 660a 3c64 6976 2063 6c61 7373  endif.<div class
+00000df0: 3d22 746f 6f6c 466f 726d 223e 0a20 2020  ="toolForm">.   
+00000e00: 203c 6469 7620 636c 6173 733d 2274 6f6f   <div class="too
+00000e10: 6c46 6f72 6d54 6974 6c65 223e 5265 706f  lFormTitle">Repo
+00000e20: 7369 746f 7279 203c 623e 247b 7265 706f  sitory <b>${repo
+00000e30: 7369 746f 7279 2e6e 616d 6520 7c20 687d  sitory.name | h}
+00000e40: 3c2f 623e 3c2f 6469 763e 0a20 2020 203c  </b></div>.    <
+00000e50: 6469 7620 636c 6173 733d 2274 6f6f 6c46  div class="toolF
+00000e60: 6f72 6d42 6f64 7922 3e0a 2020 2020 2020  ormBody">.      
+00000e70: 2020 3c64 6976 2063 6c61 7373 3d22 666f    <div class="fo
+00000e80: 726d 2d72 6f77 223e 0a20 2020 2020 2020  rm-row">.       
+00000e90: 2020 2020 203c 623e 4e61 6d65 3a3c 2f62       <b>Name:</b
+00000ea0: 3e5c 0a20 2020 2020 2020 2020 2020 2025  >\.            %
+00000eb0: 6966 2063 616e 5f62 726f 7773 655f 636f  if can_browse_co
+00000ec0: 6e74 656e 7473 3a0a 2020 2020 2020 2020  ntents:.        
+00000ed0: 2020 2020 2020 2020 3c61 2074 6974 6c65          <a title
+00000ee0: 3d22 4272 6f77 7365 2074 6865 2063 6f6e  ="Browse the con
+00000ef0: 7465 6e74 7320 6f66 2074 6869 7320 7265  tents of this re
+00000f00: 706f 7369 746f 7279 2220 6872 6566 3d22  pository" href="
+00000f10: 247b 682e 7572 6c5f 666f 7228 2063 6f6e  ${h.url_for( con
+00000f20: 7472 6f6c 6c65 723d 2772 6570 6f73 6974  troller='reposit
+00000f30: 6f72 7927 2c20 6163 7469 6f6e 3d27 6272  ory', action='br
+00000f40: 6f77 7365 5f72 6570 6f73 6974 6f72 7927  owse_repository'
+00000f50: 2c20 6964 3d74 7261 6e73 2e61 7070 2e73  , id=trans.app.s
+00000f60: 6563 7572 6974 792e 656e 636f 6465 5f69  ecurity.encode_i
+00000f70: 6428 2072 6570 6f73 6974 6f72 792e 6964  d( repository.id
+00000f80: 2029 2029 7d22 3e24 7b72 6570 6f73 6974   ) )}">${reposit
+00000f90: 6f72 792e 6e61 6d65 7d3c 2f61 3e0a 2020  ory.name}</a>.  
+00000fa0: 2020 2020 2020 2020 2020 2565 6c73 653a            %else:
+00000fb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000fc0: 2024 7b72 6570 6f73 6974 6f72 792e 6e61   ${repository.na
+00000fd0: 6d65 207c 2068 7d0a 2020 2020 2020 2020  me | h}.        
+00000fe0: 2020 2020 2565 6e64 6966 0a20 2020 2020      %endif.     
+00000ff0: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
+00001000: 2020 3c64 6976 2063 6c61 7373 3d22 666f    <div class="fo
+00001010: 726d 2d72 6f77 223e 0a20 2020 2020 2020  rm-row">.       
+00001020: 2020 2020 203c 623e 4f77 6e65 723a 3c2f       <b>Owner:</
+00001030: 623e 0a20 2020 2020 2020 2020 2020 203c  b>.            <
+00001040: 6120 7469 746c 653d 2253 6565 2061 6c6c  a title="See all
+00001050: 2072 6570 6f73 6974 6f72 6965 7320 6f77   repositories ow
+00001060: 6e65 6420 6279 2024 7b20 7265 706f 7369  ned by ${ reposi
+00001070: 746f 7279 2e75 7365 722e 7573 6572 6e61  tory.user.userna
+00001080: 6d65 207c 2068 207d 2220 6872 6566 3d22  me | h }" href="
+00001090: 247b 682e 7572 6c5f 666f 7228 2063 6f6e  ${h.url_for( con
+000010a0: 7472 6f6c 6c65 723d 2772 6570 6f73 6974  troller='reposit
+000010b0: 6f72 7927 2c20 6163 7469 6f6e 3d27 6272  ory', action='br
+000010c0: 6f77 7365 5f72 6570 6f73 6974 6f72 6965  owse_repositorie
+000010d0: 735f 6279 5f75 7365 7227 2c20 7573 6572  s_by_user', user
+000010e0: 5f69 643d 7472 616e 732e 6170 702e 7365  _id=trans.app.se
+000010f0: 6375 7269 7479 2e65 6e63 6f64 655f 6964  curity.encode_id
+00001100: 2820 7265 706f 7369 746f 7279 2e75 7365  ( repository.use
+00001110: 722e 6964 2029 2029 7d22 3e24 7b20 7265  r.id ) )}">${ re
+00001120: 706f 7369 746f 7279 2e75 7365 722e 7573  pository.user.us
+00001130: 6572 6e61 6d65 207c 2068 207d 3c2f 613e  ername | h }</a>
+00001140: 0a20 2020 2020 2020 203c 2f64 6976 3e0a  .        </div>.
+00001150: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
+00001160: 7373 3d22 666f 726d 2d72 6f77 223e 0a20  ss="form-row">. 
+00001170: 2020 2020 2020 2020 2020 203c 623e 5379             <b>Sy
+00001180: 6e6f 7073 6973 3a3c 2f62 3e0a 2020 2020  nopsis:</b>.    
+00001190: 2020 2020 2020 2020 247b 7265 706f 7369          ${reposi
+000011a0: 746f 7279 2e64 6573 6372 6970 7469 6f6e  tory.description
+000011b0: 207c 2068 7d0a 2020 2020 2020 2020 3c2f   | h}.        </
+000011c0: 6469 763e 0a20 2020 2020 2020 2025 6966  div>.        %if
+000011d0: 2072 6570 6f73 6974 6f72 792e 6c6f 6e67   repository.long
+000011e0: 5f64 6573 6372 6970 7469 6f6e 3a0a 2020  _description:.  
+000011f0: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
+00001200: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
+00001210: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001220: 2024 7b72 656e 6465 725f 6c6f 6e67 5f64   ${render_long_d
+00001230: 6573 6372 6970 7469 6f6e 2820 746f 5f68  escription( to_h
+00001240: 746d 6c5f 7374 7269 6e67 2820 7265 706f  tml_string( repo
+00001250: 7369 746f 7279 2e6c 6f6e 675f 6465 7363  sitory.long_desc
+00001260: 7269 7074 696f 6e20 2920 297d 0a20 2020  ription ) )}.   
+00001270: 2020 2020 2020 2020 203c 2f64 6976 3e0a           </div>.
+00001280: 2020 2020 2020 2020 2565 6e64 6966 0a20          %endif. 
+00001290: 2020 2020 2020 2025 6966 2072 6570 6f73         %if repos
+000012a0: 6974 6f72 792e 686f 6d65 7061 6765 5f75  itory.homepage_u
+000012b0: 726c 3a0a 2020 2020 2020 2020 3c64 6976  rl:.        <div
+000012c0: 2063 6c61 7373 3d22 666f 726d 2d72 6f77   class="form-row
+000012d0: 223e 0a20 2020 2020 2020 2020 2020 203c  ">.            <
+000012e0: 623e 436f 6e74 656e 7420 686f 6d65 7061  b>Content homepa
+000012f0: 6765 3a3c 2f62 3e0a 2020 2020 2020 2020  ge:</b>.        
+00001300: 2020 2020 3c61 2068 7265 663d 2224 7b72      <a href="${r
+00001310: 6570 6f73 6974 6f72 792e 686f 6d65 7061  epository.homepa
+00001320: 6765 5f75 726c 207c 2068 7d22 2074 6172  ge_url | h}" tar
+00001330: 6765 743d 225f 626c 616e 6b22 3e24 7b72  get="_blank">${r
+00001340: 6570 6f73 6974 6f72 792e 686f 6d65 7061  epository.homepa
+00001350: 6765 5f75 726c 207c 2068 7d3c 2f61 3e0a  ge_url | h}</a>.
+00001360: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
+00001370: 2020 2020 2020 2025 656e 6469 660a 2020         %endif.  
+00001380: 2020 2020 2020 2569 6620 7265 706f 7369        %if reposi
+00001390: 746f 7279 2e72 656d 6f74 655f 7265 706f  tory.remote_repo
+000013a0: 7369 746f 7279 5f75 726c 3a0a 2020 2020  sitory_url:.    
+000013b0: 2020 2020 3c64 6976 2063 6c61 7373 3d22      <div class="
+000013c0: 666f 726d 2d72 6f77 223e 0a20 2020 2020  form-row">.     
+000013d0: 2020 2020 2020 203c 623e 4465 7665 6c6f         <b>Develo
+000013e0: 706d 656e 7420 7265 706f 7369 746f 7279  pment repository
+000013f0: 3a3c 2f62 3e0a 2020 2020 2020 2020 2020  :</b>.          
+00001400: 2020 3c61 2068 7265 663d 2224 7b72 6570    <a href="${rep
+00001410: 6f73 6974 6f72 792e 7265 6d6f 7465 5f72  ository.remote_r
+00001420: 6570 6f73 6974 6f72 795f 7572 6c20 7c20  epository_url | 
+00001430: 687d 2220 7461 7267 6574 3d22 5f62 6c61  h}" target="_bla
+00001440: 6e6b 223e 247b 7265 706f 7369 746f 7279  nk">${repository
+00001450: 2e72 656d 6f74 655f 7265 706f 7369 746f  .remote_reposito
+00001460: 7279 5f75 726c 207c 2068 7d3c 2f61 3e0a  ry_url | h}</a>.
+00001470: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
+00001480: 2020 2020 2020 2025 656e 6469 660a 2020         %endif.  
+00001490: 2020 2020 2020 3c64 6976 2063 6c61 7373        <div class
+000014a0: 3d22 666f 726d 2d72 6f77 223e 0a20 2020  ="form-row">.   
+000014b0: 2020 2020 2020 2020 203c 623e 247b 7368           <b>${sh
+000014c0: 6172 6162 6c65 5f6c 696e 6b5f 6c61 6265  arable_link_labe
+000014d0: 6c7d 3c2f 623e 0a20 2020 2020 2020 2020  l}</b>.         
+000014e0: 2020 203c 6120 6872 6566 3d22 247b 2072     <a href="${ r
+000014f0: 6570 6f73 6974 6f72 792e 7368 6172 655f  epository.share_
+00001500: 7572 6c20 7d22 2074 6172 6765 743d 225f  url }" target="_
+00001510: 626c 616e 6b22 3e24 7b20 7265 706f 7369  blank">${ reposi
+00001520: 746f 7279 2e73 6861 7265 5f75 726c 207d  tory.share_url }
+00001530: 3c2f 613e 0a20 2020 2020 2020 2020 2020  </a>.           
+00001540: 203c 6275 7474 6f6e 2074 6974 6c65 3d22   <button title="
+00001550: 746f 2063 6c69 7062 6f61 7264 2220 636c  to clipboard" cl
+00001560: 6173 733d 2262 746e 2062 746e 2d73 6563  ass="btn btn-sec
+00001570: 6f6e 6461 7279 2062 746e 2d73 6d22 2069  ondary btn-sm" i
+00001580: 643d 2273 6861 7265 5f63 6c69 7062 6f61  d="share_clipboa
+00001590: 7264 223e 3c73 7061 6e20 636c 6173 733d  rd"><span class=
+000015a0: 2266 6120 6661 2d63 6c69 7062 6f61 7264  "fa fa-clipboard
+000015b0: 223e 3c2f 7370 616e 3e3c 2f62 7574 746f  "></span></butto
+000015c0: 6e3e 0a20 2020 2020 2020 203c 2f64 6976  n>.        </div
+000015d0: 3e0a 2020 2020 2020 2020 2569 6620 6361  >.        %if ca
+000015e0: 6e5f 646f 776e 6c6f 6164 206f 7220 6361  n_download or ca
+000015f0: 6e5f 7075 7368 3a0a 2020 2020 2020 2020  n_push:.        
+00001600: 2020 2020 3c64 6976 2063 6c61 7373 3d22      <div class="
+00001610: 666f 726d 2d72 6f77 223e 0a20 2020 2020  form-row">.     
+00001620: 2020 2020 2020 2020 2020 203c 623e 436c             <b>Cl
+00001630: 6f6e 6520 7468 6973 2072 6570 6f73 6974  one this reposit
+00001640: 6f72 793a 3c2f 623e 0a20 2020 2020 2020  ory:</b>.       
+00001650: 2020 2020 2020 2020 203c 636f 6465 3e68           <code>h
+00001660: 6720 636c 6f6e 6520 3c61 2074 6974 6c65  g clone <a title
+00001670: 3d22 5368 6f77 2069 6e20 6d65 7263 7572  ="Show in mercur
+00001680: 6961 6c20 6272 6f77 7365 7222 2068 7265  ial browser" hre
+00001690: 663d 2224 7b20 7265 706f 7369 746f 7279  f="${ repository
+000016a0: 2e63 6c6f 6e65 5f75 726c 207d 223e 247b  .clone_url }">${
+000016b0: 2072 6570 6f73 6974 6f72 792e 636c 6f6e   repository.clon
+000016c0: 655f 7572 6c20 7d3c 2f61 3e3c 2f63 6f64  e_url }</a></cod
+000016d0: 653e 0a20 2020 2020 2020 2020 2020 2020  e>.             
+000016e0: 2020 203c 6275 7474 6f6e 2074 6974 6c65     <button title
+000016f0: 3d22 746f 2063 6c69 7062 6f61 7264 2220  ="to clipboard" 
+00001700: 636c 6173 733d 2262 746e 2062 746e 2d73  class="btn btn-s
+00001710: 6563 6f6e 6461 7279 2062 746e 2d73 6d22  econdary btn-sm"
+00001720: 2069 643d 2263 6c6f 6e65 5f63 6c69 7062   id="clone_clipb
+00001730: 6f61 7264 223e 3c73 7061 6e20 636c 6173  oard"><span clas
+00001740: 733d 2266 6120 6661 2d63 6c69 7062 6f61  s="fa fa-clipboa
+00001750: 7264 223e 3c2f 7370 616e 3e3c 2f62 7574  rd"></span></but
+00001760: 746f 6e3e 0a20 2020 2020 2020 2020 2020  ton>.           
+00001770: 203c 2f64 6976 3e0a 2020 2020 2020 2020   </div>.        
+00001780: 2565 6e64 6966 0a20 2020 2020 2020 203c  %endif.        <
+00001790: 6469 7620 636c 6173 733d 2266 6f72 6d2d  div class="form-
+000017a0: 726f 7722 3e0a 2020 2020 2020 2020 2020  row">.          
+000017b0: 2020 3c62 3e54 7970 653a 3c2f 623e 0a20    <b>Type:</b>. 
+000017c0: 2020 2020 2020 2020 2020 2024 7b72 6570             ${rep
+000017d0: 6f73 6974 6f72 792e 7479 7065 207c 2068  ository.type | h
+000017e0: 7d0a 2020 2020 2020 2020 2020 2020 3c64  }.            <d
+000017f0: 6976 2073 7479 6c65 3d22 636c 6561 723a  iv style="clear:
+00001800: 2062 6f74 6822 3e3c 2f64 6976 3e0a 2020   both"></div>.  
+00001810: 2020 2020 2020 3c2f 6469 763e 0a20 2020        </div>.   
+00001820: 2020 2020 203c 6469 7620 636c 6173 733d       <div class=
+00001830: 2266 6f72 6d2d 726f 7722 3e0a 2020 2020  "form-row">.    
+00001840: 2020 2020 2020 2020 3c62 3e52 6576 6973          <b>Revis
+00001850: 696f 6e3a 3c2f 623e 0a20 2020 2020 2020  ion:</b>.       
+00001860: 2020 2020 2025 6966 2063 616e 5f76 6965       %if can_vie
+00001870: 775f 6368 616e 6765 5f6c 6f67 3a0a 2020  w_change_log:.  
+00001880: 2020 2020 2020 2020 2020 2020 2020 3c61                <a
+00001890: 2074 6974 6c65 3d22 5365 6520 7468 6520   title="See the 
+000018a0: 7265 7669 7369 6f6e 2068 6973 746f 7279  revision history
+000018b0: 2220 6872 6566 3d22 247b 682e 7572 6c5f  " href="${h.url_
+000018c0: 666f 7228 2063 6f6e 7472 6f6c 6c65 723d  for( controller=
+000018d0: 2772 6570 6f73 6974 6f72 7927 2c20 6163  'repository', ac
+000018e0: 7469 6f6e 3d27 7669 6577 5f63 6861 6e67  tion='view_chang
+000018f0: 656c 6f67 272c 2069 643d 7472 616e 732e  elog', id=trans.
+00001900: 6170 702e 7365 6375 7269 7479 2e65 6e63  app.security.enc
+00001910: 6f64 655f 6964 2820 7265 706f 7369 746f  ode_id( reposito
+00001920: 7279 2e69 6420 2920 297d 223e 247b 7265  ry.id ) )}">${re
+00001930: 7669 7369 6f6e 5f6c 6162 656c 7d3c 2f61  vision_label}</a
+00001940: 3e0a 2020 2020 2020 2020 2020 2020 2565  >.            %e
+00001950: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00001960: 2020 2020 2024 7b72 6576 6973 696f 6e5f       ${revision_
+00001970: 6c61 6265 6c7d 0a20 2020 2020 2020 2020  label}.         
+00001980: 2020 2025 656e 6469 660a 2020 2020 2020     %endif.      
+00001990: 2020 3c2f 6469 763e 0a20 2020 2020 2020    </div>.       
+000019a0: 203c 6469 7620 636c 6173 733d 2266 6f72   <div class="for
+000019b0: 6d2d 726f 7722 3e0a 2020 2020 2020 2020  m-row">.        
+000019c0: 2020 2020 3c62 3e54 6869 7320 7265 7669      <b>This revi
+000019d0: 7369 6f6e 2063 616e 2062 6520 696e 7374  sion can be inst
+000019e0: 616c 6c65 643a 3c2f 623e 0a20 2020 2020  alled:</b>.     
+000019f0: 2020 2020 2020 2024 7b72 6576 6973 696f         ${revisio
+00001a00: 6e5f 696e 7374 616c 6c61 626c 657d 0a20  n_installable}. 
+00001a10: 2020 2020 2020 203c 2f64 6976 3e0a 2020         </div>.  
+00001a20: 2020 2020 2020 3c64 6976 2063 6c61 7373        <div class
+00001a30: 3d22 666f 726d 2d72 6f77 223e 0a20 2020  ="form-row">.   
+00001a40: 2020 2020 2020 2020 203c 623e 5469 6d65           <b>Time
+00001a50: 7320 636c 6f6e 6564 202f 2069 6e73 7461  s cloned / insta
+00001a60: 6c6c 6564 3a3c 2f62 3e0a 2020 2020 2020  lled:</b>.      
+00001a70: 2020 2020 2020 247b 7265 706f 7369 746f        ${reposito
+00001a80: 7279 2e74 696d 6573 5f64 6f77 6e6c 6f61  ry.times_downloa
+00001a90: 6465 647d 0a20 2020 2020 2020 203c 2f64  ded}.        </d
+00001aa0: 6976 3e0a 2020 2020 2020 2020 2569 6620  iv>.        %if 
+00001ab0: 7472 616e 732e 7573 6572 5f69 735f 6164  trans.user_is_ad
+00001ac0: 6d69 6e3a 0a20 2020 2020 2020 2020 2020  min:.           
+00001ad0: 203c 6469 7620 636c 6173 733d 2266 6f72   <div class="for
+00001ae0: 6d2d 726f 7722 3e0a 2020 2020 2020 2020  m-row">.        
+00001af0: 2020 2020 2020 2020 3c62 3e4c 6f63 6174          <b>Locat
+00001b00: 696f 6e3a 3c2f 623e 0a20 2020 2020 2020  ion:</b>.       
+00001b10: 2020 2020 2020 2020 2024 7b72 6570 6f73           ${repos
+00001b20: 6974 6f72 792e 7265 706f 5f70 6174 6828  itory.repo_path(
+00001b30: 2074 7261 6e73 2e61 7070 2029 207c 2068   trans.app ) | h
+00001b40: 7d0a 2020 2020 2020 2020 2020 2020 3c2f  }.            </
+00001b50: 6469 763e 0a20 2020 2020 2020 2020 2020  div>.           
+00001b60: 203c 6469 7620 636c 6173 733d 2266 6f72   <div class="for
+00001b70: 6d2d 726f 7722 3e0a 2020 2020 2020 2020  m-row">.        
+00001b80: 2020 2020 2020 2020 3c62 3e44 656c 6574          <b>Delet
+00001b90: 6564 3a3c 2f62 3e0a 2020 2020 2020 2020  ed:</b>.        
+00001ba0: 2020 2020 2020 2020 247b 7265 706f 7369          ${reposi
+00001bb0: 746f 7279 2e64 656c 6574 6564 7d0a 2020  tory.deleted}.  
+00001bc0: 2020 2020 2020 2020 2020 3c2f 6469 763e            </div>
+00001bd0: 0a20 2020 2020 2020 2025 656e 6469 660a  .        %endif.
+00001be0: 2020 2020 3c2f 6469 763e 0a3c 2f64 6976      </div>.</div
+00001bf0: 3e0a 247b 7265 6e64 6572 5f72 6570 6f73  >.${render_repos
+00001c00: 6974 6f72 795f 6974 656d 7328 206d 6574  itory_items( met
+00001c10: 6164 6174 612c 2063 6f6e 7461 696e 6572  adata, container
+00001c20: 735f 6469 6374 2c20 6361 6e5f 7365 745f  s_dict, can_set_
+00001c30: 6d65 7461 6461 7461 3d46 616c 7365 2c20  metadata=False, 
+00001c40: 7265 6e64 6572 5f72 6570 6f73 6974 6f72  render_repositor
+00001c50: 795f 6163 7469 6f6e 735f 666f 723d 2774  y_actions_for='t
+00001c60: 6f6f 6c5f 7368 6564 2720 297d 0a25 6966  ool_shed' )}.%if
+00001c70: 2072 6570 6f73 6974 6f72 792e 6361 7465   repository.cate
+00001c80: 676f 7269 6573 3a0a 2020 2020 3c70 2f3e  gories:.    <p/>
+00001c90: 0a20 2020 203c 6469 7620 636c 6173 733d  .    <div class=
+00001ca0: 2274 6f6f 6c46 6f72 6d22 3e0a 2020 2020  "toolForm">.    
+00001cb0: 2020 2020 3c64 6976 2063 6c61 7373 3d22      <div class="
+00001cc0: 746f 6f6c 466f 726d 5469 746c 6522 3e43  toolFormTitle">C
+00001cd0: 6174 6567 6f72 6965 733c 2f64 6976 3e0a  ategories</div>.
+00001ce0: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
+00001cf0: 7373 3d22 746f 6f6c 466f 726d 426f 6479  ss="toolFormBody
+00001d00: 223e 0a20 2020 2020 2020 2020 2020 2025  ">.            %
+00001d10: 666f 7220 7263 6120 696e 2072 6570 6f73  for rca in repos
+00001d20: 6974 6f72 792e 6361 7465 676f 7269 6573  itory.categories
+00001d30: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00001d40: 2020 3c64 6976 2063 6c61 7373 3d22 666f    <div class="fo
+00001d50: 726d 2d72 6f77 223e 0a20 2020 2020 2020  rm-row">.       
+00001d60: 2020 2020 2020 2020 2020 2020 203c 6120               <a 
+00001d70: 6872 6566 3d22 247b 682e 7572 6c5f 666f  href="${h.url_fo
+00001d80: 7228 2063 6f6e 7472 6f6c 6c65 723d 2772  r( controller='r
+00001d90: 6570 6f73 6974 6f72 7927 2c20 6163 7469  epository', acti
+00001da0: 6f6e 3d27 6272 6f77 7365 5f72 6570 6f73  on='browse_repos
+00001db0: 6974 6f72 6965 735f 696e 5f63 6174 6567  itories_in_categ
+00001dc0: 6f72 7927 2c20 6964 3d74 7261 6e73 2e73  ory', id=trans.s
+00001dd0: 6563 7572 6974 792e 656e 636f 6465 5f69  ecurity.encode_i
+00001de0: 6428 2072 6361 2e63 6174 6567 6f72 792e  d( rca.category.
+00001df0: 6964 2029 2029 7d22 3e24 7b72 6361 2e63  id ) )}">${rca.c
+00001e00: 6174 6567 6f72 792e 6e61 6d65 207c 2068  ategory.name | h
+00001e10: 7d3c 2f61 3e20 2d20 247b 7263 612e 6361  }</a> - ${rca.ca
+00001e20: 7465 676f 7279 2e64 6573 6372 6970 7469  tegory.descripti
+00001e30: 6f6e 207c 2068 7d0a 2020 2020 2020 2020  on | h}.        
+00001e40: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
+00001e50: 2020 2020 2020 2020 2020 2025 656e 6466             %endf
+00001e60: 6f72 0a20 2020 2020 2020 2020 2020 203c  or.            <
+00001e70: 6469 7620 7374 796c 653d 2263 6c65 6172  div style="clear
+00001e80: 3a20 626f 7468 223e 3c2f 6469 763e 0a20  : both"></div>. 
+00001e90: 2020 2020 2020 203c 2f64 6976 3e0a 2020         </div>.  
+00001ea0: 2020 3c2f 6469 763e 0a25 656e 6469 660a    </div>.%endif.
+00001eb0: 2569 6620 7472 616e 732e 7765 6261 7070  %if trans.webapp
+00001ec0: 2e6e 616d 6520 3d3d 2027 746f 6f6c 5f73  .name == 'tool_s
+00001ed0: 6865 6427 2061 6e64 2074 7261 6e73 2e75  hed' and trans.u
+00001ee0: 7365 7220 616e 6420 7472 616e 732e 6170  ser and trans.ap
+00001ef0: 702e 636f 6e66 6967 2e73 6d74 705f 7365  p.config.smtp_se
+00001f00: 7276 6572 3a0a 2020 2020 3c70 2f3e 0a20  rver:.    <p/>. 
+00001f10: 2020 203c 6469 7620 636c 6173 733d 2274     <div class="t
+00001f20: 6f6f 6c46 6f72 6d22 3e0a 2020 2020 2020  oolForm">.      
+00001f30: 2020 3c64 6976 2063 6c61 7373 3d22 746f    <div class="to
+00001f40: 6f6c 466f 726d 5469 746c 6522 3e4e 6f74  olFormTitle">Not
+00001f50: 6966 6963 6174 696f 6e20 6f6e 2075 7064  ification on upd
+00001f60: 6174 653c 2f64 6976 3e0a 2020 2020 2020  ate</div>.      
+00001f70: 2020 3c64 6976 2063 6c61 7373 3d22 746f    <div class="to
+00001f80: 6f6c 466f 726d 426f 6479 223e 0a20 2020  olFormBody">.   
+00001f90: 2020 2020 2020 2020 203c 666f 726d 206e           <form n
+00001fa0: 616d 653d 2272 6563 6569 7665 5f65 6d61  ame="receive_ema
+00001fb0: 696c 5f61 6c65 7274 7322 2069 643d 2272  il_alerts" id="r
+00001fc0: 6563 6569 7665 5f65 6d61 696c 5f61 6c65  eceive_email_ale
+00001fd0: 7274 7322 2061 6374 696f 6e3d 2224 7b68  rts" action="${h
+00001fe0: 2e75 726c 5f66 6f72 2820 636f 6e74 726f  .url_for( contro
+00001ff0: 6c6c 6572 3d27 7265 706f 7369 746f 7279  ller='repository
+00002000: 272c 2061 6374 696f 6e3d 2776 6965 775f  ', action='view_
+00002010: 7265 706f 7369 746f 7279 272c 2069 643d  repository', id=
+00002020: 7472 616e 732e 7365 6375 7269 7479 2e65  trans.security.e
+00002030: 6e63 6f64 655f 6964 2820 7265 706f 7369  ncode_id( reposi
+00002040: 746f 7279 2e69 6420 2920 297d 2220 6d65  tory.id ) )}" me
+00002050: 7468 6f64 3d22 706f 7374 2220 3e0a 2020  thod="post" >.  
+00002060: 2020 2020 2020 2020 2020 2020 2020 3c64                <d
+00002070: 6976 2063 6c61 7373 3d22 666f 726d 2d72  iv class="form-r
+00002080: 6f77 223e 0a20 2020 2020 2020 2020 2020  ow">.           
+00002090: 2020 2020 2020 2020 203c 6c61 6265 6c3e           <label>
+000020a0: 5265 6365 6976 6520 656d 6169 6c20 616c  Receive email al
+000020b0: 6572 7473 3a3c 2f6c 6162 656c 3e0a 2020  erts:</label>.  
+000020c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000020d0: 2020 247b 7265 6e64 6572 5f63 6865 636b    ${render_check
+000020e0: 626f 7828 616c 6572 7473 5f63 6865 636b  box(alerts_check
+000020f0: 5f62 6f78 297d 0a20 2020 2020 2020 2020  _box)}.         
+00002100: 2020 2020 2020 2020 2020 203c 6469 7620             <div 
+00002110: 636c 6173 733d 2274 6f6f 6c50 6172 616d  class="toolParam
+00002120: 4865 6c70 2220 7374 796c 653d 2263 6c65  Help" style="cle
+00002130: 6172 3a20 626f 7468 3b22 3e0a 2020 2020  ar: both;">.    
+00002140: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002150: 2020 2020 4368 6563 6b20 7468 6520 626f      Check the bo
+00002160: 7820 616e 6420 636c 6963 6b20 3c62 3e53  x and click <b>S
+00002170: 6176 653c 2f62 3e20 746f 2072 6563 6569  ave</b> to recei
+00002180: 7665 2065 6d61 696c 2061 6c65 7274 7320  ve email alerts 
+00002190: 7768 656e 2075 7064 6174 6573 2074 6f20  when updates to 
+000021a0: 7468 6973 2072 6570 6f73 6974 6f72 7920  this repository 
+000021b0: 6f63 6375 722e 0a20 2020 2020 2020 2020  occur..         
+000021c0: 2020 2020 2020 2020 2020 203c 2f64 6976             </div
+000021d0: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+000021e0: 2020 3c2f 6469 763e 0a20 2020 2020 2020    </div>.       
+000021f0: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
+00002200: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
+00002210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002220: 2020 2020 3c69 6e70 7574 2074 7970 653d      <input type=
+00002230: 2273 7562 6d69 7422 206e 616d 653d 2272  "submit" name="r
+00002240: 6563 6569 7665 5f65 6d61 696c 5f61 6c65  eceive_email_ale
+00002250: 7274 735f 6275 7474 6f6e 2220 7661 6c75  rts_button" valu
+00002260: 653d 2253 6176 6522 2f3e 0a20 2020 2020  e="Save"/>.     
+00002270: 2020 2020 2020 2020 2020 203c 2f64 6976             </div
+00002280: 3e0a 2020 2020 2020 2020 2020 2020 3c2f  >.            </
+00002290: 666f 726d 3e0a 2020 2020 2020 2020 3c2f  form>.        </
+000022a0: 6469 763e 0a20 2020 203c 2f64 6976 3e0a  div>.    </div>.
+000022b0: 2565 6e64 6966 0a25 6966 2072 6570 6f73  %endif.%if repos
+000022c0: 6974 6f72 792e 7261 7469 6e67 733a 0a20  itory.ratings:. 
+000022d0: 2020 203c 702f 3e0a 2020 2020 3c64 6976     <p/>.    <div
+000022e0: 2063 6c61 7373 3d22 746f 6f6c 466f 726d   class="toolForm
+000022f0: 223e 0a20 2020 2020 2020 203c 6469 7620  ">.        <div 
+00002300: 636c 6173 733d 2274 6f6f 6c46 6f72 6d54  class="toolFormT
+00002310: 6974 6c65 223e 5261 7469 6e67 3c2f 6469  itle">Rating</di
+00002320: 763e 0a20 2020 2020 2020 203c 6469 7620  v>.        <div 
+00002330: 636c 6173 733d 2274 6f6f 6c46 6f72 6d42  class="toolFormB
+00002340: 6f64 7922 3e0a 2020 2020 2020 2020 2020  ody">.          
+00002350: 2020 3c64 6976 2063 6c61 7373 3d22 666f    <div class="fo
+00002360: 726d 2d72 6f77 223e 0a20 2020 2020 2020  rm-row">.       
+00002370: 2020 2020 2020 2020 203c 6c61 6265 6c3e           <label>
+00002380: 5469 6d65 7320 5261 7465 643a 3c2f 6c61  Times Rated:</la
+00002390: 6265 6c3e 0a20 2020 2020 2020 2020 2020  bel>.           
+000023a0: 2020 2020 2024 7b6e 756d 5f72 6174 696e       ${num_ratin
+000023b0: 6773 7d0a 2020 2020 2020 2020 2020 2020  gs}.            
+000023c0: 2020 2020 3c64 6976 2073 7479 6c65 3d22      <div style="
+000023d0: 636c 6561 723a 2062 6f74 6822 3e3c 2f64  clear: both"></d
+000023e0: 6976 3e0a 2020 2020 2020 2020 2020 2020  iv>.            
+000023f0: 3c2f 6469 763e 0a20 2020 2020 2020 2020  </div>.         
+00002400: 2020 203c 6469 7620 636c 6173 733d 2266     <div class="f
+00002410: 6f72 6d2d 726f 7722 3e0a 2020 2020 2020  orm-row">.      
+00002420: 2020 2020 2020 2020 2020 3c6c 6162 656c            <label
+00002430: 3e41 7665 7261 6765 2052 6174 696e 673a  >Average Rating:
+00002440: 3c2f 6c61 6265 6c3e 0a20 2020 2020 2020  </label>.       
+00002450: 2020 2020 2020 2020 2024 7b72 656e 6465           ${rende
+00002460: 725f 7374 6172 5f72 6174 696e 6728 2027  r_star_rating( '
+00002470: 6176 675f 7261 7469 6e67 272c 2061 7667  avg_rating', avg
+00002480: 5f72 6174 696e 672c 2064 6973 6162 6c65  _rating, disable
+00002490: 643d 5472 7565 2029 7d0a 2020 2020 2020  d=True )}.      
+000024a0: 2020 2020 2020 2020 2020 3c64 6976 2073            <div s
+000024b0: 7479 6c65 3d22 636c 6561 723a 2062 6f74  tyle="clear: bot
+000024c0: 6822 3e3c 2f64 6976 3e0a 2020 2020 2020  h"></div>.      
+000024d0: 2020 2020 2020 3c2f 6469 763e 0a20 2020        </div>.   
+000024e0: 2020 2020 203c 2f64 6976 3e0a 2020 2020       </div>.    
+000024f0: 3c2f 6469 763e 0a25 656e 6469 660a 3c70  </div>.%endif.<p
+00002500: 2f3e 0a                                  />.
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/admin/tool_shed_repository/view_tool_metadata.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/view_tool_metadata.mako`

 * *Files 27% similar despite different names*

```diff
@@ -1,535 +1,729 @@
 00000000: 3c25 696e 6865 7269 7420 6669 6c65 3d22  <%inherit file="
 00000010: 2f62 6173 652e 6d61 6b6f 222f 3e0a 3c25  /base.mako"/>.<%
 00000020: 6e61 6d65 7370 6163 6520 6669 6c65 3d22  namespace file="
 00000030: 2f6d 6573 7361 6765 2e6d 616b 6f22 2069  /message.mako" i
 00000040: 6d70 6f72 743d 2272 656e 6465 725f 6d73  mport="render_ms
 00000050: 6722 202f 3e0a 3c25 6e61 6d65 7370 6163  g" />.<%namespac
-00000060: 6520 6669 6c65 3d22 2f61 646d 696e 2f74  e file="/admin/t
-00000070: 6f6f 6c5f 7368 6564 5f72 6570 6f73 6974  ool_shed_reposit
-00000080: 6f72 792f 7265 706f 7369 746f 7279 5f61  ory/repository_a
-00000090: 6374 696f 6e73 5f6d 656e 752e 6d61 6b6f  ctions_menu.mako
-000000a0: 2220 696d 706f 7274 3d22 2a22 202f 3e0a  " import="*" />.
-000000b0: 0a24 7b72 656e 6465 725f 6761 6c61 7879  .${render_galaxy
-000000c0: 5f72 6570 6f73 6974 6f72 795f 6163 7469  _repository_acti
-000000d0: 6f6e 7328 2072 6570 6f73 6974 6f72 7920  ons( repository 
-000000e0: 297d 0a0a 2569 6620 6d65 7373 6167 653a  )}..%if message:
-000000f0: 0a20 2020 2024 7b72 656e 6465 725f 6d73  .    ${render_ms
-00000100: 6728 206d 6573 7361 6765 2c20 7374 6174  g( message, stat
-00000110: 7573 2029 7d0a 2565 6e64 6966 0a0a 2569  us )}.%endif..%i
-00000120: 6620 746f 6f6c 5f6d 6574 6164 6174 613a  f tool_metadata:
-00000130: 0a20 2020 203c 702f 3e0a 2020 2020 3c64  .    <p/>.    <d
-00000140: 6976 2063 6c61 7373 3d22 6361 7264 223e  iv class="card">
-00000150: 0a20 2020 2020 2020 203c 6469 7620 636c  .        <div cl
-00000160: 6173 733d 2263 6172 642d 6865 6164 6572  ass="card-header
-00000170: 223e 247b 746f 6f6c 5f6d 6574 6164 6174  ">${tool_metadat
-00000180: 615b 2027 6e61 6d65 2720 5d7c 687d 2074  a[ 'name' ]|h} t
-00000190: 6f6f 6c20 6d65 7461 6461 7461 3c2f 6469  ool metadata</di
-000001a0: 763e 0a20 2020 2020 2020 203c 6469 7620  v>.        <div 
-000001b0: 636c 6173 733d 2263 6172 642d 626f 6479  class="card-body
-000001c0: 223e 0a20 2020 2020 2020 2020 2020 203c  ">.            <
-000001d0: 6469 7620 636c 6173 733d 2266 6f72 6d2d  div class="form-
-000001e0: 726f 7722 3e0a 2020 2020 2020 2020 2020  row">.          
-000001f0: 2020 2020 2020 3c74 6162 6c65 2077 6964        <table wid
-00000200: 7468 3d22 3130 3025 223e 0a20 2020 2020  th="100%">.     
-00000210: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00000220: 7472 2062 6763 6f6c 6f72 3d22 2344 3844  tr bgcolor="#D8D
-00000230: 3844 3822 2077 6964 7468 3d22 3130 3025  8D8" width="100%
-00000240: 223e 3c74 643e 3c62 3e4d 6973 6365 6c6c  "><td><b>Miscell
-00000250: 616e 656f 7573 3c2f 7464 3e3c 2f74 723e  aneous</td></tr>
-00000260: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000270: 203c 2f74 6162 6c65 3e0a 2020 2020 2020   </table>.      
-00000280: 2020 2020 2020 3c2f 6469 763e 0a20 2020        </div>.   
-00000290: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
-000002a0: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
-000002b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000002c0: 3c6c 6162 656c 3e4e 616d 653a 3c2f 6c61  <label>Name:</la
-000002d0: 6265 6c3e 0a20 2020 2020 2020 2020 2020  bel>.           
-000002e0: 2020 2020 2024 7b74 6f6f 6c5f 6d65 7461       ${tool_meta
-000002f0: 6461 7461 5b20 276e 616d 6527 205d 7c68  data[ 'name' ]|h
-00000300: 7d0a 2020 2020 2020 2020 2020 2020 2020  }.              
-00000310: 2020 3c64 6976 2073 7479 6c65 3d22 636c    <div style="cl
-00000320: 6561 723a 2062 6f74 6822 3e3c 2f64 6976  ear: both"></div
-00000330: 3e0a 2020 2020 2020 2020 2020 2020 3c2f  >.            </
-00000340: 6469 763e 0a20 2020 2020 2020 2020 2020  div>.           
-00000350: 2025 6966 2027 6465 7363 7269 7074 696f   %if 'descriptio
-00000360: 6e27 2069 6e20 746f 6f6c 5f6d 6574 6164  n' in tool_metad
-00000370: 6174 613a 0a20 2020 2020 2020 2020 2020  ata:.           
-00000380: 2020 2020 203c 6469 7620 636c 6173 733d       <div class=
-00000390: 2266 6f72 6d2d 726f 7722 3e0a 2020 2020  "form-row">.    
-000003a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000003b0: 3c6c 6162 656c 3e44 6573 6372 6970 7469  <label>Descripti
-000003c0: 6f6e 3a3c 2f6c 6162 656c 3e0a 2020 2020  on:</label>.    
-000003d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000003e0: 247b 746f 6f6c 5f6d 6574 6164 6174 615b  ${tool_metadata[
-000003f0: 2027 6465 7363 7269 7074 696f 6e27 205d   'description' ]
-00000400: 7c68 7d0a 2020 2020 2020 2020 2020 2020  |h}.            
-00000410: 2020 2020 2020 2020 3c64 6976 2073 7479          <div sty
-00000420: 6c65 3d22 636c 6561 723a 2062 6f74 6822  le="clear: both"
-00000430: 3e3c 2f64 6976 3e0a 2020 2020 2020 2020  ></div>.        
-00000440: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
-00000450: 2020 2020 2020 2020 2020 2025 656e 6469             %endi
-00000460: 660a 2020 2020 2020 2020 2020 2020 2569  f.            %i
-00000470: 6620 2769 6427 2069 6e20 746f 6f6c 5f6d  f 'id' in tool_m
-00000480: 6574 6164 6174 613a 0a20 2020 2020 2020  etadata:.       
-00000490: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
-000004a0: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
-000004b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000004c0: 2020 2020 3c6c 6162 656c 3e49 643a 3c2f      <label>Id:</
-000004d0: 6c61 6265 6c3e 0a20 2020 2020 2020 2020  label>.         
-000004e0: 2020 2020 2020 2020 2020 2024 7b74 6f6f             ${too
-000004f0: 6c5f 6d65 7461 6461 7461 5b20 2769 6427  l_metadata[ 'id'
-00000500: 205d 7c68 7d0a 2020 2020 2020 2020 2020   ]|h}.          
-00000510: 2020 2020 2020 2020 2020 3c64 6976 2073            <div s
-00000520: 7479 6c65 3d22 636c 6561 723a 2062 6f74  tyle="clear: bot
-00000530: 6822 3e3c 2f64 6976 3e0a 2020 2020 2020  h"></div>.      
-00000540: 2020 2020 2020 2020 2020 3c2f 6469 763e            </div>
-00000550: 0a20 2020 2020 2020 2020 2020 2025 656e  .            %en
-00000560: 6469 660a 2020 2020 2020 2020 2020 2020  dif.            
-00000570: 2569 6620 2767 7569 6427 2069 6e20 746f  %if 'guid' in to
-00000580: 6f6c 5f6d 6574 6164 6174 613a 0a20 2020  ol_metadata:.   
-00000590: 2020 2020 2020 2020 2020 2020 203c 6469               <di
-000005a0: 7620 636c 6173 733d 2266 6f72 6d2d 726f  v class="form-ro
-000005b0: 7722 3e0a 2020 2020 2020 2020 2020 2020  w">.            
-000005c0: 2020 2020 2020 2020 3c6c 6162 656c 3e47          <label>G
-000005d0: 7569 643a 3c2f 6c61 6265 6c3e 0a20 2020  uid:</label>.   
-000005e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000005f0: 2024 7b74 6f6f 6c5f 6d65 7461 6461 7461   ${tool_metadata
-00000600: 5b20 2767 7569 6427 205d 7c68 7d0a 2020  [ 'guid' ]|h}.  
-00000610: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000620: 2020 3c64 6976 2073 7479 6c65 3d22 636c    <div style="cl
-00000630: 6561 723a 2062 6f74 6822 3e3c 2f64 6976  ear: both"></div
-00000640: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
-00000650: 2020 3c2f 6469 763e 0a20 2020 2020 2020    </div>.       
-00000660: 2020 2020 2025 656e 6469 660a 2020 2020       %endif.    
-00000670: 2020 2020 2020 2020 2569 6620 2776 6572          %if 'ver
-00000680: 7369 6f6e 2720 696e 2074 6f6f 6c5f 6d65  sion' in tool_me
-00000690: 7461 6461 7461 3a0a 2020 2020 2020 2020  tadata:.        
-000006a0: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
-000006b0: 7373 3d22 666f 726d 2d72 6f77 223e 0a20  ss="form-row">. 
-000006c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000006d0: 2020 203c 6c61 6265 6c3e 5665 7273 696f     <label>Versio
-000006e0: 6e3a 3c2f 6c61 6265 6c3e 0a20 2020 2020  n:</label>.     
-000006f0: 2020 2020 2020 2020 2020 2020 2020 2024                 $
-00000700: 7b74 6f6f 6c5f 6d65 7461 6461 7461 5b20  {tool_metadata[ 
-00000710: 2776 6572 7369 6f6e 2720 5d7c 687d 0a20  'version' ]|h}. 
-00000720: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000730: 2020 203c 6469 7620 7374 796c 653d 2263     <div style="c
-00000740: 6c65 6172 3a20 626f 7468 223e 3c2f 6469  lear: both"></di
-00000750: 763e 0a20 2020 2020 2020 2020 2020 2020  v>.             
-00000760: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
-00000770: 2020 2020 2020 2565 6e64 6966 0a20 2020        %endif.   
-00000780: 2020 2020 2020 2020 2025 6966 2027 7665           %if 've
-00000790: 7273 696f 6e5f 7374 7269 6e67 5f63 6d64  rsion_string_cmd
-000007a0: 2720 696e 2074 6f6f 6c5f 6d65 7461 6461  ' in tool_metada
-000007b0: 7461 3a0a 2020 2020 2020 2020 2020 2020  ta:.            
-000007c0: 2020 2020 3c64 6976 2063 6c61 7373 3d22      <div class="
-000007d0: 666f 726d 2d72 6f77 223e 0a20 2020 2020  form-row">.     
-000007e0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-000007f0: 6c61 6265 6c3e 5665 7273 696f 6e20 636f  label>Version co
-00000800: 6d6d 616e 6420 7374 7269 6e67 3a3c 2f6c  mmand string:</l
-00000810: 6162 656c 3e0a 2020 2020 2020 2020 2020  abel>.          
-00000820: 2020 2020 2020 2020 2020 247b 746f 6f6c            ${tool
-00000830: 5f6d 6574 6164 6174 615b 2027 7665 7273  _metadata[ 'vers
-00000840: 696f 6e5f 7374 7269 6e67 5f63 6d64 2720  ion_string_cmd' 
-00000850: 5d7c 687d 0a20 2020 2020 2020 2020 2020  ]|h}.           
-00000860: 2020 2020 2020 2020 203c 6469 7620 7374           <div st
-00000870: 796c 653d 2263 6c65 6172 3a20 626f 7468  yle="clear: both
-00000880: 223e 3c2f 6469 763e 0a20 2020 2020 2020  "></div>.       
-00000890: 2020 2020 2020 2020 203c 2f64 6976 3e0a           </div>.
-000008a0: 2020 2020 2020 2020 2020 2020 2565 6e64              %end
-000008b0: 6966 0a20 2020 2020 2020 2020 2020 203c  if.            <
-000008c0: 6469 7620 636c 6173 733d 2266 6f72 6d2d  div class="form-
-000008d0: 726f 7722 3e0a 2020 2020 2020 2020 2020  row">.          
-000008e0: 2020 2020 2020 3c74 6162 6c65 2077 6964        <table wid
-000008f0: 7468 3d22 3130 3025 223e 0a20 2020 2020  th="100%">.     
-00000900: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00000910: 7472 2062 6763 6f6c 6f72 3d22 2344 3844  tr bgcolor="#D8D
-00000920: 3844 3822 2077 6964 7468 3d22 3130 3025  8D8" width="100%
-00000930: 223e 3c74 643e 3c62 3e56 6572 7369 6f6e  "><td><b>Version
-00000940: 206c 696e 6561 6765 206f 6620 7468 6973   lineage of this
-00000950: 2074 6f6f 6c20 2867 7569 6473 206f 7264   tool (guids ord
-00000960: 6572 6564 206d 6f73 7420 7265 6365 6e74  ered most recent
-00000970: 2074 6f20 6f6c 6465 7374 293c 2f74 643e   to oldest)</td>
-00000980: 3c2f 7472 3e0a 2020 2020 2020 2020 2020  </tr>.          
-00000990: 2020 2020 2020 3c2f 7461 626c 653e 0a20        </table>. 
-000009a0: 2020 2020 2020 2020 2020 203c 2f64 6976             </div
-000009b0: 3e0a 2020 2020 2020 2020 2020 2020 3c64  >.            <d
-000009c0: 6976 2063 6c61 7373 3d22 666f 726d 2d72  iv class="form-r
-000009d0: 6f77 223e 0a20 2020 2020 2020 2020 2020  ow">.           
-000009e0: 2020 2020 2025 6966 2074 6f6f 6c5f 6c69       %if tool_li
-000009f0: 6e65 6167 653a 0a20 2020 2020 2020 2020  neage:.         
-00000a00: 2020 2020 2020 2020 2020 203c 7461 626c             <tabl
-00000a10: 6520 636c 6173 733d 2267 7269 6422 3e0a  e class="grid">.
-00000a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000a30: 2020 2020 2020 2020 2566 6f72 2067 7569          %for gui
-00000a40: 6420 696e 2074 6f6f 6c5f 6c69 6e65 6167  d in tool_lineag
-00000a50: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
-00000a60: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00000a70: 7472 3e0a 2020 2020 2020 2020 2020 2020  tr>.            
-00000a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000a90: 2020 2020 3c74 643e 0a20 2020 2020 2020      <td>.       
-00000aa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ab0: 2020 2020 2020 2020 2020 2020 2025 6966               %if
-00000ac0: 2067 7569 6420 3d3d 2074 6f6f 6c5f 6d65   guid == tool_me
-00000ad0: 7461 6461 7461 5b20 2767 7569 6427 205d  tadata[ 'guid' ]
-00000ae0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00000af0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b00: 2020 2020 2020 2020 2020 247b 6775 6964            ${guid
-00000b10: 7c68 7d20 3c62 3e28 7468 6973 2074 6f6f  |h} <b>(this too
-00000b20: 6c29 3c2f 623e 0a20 2020 2020 2020 2020  l)</b>.         
-00000b30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b40: 2020 2020 2020 2020 2020 2025 656c 7365             %else
-00000b50: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00000b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000b70: 2020 2020 2020 2020 2020 247b 6775 6964            ${guid
-00000b80: 7c68 7d0a 2020 2020 2020 2020 2020 2020  |h}.            
-00000b90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000ba0: 2020 2020 2020 2020 2565 6e64 6966 0a20          %endif. 
-00000bb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000bc0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00000bd0: 2f74 643e 0a20 2020 2020 2020 2020 2020  /td>.           
-00000be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000bf0: 203c 2f74 723e 0a20 2020 2020 2020 2020   </tr>.         
-00000c00: 2020 2020 2020 2020 2020 2020 2020 2025                 %
-00000c10: 656e 6466 6f72 0a20 2020 2020 2020 2020  endfor.         
-00000c20: 2020 2020 2020 2020 2020 203c 2f74 6162             </tab
-00000c30: 6c65 3e0a 2020 2020 2020 2020 2020 2020  le>.            
-00000c40: 2020 2020 2565 6e64 6966 0a20 2020 2020      %endif.     
-00000c50: 2020 2020 2020 203c 2f64 6976 3e0a 2020         </div>.  
-00000c60: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
-00000c70: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
-00000c80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000c90: 203c 7461 626c 6520 7769 6474 683d 2231   <table width="1
-00000ca0: 3030 2522 3e0a 2020 2020 2020 2020 2020  00%">.          
-00000cb0: 2020 2020 2020 2020 2020 3c74 7220 6267            <tr bg
-00000cc0: 636f 6c6f 723d 2223 4438 4438 4438 2220  color="#D8D8D8" 
-00000cd0: 7769 6474 683d 2231 3030 2522 3e3c 7464  width="100%"><td
-00000ce0: 3e3c 623e 5265 7175 6972 656d 656e 7473  ><b>Requirements
-00000cf0: 2028 6465 7065 6e64 656e 6369 6573 2064   (dependencies d
-00000d00: 6566 696e 6564 2069 6e20 7468 6520 266c  efined in the &l
-00000d10: 743b 7265 7175 6972 656d 656e 7473 2667  t;requirements&g
-00000d20: 743b 2074 6167 2073 6574 293c 2f74 643e  t; tag set)</td>
-00000d30: 3c2f 7472 3e0a 2020 2020 2020 2020 2020  </tr>.          
-00000d40: 2020 2020 2020 3c2f 7461 626c 653e 0a20        </table>. 
-00000d50: 2020 2020 2020 2020 2020 203c 2f64 6976             </div
-00000d60: 3e0a 2020 2020 2020 2020 2020 2020 3c25  >.            <%
-00000d70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000d80: 2069 6620 2772 6571 7569 7265 6d65 6e74   if 'requirement
-00000d90: 7327 2069 6e20 746f 6f6c 5f6d 6574 6164  s' in tool_metad
-00000da0: 6174 613a 0a20 2020 2020 2020 2020 2020  ata:.           
-00000db0: 2020 2020 2020 2020 2072 6571 7569 7265           require
-00000dc0: 6d65 6e74 7320 3d20 746f 6f6c 5f6d 6574  ments = tool_met
-00000dd0: 6164 6174 615b 2027 7265 7175 6972 656d  adata[ 'requirem
-00000de0: 656e 7473 2720 5d0a 2020 2020 2020 2020  ents' ].        
-00000df0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-00000e00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000e10: 2020 7265 7175 6972 656d 656e 7473 203d    requirements =
-00000e20: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          
-00000e30: 2020 253e 0a20 2020 2020 2020 2020 2020    %>.           
-00000e40: 2025 6966 2072 6571 7569 7265 6d65 6e74   %if requirement
-00000e50: 733a 0a20 2020 2020 2020 2020 2020 2020  s:.             
-00000e60: 2020 203c 6469 7620 636c 6173 733d 2266     <div class="f
-00000e70: 6f72 6d2d 726f 7722 3e0a 2020 2020 2020  orm-row">.      
-00000e80: 2020 2020 2020 2020 2020 2020 2020 3c6c                <l
-00000e90: 6162 656c 3e52 6571 7569 7265 6d65 6e74  abel>Requirement
-00000ea0: 733a 3c2f 6c61 6265 6c3e 0a20 2020 2020  s:</label>.     
-00000eb0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00000ec0: 7461 626c 6520 636c 6173 733d 2267 7269  table class="gri
-00000ed0: 6422 3e0a 2020 2020 2020 2020 2020 2020  d">.            
-00000ee0: 2020 2020 2020 2020 2020 2020 3c74 723e              <tr>
-00000ef0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000f00: 2020 2020 2020 2020 2020 2020 203c 7464               <td
-00000f10: 3e3c 623e 6e61 6d65 3c2f 623e 3c2f 7464  ><b>name</b></td
-00000f20: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
-00000f30: 2020 2020 2020 2020 2020 2020 2020 3c74                <t
-00000f40: 643e 3c62 3e76 6572 7369 6f6e 3c2f 623e  d><b>version</b>
-00000f50: 3c2f 7464 3e0a 2020 2020 2020 2020 2020  </td>.          
-00000f60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000f70: 2020 3c74 643e 3c62 3e74 7970 653c 2f62    <td><b>type</b
-00000f80: 3e3c 2f74 643e 0a20 2020 2020 2020 2020  ></td>.         
-00000f90: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00000fa0: 2f74 723e 0a20 2020 2020 2020 2020 2020  /tr>.           
-00000fb0: 2020 2020 2020 2020 2020 2020 2025 666f               %fo
-00000fc0: 7220 7265 7175 6972 656d 656e 745f 6469  r requirement_di
-00000fd0: 6374 2069 6e20 7265 7175 6972 656d 656e  ct in requiremen
-00000fe0: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
-00000ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001000: 3c25 0a20 2020 2020 2020 2020 2020 2020  <%.             
-00001010: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001020: 2020 2072 6571 7569 7265 6d65 6e74 5f6e     requirement_n
-00001030: 616d 6520 3d20 7265 7175 6972 656d 656e  ame = requiremen
-00001040: 745f 6469 6374 5b20 276e 616d 6527 205d  t_dict[ 'name' ]
-00001050: 206f 7220 276e 6f74 2070 726f 7669 6465   or 'not provide
-00001060: 6427 0a20 2020 2020 2020 2020 2020 2020  d'.             
-00001070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001080: 2020 2072 6571 7569 7265 6d65 6e74 5f76     requirement_v
-00001090: 6572 7369 6f6e 203d 2072 6571 7569 7265  ersion = require
-000010a0: 6d65 6e74 5f64 6963 745b 2027 7665 7273  ment_dict[ 'vers
-000010b0: 696f 6e27 205d 206f 7220 276e 6f74 2070  ion' ] or 'not p
-000010c0: 726f 7669 6465 6427 0a20 2020 2020 2020  rovided'.       
-000010d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000010e0: 2020 2020 2020 2020 2072 6571 7569 7265           require
-000010f0: 6d65 6e74 5f74 7970 6520 3d20 7265 7175  ment_type = requ
-00001100: 6972 656d 656e 745f 6469 6374 5b20 2774  irement_dict[ 't
-00001110: 7970 6527 205d 206f 7220 276e 6f74 2070  ype' ] or 'not p
-00001120: 726f 7669 6465 6427 0a20 2020 2020 2020  rovided'.       
-00001130: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001140: 2020 2020 2025 3e0a 2020 2020 2020 2020       %>.        
-00001150: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001160: 2020 2020 3c74 723e 0a20 2020 2020 2020      <tr>.       
-00001170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001180: 2020 2020 2020 2020 203c 7464 3e24 7b72           <td>${r
-00001190: 6571 7569 7265 6d65 6e74 5f6e 616d 657c  equirement_name|
-000011a0: 687d 3c2f 7464 3e0a 2020 2020 2020 2020  h}</td>.        
-000011b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000011c0: 2020 2020 2020 2020 3c74 643e 247b 7265          <td>${re
-000011d0: 7175 6972 656d 656e 745f 7665 7273 696f  quirement_versio
-000011e0: 6e7c 687d 3c2f 7464 3e0a 2020 2020 2020  n|h}</td>.      
-000011f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001200: 2020 2020 2020 2020 2020 3c74 643e 247b            <td>${
-00001210: 7265 7175 6972 656d 656e 745f 7479 7065  requirement_type
-00001220: 7c68 7d3c 2f74 643e 0a20 2020 2020 2020  |h}</td>.       
-00001230: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001240: 2020 2020 203c 2f74 723e 0a20 2020 2020       </tr>.     
-00001250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001260: 2020 2025 656e 6466 6f72 0a20 2020 2020     %endfor.     
-00001270: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00001280: 2f74 6162 6c65 3e0a 2020 2020 2020 2020  /table>.        
-00001290: 2020 2020 2020 2020 2020 2020 3c64 6976              <div
-000012a0: 2073 7479 6c65 3d22 636c 6561 723a 2062   style="clear: b
-000012b0: 6f74 6822 3e3c 2f64 6976 3e0a 2020 2020  oth"></div>.    
-000012c0: 2020 2020 2020 2020 2020 2020 3c2f 6469              </di
-000012d0: 763e 0a20 2020 2020 2020 2020 2020 2025  v>.            %
-000012e0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-000012f0: 2020 2020 2020 3c64 6976 2063 6c61 7373        <div class
-00001300: 3d22 666f 726d 2d72 6f77 223e 0a20 2020  ="form-row">.   
-00001310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001320: 204e 6f20 7265 7175 6972 656d 656e 7473   No requirements
-00001330: 2064 6566 696e 6564 0a20 2020 2020 2020   defined.       
-00001340: 2020 2020 2020 2020 203c 2f64 6976 3e0a           </div>.
-00001350: 2020 2020 2020 2020 2020 2020 2565 6e64              %end
-00001360: 6966 0a20 2020 2020 2020 2020 2020 2025  if.            %
-00001370: 6966 2074 6f6f 6c3a 0a20 2020 2020 2020  if tool:.       
-00001380: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
-00001390: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
+00000060: 6520 6669 6c65 3d22 2f77 6562 6170 7073  e file="/webapps
+00000070: 2f74 6f6f 6c5f 7368 6564 2f63 6f6d 6d6f  /tool_shed/commo
+00000080: 6e2f 636f 6d6d 6f6e 2e6d 616b 6f22 2069  n/common.mako" i
+00000090: 6d70 6f72 743d 222a 2220 2f3e 0a3c 256e  mport="*" />.<%n
+000000a0: 616d 6573 7061 6365 2066 696c 653d 222f  amespace file="/
+000000b0: 7765 6261 7070 732f 746f 6f6c 5f73 6865  webapps/tool_she
+000000c0: 642f 7265 706f 7369 746f 7279 2f63 6f6d  d/repository/com
+000000d0: 6d6f 6e2e 6d61 6b6f 2220 696d 706f 7274  mon.mako" import
+000000e0: 3d22 2a22 202f 3e0a 3c25 6e61 6d65 7370  ="*" />.<%namesp
+000000f0: 6163 6520 6669 6c65 3d22 2f77 6562 6170  ace file="/webap
+00000100: 7073 2f74 6f6f 6c5f 7368 6564 2f63 6f6d  ps/tool_shed/com
+00000110: 6d6f 6e2f 7265 706f 7369 746f 7279 5f61  mon/repository_a
+00000120: 6374 696f 6e73 5f6d 656e 752e 6d61 6b6f  ctions_menu.mako
+00000130: 2220 696d 706f 7274 3d22 2a22 202f 3e0a  " import="*" />.
+00000140: 0a3c 250a 2020 2020 6973 5f6e 6577 203d  .<%.    is_new =
+00000150: 2072 6570 6f73 6974 6f72 792e 6973 5f6e   repository.is_n
+00000160: 6577 2829 0a0a 2020 2020 6361 6e5f 7075  ew()..    can_pu
+00000170: 7368 203d 2074 7261 6e73 2e61 7070 2e73  sh = trans.app.s
+00000180: 6563 7572 6974 795f 6167 656e 742e 6361  ecurity_agent.ca
+00000190: 6e5f 7075 7368 2820 7472 616e 732e 6170  n_push( trans.ap
+000001a0: 702c 2074 7261 6e73 2e75 7365 722c 2072  p, trans.user, r
+000001b0: 6570 6f73 6974 6f72 7920 290a 2020 2020  epository ).    
+000001c0: 6361 6e5f 646f 776e 6c6f 6164 203d 206e  can_download = n
+000001d0: 6f74 2069 735f 6e65 7720 616e 6420 2820  ot is_new and ( 
+000001e0: 6e6f 7420 6973 5f6d 616c 6963 696f 7573  not is_malicious
+000001f0: 206f 7220 6361 6e5f 7075 7368 2029 0a20   or can_push ). 
+00000200: 2020 2063 616e 5f76 6965 775f 6368 616e     can_view_chan
+00000210: 6765 5f6c 6f67 203d 2074 7261 6e73 2e77  ge_log = trans.w
+00000220: 6562 6170 702e 6e61 6d65 203d 3d20 2774  ebapp.name == 't
+00000230: 6f6f 6c5f 7368 6564 2720 616e 6420 6e6f  ool_shed' and no
+00000240: 7420 6973 5f6e 6577 0a25 3e0a 0a3c 2521  t is_new.%>..<%!
+00000250: 0a20 2020 6465 6620 696e 6865 7269 7428  .   def inherit(
+00000260: 636f 6e74 6578 7429 3a0a 2020 2020 2020  context):.      
+00000270: 2069 6620 636f 6e74 6578 742e 6765 7428   if context.get(
+00000280: 2775 7365 5f70 616e 656c 7327 293a 0a20  'use_panels'):. 
+00000290: 2020 2020 2020 2020 2020 7265 7475 726e            return
+000002a0: 2027 2f77 6562 6170 7073 2f74 6f6f 6c5f   '/webapps/tool_
+000002b0: 7368 6564 2f62 6173 655f 7061 6e65 6c73  shed/base_panels
+000002c0: 2e6d 616b 6f27 0a20 2020 2020 2020 656c  .mako'.       el
+000002d0: 7365 3a0a 2020 2020 2020 2020 2020 2072  se:.           r
+000002e0: 6574 7572 6e20 272f 6261 7365 2e6d 616b  eturn '/base.mak
+000002f0: 6f27 0a25 3e0a 3c25 696e 6865 7269 7420  o'.%>.<%inherit 
+00000300: 6669 6c65 3d22 247b 696e 6865 7269 7428  file="${inherit(
+00000310: 636f 6e74 6578 7429 7d22 2f3e 0a0a 2569  context)}"/>..%i
+00000320: 6620 7265 6e64 6572 5f72 6570 6f73 6974  f render_reposit
+00000330: 6f72 795f 6163 7469 6f6e 735f 666f 7220  ory_actions_for 
+00000340: 3d3d 2027 746f 6f6c 5f73 6865 6427 3a0a  == 'tool_shed':.
+00000350: 2020 2020 247b 7265 6e64 6572 5f74 6f6f      ${render_too
+00000360: 6c5f 7368 6564 5f72 6570 6f73 6974 6f72  l_shed_repositor
+00000370: 795f 6163 7469 6f6e 7328 2072 6570 6f73  y_actions( repos
+00000380: 6974 6f72 793d 7265 706f 7369 746f 7279  itory=repository
+00000390: 2c20 6d65 7461 6461 7461 3d6d 6574 6164  , metadata=metad
+000003a0: 6174 612c 2063 6861 6e67 6573 6574 5f72  ata, changeset_r
+000003b0: 6576 6973 696f 6e3d 6368 616e 6765 7365  evision=changese
+000003c0: 745f 7265 7669 7369 6f6e 2029 7d0a 2565  t_revision )}.%e
+000003d0: 6c73 653a 0a20 2020 2024 7b72 656e 6465  lse:.    ${rende
+000003e0: 725f 6761 6c61 7879 5f72 6570 6f73 6974  r_galaxy_reposit
+000003f0: 6f72 795f 6163 7469 6f6e 7328 2072 6570  ory_actions( rep
+00000400: 6f73 6974 6f72 793d 7265 706f 7369 746f  ository=reposito
+00000410: 7279 2029 7d0a 2565 6e64 6966 0a0a 2569  ry )}.%endif..%i
+00000420: 6620 6d65 7373 6167 653a 0a20 2020 2024  f message:.    $
+00000430: 7b72 656e 6465 725f 6d73 6728 206d 6573  {render_msg( mes
+00000440: 7361 6765 2c20 7374 6174 7573 2029 7d0a  sage, status )}.
+00000450: 2565 6e64 6966 0a0a 3c64 6976 2063 6c61  %endif..<div cla
+00000460: 7373 3d22 746f 6f6c 466f 726d 223e 0a20  ss="toolForm">. 
+00000470: 2020 203c 6469 7620 636c 6173 733d 2274     <div class="t
+00000480: 6f6f 6c46 6f72 6d54 6974 6c65 223e 5265  oolFormTitle">Re
+00000490: 706f 7369 746f 7279 2072 6576 6973 696f  pository revisio
+000004a0: 6e3c 2f64 6976 3e0a 2020 2020 3c64 6976  n</div>.    <div
+000004b0: 2063 6c61 7373 3d22 746f 6f6c 466f 726d   class="toolForm
+000004c0: 426f 6479 223e 0a20 2020 2020 2020 203c  Body">.        <
+000004d0: 6469 7620 636c 6173 733d 2266 6f72 6d2d  div class="form-
+000004e0: 726f 7722 3e0a 2020 2020 2020 2020 2020  row">.          
+000004f0: 2020 3c6c 6162 656c 3e52 6576 6973 696f    <label>Revisio
+00000500: 6e3a 3c2f 6c61 6265 6c3e 0a20 2020 2020  n:</label>.     
+00000510: 2020 2020 2020 2025 6966 2063 616e 5f76         %if can_v
+00000520: 6965 775f 6368 616e 6765 5f6c 6f67 3a0a  iew_change_log:.
+00000530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000540: 3c61 2068 7265 663d 2224 7b68 2e75 726c  <a href="${h.url
+00000550: 5f66 6f72 2820 636f 6e74 726f 6c6c 6572  _for( controller
+00000560: 3d27 7265 706f 7369 746f 7279 272c 2061  ='repository', a
+00000570: 6374 696f 6e3d 2776 6965 775f 6368 616e  ction='view_chan
+00000580: 6765 6c6f 6727 2c20 6964 3d74 7261 6e73  gelog', id=trans
+00000590: 2e61 7070 2e73 6563 7572 6974 792e 656e  .app.security.en
+000005a0: 636f 6465 5f69 6428 2072 6570 6f73 6974  code_id( reposit
+000005b0: 6f72 792e 6964 2029 2029 7d22 3e24 7b72  ory.id ) )}">${r
+000005c0: 6576 6973 696f 6e5f 6c61 6265 6c7d 3c2f  evision_label}</
+000005d0: 613e 0a20 2020 2020 2020 2020 2020 2025  a>.            %
+000005e0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+000005f0: 2020 2020 2020 247b 7265 7669 7369 6f6e        ${revision
+00000600: 5f6c 6162 656c 7d0a 2020 2020 2020 2020  _label}.        
+00000610: 2020 2020 2565 6e64 6966 0a20 2020 2020      %endif.     
+00000620: 2020 203c 2f64 6976 3e0a 2020 2020 3c2f     </div>.    </
+00000630: 6469 763e 0a3c 2f64 6976 3e0a 3c70 2f3e  div>.</div>.<p/>
+00000640: 0a25 6966 2063 616e 5f64 6f77 6e6c 6f61  .%if can_downloa
+00000650: 643a 0a20 2020 203c 6469 7620 636c 6173  d:.    <div clas
+00000660: 733d 2274 6f6f 6c46 6f72 6d22 3e0a 2020  s="toolForm">.  
+00000670: 2020 2020 2020 3c64 6976 2063 6c61 7373        <div class
+00000680: 3d22 746f 6f6c 466f 726d 5469 746c 6522  ="toolFormTitle"
+00000690: 3e52 6570 6f73 6974 6f72 7920 2724 7b72  >Repository '${r
+000006a0: 6570 6f73 6974 6f72 792e 6e61 6d65 207c  epository.name |
+000006b0: 2068 7d27 3c2f 6469 763e 0a20 2020 2020   h}'</div>.     
+000006c0: 2020 203c 6469 7620 636c 6173 733d 2274     <div class="t
+000006d0: 6f6f 6c46 6f72 6d42 6f64 7922 3e0a 2020  oolFormBody">.  
+000006e0: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
+000006f0: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
+00000700: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000710: 203c 6c61 6265 6c3e 436c 6f6e 6520 7468   <label>Clone th
+00000720: 6973 2072 6570 6f73 6974 6f72 793a 3c2f  is repository:</
+00000730: 6c61 6265 6c3e 0a20 2020 2020 2020 2020  label>.         
+00000740: 2020 2020 2020 2024 7b72 656e 6465 725f         ${render_
+00000750: 636c 6f6e 655f 7374 7228 2072 6570 6f73  clone_str( repos
+00000760: 6974 6f72 7920 297d 0a20 2020 2020 2020  itory )}.       
+00000770: 2020 2020 203c 2f64 6976 3e0a 2020 2020       </div>.    
+00000780: 2020 2020 3c2f 6469 763e 0a20 2020 203c      </div>.    <
+00000790: 2f64 6976 3e0a 2565 6c73 653a 0a20 2020  /div>.%else:.   
+000007a0: 203c 623e 5265 706f 7369 746f 7279 206e   <b>Repository n
+000007b0: 616d 653a 3c2f 623e 3c62 722f 3e0a 2020  ame:</b><br/>.  
+000007c0: 2020 247b 7265 706f 7369 746f 7279 2e6e    ${repository.n
+000007d0: 616d 657d 0a25 656e 6469 660a 2569 6620  ame}.%endif.%if 
+000007e0: 746f 6f6c 5f6d 6574 6164 6174 615f 6469  tool_metadata_di
+000007f0: 6374 3a0a 2020 2020 3c70 2f3e 0a20 2020  ct:.    <p/>.   
+00000800: 203c 6469 7620 636c 6173 733d 2274 6f6f   <div class="too
+00000810: 6c46 6f72 6d22 3e0a 2020 2020 2020 2020  lForm">.        
+00000820: 3c64 6976 2063 6c61 7373 3d22 746f 6f6c  <div class="tool
+00000830: 466f 726d 5469 746c 6522 3e24 7b74 6f6f  FormTitle">${too
+00000840: 6c5f 6d65 7461 6461 7461 5f64 6963 745b  l_metadata_dict[
+00000850: 2027 6e61 6d65 2720 5d7d 2074 6f6f 6c20   'name' ]} tool 
+00000860: 6d65 7461 6461 7461 3c2f 6469 763e 0a20  metadata</div>. 
+00000870: 2020 2020 2020 203c 6469 7620 636c 6173         <div clas
+00000880: 733d 2274 6f6f 6c46 6f72 6d42 6f64 7922  s="toolFormBody"
+00000890: 3e0a 2020 2020 2020 2020 2020 2020 3c64  >.            <d
+000008a0: 6976 2063 6c61 7373 3d22 666f 726d 2d72  iv class="form-r
+000008b0: 6f77 223e 0a20 2020 2020 2020 2020 2020  ow">.           
+000008c0: 2020 2020 203c 7461 626c 6520 7769 6474       <table widt
+000008d0: 683d 2231 3030 2522 3e0a 2020 2020 2020  h="100%">.      
+000008e0: 2020 2020 2020 2020 2020 2020 2020 3c74                <t
+000008f0: 7220 6267 636f 6c6f 723d 2223 4438 4438  r bgcolor="#D8D8
+00000900: 4438 2220 7769 6474 683d 2231 3030 2522  D8" width="100%"
+00000910: 3e3c 7464 3e3c 623e 4d69 7363 656c 6c61  ><td><b>Miscella
+00000920: 6e65 6f75 733c 2f74 643e 3c2f 7472 3e0a  neous</td></tr>.
+00000930: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000940: 3c2f 7461 626c 653e 0a20 2020 2020 2020  </table>.       
+00000950: 2020 2020 203c 2f64 6976 3e0a 2020 2020       </div>.    
+00000960: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
+00000970: 7373 3d22 666f 726d 2d72 6f77 223e 0a20  ss="form-row">. 
+00000980: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+00000990: 6c61 6265 6c3e 4e61 6d65 3a3c 2f6c 6162  label>Name:</lab
+000009a0: 656c 3e0a 2020 2020 2020 2020 2020 2020  el>.            
+000009b0: 2020 2020 3c61 2068 7265 663d 2224 7b68      <a href="${h
+000009c0: 2e75 726c 5f66 6f72 2820 636f 6e74 726f  .url_for( contro
+000009d0: 6c6c 6572 3d27 7265 706f 7369 746f 7279  ller='repository
+000009e0: 272c 2061 6374 696f 6e3d 2764 6973 706c  ', action='displ
+000009f0: 6179 5f74 6f6f 6c27 2c20 7265 706f 7369  ay_tool', reposi
+00000a00: 746f 7279 5f69 643d 7472 616e 732e 7365  tory_id=trans.se
+00000a10: 6375 7269 7479 2e65 6e63 6f64 655f 6964  curity.encode_id
+00000a20: 2820 7265 706f 7369 746f 7279 2e69 6420  ( repository.id 
+00000a30: 292c 2074 6f6f 6c5f 636f 6e66 6967 3d74  ), tool_config=t
+00000a40: 6f6f 6c5f 6d65 7461 6461 7461 5f64 6963  ool_metadata_dic
+00000a50: 745b 2027 746f 6f6c 5f63 6f6e 6669 6727  t[ 'tool_config'
+00000a60: 205d 2c20 6368 616e 6765 7365 745f 7265   ], changeset_re
+00000a70: 7669 7369 6f6e 3d63 6861 6e67 6573 6574  vision=changeset
+00000a80: 5f72 6576 6973 696f 6e20 297d 223e 247b  _revision )}">${
+00000a90: 746f 6f6c 5f6d 6574 6164 6174 615f 6469  tool_metadata_di
+00000aa0: 6374 5b20 276e 616d 6527 205d 7d3c 2f61  ct[ 'name' ]}</a
+00000ab0: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+00000ac0: 2020 3c64 6976 2073 7479 6c65 3d22 636c    <div style="cl
+00000ad0: 6561 723a 2062 6f74 6822 3e3c 2f64 6976  ear: both"></div
+00000ae0: 3e0a 2020 2020 2020 2020 2020 2020 3c2f  >.            </
+00000af0: 6469 763e 0a20 2020 2020 2020 2020 2020  div>.           
+00000b00: 2025 6966 2027 6465 7363 7269 7074 696f   %if 'descriptio
+00000b10: 6e27 2069 6e20 746f 6f6c 5f6d 6574 6164  n' in tool_metad
+00000b20: 6174 615f 6469 6374 3a0a 2020 2020 2020  ata_dict:.      
+00000b30: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
+00000b40: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
+00000b50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000b60: 2020 2020 203c 6c61 6265 6c3e 4465 7363       <label>Desc
+00000b70: 7269 7074 696f 6e3a 3c2f 6c61 6265 6c3e  ription:</label>
+00000b80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000b90: 2020 2020 2024 7b74 6f6f 6c5f 6d65 7461       ${tool_meta
+00000ba0: 6461 7461 5f64 6963 745b 2027 6465 7363  data_dict[ 'desc
+00000bb0: 7269 7074 696f 6e27 205d 207c 2068 7d0a  ription' ] | h}.
+00000bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000bd0: 2020 2020 3c64 6976 2073 7479 6c65 3d22      <div style="
+00000be0: 636c 6561 723a 2062 6f74 6822 3e3c 2f64  clear: both"></d
+00000bf0: 6976 3e0a 2020 2020 2020 2020 2020 2020  iv>.            
+00000c00: 2020 2020 3c2f 6469 763e 0a20 2020 2020      </div>.     
+00000c10: 2020 2020 2020 2025 656e 6469 660a 2020         %endif.  
+00000c20: 2020 2020 2020 2020 2020 2569 6620 2769            %if 'i
+00000c30: 6427 2069 6e20 746f 6f6c 5f6d 6574 6164  d' in tool_metad
+00000c40: 6174 615f 6469 6374 3a0a 2020 2020 2020  ata_dict:.      
+00000c50: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
+00000c60: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
+00000c70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000c80: 2020 2020 203c 6c61 6265 6c3e 4964 3a3c       <label>Id:<
+00000c90: 2f6c 6162 656c 3e0a 2020 2020 2020 2020  /label>.        
+00000ca0: 2020 2020 2020 2020 2020 2020 247b 746f              ${to
+00000cb0: 6f6c 5f6d 6574 6164 6174 615f 6469 6374  ol_metadata_dict
+00000cc0: 5b20 2769 6427 205d 207c 2068 7d0a 2020  [ 'id' ] | h}.  
+00000cd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000ce0: 2020 3c64 6976 2073 7479 6c65 3d22 636c    <div style="cl
+00000cf0: 6561 723a 2062 6f74 6822 3e3c 2f64 6976  ear: both"></div
+00000d00: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+00000d10: 2020 3c2f 6469 763e 0a20 2020 2020 2020    </div>.       
+00000d20: 2020 2020 2025 656e 6469 660a 2020 2020       %endif.    
+00000d30: 2020 2020 2020 2020 2569 6620 2767 7569          %if 'gui
+00000d40: 6427 2069 6e20 746f 6f6c 5f6d 6574 6164  d' in tool_metad
+00000d50: 6174 615f 6469 6374 3a0a 2020 2020 2020  ata_dict:.      
+00000d60: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
+00000d70: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
+00000d80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000d90: 2020 2020 203c 6c61 6265 6c3e 4775 6964       <label>Guid
+00000da0: 3a3c 2f6c 6162 656c 3e0a 2020 2020 2020  :</label>.      
+00000db0: 2020 2020 2020 2020 2020 2020 2020 247b                ${
+00000dc0: 746f 6f6c 5f6d 6574 6164 6174 615f 6469  tool_metadata_di
+00000dd0: 6374 5b20 2767 7569 6427 205d 207c 2068  ct[ 'guid' ] | h
+00000de0: 7d0a 2020 2020 2020 2020 2020 2020 2020  }.              
+00000df0: 2020 2020 2020 3c64 6976 2073 7479 6c65        <div style
+00000e00: 3d22 636c 6561 723a 2062 6f74 6822 3e3c  ="clear: both"><
+00000e10: 2f64 6976 3e0a 2020 2020 2020 2020 2020  /div>.          
+00000e20: 2020 2020 2020 3c2f 6469 763e 0a20 2020        </div>.   
+00000e30: 2020 2020 2020 2020 2025 656e 6469 660a           %endif.
+00000e40: 2020 2020 2020 2020 2020 2020 2569 6620              %if 
+00000e50: 2776 6572 7369 6f6e 2720 696e 2074 6f6f  'version' in too
+00000e60: 6c5f 6d65 7461 6461 7461 5f64 6963 743a  l_metadata_dict:
+00000e70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000e80: 203c 6469 7620 636c 6173 733d 2266 6f72   <div class="for
+00000e90: 6d2d 726f 7722 3e0a 2020 2020 2020 2020  m-row">.        
+00000ea0: 2020 2020 2020 2020 2020 2020 3c6c 6162              <lab
+00000eb0: 656c 3e56 6572 7369 6f6e 3a3c 2f6c 6162  el>Version:</lab
+00000ec0: 656c 3e0a 2020 2020 2020 2020 2020 2020  el>.            
+00000ed0: 2020 2020 2020 2020 247b 746f 6f6c 5f6d          ${tool_m
+00000ee0: 6574 6164 6174 615f 6469 6374 5b20 2776  etadata_dict[ 'v
+00000ef0: 6572 7369 6f6e 2720 5d20 7c20 687d 0a20  ersion' ] | h}. 
+00000f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000f10: 2020 203c 6469 7620 7374 796c 653d 2263     <div style="c
+00000f20: 6c65 6172 3a20 626f 7468 223e 3c2f 6469  lear: both"></di
+00000f30: 763e 0a20 2020 2020 2020 2020 2020 2020  v>.             
+00000f40: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
+00000f50: 2020 2020 2020 2565 6e64 6966 0a20 2020        %endif.   
+00000f60: 2020 2020 2020 2020 2025 6966 2027 7665           %if 've
+00000f70: 7273 696f 6e5f 7374 7269 6e67 5f63 6d64  rsion_string_cmd
+00000f80: 2720 696e 2074 6f6f 6c5f 6d65 7461 6461  ' in tool_metada
+00000f90: 7461 5f64 6963 743a 0a20 2020 2020 2020  ta_dict:.       
+00000fa0: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
+00000fb0: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
+00000fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000fd0: 2020 2020 3c6c 6162 656c 3e56 6572 7369      <label>Versi
+00000fe0: 6f6e 2063 6f6d 6d61 6e64 2073 7472 696e  on command strin
+00000ff0: 673a 3c2f 6c61 6265 6c3e 0a20 2020 2020  g:</label>.     
+00001000: 2020 2020 2020 2020 2020 2020 2020 2024                 $
+00001010: 7b74 6f6f 6c5f 6d65 7461 6461 7461 5f64  {tool_metadata_d
+00001020: 6963 745b 2027 7665 7273 696f 6e5f 7374  ict[ 'version_st
+00001030: 7269 6e67 5f63 6d64 2720 5d20 7c20 687d  ring_cmd' ] | h}
+00001040: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001050: 2020 2020 203c 6469 7620 7374 796c 653d       <div style=
+00001060: 2263 6c65 6172 3a20 626f 7468 223e 3c2f  "clear: both"></
+00001070: 6469 763e 0a20 2020 2020 2020 2020 2020  div>.           
+00001080: 2020 2020 203c 2f64 6976 3e0a 2020 2020       </div>.    
+00001090: 2020 2020 2020 2020 2565 6e64 6966 0a20          %endif. 
+000010a0: 2020 2020 2020 2020 2020 2025 6966 2027             %if '
+000010b0: 6164 645f 746f 5f74 6f6f 6c5f 7061 6e65  add_to_tool_pane
+000010c0: 6c27 2069 6e20 746f 6f6c 5f6d 6574 6164  l' in tool_metad
+000010d0: 6174 615f 6469 6374 3a0a 2020 2020 2020  ata_dict:.      
+000010e0: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
+000010f0: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
+00001100: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001110: 2020 2020 203c 6c61 6265 6c3e 4469 7370       <label>Disp
+00001120: 6c61 7920 696e 2074 6f6f 6c20 7061 6e65  lay in tool pane
+00001130: 6c3a 3c2f 6c61 6265 6c3e 0a20 2020 2020  l:</label>.     
+00001140: 2020 2020 2020 2020 2020 2020 2020 2024                 $
+00001150: 7b74 6f6f 6c5f 6d65 7461 6461 7461 5f64  {tool_metadata_d
+00001160: 6963 745b 2027 6164 645f 746f 5f74 6f6f  ict[ 'add_to_too
+00001170: 6c5f 7061 6e65 6c27 205d 207c 2068 7d0a  l_panel' ] | h}.
+00001180: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001190: 2020 2020 3c64 6976 2073 7479 6c65 3d22      <div style="
+000011a0: 636c 6561 723a 2062 6f74 6822 3e3c 2f64  clear: both"></d
+000011b0: 6976 3e0a 2020 2020 2020 2020 2020 2020  iv>.            
+000011c0: 2020 2020 3c2f 6469 763e 0a20 2020 2020      </div>.     
+000011d0: 2020 2020 2020 2025 656e 6469 660a 2020         %endif.  
+000011e0: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
+000011f0: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
+00001200: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001210: 203c 7461 626c 6520 7769 6474 683d 2231   <table width="1
+00001220: 3030 2522 3e0a 2020 2020 2020 2020 2020  00%">.          
+00001230: 2020 2020 2020 2020 2020 3c74 7220 6267            <tr bg
+00001240: 636f 6c6f 723d 2223 4438 4438 4438 2220  color="#D8D8D8" 
+00001250: 7769 6474 683d 2231 3030 2522 3e3c 7464  width="100%"><td
+00001260: 3e3c 623e 5665 7273 696f 6e20 6c69 6e65  ><b>Version line
+00001270: 6167 6520 6f66 2074 6869 7320 746f 6f6c  age of this tool
+00001280: 2028 6775 6964 7320 6f72 6465 7265 6420   (guids ordered 
+00001290: 6d6f 7374 2072 6563 656e 7420 746f 206f  most recent to o
+000012a0: 6c64 6573 7429 3c2f 7464 3e3c 2f74 723e  ldest)</td></tr>
+000012b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000012c0: 203c 2f74 6162 6c65 3e0a 2020 2020 2020   </table>.      
+000012d0: 2020 2020 2020 3c2f 6469 763e 0a20 2020        </div>.   
+000012e0: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
+000012f0: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
+00001300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001310: 2569 6620 746f 6f6c 5f6c 696e 6561 6765  %if tool_lineage
+00001320: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00001330: 2020 2020 2020 3c74 6162 6c65 2063 6c61        <table cla
+00001340: 7373 3d22 6772 6964 223e 0a20 2020 2020  ss="grid">.     
+00001350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001360: 2020 2025 666f 7220 6775 6964 2069 6e20     %for guid in 
+00001370: 746f 6f6c 5f6c 696e 6561 6765 3a0a 2020  tool_lineage:.  
+00001380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001390: 2020 2020 2020 2020 2020 3c74 723e 0a20            <tr>. 
 000013a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000013b0: 2020 2020 3c74 6162 6c65 2077 6964 7468      <table width
-000013c0: 3d22 3130 3025 223e 0a20 2020 2020 2020  ="100%">.       
+000013b0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+000013c0: 7464 3e0a 2020 2020 2020 2020 2020 2020  td>.            
 000013d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000013e0: 203c 7472 2062 6763 6f6c 6f72 3d22 2344   <tr bgcolor="#D
-000013f0: 3844 3844 3822 2077 6964 7468 3d22 3130  8D8D8" width="10
-00001400: 3025 223e 3c74 643e 3c62 3e41 6464 6974  0%"><td><b>Addit
-00001410: 696f 6e61 6c20 696e 666f 726d 6174 696f  ional informatio
-00001420: 6e20 6162 6f75 7420 7468 6973 2074 6f6f  n about this too
-00001430: 6c3c 2f74 643e 3c2f 7472 3e0a 2020 2020  l</td></tr>.    
-00001440: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001450: 3c2f 7461 626c 653e 0a20 2020 2020 2020  </table>.       
-00001460: 2020 2020 2020 2020 203c 2f64 6976 3e0a           </div>.
-00001470: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001480: 3c64 6976 2063 6c61 7373 3d22 666f 726d  <div class="form
-00001490: 2d72 6f77 223e 0a20 2020 2020 2020 2020  -row">.         
-000014a0: 2020 2020 2020 2020 2020 203c 6c61 6265             <labe
-000014b0: 6c3e 436f 6d6d 616e 643a 3c2f 6c61 6265  l>Command:</labe
-000014c0: 6c3e 0a20 2020 2020 2020 2020 2020 2020  l>.             
-000014d0: 2020 2020 2020 203c 7072 653e 247b 746f         <pre>${to
-000014e0: 6f6c 2e63 6f6d 6d61 6e64 7c68 7d3c 2f70  ol.command|h}</p
-000014f0: 7265 3e0a 2020 2020 2020 2020 2020 2020  re>.            
-00001500: 2020 2020 2020 2020 3c64 6976 2073 7479          <div sty
-00001510: 6c65 3d22 636c 6561 723a 2062 6f74 6822  le="clear: both"
-00001520: 3e3c 2f64 6976 3e0a 2020 2020 2020 2020  ></div>.        
-00001530: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
-00001540: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00001550: 6469 7620 636c 6173 733d 2266 6f72 6d2d  div class="form-
-00001560: 726f 7722 3e0a 2020 2020 2020 2020 2020  row">.          
-00001570: 2020 2020 2020 2020 2020 3c6c 6162 656c            <label
-00001580: 3e49 6e74 6572 7072 6574 6572 3a3c 2f6c  >Interpreter:</l
-00001590: 6162 656c 3e0a 2020 2020 2020 2020 2020  abel>.          
-000015a0: 2020 2020 2020 2020 2020 247b 746f 6f6c            ${tool
-000015b0: 2e69 6e74 6572 7072 6574 6572 7c68 7d0a  .interpreter|h}.
-000015c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000015d0: 2020 2020 3c64 6976 2073 7479 6c65 3d22      <div style="
-000015e0: 636c 6561 723a 2062 6f74 6822 3e3c 2f64  clear: both"></d
-000015f0: 6976 3e0a 2020 2020 2020 2020 2020 2020  iv>.            
-00001600: 2020 2020 3c2f 6469 763e 0a20 2020 2020      </div>.     
-00001610: 2020 2020 2020 2020 2020 203c 6469 7620             <div 
-00001620: 636c 6173 733d 2266 6f72 6d2d 726f 7722  class="form-row"
-00001630: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
-00001640: 2020 2020 2020 3c6c 6162 656c 3e49 7320        <label>Is 
-00001650: 6d75 6c74 692d 6279 7465 3a3c 2f6c 6162  multi-byte:</lab
-00001660: 656c 3e0a 2020 2020 2020 2020 2020 2020  el>.            
-00001670: 2020 2020 2020 2020 247b 746f 6f6c 2e69          ${tool.i
-00001680: 735f 6d75 6c74 695f 6279 7465 7c68 7d0a  s_multi_byte|h}.
-00001690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000016a0: 2020 2020 3c64 6976 2073 7479 6c65 3d22      <div style="
-000016b0: 636c 6561 723a 2062 6f74 6822 3e3c 2f64  clear: both"></d
-000016c0: 6976 3e0a 2020 2020 2020 2020 2020 2020  iv>.            
-000016d0: 2020 2020 3c2f 6469 763e 0a20 2020 2020      </div>.     
-000016e0: 2020 2020 2020 2020 2020 203c 6469 7620             <div 
-000016f0: 636c 6173 733d 2266 6f72 6d2d 726f 7722  class="form-row"
-00001700: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
-00001710: 2020 2020 2020 3c6c 6162 656c 3e46 6f72        <label>For
-00001720: 6365 7320 6120 6869 7374 6f72 7920 7265  ces a history re
-00001730: 6672 6573 683a 3c2f 6c61 6265 6c3e 0a20  fresh:</label>. 
-00001740: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001750: 2020 2024 7b74 6f6f 6c2e 666f 7263 655f     ${tool.force_
-00001760: 6869 7374 6f72 795f 7265 6672 6573 687c  history_refresh|
-00001770: 687d 0a20 2020 2020 2020 2020 2020 2020  h}.             
-00001780: 2020 2020 2020 203c 6469 7620 7374 796c         <div styl
-00001790: 653d 2263 6c65 6172 3a20 626f 7468 223e  e="clear: both">
-000017a0: 3c2f 6469 763e 0a20 2020 2020 2020 2020  </div>.         
-000017b0: 2020 2020 2020 203c 2f64 6976 3e0a 2020         </div>.  
-000017c0: 2020 2020 2020 2020 2020 2020 2020 3c64                <d
-000017d0: 6976 2063 6c61 7373 3d22 666f 726d 2d72  iv class="form-r
-000017e0: 6f77 223e 0a20 2020 2020 2020 2020 2020  ow">.           
-000017f0: 2020 2020 2020 2020 203c 6c61 6265 6c3e           <label>
-00001800: 5061 7261 6c6c 656c 6973 6d3a 3c2f 6c61  Parallelism:</la
-00001810: 6265 6c3e 0a20 2020 2020 2020 2020 2020  bel>.           
-00001820: 2020 2020 2020 2020 2024 7b74 6f6f 6c2e           ${tool.
-00001830: 7061 7261 6c6c 656c 6973 6d7c 687d 0a20  parallelism|h}. 
-00001840: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001850: 2020 203c 6469 7620 7374 796c 653d 2263     <div style="c
-00001860: 6c65 6172 3a20 626f 7468 223e 3c2f 6469  lear: both"></di
-00001870: 763e 0a20 2020 2020 2020 2020 2020 2020  v>.             
-00001880: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
-00001890: 2020 2020 2020 2565 6e64 6966 0a20 2020        %endif.   
-000018a0: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
-000018b0: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
-000018c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000018d0: 3c74 6162 6c65 2077 6964 7468 3d22 3130  <table width="10
-000018e0: 3025 223e 0a20 2020 2020 2020 2020 2020  0%">.           
-000018f0: 2020 2020 2020 2020 203c 7472 2062 6763           <tr bgc
-00001900: 6f6c 6f72 3d22 2344 3844 3844 3822 2077  olor="#D8D8D8" w
-00001910: 6964 7468 3d22 3130 3025 223e 3c74 643e  idth="100%"><td>
-00001920: 3c62 3e46 756e 6374 696f 6e61 6c20 7465  <b>Functional te
-00001930: 7374 733c 2f74 643e 3c2f 7472 3e0a 2020  sts</td></tr>.  
-00001940: 2020 2020 2020 2020 2020 2020 2020 3c2f                </
-00001950: 7461 626c 653e 0a20 2020 2020 2020 2020  table>.         
-00001960: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
-00001970: 2020 2020 2020 3c25 0a20 2020 2020 2020        <%.       
-00001980: 2020 2020 2020 2020 2069 6620 2774 6573           if 'tes
-00001990: 7473 2720 696e 2074 6f6f 6c5f 6d65 7461  ts' in tool_meta
-000019a0: 6461 7461 3a0a 2020 2020 2020 2020 2020  data:.          
-000019b0: 2020 2020 2020 2020 2020 7465 7374 7320            tests 
-000019c0: 3d20 746f 6f6c 5f6d 6574 6164 6174 615b  = tool_metadata[
-000019d0: 2027 7465 7374 7327 205d 0a20 2020 2020   'tests' ].     
-000019e0: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
-000019f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00001a00: 2020 2020 2074 6573 7473 203d 204e 6f6e       tests = Non
-00001a10: 650a 2020 2020 2020 2020 2020 2020 253e  e.            %>
-00001a20: 0a20 2020 2020 2020 2020 2020 2025 6966  .            %if
-00001a30: 2074 6573 7473 3a0a 2020 2020 2020 2020   tests:.        
-00001a40: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
-00001a50: 7373 3d22 666f 726d 2d72 6f77 223e 0a20  ss="form-row">. 
-00001a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001a70: 2020 203c 7461 626c 6520 636c 6173 733d     <table class=
-00001a80: 2267 7269 6422 3e0a 2020 2020 2020 2020  "grid">.        
-00001a90: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001aa0: 3c74 723e 0a20 2020 2020 2020 2020 2020  <tr>.           
-00001ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001ac0: 203c 7464 3e3c 623e 6e61 6d65 3c2f 623e   <td><b>name</b>
-00001ad0: 3c2f 7464 3e0a 2020 2020 2020 2020 2020  </td>.          
+000013e0: 2020 2020 2020 2020 2569 6620 6775 6964          %if guid
+000013f0: 203d 3d20 746f 6f6c 5f6d 6574 6164 6174   == tool_metadat
+00001400: 615f 6469 6374 5b20 2767 7569 6427 205d  a_dict[ 'guid' ]
+00001410: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00001420: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001430: 2020 2020 2020 2020 2020 247b 6775 6964            ${guid
+00001440: 207c 2068 7d20 3c62 3e28 7468 6973 2074   | h} <b>(this t
+00001450: 6f6f 6c29 3c2f 623e 0a20 2020 2020 2020  ool)</b>.       
+00001460: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001470: 2020 2020 2020 2020 2020 2020 2025 656c               %el
+00001480: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00001490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000014a0: 2020 2020 2020 2020 2020 2020 247b 6775              ${gu
+000014b0: 6964 207c 2068 7d0a 2020 2020 2020 2020  id | h}.        
+000014c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000014d0: 2020 2020 2020 2020 2020 2020 2565 6e64              %end
+000014e0: 6966 0a20 2020 2020 2020 2020 2020 2020  if.             
+000014f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001500: 2020 203c 2f74 643e 0a20 2020 2020 2020     </td>.       
+00001510: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001520: 2020 2020 203c 2f74 723e 0a20 2020 2020       </tr>.     
+00001530: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001540: 2020 2025 656e 6466 6f72 0a20 2020 2020     %endfor.     
+00001550: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+00001560: 2f74 6162 6c65 3e0a 2020 2020 2020 2020  /table>.        
+00001570: 2020 2020 2020 2020 2565 6c73 653a 0a20          %else:. 
+00001580: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001590: 2020 204e 6f20 746f 6f6c 2076 6572 7369     No tool versi
+000015a0: 6f6e 7320 6172 6520 6465 6669 6e65 6420  ons are defined 
+000015b0: 666f 7220 7468 6973 2074 6f6f 6c20 736f  for this tool so
+000015c0: 2069 7420 6973 2063 7269 7469 6361 6c20   it is critical 
+000015d0: 7468 6174 2079 6f75 203c 623e 5265 7365  that you <b>Rese
+000015e0: 7420 616c 6c20 7265 706f 7369 746f 7279  t all repository
+000015f0: 206d 6574 6164 6174 613c 2f62 3e20 6672   metadata</b> fr
+00001600: 6f6d 2074 6865 0a20 2020 2020 2020 2020  om the.         
+00001610: 2020 2020 2020 2020 2020 203c 623e 4d61             <b>Ma
+00001620: 6e61 6765 2072 6570 6f73 6974 6f72 793c  nage repository<
+00001630: 2f62 3e20 7061 6765 2e0a 2020 2020 2020  /b> page..      
+00001640: 2020 2020 2020 2020 2020 2565 6e64 6966            %endif
+00001650: 0a20 2020 2020 2020 2020 2020 203c 2f64  .            </d
+00001660: 6976 3e0a 2020 2020 2020 2020 2020 2020  iv>.            
+00001670: 3c64 6976 2063 6c61 7373 3d22 666f 726d  <div class="form
+00001680: 2d72 6f77 223e 0a20 2020 2020 2020 2020  -row">.         
+00001690: 2020 2020 2020 203c 7461 626c 6520 7769         <table wi
+000016a0: 6474 683d 2231 3030 2522 3e0a 2020 2020  dth="100%">.    
+000016b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000016c0: 3c74 7220 6267 636f 6c6f 723d 2223 4438  <tr bgcolor="#D8
+000016d0: 4438 4438 2220 7769 6474 683d 2231 3030  D8D8" width="100
+000016e0: 2522 3e3c 7464 3e3c 623e 5265 7175 6972  %"><td><b>Requir
+000016f0: 656d 656e 7473 2028 6465 7065 6e64 656e  ements (dependen
+00001700: 6369 6573 2064 6566 696e 6564 2069 6e20  cies defined in 
+00001710: 7468 6520 266c 743b 7265 7175 6972 656d  the &lt;requirem
+00001720: 656e 7473 2667 743b 2074 6167 2073 6574  ents&gt; tag set
+00001730: 293c 2f74 643e 3c2f 7472 3e0a 2020 2020  )</td></tr>.    
+00001740: 2020 2020 2020 2020 2020 2020 3c2f 7461              </ta
+00001750: 626c 653e 0a20 2020 2020 2020 2020 2020  ble>.           
+00001760: 203c 2f64 6976 3e0a 2020 2020 2020 2020   </div>.        
+00001770: 2020 2020 3c25 0a20 2020 2020 2020 2020      <%.         
+00001780: 2020 2020 2020 2069 6620 2772 6571 7569         if 'requi
+00001790: 7265 6d65 6e74 7327 2069 6e20 746f 6f6c  rements' in tool
+000017a0: 5f6d 6574 6164 6174 615f 6469 6374 3a0a  _metadata_dict:.
+000017b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000017c0: 2020 2020 7265 7175 6972 656d 656e 7473      requirements
+000017d0: 203d 2074 6f6f 6c5f 6d65 7461 6461 7461   = tool_metadata
+000017e0: 5f64 6963 745b 2027 7265 7175 6972 656d  _dict[ 'requirem
+000017f0: 656e 7473 2720 5d0a 2020 2020 2020 2020  ents' ].        
+00001800: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00001810: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001820: 2020 7265 7175 6972 656d 656e 7473 203d    requirements =
+00001830: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          
+00001840: 2020 253e 0a20 2020 2020 2020 2020 2020    %>.           
+00001850: 2025 6966 2072 6571 7569 7265 6d65 6e74   %if requirement
+00001860: 733a 0a20 2020 2020 2020 2020 2020 2020  s:.             
+00001870: 2020 203c 6469 7620 636c 6173 733d 2266     <div class="f
+00001880: 6f72 6d2d 726f 7722 3e0a 2020 2020 2020  orm-row">.      
+00001890: 2020 2020 2020 2020 2020 2020 2020 3c6c                <l
+000018a0: 6162 656c 3e52 6571 7569 7265 6d65 6e74  abel>Requirement
+000018b0: 733a 3c2f 6c61 6265 6c3e 0a20 2020 2020  s:</label>.     
+000018c0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+000018d0: 7461 626c 6520 636c 6173 733d 2267 7269  table class="gri
+000018e0: 6422 3e0a 2020 2020 2020 2020 2020 2020  d">.            
+000018f0: 2020 2020 2020 2020 2020 2020 3c74 723e              <tr>
+00001900: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001910: 2020 2020 2020 2020 2020 2020 203c 7464               <td
+00001920: 3e3c 623e 6e61 6d65 3c2f 623e 3c2f 7464  ><b>name</b></td
+00001930: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+00001940: 2020 2020 2020 2020 2020 2020 2020 3c74                <t
+00001950: 643e 3c62 3e76 6572 7369 6f6e 3c2f 623e  d><b>version</b>
+00001960: 3c2f 7464 3e0a 2020 2020 2020 2020 2020  </td>.          
+00001970: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001980: 2020 3c74 643e 3c62 3e74 7970 653c 2f62    <td><b>type</b
+00001990: 3e3c 2f74 643e 0a20 2020 2020 2020 2020  ></td>.         
+000019a0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+000019b0: 2f74 723e 0a20 2020 2020 2020 2020 2020  /tr>.           
+000019c0: 2020 2020 2020 2020 2020 2020 2025 666f               %fo
+000019d0: 7220 7265 7175 6972 656d 656e 745f 6469  r requirement_di
+000019e0: 6374 2069 6e20 7265 7175 6972 656d 656e  ct in requiremen
+000019f0: 7473 3a0a 2020 2020 2020 2020 2020 2020  ts:.            
+00001a00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001a10: 3c25 0a20 2020 2020 2020 2020 2020 2020  <%.             
+00001a20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001a30: 2020 2072 6571 7569 7265 6d65 6e74 5f6e     requirement_n
+00001a40: 616d 6520 3d20 7265 7175 6972 656d 656e  ame = requiremen
+00001a50: 745f 6469 6374 5b20 276e 616d 6527 205d  t_dict[ 'name' ]
+00001a60: 206f 7220 276e 6f74 2070 726f 7669 6465   or 'not provide
+00001a70: 6427 0a20 2020 2020 2020 2020 2020 2020  d'.             
+00001a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001a90: 2020 2072 6571 7569 7265 6d65 6e74 5f76     requirement_v
+00001aa0: 6572 7369 6f6e 203d 2072 6571 7569 7265  ersion = require
+00001ab0: 6d65 6e74 5f64 6963 745b 2027 7665 7273  ment_dict[ 'vers
+00001ac0: 696f 6e27 205d 206f 7220 276e 6f74 2070  ion' ] or 'not p
+00001ad0: 726f 7669 6465 6427 0a20 2020 2020 2020  rovided'.       
 00001ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001af0: 2020 3c74 643e 3c62 3e69 6e70 7574 733c    <td><b>inputs<
-00001b00: 2f62 3e3c 2f74 643e 0a20 2020 2020 2020  /b></td>.       
-00001b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001b20: 2020 2020 203c 7464 3e3c 623e 6f75 7470       <td><b>outp
-00001b30: 7574 733c 2f62 3e3c 2f74 643e 0a20 2020  uts</b></td>.   
+00001af0: 2020 2020 2020 2020 2072 6571 7569 7265           require
+00001b00: 6d65 6e74 5f74 7970 6520 3d20 7265 7175  ment_type = requ
+00001b10: 6972 656d 656e 745f 6469 6374 5b20 2774  irement_dict[ 't
+00001b20: 7970 6527 205d 206f 7220 276e 6f74 2070  ype' ] or 'not p
+00001b30: 726f 7669 6465 6427 0a20 2020 2020 2020  rovided'.       
 00001b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001b50: 2020 2020 2020 2020 203c 7464 3e3c 623e           <td><b>
-00001b60: 7265 7175 6972 6564 2066 696c 6573 3c2f  required files</
-00001b70: 623e 3c2f 7464 3e0a 2020 2020 2020 2020  b></td>.        
+00001b50: 2020 2020 2025 3e0a 2020 2020 2020 2020       %>.        
+00001b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001b70: 2020 2020 3c74 723e 0a20 2020 2020 2020      <tr>.       
 00001b80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001b90: 3c2f 7472 3e0a 2020 2020 2020 2020 2020  </tr>.          
-00001ba0: 2020 2020 2020 2020 2020 2020 2020 2566                %f
-00001bb0: 6f72 2074 6573 745f 6469 6374 2069 6e20  or test_dict in 
-00001bc0: 7465 7374 733a 0a20 2020 2020 2020 2020  tests:.         
-00001bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001be0: 2020 203c 250a 2020 2020 2020 2020 2020     <%.          
-00001bf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001c00: 2020 2020 2020 696e 7075 7473 203d 2074        inputs = t
-00001c10: 6573 745f 6469 6374 5b20 2769 6e70 7574  est_dict[ 'input
-00001c20: 7327 205d 0a20 2020 2020 2020 2020 2020  s' ].           
-00001c30: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001c40: 2020 2020 206f 7574 7075 7473 203d 2074       outputs = t
-00001c50: 6573 745f 6469 6374 5b20 276f 7574 7075  est_dict[ 'outpu
-00001c60: 7473 2720 5d0a 2020 2020 2020 2020 2020  ts' ].          
-00001c70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001c80: 2020 2020 2020 7265 7175 6972 6564 5f66        required_f
-00001c90: 696c 6573 203d 2074 6573 745f 6469 6374  iles = test_dict
-00001ca0: 5b20 2772 6571 7569 7265 645f 6669 6c65  [ 'required_file
-00001cb0: 7327 205d 0a20 2020 2020 2020 2020 2020  s' ].           
-00001cc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001cd0: 2025 3e0a 2020 2020 2020 2020 2020 2020   %>.            
-00001ce0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001cf0: 3c74 723e 0a20 2020 2020 2020 2020 2020  <tr>.           
-00001d00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001d10: 2020 2020 203c 7464 3e24 7b74 6573 745f       <td>${test_
-00001d20: 6469 6374 5b20 276e 616d 6527 205d 7c68  dict[ 'name' ]|h
-00001d30: 7d3c 2f74 643e 0a20 2020 2020 2020 2020  }</td>.         
-00001d40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001d50: 2020 2020 2020 203c 7464 3e0a 2020 2020         <td>.    
-00001d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001d70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001d80: 2566 6f72 2069 6e70 7574 2069 6e20 696e  %for input in in
-00001d90: 7075 7473 3a0a 2020 2020 2020 2020 2020  puts:.          
-00001da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001db0: 2020 2020 2020 2020 2020 2020 2020 3c62                <b
-00001dc0: 3e24 7b69 6e70 7574 5b30 5d7c 687d 3a3c  >${input[0]|h}:<
-00001dd0: 2f62 3e20 247b 696e 7075 745b 315d 7c68  /b> ${input[1]|h
-00001de0: 7d3c 6272 2f3e 0a20 2020 2020 2020 2020  }<br/>.         
-00001df0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001e00: 2020 2020 2020 2020 2020 2025 656e 6466             %endf
-00001e10: 6f72 0a20 2020 2020 2020 2020 2020 2020  or.             
-00001e20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001e30: 2020 203c 2f74 643e 0a20 2020 2020 2020     </td>.       
-00001e40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001e50: 2020 2020 2020 2020 203c 7464 3e0a 2020           <td>.  
-00001e60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001e70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001e80: 2020 2566 6f72 206f 7574 7075 7420 696e    %for output in
-00001e90: 206f 7574 7075 7473 3a0a 2020 2020 2020   outputs:.      
-00001ea0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001b90: 2020 2020 2020 2020 203c 7464 3e24 7b72           <td>${r
+00001ba0: 6571 7569 7265 6d65 6e74 5f6e 616d 6520  equirement_name 
+00001bb0: 7c20 687d 3c2f 7464 3e0a 2020 2020 2020  | h}</td>.      
+00001bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001bd0: 2020 2020 2020 2020 2020 3c74 643e 247b            <td>${
+00001be0: 7265 7175 6972 656d 656e 745f 7665 7273  requirement_vers
+00001bf0: 696f 6e20 7c20 687d 3c2f 7464 3e0a 2020  ion | h}</td>.  
+00001c00: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001c10: 2020 2020 2020 2020 2020 2020 2020 3c74                <t
+00001c20: 643e 247b 7265 7175 6972 656d 656e 745f  d>${requirement_
+00001c30: 7479 7065 207c 2068 7d3c 2f74 643e 0a20  type | h}</td>. 
+00001c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001c50: 2020 2020 2020 2020 2020 203c 2f74 723e             </tr>
+00001c60: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001c70: 2020 2020 2020 2020 2025 656e 6466 6f72           %endfor
+00001c80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001c90: 2020 2020 203c 2f74 6162 6c65 3e0a 2020       </table>.  
+00001ca0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001cb0: 2020 3c64 6976 2073 7479 6c65 3d22 636c    <div style="cl
+00001cc0: 6561 723a 2062 6f74 6822 3e3c 2f64 6976  ear: both"></div
+00001cd0: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+00001ce0: 2020 3c2f 6469 763e 0a20 2020 2020 2020    </div>.       
+00001cf0: 2020 2020 2025 656c 7365 3a0a 2020 2020       %else:.    
+00001d00: 2020 2020 2020 2020 2020 2020 3c64 6976              <div
+00001d10: 2063 6c61 7373 3d22 666f 726d 2d72 6f77   class="form-row
+00001d20: 223e 0a20 2020 2020 2020 2020 2020 2020  ">.             
+00001d30: 2020 2020 2020 204e 6f20 7265 7175 6972         No requir
+00001d40: 656d 656e 7473 2064 6566 696e 6564 0a20  ements defined. 
+00001d50: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+00001d60: 2f64 6976 3e0a 2020 2020 2020 2020 2020  /div>.          
+00001d70: 2020 2565 6e64 6966 0a20 2020 2020 2020    %endif.       
+00001d80: 2020 2020 2025 6966 2074 6f6f 6c3a 0a20       %if tool:. 
+00001d90: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+00001da0: 6469 7620 636c 6173 733d 2266 6f72 6d2d  div class="form-
+00001db0: 726f 7722 3e0a 2020 2020 2020 2020 2020  row">.          
+00001dc0: 2020 2020 2020 2020 2020 3c74 6162 6c65            <table
+00001dd0: 2077 6964 7468 3d22 3130 3025 223e 0a20   width="100%">. 
+00001de0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001df0: 2020 2020 2020 203c 7472 2062 6763 6f6c         <tr bgcol
+00001e00: 6f72 3d22 2344 3844 3844 3822 2077 6964  or="#D8D8D8" wid
+00001e10: 7468 3d22 3130 3025 223e 3c74 643e 3c62  th="100%"><td><b
+00001e20: 3e41 6464 6974 696f 6e61 6c20 696e 666f  >Additional info
+00001e30: 726d 6174 696f 6e20 6162 6f75 7420 7468  rmation about th
+00001e40: 6973 2074 6f6f 6c3c 2f74 643e 3c2f 7472  is tool</td></tr
+00001e50: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+00001e60: 2020 2020 2020 3c2f 7461 626c 653e 0a20        </table>. 
+00001e70: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+00001e80: 2f64 6976 3e0a 2020 2020 2020 2020 2020  /div>.          
+00001e90: 2020 2020 2020 3c64 6976 2063 6c61 7373        <div class
+00001ea0: 3d22 666f 726d 2d72 6f77 223e 0a20 2020  ="form-row">.   
 00001eb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001ec0: 2020 3c62 3e24 7b6f 7574 7075 745b 305d    <b>${output[0]
-00001ed0: 7c68 7d3a 3c2f 623e 2024 7b6f 7574 7075  |h}:</b> ${outpu
-00001ee0: 745b 315d 7c68 7d3c 6272 2f3e 0a20 2020  t[1]|h}<br/>.   
-00001ef0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001f00: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001f10: 2025 656e 6466 6f72 0a20 2020 2020 2020   %endfor.       
-00001f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001f30: 2020 2020 2020 2020 203c 2f74 643e 0a20           </td>. 
+00001ec0: 203c 6c61 6265 6c3e 436f 6d6d 616e 643a   <label>Command:
+00001ed0: 3c2f 6c61 6265 6c3e 0a20 2020 2020 2020  </label>.       
+00001ee0: 2020 2020 2020 2020 2020 2020 203c 7072               <pr
+00001ef0: 653e 247b 746f 6f6c 2e63 6f6d 6d61 6e64  e>${tool.command
+00001f00: 207c 2068 7d3c 2f70 7265 3e0a 2020 2020   | h}</pre>.    
+00001f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001f20: 3c64 6976 2073 7479 6c65 3d22 636c 6561  <div style="clea
+00001f30: 723a 2062 6f74 6822 3e3c 2f64 6976 3e0a  r: both"></div>.
 00001f40: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001f50: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00001f60: 7464 3e0a 2020 2020 2020 2020 2020 2020  td>.            
-00001f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001f80: 2020 2020 2020 2020 2566 6f72 2072 6571          %for req
-00001f90: 7569 7265 645f 6669 6c65 2069 6e20 7265  uired_file in re
-00001fa0: 7175 6972 6564 5f66 696c 6573 3a0a 2020  quired_files:.  
+00001f50: 3c2f 6469 763e 0a20 2020 2020 2020 2020  </div>.         
+00001f60: 2020 2020 2020 203c 6469 7620 636c 6173         <div clas
+00001f70: 733d 2266 6f72 6d2d 726f 7722 3e0a 2020  s="form-row">.  
+00001f80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001f90: 2020 3c6c 6162 656c 3e49 6e74 6572 7072    <label>Interpr
+00001fa0: 6574 6572 3a3c 2f6c 6162 656c 3e0a 2020  eter:</label>.  
 00001fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001fc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00001fd0: 2020 2020 2020 247b 7265 7175 6972 6564        ${required
-00001fe0: 5f66 696c 657c 687d 3c62 722f 3e0a 2020  _file|h}<br/>.  
-00001ff0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002010: 2020 2565 6e64 666f 720a 2020 2020 2020    %endfor.      
-00002020: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002030: 2020 2020 2020 2020 2020 3c2f 7464 3e0a            </td>.
-00002040: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002050: 2020 2020 2020 2020 2020 2020 3c2f 7472              </tr
-00002060: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
-00002070: 2020 2020 2020 2020 2020 2565 6e64 666f            %endfo
-00002080: 720a 2020 2020 2020 2020 2020 2020 2020  r.              
-00002090: 2020 2020 2020 3c2f 7461 626c 653e 0a20        </table>. 
-000020a0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-000020b0: 2f64 6976 3e0a 2020 2020 2020 2020 2020  /div>.          
-000020c0: 2020 2565 6c73 653a 0a20 2020 2020 2020    %else:.       
-000020d0: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
-000020e0: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
-000020f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002100: 2020 2020 4e6f 2066 756e 6374 696f 6e61      No functiona
-00002110: 6c20 7465 7374 7320 6465 6669 6e65 640a  l tests defined.
-00002120: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002130: 3c2f 6469 763e 0a20 2020 2020 2020 2020  </div>.         
-00002140: 2020 2025 656e 6469 660a 2020 2020 2020     %endif.      
-00002150: 2020 3c2f 6469 763e 0a20 2020 203c 2f64    </div>.    </d
-00002160: 6976 3e0a 2565 6e64 6966 0a              iv>.%endif.
+00001fc0: 2020 247b 746f 6f6c 2e69 6e74 6572 7072    ${tool.interpr
+00001fd0: 6574 6572 207c 2068 7d0a 2020 2020 2020  eter | h}.      
+00001fe0: 2020 2020 2020 2020 2020 2020 2020 3c64                <d
+00001ff0: 6976 2073 7479 6c65 3d22 636c 6561 723a  iv style="clear:
+00002000: 2062 6f74 6822 3e3c 2f64 6976 3e0a 2020   both"></div>.  
+00002010: 2020 2020 2020 2020 2020 2020 2020 3c2f                </
+00002020: 6469 763e 0a20 2020 2020 2020 2020 2020  div>.           
+00002030: 2020 2020 203c 6469 7620 636c 6173 733d       <div class=
+00002040: 2266 6f72 6d2d 726f 7722 3e0a 2020 2020  "form-row">.    
+00002050: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002060: 3c6c 6162 656c 3e46 6f72 6365 7320 6120  <label>Forces a 
+00002070: 6869 7374 6f72 7920 7265 6672 6573 683a  history refresh:
+00002080: 3c2f 6c61 6265 6c3e 0a20 2020 2020 2020  </label>.       
+00002090: 2020 2020 2020 2020 2020 2020 2024 7b74               ${t
+000020a0: 6f6f 6c2e 666f 7263 655f 6869 7374 6f72  ool.force_histor
+000020b0: 795f 7265 6672 6573 6820 7c20 687d 0a20  y_refresh | h}. 
+000020c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000020d0: 2020 203c 6469 7620 7374 796c 653d 2263     <div style="c
+000020e0: 6c65 6172 3a20 626f 7468 223e 3c2f 6469  lear: both"></di
+000020f0: 763e 0a20 2020 2020 2020 2020 2020 2020  v>.             
+00002100: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
+00002110: 2020 2020 2020 2020 2020 3c25 2070 6172            <% par
+00002120: 616c 6c65 6c69 736d 5f69 6e66 6f20 3d20  allelism_info = 
+00002130: 746f 6f6c 2e70 6172 616c 6c65 6c69 736d  tool.parallelism
+00002140: 2025 3e0a 2020 2020 2020 2020 2020 2020   %>.            
+00002150: 2020 2020 2569 6620 7061 7261 6c6c 656c      %if parallel
+00002160: 6973 6d5f 696e 666f 3a0a 2020 2020 2020  ism_info:.      
+00002170: 2020 2020 2020 2020 2020 2020 2020 3c64                <d
+00002180: 6976 2063 6c61 7373 3d22 666f 726d 2d72  iv class="form-r
+00002190: 6f77 223e 0a20 2020 2020 2020 2020 2020  ow">.           
+000021a0: 2020 2020 2020 2020 2020 2020 203c 7461               <ta
+000021b0: 626c 6520 7769 6474 683d 2231 3030 2522  ble width="100%"
+000021c0: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+000021d0: 2020 2020 2020 2020 2020 2020 2020 3c74                <t
+000021e0: 7220 6267 636f 6c6f 723d 2223 4438 4438  r bgcolor="#D8D8
+000021f0: 4438 2220 7769 6474 683d 2231 3030 2522  D8" width="100%"
+00002200: 3e3c 7464 3e3c 623e 5061 7261 6c6c 656c  ><td><b>Parallel
+00002210: 6973 6d3c 2f74 643e 3c2f 7472 3e0a 2020  ism</td></tr>.  
+00002220: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002230: 2020 2020 2020 3c2f 7461 626c 653e 0a20        </table>. 
+00002240: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002250: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
+00002260: 2020 2020 2020 2020 2020 2020 2020 3c64                <d
+00002270: 6976 2063 6c61 7373 3d22 666f 726d 2d72  iv class="form-r
+00002280: 6f77 223e 0a20 2020 2020 2020 2020 2020  ow">.           
+00002290: 2020 2020 2020 2020 2020 2020 203c 6c61               <la
+000022a0: 6265 6c3e 4d65 7468 6f64 3a3c 2f6c 6162  bel>Method:</lab
+000022b0: 656c 3e0a 2020 2020 2020 2020 2020 2020  el>.            
+000022c0: 2020 2020 2020 2020 2020 2020 247b 7061              ${pa
+000022d0: 7261 6c6c 656c 6973 6d5f 696e 666f 2e6d  rallelism_info.m
+000022e0: 6574 686f 6420 7c20 687d 0a20 2020 2020  ethod | h}.     
+000022f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002300: 2020 203c 6469 7620 7374 796c 653d 2263     <div style="c
+00002310: 6c65 6172 3a20 626f 7468 223e 3c2f 6469  lear: both"></di
+00002320: 763e 0a20 2020 2020 2020 2020 2020 2020  v>.             
+00002330: 2020 2020 2020 203c 2f64 6976 3e0a 2020         </div>.  
+00002340: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002350: 2020 2566 6f72 206b 6579 2c20 7661 6c20    %for key, val 
+00002360: 696e 2070 6172 616c 6c65 6c69 736d 5f69  in parallelism_i
+00002370: 6e66 6f2e 6174 7472 6962 7574 6573 2e69  nfo.attributes.i
+00002380: 7465 6d73 2829 3a0a 2020 2020 2020 2020  tems():.        
+00002390: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000023a0: 3c64 6976 2063 6c61 7373 3d22 666f 726d  <div class="form
+000023b0: 2d72 6f77 223e 0a20 2020 2020 2020 2020  -row">.         
+000023c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000023d0: 2020 203c 6c61 6265 6c3e 247b 6b65 797d     <label>${key}
+000023e0: 3a3c 2f6c 6162 656c 3e0a 2020 2020 2020  :</label>.      
+000023f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002400: 2020 2020 2020 247b 7661 6c20 7c20 687d        ${val | h}
+00002410: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002420: 2020 2020 2020 2020 2020 2020 203c 6469               <di
+00002430: 7620 7374 796c 653d 2263 6c65 6172 3a20  v style="clear: 
+00002440: 626f 7468 223e 3c2f 6469 763e 0a20 2020  both"></div>.   
+00002450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002460: 2020 2020 203c 2f64 6976 3e0a 2020 2020       </div>.    
+00002470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002480: 2565 6e64 666f 720a 2020 2020 2020 2020  %endfor.        
+00002490: 2020 2020 2020 2020 2565 6e64 6966 0a20          %endif. 
+000024a0: 2020 2020 2020 2020 2020 2025 656e 6469             %endi
+000024b0: 660a 2020 2020 2020 2020 2020 2020 3c64  f.            <d
+000024c0: 6976 2063 6c61 7373 3d22 666f 726d 2d72  iv class="form-r
+000024d0: 6f77 223e 0a20 2020 2020 2020 2020 2020  ow">.           
+000024e0: 2020 2020 203c 7461 626c 6520 7769 6474       <table widt
+000024f0: 683d 2231 3030 2522 3e0a 2020 2020 2020  h="100%">.      
+00002500: 2020 2020 2020 2020 2020 2020 2020 3c74                <t
+00002510: 7220 6267 636f 6c6f 723d 2223 4438 4438  r bgcolor="#D8D8
+00002520: 4438 2220 7769 6474 683d 2231 3030 2522  D8" width="100%"
+00002530: 3e3c 7464 3e3c 623e 4675 6e63 7469 6f6e  ><td><b>Function
+00002540: 616c 2074 6573 7473 3c2f 7464 3e3c 2f74  al tests</td></t
+00002550: 723e 0a20 2020 2020 2020 2020 2020 2020  r>.             
+00002560: 2020 203c 2f74 6162 6c65 3e0a 2020 2020     </table>.    
+00002570: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
+00002580: 2020 2020 2020 2020 2020 203c 250a 2020             <%.  
+00002590: 2020 2020 2020 2020 2020 2020 2020 6966                if
+000025a0: 2027 7465 7374 7327 2069 6e20 746f 6f6c   'tests' in tool
+000025b0: 5f6d 6574 6164 6174 615f 6469 6374 3a0a  _metadata_dict:.
+000025c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000025d0: 2020 2020 7465 7374 7320 3d20 746f 6f6c      tests = tool
+000025e0: 5f6d 6574 6164 6174 615f 6469 6374 5b20  _metadata_dict[ 
+000025f0: 2774 6573 7473 2720 5d0a 2020 2020 2020  'tests' ].      
+00002600: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
+00002610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002620: 2020 2020 7465 7374 7320 3d20 4e6f 6e65      tests = None
+00002630: 0a20 2020 2020 2020 2020 2020 2025 3e0a  .            %>.
+00002640: 2020 2020 2020 2020 2020 2020 2569 6620              %if 
+00002650: 7465 7374 733a 0a20 2020 2020 2020 2020  tests:.         
+00002660: 2020 2020 2020 203c 6469 7620 636c 6173         <div clas
+00002670: 733d 2266 6f72 6d2d 726f 7722 3e0a 2020  s="form-row">.  
+00002680: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002690: 2020 3c74 6162 6c65 2063 6c61 7373 3d22    <table class="
+000026a0: 6772 6964 223e 0a20 2020 2020 2020 2020  grid">.         
+000026b0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+000026c0: 7472 3e0a 2020 2020 2020 2020 2020 2020  tr>.            
+000026d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000026e0: 3c74 643e 3c62 3e6e 616d 653c 2f62 3e3c  <td><b>name</b><
+000026f0: 2f74 643e 0a20 2020 2020 2020 2020 2020  /td>.           
+00002700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002710: 203c 7464 3e3c 623e 696e 7075 7473 3c2f   <td><b>inputs</
+00002720: 623e 3c2f 7464 3e0a 2020 2020 2020 2020  b></td>.        
+00002730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002740: 2020 2020 3c74 643e 3c62 3e6f 7574 7075      <td><b>outpu
+00002750: 7473 3c2f 623e 3c2f 7464 3e0a 2020 2020  ts</b></td>.    
+00002760: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002770: 2020 2020 2020 2020 3c74 643e 3c62 3e72          <td><b>r
+00002780: 6571 7569 7265 6420 6669 6c65 733c 2f62  equired files</b
+00002790: 3e3c 2f74 643e 0a20 2020 2020 2020 2020  ></td>.         
+000027a0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+000027b0: 2f74 723e 0a20 2020 2020 2020 2020 2020  /tr>.           
+000027c0: 2020 2020 2020 2020 2020 2020 2025 666f               %fo
+000027d0: 7220 7465 7374 5f64 6963 7420 696e 2074  r test_dict in t
+000027e0: 6573 7473 3a0a 2020 2020 2020 2020 2020  ests:.          
+000027f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002800: 2020 3c25 0a20 2020 2020 2020 2020 2020    <%.           
+00002810: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002820: 2020 2020 2069 6e70 7574 7320 3d20 7465       inputs = te
+00002830: 7374 5f64 6963 745b 2027 696e 7075 7473  st_dict[ 'inputs
+00002840: 2720 5d0a 2020 2020 2020 2020 2020 2020  ' ].            
+00002850: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002860: 2020 2020 6f75 7470 7574 7320 3d20 7465      outputs = te
+00002870: 7374 5f64 6963 745b 2027 6f75 7470 7574  st_dict[ 'output
+00002880: 7327 205d 0a20 2020 2020 2020 2020 2020  s' ].           
+00002890: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000028a0: 2020 2020 2072 6571 7569 7265 645f 6669       required_fi
+000028b0: 6c65 7320 3d20 7465 7374 5f64 6963 745b  les = test_dict[
+000028c0: 2027 7265 7175 6972 6564 5f66 696c 6573   'required_files
+000028d0: 2720 5d0a 2020 2020 2020 2020 2020 2020  ' ].            
+000028e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000028f0: 253e 0a20 2020 2020 2020 2020 2020 2020  %>.             
+00002900: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+00002910: 7472 3e0a 2020 2020 2020 2020 2020 2020  tr>.            
+00002920: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002930: 2020 2020 3c74 643e 247b 7465 7374 5f64      <td>${test_d
+00002940: 6963 745b 2027 6e61 6d65 2720 5d7d 3c2f  ict[ 'name' ]}</
+00002950: 7464 3e0a 2020 2020 2020 2020 2020 2020  td>.            
+00002960: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002970: 2020 2020 3c74 643e 0a20 2020 2020 2020      <td>.       
+00002980: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002990: 2020 2020 2020 2020 2020 2020 2025 666f               %fo
+000029a0: 7220 696e 7075 7420 696e 2069 6e70 7574  r input in input
+000029b0: 733a 0a20 2020 2020 2020 2020 2020 2020  s:.             
+000029c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000029d0: 2020 2020 2020 2020 2020 203c 623e 247b             <b>${
+000029e0: 696e 7075 745b 305d 7d3a 3c2f 623e 2024  input[0]}:</b> $
+000029f0: 7b69 6e70 7574 5b31 5d20 7c20 687d 3c62  {input[1] | h}<b
+00002a00: 722f 3e0a 2020 2020 2020 2020 2020 2020  r/>.            
+00002a10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002a20: 2020 2020 2020 2020 2565 6e64 666f 720a          %endfor.
+00002a30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002a40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002a50: 3c2f 7464 3e0a 2020 2020 2020 2020 2020  </td>.          
+00002a60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002a70: 2020 2020 2020 3c74 643e 0a20 2020 2020        <td>.     
+00002a80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002a90: 2020 2020 2020 2020 2020 2020 2020 2025                 %
+00002aa0: 666f 7220 6f75 7470 7574 2069 6e20 6f75  for output in ou
+00002ab0: 7470 7574 733a 0a20 2020 2020 2020 2020  tputs:.         
+00002ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002ad0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+00002ae0: 623e 247b 6f75 7470 7574 5b30 5d7d 3a3c  b>${output[0]}:<
+00002af0: 2f62 3e20 247b 6f75 7470 7574 5b31 5d20  /b> ${output[1] 
+00002b00: 7c20 687d 3c62 722f 3e0a 2020 2020 2020  | h}<br/>.      
+00002b10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002b20: 2020 2020 2020 2020 2020 2020 2020 2565                %e
+00002b30: 6e64 666f 720a 2020 2020 2020 2020 2020  ndfor.          
+00002b40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002b50: 2020 2020 2020 3c2f 7464 3e0a 2020 2020        </td>.    
+00002b60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002b70: 2020 2020 2020 2020 2020 2020 3c74 643e              <td>
+00002b80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002b90: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002ba0: 2020 2020 2025 666f 7220 7265 7175 6972       %for requir
+00002bb0: 6564 5f66 696c 6520 696e 2072 6571 7569  ed_file in requi
+00002bc0: 7265 645f 6669 6c65 733a 0a20 2020 2020  red_files:.     
+00002bd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002be0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002bf0: 2020 2024 7b72 6571 7569 7265 645f 6669     ${required_fi
+00002c00: 6c65 207c 2068 7d3c 6272 2f3e 0a20 2020  le | h}<br/>.   
+00002c10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002c20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002c30: 2025 656e 6466 6f72 0a20 2020 2020 2020   %endfor.       
+00002c40: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002c50: 2020 2020 2020 2020 203c 2f74 643e 0a20           </td>. 
+00002c60: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002c70: 2020 2020 2020 2020 2020 203c 2f74 723e             </tr>
+00002c80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002c90: 2020 2020 2020 2020 2025 656e 6466 6f72           %endfor
+00002ca0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002cb0: 2020 2020 203c 2f74 6162 6c65 3e0a 2020       </table>.  
+00002cc0: 2020 2020 2020 2020 2020 2020 2020 3c2f                </
+00002cd0: 6469 763e 0a20 2020 2020 2020 2020 2020  div>.           
+00002ce0: 2025 656c 7365 3a0a 2020 2020 2020 2020   %else:.        
+00002cf0: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
+00002d00: 7373 3d22 666f 726d 2d72 6f77 223e 0a20  ss="form-row">. 
+00002d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002d20: 2020 204e 6f20 6675 6e63 7469 6f6e 616c     No functional
+00002d30: 2074 6573 7473 2064 6566 696e 6564 0a20   tests defined. 
+00002d40: 2020 2020 2020 2020 2020 2020 2020 203c                 <
+00002d50: 2f64 6976 3e0a 2020 2020 2020 2020 2020  /div>.          
+00002d60: 2020 2565 6e64 6966 0a20 2020 2020 2020    %endif.       
+00002d70: 203c 2f64 6976 3e0a 2020 2020 3c2f 6469   </div>.    </di
+00002d80: 763e 0a25 656e 6469 660a                 v>.%endif.
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/base/base_panels.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/base/base_panels.mako`

 * *Files 5% similar despite different names*

```diff
@@ -17,63 +17,53 @@
 </%def>
 
 ## Default stylesheets
 <%def name="stylesheets()">
     <!--- base/base_panels.mako stylesheets() -->
     ${h.css(
         'bootstrap-tour',
+    )}
+    ${h.dist_css(
         'base'
     )}
 </%def>
 
 ## Default javascripts
 ## TODO: remove when all libs are required directly in modules
 <%def name="javascripts()">
     <!--- base/base_panels.mako javascripts() -->
-    ${h.js(
-        'bundled/libs.chunk',
-        'bundled/base.chunk'
+    ${h.dist_js(
+        'libs.bundled',
     )}
     ${ javascript_entry() }
 </%def>
 
 <%def name="javascript_entry()">
     <!-- base/base_panels.mako javascript_entry -->
-    ${ h.js('bundled/generic.bundled')}
+    ${ h.dist_js('toolshed.bundled')}
 </%def>
 
 <%def name="javascript_app()">
     <!--- base/base_panels.mako javascript_app() -->
     ${ galaxy_client.load() }
 </%def>
 
 ## Default late-load javascripts
 <%def name="late_javascripts()">
     <!--- base/base_panels.mako late_javascripts() -->
 
-    <script type="text/javascript">
-
-        var panelConfig = {
-            left_panel: ${h.to_js_bool(self.has_left_panel)},
-            right_panel: ${h.to_js_bool(self.has_right_panel)},
-            rightPanelSelector: '#right',
-            leftPanelSelector: '#left'
-        };
-
-        // "late javascripts"
-        config.addInitialization(function() {
-            console.log("base/base_panels.mako, panel init");
-            window.bundleEntries.panelManagement(panelConfig);
-        });
-
-    </script>
-
     %if t.webapp.name == 'galaxy' and app.config.ga_code:
         ${galaxy_client.config_google_analytics(app.config.ga_code)}
     %endif
+    %if t.webapp.name == 'galaxy' and app.config.plausible_server and app.config.plausible_domain:
+        ${ galaxy_client.config_plausible_analytics(app.config.plausible_server, app.config.plausible_domain) }
+    %endif
+    %if t.webapp.name == 'galaxy' and app.config.matomo_server and app.config.matomo_site_id:
+        ${ galaxy_client.config_matomo_analytics(app.config.matomo_server, app.config.matomo_site_id) }
+    %endif
 
 </%def>
 
 ## Masthead
 <%def name="masthead()">
     ## Override
 </%def>
@@ -108,22 +98,18 @@
             </div>
         </div>
     </div>
 </%def>
 
 ## Document
 <html>
-    <!--base_panels.mako-->
+    <!-- toolshed webapp base_panels.mako-->
     ${self.init()}
     <head>
         <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
-        ## For mobile browsers, don't scale up
-        <meta name = "viewport" content = "maximum-scale=1.0">
-        ## Force IE to standards mode, and prefer Google Chrome Frame if the user has already installed it
-        <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
 
         <title>
             Galaxy
             %if app.config.brand:
             | ${app.config.brand}
             %endif
             | ${self.title()}
@@ -166,15 +152,15 @@
         %endif
         <div id="everything">
 
             ## Background displays first
             <div id="background"></div>
 
             ## Layer iframes over backgrounds
-            <div id="masthead" class="navbar navbar-fixed-top navbar-inverse">
+            <div>
                 ${self.masthead()}
             </div>
 
             %if self.message_box_visible:
                 <div id="messagebox" class="panel-${app.config.message_box_class}-message" style="display:block">
                     ${app.config.message_box_content}
                 </div>
```

#### html2text {}

```diff
@@ -1,38 +1,41 @@
  <%namespace name="galaxy_client" file="/galaxy_client_app.mako" /> <%
 self.has_left_panel = hasattr( self, 'left_panel' ) self.has_right_panel =
 hasattr( self, 'right_panel' ) self.message_box_visible =
 app.config.message_box_visible self.show_inactivity_warning = False
 self.overlay_visible=False self.active_view=None self.body_class=""
 self.require_javascript=False %> <%def name="init()"> ## Override
 def> ## Default stylesheets <%def name="stylesheets()">  ${h.css( 'bootstrap-
-tour', 'base' )}
+tour', )} ${h.dist_css( 'base' )}
 def> ## Default javascripts ## TODO: remove when all libs are required directly
-in modules <%def name="javascripts()">  ${h.js( 'bundled/libs.chunk', 'bundled/
-base.chunk' )} ${ javascript_entry() }
-def> <%def name="javascript_entry()">  ${ h.js('bundled/generic.bundled')}
+in modules <%def name="javascripts()">  ${h.dist_js( 'libs.bundled', )} $
+{ javascript_entry() }
+def> <%def name="javascript_entry()">  ${ h.dist_js('toolshed.bundled')}
 def> <%def name="javascript_app()">  ${ galaxy_client.load() }
-def> ## Default late-load javascripts <%def name="late_javascripts()">
- %if t.webapp.name == 'galaxy' and app.config.ga_code: $
-{galaxy_client.config_google_analytics(app.config.ga_code)} %endif
+def> ## Default late-load javascripts <%def name="late_javascripts()">  %if
+t.webapp.name == 'galaxy' and app.config.ga_code: $
+{galaxy_client.config_google_analytics(app.config.ga_code)} %endif %if
+t.webapp.name == 'galaxy' and app.config.plausible_server and
+app.config.plausible_domain: ${ galaxy_client.config_plausible_analytics
+(app.config.plausible_server, app.config.plausible_domain) } %endif %if
+t.webapp.name == 'galaxy' and app.config.matomo_server and
+app.config.matomo_site_id: ${ galaxy_client.config_matomo_analytics
+(app.config.matomo_server, app.config.matomo_site_id) } %endif
 def> ## Masthead <%def name="masthead()"> ## Override
 def> <%def name="overlay( title='', content='', visible=False )"> <%def
 name="title()">
 def> <%def name="content()">
 def> <% if visible: display = "style='display: block;'" overlay_class = "in"
 else: display = "style='display: none;'" overlay_class = "" %>
 {display}>
 ×
 *** ${title} ***
 ${content}
 def> ## Document
  ${self.init()}
- ## For mobile browsers, don't scale up
- ## Force IE to standards mode, and prefer Google Chrome Frame if the user has
-already installed it
 ## relative href for site root
  ${self.stylesheets()} ## Normally, we'd put all the javascripts at the bottom
 of the
 ## but during this transitional period we need access to the config ##
 functions which are only available when these scripts are executed ## and I
 can't yet control when the templates are going to write scripts ## to the
 output ${self.javascripts()} ${self.javascript_app()}
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/base.mako` & `galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/base.mako`

 * *Files 21% similar despite different names*

```diff
@@ -4,18 +4,25 @@
 <% _=n_ %>
 <!DOCTYPE HTML>
 <html>
     <!--base.mako-->
     ${self.init()}
     <head>
         <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
-        ## For mobile browsers, don't scale up
-        <meta name = "viewport" content = "maximum-scale=1.0">
-        ## Force IE to standards mode, and prefer Google Chrome Frame if the user has already installed it
-        <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
+
+        <!-- Set meta description -->
+        <%
+            if request.path.startswith('/login'):
+                meta_description = "Log in to Galaxy to get access to more tools and resources. Register now for a free account."
+            elif request.path.startswith('/workflows'):
+                meta_description = "Galaxy Workflows facilitate rigorous, reproducible analysis pipelines that can be shared with the community."
+            else:
+                meta_description = "Galaxy is a community-driven web-based analysis platform for life science research."
+        %>
+        <meta name="description" content="${meta_description}" />
 
         <title>
             Galaxy
             %if app.config.brand:
             | ${app.config.brand}
             %endif
             | ${self.title()}
@@ -40,38 +47,39 @@
 
 ## Default init
 <%def name="init()"></%def>
 
 ## Default stylesheets
 <%def name="stylesheets()">
     ${h.css('bootstrap-tour')}
-    ${h.css('base')}
+    ${h.dist_css('base')}
 </%def>
 
 ## Default javascripts
 <%def name="javascripts()">
     ## TODO: remove when all libs are required directly in modules
-    ${h.js(
-        'bundled/libs.chunk',
-        'bundled/base.chunk'
+    ${h.dist_js(
+        'libs.bundled',
+        'generic.bundled'
     )}
-    ${self.javascript_entry()}
-</%def>
-
-<%def name="javascript_entry()">
-    ${h.js('bundled/generic.bundled')}
 </%def>
 
 <%def name="javascript_app()">
 
     ${ galaxy_client.load( app=self.js_app ) }
     ${ galaxy_client.config_sentry( app=self.js_app ) }
     %if self.js_app and self.js_app.config and self.js_app.config.ga_code:
         ${ galaxy_client.config_google_analytics(self.js_app.config.ga_code) }
     %endif
+    %if self.js_app and self.js_app.config and self.js_app.config.plausible_server and self.js_app.config.plausible_domain:
+        ${ galaxy_client.config_plausible_analytics(self.js_app.config.plausible_server, self.js_app.config.plausible_domain) }
+    %endif
+    %if self.js_app and self.js_app.config and self.js_app.config.matomo_server and self.js_app.config.matomo_site_id:
+        ${ galaxy_client.config_matomo_analytics(self.js_app.config.matomo_server, self.js_app.config.matomo_site_id) }
+    %endif
 
     %if not form_input_auto_focus is UNDEFINED and form_input_auto_focus:
         <script type="text/javascript">
             // Auto Focus on first item on form
             config.addInitialization(function() {
                 console.log("base.mako", "auto focus on first item on form");
                 if ( $("*:focus").html() == null ) {
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/display_common.mako` & `galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/display_common.mako`

 * *Files 1% similar despite different names*

```diff
@@ -121,18 +121,15 @@
     %>
 </%def>
 
 ## Returns item user/owner.
 <%def name="get_item_user( item )">
     <%
         # Exceptions first, default last.
-        if isinstance( item, model.HistoryDatasetAssociation ):
-            return item.history.user
-        else:
-            return item.user
+        return item.user
     %>
 </%def>
 
 ## Returns item slug.
 <%def name="get_item_slug( item )">
     <%
         # Exceptions first, default last.
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/galaxy_client_app.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/galaxy_client_app.mako`

 * *Files 25% similar despite different names*

```diff
@@ -66,14 +66,49 @@
             ga('send', 'pageview');
         %else:
             console.warn("Missing google analytics code");
         %endif
     </script>
 </%def>
 
+<%def name="config_plausible_analytics(plausible_server, plausible_domain)">
+    %if plausible_server and plausible_domain:
+        <script async defer data-domain="${plausible_domain}" src="${plausible_server}/js/plausible.js"></script>
+    %else:
+        <script>console.warn("Missing plausible server or plausible domain");</script>
+    %endif
+</%def>
+
+<%def name="config_matomo_analytics(matomo_server, matomo_site_id)">
+    <script type="text/javascript">
+        console.log("config_matomo_analytics matomo_server:", '${matomo_server}');
+        console.log("config_matomo_analytics matomo_site_id:", '${matomo_site_id}');
+        %if ga_code:
+            var _paq = window._paq = window._paq || [];
+            /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
+            _paq.push(['trackPageView']);
+            _paq.push(['enableLinkTracking']);
+            (function () {
+                var u = "${matomo_server}/";
+                _paq.push(['setTrackerUrl', u + 'matomo.php']);
+                _paq.push(['setSiteId', '${matomo_site_id}']);
+                var d = document;
+                var g = d.createElement('script');
+                var s = d.getElementsByTagName('script')[0];
+                g.type = 'text/javascript';
+                g.async = true;
+                g.src = u + 'matomo.js';
+                s.parentNode.insertBefore(g, s);
+            })();
+        %else:
+            console.warn("Missing matomo server or matomo site id");
+        %endif
+    </script>
+</%def>
+
 
 ## ----------------------------------------------------------------------------
 <%def name="get_user_dict()">
     ## Return a dictionary of user or anonymous user data including:
     ##  email, id, disk space used, quota percent, and tags used
     <%
         from markupsafe import escape
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/legacy/grid_base.mako` & `galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/legacy/grid_base.mako`

 * *Files 4% similar despite different names*

```diff
@@ -1,33 +1,23 @@
 <%!
-    import six
     from galaxy.util import unicodify
-    from galaxy.webapps.reports.framework.grids import TextColumn
+    from galaxy.web.legacy_framework.grids import TextColumn
 
     def inherit(context):
         kwargs = context.get( 'kwargs', {} )
         if kwargs.get( 'embedded', False ):
             # No inheritance - using only embeddable content (self.body)
             return None
-        if context.get('use_panels'):
-            if context.get('webapp'):
-                app_name = context.get('webapp')
-            elif context.get('app'):
-                app_name = context.get('app').name
-            else:
-                app_name = 'galaxy'
-            return '/webapps/%s/base_panels.mako' % app_name
-        else:
-            return '/base.mako'
+        return '/base.mako'
 %>
 <%inherit file="${inherit(context)}"/>
 <%namespace file="/display_common.mako" import="get_class_plural" />
 
 ##
-## Override methods from base.mako and base_panels.mako
+## Override methods from base.mako
 ##
 
 <%def name="init( embedded=False, insert=None )">
 <%
     self.has_left_panel         = False
     self.has_right_panel        = False
     self.message_box_visible    = False
@@ -206,15 +196,15 @@
                 ## target
                 target = column.target
 
                 ## get value
                 value = column.get_value( trans, grid, item )
 
                 # Handle non-ascii chars.
-                if isinstance(value, six.binary_type):
+                if isinstance(value, bytes):
                     value = unicodify(value, 'utf-8')
                     value = value.replace('/', '//')
                 endif
 
                 ## Item dictionary
                 item_dict['column_config'][column.label] = {
                     'link'      : link,
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/message.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/message.mako`

 * *Files 0% similar despite different names*

```diff
@@ -59,9 +59,9 @@
         if status == "done":
             status = "success"
         elif status == "error":
             status = "danger"
         if status not in ("danger", "info", "success", "warning"):
             status = "info"
     %>
-    <div class="message mt-2 alert alert-${status}">${_(sanitize_html(msg))}</div>
+    <div class="message mt-2 alert alert-${status}">${sanitize_html(msg)}</div>
 </%def>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/refresh_frames.mako` & `galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/refresh_frames.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/center.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/center.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group_create.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group_create.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group_rename.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/group/group_rename.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role_create.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role_create.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role_rename.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/dataset_security/role/role_rename.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/index.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/index.mako`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,14 @@
                 top.location.href = location.href;
             }
         </script>
     %endif
 </%def>
 
 <%def name="left_panel()">
-    <% can_review_repositories = trans.app.security_agent.user_can_review_repositories( trans.user ) %>
     <div class="unified-panel-header" unselectable="on">
         <div class='unified-panel-header-inner'>Administration</div>
     </div>
     <div class="unified-panel-body">
         <div class="toolMenu">
             <div class="toolSectionList">
                 <div class="toolSectionTitle">
```

#### html2text {}

```diff
@@ -3,16 +3,15 @@
 "base.css" for styling tool menu and forms (details) ${h.css( "base" )} ## But
 make sure styles for the layout take precedence ${parent.stylesheets()}
 def> <%def name="javascripts()"> ${parent.javascripts()}
 def> <%def name="init()"> ${parent.init()} <% self.has_left_panel=True
 self.has_right_panel=False self.active_view="tools" %> %if
 trans.app.config.require_login and not trans.user:
  %endif
-def> <%def name="left_panel()"> <% can_review_repositories =
-trans.app.security_agent.user_can_review_repositories( trans.user ) %>
+def> <%def name="left_panel()">
 Administration
 Repositories
 Browse_by_category
 Browse_all_repositories
 Reset_selected_metadata
 Browse_metadata
 Categories
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/statistics.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/statistics.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/user/reset_password.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/user/reset_password.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/admin/user/user.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/admin/user/user.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/base_panels.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/base_panels.mako`

 * *Files 11% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 </%def>
 
 <%def name="javascripts()">
     ${parent.javascripts()}
     <script type="text/javascript">
         config.addInitialization(function() {
             console.log("toolshed/base_panels.mako", "hardcoded dropdown init");
-            
+
             // Masthead dropdown menus
             var $dropdowns = $("#masthead ul.nav > li.dropdown > .dropdown-menu");
             $("body").on( "click.nav_popups", function( e ) {
                 $dropdowns.hide();
                 $("#dd-helper").hide();
                 // If the target is in the menu, treat normally
                 if ( $(e.target).closest( "#masthead ul.nav > li.dropdown > .dropdown-menu" ).length ) {
@@ -44,139 +44,140 @@
 
 ## Masthead
 <%def name="masthead()">
 
     %if app.config.ga_code:
         ${ galaxy_client.config_google_analytics(app.config.ga_code)}
     %endif
+    %if app.config.plausible_server and app.config.plausible_domain:
+        ${ galaxy_client.config_plausible_analytics(app.config.plausible_server, app.config.plausible_domain) }
+    %endif
+    %if app.config.matomo_server:
+        ${ galaxy_client.config_matomo_analytics(app.config.matomo_server) }
+    %endif
 
     ## start main tag
-    <nav id="masthead" class="navbar navbar-expand fixed-top justify-content-center navbar-dark">
-
-      ## Logo, layered over tabs to be clickable
-      <a href="${h.url_for( app.config.get( 'logo_url', '/' ) )}" aria-label="homepage" class="navbar-brand">
-          <img alt="logo" class="navbar-brand-image" src="${h.url_for('/static/favicon.png')}">
-          <span class="navbar-brand-title">
-          Galaxy Tool Shed
-          %if app.config.brand:
-              / ${app.config.brand}
-          %endif
-      </a>
-
-
-    ## Tab area, fills entire width
-            <ul class="navbar-nav">
-                <%def name="tab( id, display, href, target='_parent', visible=True, extra_class='', menu_options=None )">
-                    <%
-                    cls = "nav-item"
-                    a_cls = "nav-link"
-                    extra = ""
-                    if extra_class:
-                        cls = extra_class
-                    if self.active_view == id:
-                        cls += " active"
-                    if menu_options:
-                        cls += " dropdown"
-                        a_cls += " dropdown-toggle"
-                        extra = "<b class='caret'></b>"
-                    style = ""
-                    if not visible:
-                        style = "display: none;"
-                    %>
-
-                    <li class="${cls}" style="${style}">
-                      <a
-                        %if href:
-                            class="${a_cls}" target="${target}" href="${href}"
-                        %else:
-                            class="${a_cls}"
-                        %endif
-						%if menu_options:
-							role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"
-						%endif
-                        >
-							${display}${extra}
-						</a>
-                        %if menu_options:
-                            <div class="dropdown-menu" aria-labelledby="navbarDropdown">
-                                %for menu_item in menu_options:
-                                    %if not menu_item:
-                                        <div class="dropdown-divider"></div>
-                                    %else:
-                                        %if len ( menu_item ) == 1:
-                                            <a class="dropdown-item" href="javascript:void(0)" role="button">
-                                                ${menu_item[0]}
-                                            </a>
-                                        %elif len ( menu_item ) == 2:
-                                            <% name, link = menu_item %>
-                                            <a class="dropdown-item" href="${link}">${name | h}</a>
-                                        %else:
-                                            <% name, link, target = menu_item %>
-                                            <a class="dropdown-item" target="${target}" href="${link}">${name | h}</a>
-                                        %endif
-                                    %endif
-                                %endfor
-                            </div>
-                        %endif
-                    </li>
-                </%def>
-
-                ## Repositories tab.
-                ${tab( "repositories", "Repositories", h.url_for( controller='/repository', action='index' ) )}
+    <nav id="masthead" class="masthead-toolshed navbar navbar-expand navbar-fixed-top justify-content-center navbar-dark">
 
-                ## Groups tab.
-                ${tab( "groups", "Groups", h.url_for( controller='/groups', action='index' ) )}
-
-                ## Admin tab.
-                ${tab( "admin", "Admin", h.url_for( controller='/admin', action='index' ), extra_class="admin-only", visible=( trans.user and app.config.is_admin_user( trans.user ) ) )}
-
-                ## Help tab.
+        ## Logo, layered over tabs to be clickable
+        <a href="${h.url_for( app.config.get( 'logo_url', '/' ) )}" aria-label="homepage" class="navbar-brand">
+            <img alt="logo" class="navbar-brand-image" src="${h.url_for('/static/favicon.svg')}">
+            <span class="navbar-brand-title">
+                Tool Shed
+                %if app.config.brand:
+                    / ${app.config.brand}
+                %endif
+            </span>
+        </a>
+
+        ## Tab area, fills entire width
+        <ul class="navbar-nav">
+            <%def name="tab( id, display, href, target='_parent', visible=True, extra_class='', menu_options=None )">
                 <%
-                    menu_options = []
-                    qa_url = app.config.get( "qa_url", None )
-                    if qa_url:
-                        menu_options = [ [_('Galaxy Q&A'), qa_url, "_blank" ] ]
-                    menu_options.extend( [
-                        [_('Tool Shed Wiki'), app.config.get( "wiki_url", "https://galaxyproject.org/toolshed" ), "_blank" ],
-                        [_('Support'), app.config.get( "support_url", "https://galaxyproject.org/support" ), "_blank" ],
-                        [_('Search'), app.config.get( "search_url", "http://galaxyproject.org/search/" ), "_blank" ],
-                        [_('Mailing Lists'), app.config.get( "mailing_lists_url", "https://galaxyproject.org/mailing-lists" ), "_blank" ],
-                        [_('Videos'), app.config.get( "screencasts_url", "https://vimeo.com/galaxyproject" ), "_blank" ],
-                        [_('Wiki'), app.config.get( "wiki_url", "http://galaxyproject.org/" ), "_blank" ],
-                        [_('How to Cite Galaxy'), app.config.get( "citation_url", "https://galaxyproject.org/citing-galaxy" ), "_blank" ]
-                    ] )
-                    tab( "help", _("Help"), None, menu_options=menu_options )
+                cls = "nav-item"
+                a_cls = "nav-link"
+                extra = ""
+                if extra_class:
+                    cls = extra_class
+                if self.active_view == id:
+                    cls += " active"
+                if menu_options:
+                    cls += " dropdown"
+                    a_cls += " dropdown-toggle"
+                    extra = "<b class='caret'></b>"
+                style = ""
+                if not visible:
+                    style = "display: none;"
                 %>
 
-                ## User tabs.
-                <%
-                    from markupsafe import escape
-                    # Menu for user who is not logged in.
-                    menu_options = [ [ _("Login"), h.url_for( controller='/user', action='login' ), "galaxy_main" ] ]
-                    if app.config.allow_user_creation:
-                        menu_options.append( [ _("Register"), h.url_for( controller='/user', action='create', cntrller='user' ), "galaxy_main" ] )
-                    extra_class = "loggedout-only"
-                    visible = ( trans.user == None )
-                    tab( "user", _("User"), None, visible=visible, menu_options=menu_options )
-                    # Menu for user who is logged in.
-                    if trans.user:
-                        email = escape( trans.user.email )
-                    else:
-                        email = ""
-                    menu_options = [ [ 'Logged in as <span id="user-email">%s</span>' %  email ] ]
-                    if app.config.use_remote_user:
-                        if app.config.remote_user_logout_href:
-                            menu_options.append( [ _('Logout'), app.config.remote_user_logout_href, "_top" ] )
-                    else:
-                        menu_options.append( [ _('Preferences'), h.url_for( controller='/user', action='index', cntrller='user' ), "galaxy_main" ] )
-                        menu_options.append( [ _('API Keys'), h.url_for( controller='/user', action='api_keys', cntrller='user' ), "galaxy_main" ] )
-                        logout_url = h.url_for( controller='/user', action='logout' )
-                        menu_options.append( [ 'Logout', logout_url, "_top" ] )
-                    if app.config.use_remote_user:
-                        menu_options.append( [ _('Public Name'), h.url_for( controller='/user', action='edit_username', cntrller='user' ), "galaxy_main" ] )
-
-                    extra_class = "loggedin-only"
-                    visible = ( trans.user != None )
-                    tab( "user", "User", None, visible=visible, menu_options=menu_options )
-                %>
-            </ul>
+                <li class="${cls}" style="${style}">
+                    <a
+                    %if href:
+                        class="${a_cls}" target="${target}" href="${href}"
+                    %else:
+                        class="${a_cls}"
+                    %endif
+                    %if menu_options:
+                        role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"
+                    %endif
+                    >
+                        ${display}${extra}
+                    </a>
+                    %if menu_options:
+                        <div class="dropdown-menu" aria-labelledby="navbarDropdown">
+                            %for menu_item in menu_options:
+                                %if not menu_item:
+                                    <div class="dropdown-divider"></div>
+                                %else:
+                                    %if len ( menu_item ) == 1:
+                                        <a class="dropdown-item" href="javascript:void(0)" role="button">
+                                            ${menu_item[0]}
+                                        </a>
+                                    %elif len ( menu_item ) == 2:
+                                        <% name, link = menu_item %>
+                                        <a class="dropdown-item" href="${link}">${name | h}</a>
+                                    %else:
+                                        <% name, link, target = menu_item %>
+                                        <a class="dropdown-item" target="${target}" href="${link}">${name | h}</a>
+                                    %endif
+                                %endif
+                            %endfor
+                        </div>
+                    %endif
+                </li>
+            </%def>
+
+            ## Repositories tab.
+            ${tab( "repositories", "Repositories", h.url_for( controller='/repository', action='index' ) )}
+
+            ## Groups tab.
+            ${tab( "groups", "Groups", h.url_for( controller='/groups', action='index' ) )}
+
+            ## Admin tab.
+            ${tab( "admin", "Admin", h.url_for( controller='/admin', action='index' ), extra_class="admin-only", visible=( trans.user and app.config.is_admin_user( trans.user ) ) )}
+
+            ## Help tab.
+            <%
+                menu_options = []
+                menu_options.extend( [
+                    ['About Tool Shed', app.config.get( "wiki_url", "https://galaxyproject.org/toolshed" ), "_blank" ],
+                    ['Support', app.config.get( "support_url", "https://galaxyproject.org/support" ), "_blank" ],
+                    ['Videos', app.config.get( "screencasts_url", "https://vimeo.com/galaxyproject" ), "_blank" ],
+                    ['How to Cite Tool Shed', app.config.get( "citation_url", "https://galaxyproject.org/citing-galaxy" ), "_blank" ]
+                ] )
+                tab( "help", "Help", None, menu_options=menu_options )
+            %>
+
+            ## User tabs.
+            <%
+                from markupsafe import escape
+                # Menu for user who is not logged in.
+                menu_options = [ [ "Login", h.url_for( controller='/user', action='login' ), "galaxy_main" ] ]
+                if app.config.allow_user_creation:
+                    menu_options.append( [ "Register", h.url_for( controller='/user', action='create', cntrller='user' ), "galaxy_main" ] )
+                extra_class = "loggedout-only"
+                visible = ( trans.user == None )
+                tab( "user", "User", None, visible=visible, menu_options=menu_options )
+                # Menu for user who is logged in.
+                if trans.user:
+                    email = escape( trans.user.email )
+                else:
+                    email = ""
+                menu_options = [ [ 'Logged in as <span id="user-email">%s</span>' %  email ] ]
+                if app.config.use_remote_user:
+                    if app.config.remote_user_logout_href:
+                        menu_options.append( [ 'Logout', app.config.remote_user_logout_href, "_top" ] )
+                else:
+                    menu_options.append( [ 'Preferences', h.url_for( controller='/user', action='index', cntrller='user' ), "galaxy_main" ] )
+                    menu_options.append( [ 'API Keys', h.url_for( controller='/user', action='api_keys', cntrller='user' ), "galaxy_main" ] )
+                    logout_url = h.url_for( controller='/user', action='logout' )
+                    menu_options.append( [ 'Logout', logout_url, "_top" ] )
+                if app.config.use_remote_user:
+                    menu_options.append( [ 'Public Name', h.url_for( controller='/user', action='edit_username', cntrller='user' ), "galaxy_main" ] )
+
+                extra_class = "loggedin-only"
+                visible = ( trans.user != None )
+                tab( "user", "User", None, visible=visible, menu_options=menu_options )
+            %>
+        </ul>
+    </nav>
 </%def>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/category/create_category.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/category/create_category.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/category/edit_category.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/category/edit_category.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/common.mako` & `galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/tool_shed/common/common.mako`

 * *Files 2% similar despite different names*

```diff
@@ -105,15 +105,14 @@
 <%def name="render_star_rating( name, rating, disabled=False )">
     <%
         if disabled:
             disabled_str = ' disabled="disabled"'
         else:
             disabled_str = ''
         html = ''
-        rating = rating or 0
         for index in range( 1, 6 ):
             html += '<input name="%s" type="radio" class="star" value="%s" %s' % ( str( name ), str( index ), disabled_str )
             if rating > ( index - 0.5 ) and rating < ( index + 0.5 ):
                 html += ' checked="checked"'
             html += '/>'
     %>
     ${html}
```

#### html2text {}

```diff
@@ -29,15 +29,15 @@
       ( deprecated_repository_dependency_tup ) msg += '
     * Revision %s of repository %s owned by %s
     * ' % \ ( changeset_revision, name, owner ) msg += '
 ' %> This repository depends upon the following deprecated repositories
 ${msg}
 def> <%def name="render_star_rating( name, rating, disabled=False )"> <% if
 disabled: disabled_str = ' disabled="disabled"' else: disabled_str = '' html =
-'' rating = rating or 0 for index in range( 1, 6 ): html += '
+'' for index in range( 1, 6 ): html += '
 s' % ( str( name ), str( index ), disabled_str ) if rating > ( index - 0.5 )
 and rating < ( index + 0.5 ): html += ' checked="checked"' html += '/>' %> $
 {html}
 def> <%def name="render_long_description( description_text )">
 Detailed description:
 ${description_text}
 def> <%def name="render_multiple_heads_message( heads )">
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/grid_common.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/common/grid_common.mako`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 <%!
-    from galaxy.webapps.reports.framework.grids import TextColumn, StateColumn, GridColumnFilter
+    from galaxy.web.legacy_framework.grids import TextColumn, StateColumn, GridColumnFilter
     from galaxy.web.framework.helpers import iff
 %>
 
 ## Render a filter UI for a grid column. Filter is rendered as a table row.
 <%def name="render_grid_column_filter( grid, column )">
     <tr>
         <%
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-<%! from galaxy.webapps.reports.framework.grids import TextColumn, StateColumn,
+<%! from galaxy.web.legacy_framework.grids import TextColumn, StateColumn,
 GridColumnFilter from galaxy.web.framework.helpers import iff %> ## Render a
 filter UI for a grid column. Filter is rendered as a table row. <%def
 name="render_grid_column_filter( grid, column )">
 <% column_label = column.label if column.filterable == "advanced": column_label
 = column_label.lower() %> %if column.filterable == "advanced":
 ${column_label}:
 %endif
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/repository_actions_menu.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/common/repository_actions_menu.mako`

 * *Files 15% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 <%inherit file="/base.mako"/>
 
 <%def name="render_tool_shed_repository_actions( repository, metadata=None, changeset_revision=None )">
     <%
-        from tool_shed.util.review_util import can_browse_repository_reviews, changeset_revision_reviewed_by_user, get_review_by_repository_id_changeset_revision_user_id
         from tool_shed.util.metadata_util import is_malicious
 
         if repository.metadata_revisions:
             has_metadata = True
         else:
             has_metadata = False
 
@@ -30,19 +29,14 @@
         if is_malicious( trans.app, trans.security.encode_id( repository.id ), repository.tip() ):
             changeset_is_malicious = True
         else:
             changeset_is_malicious = False
 
         can_browse_contents = not is_new
 
-        if can_browse_repository_reviews( trans.app, trans.user, repository ):
-            can_browse_reviews = True
-        else:
-            can_browse_reviews = False
-
         if trans.user and trans.user != repository.user:
             can_contact_owner = True
         else:
             can_contact_owner = False
         
         if not is_new and trans.user and ( is_admin or repository.user == trans.user ) and not is_deprecated:
             can_deprecate = True
@@ -69,36 +63,14 @@
         else:
             can_upload = False
 
         if not is_new and not is_deprecated and trans.user and repository.user != trans.user:
             can_rate = True
         else:
             can_rate = False
-        
-        if metadata is not None and changeset_revision is not None:
-            if has_metadata and not is_deprecated and trans.app.security_agent.user_can_review_repositories( trans.user ):
-                can_review_repository = True
-            else:
-                can_review_repository = False
-            if changeset_revision_reviewed_by_user( trans.user, repository, changeset_revision ):
-                reviewed_by_user = True
-            else:
-                reviewed_by_user = False
-        else:
-            can_review_repository = False
-            reviewed_by_user = False
-
-        if reviewed_by_user:
-            review = get_review_by_repository_id_changeset_revision_user_id( app=trans.app,
-                                                                             repository_id=trans.security.encode_id( repository.id ),
-                                                                             changeset_revision=changeset_revision,
-                                                                             user_id=trans.security.encode_id( trans.user.id ) )
-            review_id = trans.security.encode_id( review.id )
-        else:
-            review_id = None
 
         if not is_new and not is_deprecated:
             can_set_metadata = True
         else:
             can_set_metadata = False
 
         if changeset_revision is not None:
@@ -130,24 +102,14 @@
             %endif
             %if can_undeprecate:
                 <a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository', action='deprecate', id=trans.security.encode_id( repository.id ), mark_deprecated=False )}">Mark repository as not deprecated</a>
             %endif
         %else:
             <li><a class="action-button" id="repository-${repository.id}-popup" class="menubutton">Repository Actions</a></li>
             <div popupmenu="repository-${repository.id}-popup">
-                %if can_review_repository:
-                    %if reviewed_by_user:
-                        <a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository_review', action='edit_review', id=review_id )}">Manage my review of this revision</a>
-                    %else:
-                        <a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository_review', action='create_review', id=trans.app.security.encode_id( repository.id ), changeset_revision=changeset_revision )}">Add a review to this revision</a>
-                    %endif
-                %endif
-                %if can_browse_reviews:
-                    <a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository_review', action='manage_repository_reviews', id=trans.app.security.encode_id( repository.id ) )}">Browse reviews of this repository</a>
-                %endif
                 %if can_upload:
                     <a class="action-button" target="galaxy_main" href="${h.url_for( controller='upload', action='upload', repository_id=trans.security.encode_id( repository.id ) )}">Upload files to repository</a>
                 %endif
                 %if can_administer:
                     <a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository', action='manage_repository', id=trans.app.security.encode_id( repository.id ), changeset_revision=repository.tip() )}">Manage repository</a>
                 %else:
                     <a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository', action='view_repository', id=trans.app.security.encode_id( repository.id ), changeset_revision=repository.tip() )}">View repository</a>
@@ -189,15 +151,14 @@
     </ul>
 </%def>
 
 <%def name="render_galaxy_repository_actions( repository=None )">
     <br/><br/>
     <ul class="manage-table-actions">
         %if repository:
-            <li><a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository', action='install_repositories_by_revision', repository_ids=trans.security.encode_id( repository.id ), changeset_revisions=changeset_revision )}">Install to Galaxy</a></li>
             <li><a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository', action='preview_tools_in_changeset', repository_id=trans.security.encode_id( repository.id ), changeset_revision=changeset_revision )}">Browse repository</a></li>
             <li><a class="action-button" id="repository-${repository.id}-popup" class="menubutton">Tool Shed Actions</a></li>
             <div popupmenu="repository-${repository.id}-popup">
                 <a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository', action='browse_valid_categories' )}">Browse valid repositories</a>
                 <a class="action-button" target="galaxy_main" href="${h.url_for( controller='repository', action='find_tools' )}">Search for valid tools</a>
             </div>
         %else:
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/common/reset_metadata_on_selected_repositories.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/common/reset_metadata_on_selected_repositories.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/group/index.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/group/index.mako`

 * *Files 2% similar despite different names*

```diff
@@ -25,12 +25,12 @@
     %endif
 </%def>
 
 <%def name="center_panel()">
     <script type="text/javascript">
         window.globalTS = new Object();
         $( function(){
-            new window.bundleEntries.ToolshedGroups.ToolshedGroups();
+            new window.bundleToolshed.ToolshedGroups.ToolshedGroups();
         });
     </script>
     <div id="groups_element" style="width: 95%; margin:auto; margin-top:2em; "></div>
 </%def>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/index.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/index.mako`

 * *Files 20% similar despite different names*

```diff
@@ -1,14 +1,11 @@
 <%inherit file="/webapps/tool_shed/base_panels.mako"/>
 <%namespace file="/message.mako" import="render_msg" />
 
 <%def name="stylesheets()">
-    ## Include "base.css" for styling tool menu and forms (details)
-    ${h.css( "base" )}
-
     ## But make sure styles for the layout take precedence
     ${parent.stylesheets()}
 
 </%def>
 
 <%def name="javascripts()">
     ${parent.javascripts()}
@@ -27,15 +24,14 @@
                 top.location.href = location.href;
             }
         </script>
     %endif
 </%def>
 
 <%def name="left_panel()">
-    <% can_review_repositories = trans.app.security_agent.user_can_review_repositories( trans.user ) %>
     <div class="unified-panel-header" unselectable="on">
         <div class='unified-panel-header-inner'>${trans.app.shed_counter.unique_valid_tools | h} valid tools on ${util.unicodify( trans.app.shed_counter.generation_time ) | h}</div>
     </div>
     <div class="unified-panel-body">
         <div class="toolMenu">
             <div class="toolSectionList">
                 %if user_id or repository_id:
@@ -92,19 +88,14 @@
                                 <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories_i_own' )}">Repositories I own</a>
                             </div>
                             %if can_administer_repositories:
                                 <div class="toolTitle">
                                     <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories_i_can_administer' )}">Repositories I can administer</a>
                                 </div>
                             %endif
-                            %if has_reviewed_repositories:
-                                <div class="toolTitle">
-                                    <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories', operation='reviewed_repositories_i_own' )}">Reviewed repositories I own</a>
-                                </div>
-                            %endif
                             %if has_deprecated_repositories:
                                 <div class="toolTitle">
                                     <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_deprecated_repositories_i_own' )}">Deprecated repositories I own</a>
                                 </div>
                             %endif
                             <div class="toolTitle">
                                 <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_my_writable_repositories' )}">My writable repositories</a>
@@ -127,67 +118,14 @@
                             <a target="galaxy_main" href="${h.url_for( controller='repository', action='create_repository' )}">Create new repository</a>
                         </div>
                         %if trans.app.config.enable_galaxy_flavor_docker_image:
                             <div class="toolTitle">
                                 <a target="galaxy_main" href="${h.url_for( controller='repository', action='create_galaxy_docker_image' )}">Create Galaxy Docker Image</a>
                             </div>
                         %endif
-                        %if can_review_repositories:
-                            <div class="toolSectionPad"></div>
-                            <div class="toolSectionTitle">
-                                Reviewing Repositories
-                            </div>
-                            <div class="toolSectionBody">
-                                <div class="toolSectionBg">
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository_review', action='manage_repositories_ready_for_review' )}">Repositories ready for review</a>
-                                    </div>
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository_review', action='manage_repositories_without_reviews' )}">All repositories with no reviews</a>
-                                    </div>
-                                    %if trans.user.repository_reviews:
-                                        <div class="toolTitle">
-                                            <a target="galaxy_main" href="${h.url_for( controller='repository_review', action='manage_repositories_reviewed_by_me' )}">Repositories reviewed by me</a>
-                                        </div>
-                                    %endif
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository_review', action='manage_repositories_with_reviews' )}">All reviewed repositories</a>
-                                    </div>
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository_review', action='manage_components' )}">Manage review components</a>
-                                    </div>
-                                </div>
-                            </div>
-                            <div class="toolSectionPad"></div>
-                            <div class="toolSectionTitle">
-                                Reviewing Repositories With Tools
-                            </div>
-                            <div class="toolSectionBody">
-                                <div class="toolSectionBg">
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories_missing_tool_test_components' )}">Latest revision: missing tool tests</a>
-                                    </div>
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories_with_install_errors' )}">Latest revision: installation errors</a>
-                                    </div>
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories_with_failing_tool_tests' )}">Latest revision: failing tool tests</a>
-                                    </div>
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories_with_skip_tool_test_checked' )}">Latest revision: skip tool tests</a>
-                                    </div>
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories_with_no_failing_tool_tests' )}">Latest revision: all tool tests pass</a>
-                                    </div>
-                                    <div class="toolTitle">
-                                        <a target="galaxy_main" href="${h.url_for( controller='repository', action='browse_repositories_with_invalid_tools' )}">Latest revision: invalid tools</a>
-                                    </div>
-                                </div>
-                            </div>
-                        %endif
                     %else:
                         <div class="toolSectionPad"></div>
                         <div class="toolSectionTitle">
                             Available Actions
                         </div>
                         <div class="toolTitle">
                             <a target="galaxy_main" href="${h.url_for( controller='/user', action='login' )}">Login to create a repository</a>
```

#### html2text {}

```diff
@@ -1,18 +1,16 @@
 <%inherit file="/webapps/tool_shed/base_panels.mako"/> <%namespace file="/
-message.mako" import="render_msg" /> <%def name="stylesheets()"> ## Include
-"base.css" for styling tool menu and forms (details) ${h.css( "base" )} ## But
-make sure styles for the layout take precedence ${parent.stylesheets()}
+message.mako" import="render_msg" /> <%def name="stylesheets()"> ## But make
+sure styles for the layout take precedence ${parent.stylesheets()}
 def> <%def name="javascripts()"> ${parent.javascripts()}
 def> <%def name="init()"> ${parent.init()} <% self.has_left_panel=True
 self.has_right_panel=False self.active_view="tools" %> %if
 trans.app.config.require_login and not trans.user:
  %endif
-def> <%def name="left_panel()"> <% can_review_repositories =
-trans.app.security_agent.user_can_review_repositories( trans.user ) %>
+def> <%def name="left_panel()">
 ${trans.app.shed_counter.unique_valid_tools | h} valid tools on $
 {util.unicodify( trans.app.shed_counter.generation_time ) | h}
 %if user_id or repository_id: ## The route in was a sharable url, and may have
 included a changeset_revision, although we don't check for it.
 All Repositories
 Browse_by_category
 %else: %if repository_metadata:
@@ -28,44 +26,26 @@
 Browse_by_category
 %if trans.user: %if trans.user.active_repositories or
 can_administer_repositories:
 Repositories I Can Change
 Repositories_I_own
 %if can_administer_repositories:
 Repositories_I_can_administer
-%endif %if has_reviewed_repositories:
-Reviewed_repositories_I_own
 %endif %if has_deprecated_repositories:
 Deprecated_repositories_I_own
 %endif
 My_writable_repositories
 Reset_metadata_on_my_repositories
 Latest_revision:_missing_tool_tests
 Latest_revision:_invalid_tools
 %endif
 Available Actions
 Create_new_repository
 %if trans.app.config.enable_galaxy_flavor_docker_image:
 Create_Galaxy_Docker_Image
-%endif %if can_review_repositories:
-Reviewing Repositories
-Repositories_ready_for_review
-All_repositories_with_no_reviews
-%if trans.user.repository_reviews:
-Repositories_reviewed_by_me
-%endif
-All_reviewed_repositories
-Manage_review_components
-Reviewing Repositories With Tools
-Latest_revision:_missing_tool_tests
-Latest_revision:_installation_errors
-Latest_revision:_failing_tool_tests
-Latest_revision:_skip_tool_tests
-Latest_revision:_all_tool_tests_pass
-Latest_revision:_invalid_tools
 %endif %else:
 Available Actions
 Login_to_create_a_repository
 %if trans.app.config.enable_galaxy_flavor_docker_image:
 Create_Galaxy_Docker_Image
 %endif %endif %endif
 def> <%def name="center_panel()"> <% if trans.app.config.require_login and not
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/common.mako` & `galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/tool_shed/repository/common.mako`

 * *Files 0% similar despite different names*

```diff
@@ -73,15 +73,15 @@
 </%def>
 
 <%def name="container_javascripts()">
     <script type="text/javascript">
         config.addInitialization(function() {
             console.log("common.mako, container_javascripts");
 
-            var store = window.bundleEntries.store;
+            var store = window.bundleToolshed.store;
             var init_dependencies = function() {
                 var storage_id = "library-expand-state-${trans.security.encode_id(10000)}";
                 var restore_folder_state = function() {
                     var state = store.get(storage_id);
                     if (state) {
                         for (var id in state) {
                             if (state[id] === true) {
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/contact_owner.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/contact_owner.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/create_repository.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/create_repository.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/docker_image_repositories.mako` & `galaxy-web-apps-23.0.2/galaxy/webapps/base/templates/webapps/reports/registered_users_specified_month.mako`

 * *Files 24% similar despite different names*

```diff
@@ -1,137 +1,109 @@
-00000000: 3c25 6e61 6d65 7370 6163 6520 6669 6c65  <%namespace file
-00000010: 3d22 2f6d 6573 7361 6765 2e6d 616b 6f22  ="/message.mako"
-00000020: 2069 6d70 6f72 743d 2272 656e 6465 725f   import="render_
-00000030: 6d73 6722 202f 3e0a 0a3c 2521 0a20 2020  msg" />..<%!.   
-00000040: 6465 6620 696e 6865 7269 7428 636f 6e74  def inherit(cont
-00000050: 6578 7429 3a0a 2020 2020 2020 2069 6620  ext):.       if 
-00000060: 636f 6e74 6578 742e 6765 7428 2775 7365  context.get('use
-00000070: 5f70 616e 656c 7327 293a 0a20 2020 2020  _panels'):.     
-00000080: 2020 2020 2020 7265 7475 726e 2027 2f77        return '/w
-00000090: 6562 6170 7073 2f74 6f6f 6c5f 7368 6564  ebapps/tool_shed
-000000a0: 2f62 6173 655f 7061 6e65 6c73 2e6d 616b  /base_panels.mak
-000000b0: 6f27 0a20 2020 2020 2020 656c 7365 3a0a  o'.       else:.
-000000c0: 2020 2020 2020 2020 2020 2072 6574 7572             retur
-000000d0: 6e20 272f 6261 7365 2e6d 616b 6f27 0a25  n '/base.mako'.%
-000000e0: 3e0a 0a3c 2569 6e68 6572 6974 2066 696c  >..<%inherit fil
-000000f0: 653d 2224 7b69 6e68 6572 6974 2863 6f6e  e="${inherit(con
-00000100: 7465 7874 297d 222f 3e0a 0a3c 2564 6566  text)}"/>..<%def
-00000110: 206e 616d 653d 2273 7479 6c65 7368 6565   name="styleshee
-00000120: 7473 2829 223e 0a20 2020 2024 7b70 6172  ts()">.    ${par
-00000130: 656e 742e 7374 796c 6573 6865 6574 7328  ent.stylesheets(
-00000140: 297d 0a3c 2f25 6465 663e 0a0a 3c25 6465  )}.</%def>..<%de
-00000150: 6620 6e61 6d65 3d22 6a61 7661 7363 7269  f name="javascri
-00000160: 7074 7328 2922 3e0a 2020 2020 247b 7061  pts()">.    ${pa
-00000170: 7265 6e74 2e6a 6176 6173 6372 6970 7473  rent.javascripts
-00000180: 2829 7d0a 3c2f 2564 6566 3e0a 0a25 6966  ()}.</%def>..%if
-00000190: 206d 6573 7361 6765 3a0a 2020 2020 247b   message:.    ${
-000001a0: 7265 6e64 6572 5f6d 7367 2820 6d65 7373  render_msg( mess
-000001b0: 6167 652c 2073 7461 7475 7320 297d 0a25  age, status )}.%
-000001c0: 656e 6469 660a 0a3c 6469 7620 636c 6173  endif..<div clas
-000001d0: 733d 2274 6f6f 6c46 6f72 6d22 3e0a 2020  s="toolForm">.  
-000001e0: 2020 3c64 6976 2063 6c61 7373 3d22 746f    <div class="to
-000001f0: 6f6c 466f 726d 426f 6479 223e 0a20 2020  olFormBody">.   
-00000200: 2020 2020 203c 6469 7620 636c 6173 733d       <div class=
-00000210: 2266 6f72 6d2d 726f 7722 3e0a 2020 2020  "form-row">.    
-00000220: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
-00000230: 7373 3d22 7761 726e 696e 676d 6573 7361  ss="warningmessa
-00000240: 6765 223e 0a20 2020 2020 2020 2020 2020  ge">.           
-00000250: 2020 2020 2043 6c69 636b 2074 6865 203c       Click the <
-00000260: 623e 4372 6561 7465 2044 6f63 6b65 7220  b>Create Docker 
-00000270: 496d 6167 653c 2f62 3e20 6275 7474 6f6e  Image</b> button
-00000280: 2062 656c 6f77 2074 6f20 6372 6561 7465   below to create
-00000290: 2061 2044 6f63 6b65 7220 496d 6167 6520   a Docker Image 
-000002a0: 7468 6174 2077 696c 6c20 696e 7374 616c  that will instal
-000002b0: 6c20 7468 6520 666f 6c6c 6f77 696e 6720  l the following 
-000002c0: 7265 706f 7369 746f 7269 6573 2e0a 2020  repositories..  
-000002d0: 2020 2020 2020 2020 2020 3c2f 6469 763e            </div>
-000002e0: 0a20 2020 2020 2020 2020 2020 203c 6469  .            <di
-000002f0: 7620 7374 796c 653d 2263 6c65 6172 3a20  v style="clear: 
-00000300: 626f 7468 223e 3c2f 6469 763e 0a20 2020  both"></div>.   
-00000310: 2020 2020 203c 2f64 6976 3e0a 2020 2020       </div>.    
-00000320: 3c2f 6469 763e 0a3c 2f64 6976 3e0a 3c64  </div>.</div>.<d
-00000330: 6976 2063 6c61 7373 3d22 746f 6f6c 466f  iv class="toolFo
-00000340: 726d 223e 0a20 2020 203c 6469 7620 636c  rm">.    <div cl
-00000350: 6173 733d 2274 6f6f 6c46 6f72 6d54 6974  ass="toolFormTit
-00000360: 6c65 223e 5265 706f 7369 746f 7269 6573  le">Repositories
-00000370: 2066 6f72 2069 6e63 6c75 7369 6f6e 2069   for inclusion i
-00000380: 6e20 446f 636b 6572 2049 6d61 6765 3c2f  n Docker Image</
-00000390: 6469 763e 0a20 2020 2020 2020 203c 666f  div>.        <fo
-000003a0: 726d 2069 643d 2264 6f63 6b65 725f 696d  rm id="docker_im
-000003b0: 6167 655f 666f 726d 2220 6e61 6d65 3d22  age_form" name="
-000003c0: 646f 636b 6572 5f69 6d61 6765 5f66 6f72  docker_image_for
-000003d0: 6d22 2061 6374 696f 6e3d 2224 7b68 2e75  m" action="${h.u
-000003e0: 726c 5f66 6f72 2820 636f 6e74 726f 6c6c  rl_for( controll
-000003f0: 6572 3d27 7265 706f 7369 746f 7279 272c  er='repository',
-00000400: 2061 6374 696f 6e3d 2763 7265 6174 655f   action='create_
-00000410: 6761 6c61 7879 5f64 6f63 6b65 725f 696d  galaxy_docker_im
-00000420: 6167 6527 2029 7d22 2065 6e63 7479 7065  age' )}" enctype
-00000430: 3d22 6d75 6c74 6970 6172 742f 666f 726d  ="multipart/form
-00000440: 2d64 6174 6122 206d 6574 686f 643d 2270  -data" method="p
-00000450: 6f73 7422 3e0a 2020 2020 2020 2020 2020  ost">.          
-00000460: 2020 3c64 6976 2063 6c61 7373 3d22 666f    <div class="fo
-00000470: 726d 2d72 6f77 223e 0a20 2020 2020 2020  rm-row">.       
-00000480: 2020 2020 2020 2020 203c 696e 7075 7420           <input 
-00000490: 7479 7065 3d22 6869 6464 656e 2220 6e61  type="hidden" na
-000004a0: 6d65 3d22 6964 2220 7661 6c75 653d 2224  me="id" value="$
-000004b0: 7b69 647d 2220 2f3e 0a20 2020 2020 2020  {id}" />.       
-000004c0: 2020 2020 203c 2f64 6976 3e0a 2020 2020       </div>.    
-000004d0: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
-000004e0: 7373 3d22 666f 726d 2d72 6f77 223e 0a20  ss="form-row">. 
-000004f0: 2020 2020 2020 2020 2020 2020 2020 203c                 <
-00000500: 7461 626c 6520 636c 6173 733d 2267 7269  table class="gri
-00000510: 6422 3e0a 2020 2020 2020 2020 2020 2020  d">.            
-00000520: 2020 2020 2020 2020 3c74 723e 0a20 2020          <tr>.   
+00000000: 3c25 696e 6865 7269 7420 6669 6c65 3d22  <%inherit file="
+00000010: 2f62 6173 652e 6d61 6b6f 222f 3e0a 3c25  /base.mako"/>.<%
+00000020: 6e61 6d65 7370 6163 6520 6669 6c65 3d22  namespace file="
+00000030: 2f6d 6573 7361 6765 2e6d 616b 6f22 2069  /message.mako" i
+00000040: 6d70 6f72 743d 2272 656e 6465 725f 6d73  mport="render_ms
+00000050: 6722 202f 3e0a 0a25 6966 206d 6573 7361  g" />..%if messa
+00000060: 6765 3a0a 2020 2020 247b 7265 6e64 6572  ge:.    ${render
+00000070: 5f6d 7367 2820 6d65 7373 6167 652c 2027  _msg( message, '
+00000080: 646f 6e65 2720 297d 0a25 656e 6469 660a  done' )}.%endif.
+00000090: 0a3c 6469 7620 636c 6173 733d 2272 6570  .<div class="rep
+000000a0: 6f72 7422 3e0a 2020 2020 3c64 6976 2063  ort">.    <div c
+000000b0: 6c61 7373 3d22 7265 706f 7274 426f 6479  lass="reportBody
+000000c0: 223e 0a20 2020 2020 2020 203c 6833 2061  ">.        <h3 a
+000000d0: 6c69 676e 3d22 6365 6e74 6572 223e 0a20  lign="center">. 
+000000e0: 2020 2020 2020 2020 2020 2055 7365 7220             User 
+000000f0: 5265 6769 7374 7261 7469 6f6e 7320 666f  Registrations fo
+00000100: 7220 247b 6d6f 6e74 685f 6c61 6265 6c7d  r ${month_label}
+00000110: 266e 6273 703b 247b 7965 6172 5f6c 6162  &nbsp;${year_lab
+00000120: 656c 7d0a 2020 2020 2020 2020 3c2f 6833  el}.        </h3
+00000130: 3e0a 2020 2020 2020 2020 3c68 3420 616c  >.        <h4 al
+00000140: 6967 6e3d 2263 656e 7465 7222 3e0a 2020  ign="center">.  
+00000150: 2020 2020 2020 2020 2020 436c 6963 6b20            Click 
+00000160: 4461 7920 746f 2073 6565 2075 7365 7220  Day to see user 
+00000170: 7265 6769 7374 7261 7469 6f6e 7320 666f  registrations fo
+00000180: 7220 7468 6174 2064 6179 0a20 2020 2020  r that day.     
+00000190: 2020 203c 2f68 343e 0a20 2020 2020 2020     </h4>.       
+000001a0: 203c 7461 626c 6520 616c 6967 6e3d 2263   <table align="c
+000001b0: 656e 7465 7222 2077 6964 7468 3d22 3630  enter" width="60
+000001c0: 2522 2063 6c61 7373 3d22 636f 6c6f 7265  %" class="colore
+000001d0: 6422 3e0a 2020 2020 2020 2020 2020 2020  d">.            
+000001e0: 2569 6620 6c65 6e28 2075 7365 7273 2029  %if len( users )
+000001f0: 203d 3d20 303a 0a20 2020 2020 2020 2020   == 0:.         
+00000200: 2020 2020 2020 203c 7472 3e0a 2020 2020         <tr>.    
+00000210: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000220: 3c74 6420 636f 6c73 7061 6e3d 2232 223e  <td colspan="2">
+00000230: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000240: 2020 2020 2020 2020 2054 6865 7265 2061           There a
+00000250: 7265 206e 6f20 7573 6572 2072 6567 6973  re no user regis
+00000260: 7472 6174 696f 6e73 2066 6f72 0a20 2020  trations for.   
+00000270: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000280: 2020 2020 2024 7b6d 6f6e 7468 5f6c 6162       ${month_lab
+00000290: 656c 7d26 6e62 7370 3b24 7b79 6561 725f  el}&nbsp;${year_
+000002a0: 6c61 6265 6c7d 0a20 2020 2020 2020 2020  label}.         
+000002b0: 2020 2020 2020 2020 2020 203c 2f74 643e             </td>
+000002c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000002d0: 203c 2f74 723e 0a20 2020 2020 2020 2020   </tr>.         
+000002e0: 2020 2025 656c 7365 3a0a 2020 2020 2020     %else:.      
+000002f0: 2020 2020 2020 2020 2020 3c74 7220 636c            <tr cl
+00000300: 6173 733d 2268 6561 6465 7222 3e0a 2020  ass="header">.  
+00000310: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000320: 2020 3c74 6420 636c 6173 733d 2268 616c    <td class="hal
+00000330: 665f 7769 6474 6822 3e44 6179 3c2f 7464  f_width">Day</td
+00000340: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+00000350: 2020 2020 2020 3c74 6420 636c 6173 733d        <td class=
+00000360: 2268 616c 665f 7769 6474 6822 3e4e 6577  "half_width">New
+00000370: 2052 6567 6973 7472 6174 696f 6e73 3c2f   Registrations</
+00000380: 7464 3e0a 2020 2020 2020 2020 2020 2020  td>.            
+00000390: 2020 2020 3c2f 7472 3e0a 2020 2020 2020      </tr>.      
+000003a0: 2020 2020 2020 2020 2020 3c25 2063 7472            <% ctr
+000003b0: 203d 2030 2025 3e0a 2020 2020 2020 2020   = 0 %>.        
+000003c0: 2020 2020 2020 2020 2566 6f72 2075 7365          %for use
+000003d0: 7220 696e 2075 7365 7273 3a0a 2020 2020  r in users:.    
+000003e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000003f0: 2569 6620 6374 7220 2520 3220 3d3d 2031  %if ctr % 2 == 1
+00000400: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00000410: 2020 2020 2020 2020 2020 3c74 7220 636c            <tr cl
+00000420: 6173 733d 226f 6464 5f72 6f77 223e 0a20  ass="odd_row">. 
+00000430: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000440: 2020 2025 656c 7365 3a0a 2020 2020 2020     %else:.      
+00000450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000460: 2020 3c74 7220 636c 6173 733d 2274 7222    <tr class="tr"
+00000470: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
+00000480: 2020 2020 2020 2565 6e64 6966 0a20 2020        %endif.   
+00000490: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000004a0: 2020 2020 203c 7464 3e0a 2020 2020 2020       <td>.      
+000004b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000004c0: 2020 2020 2020 3c61 2068 7265 663d 2224        <a href="$
+000004d0: 7b68 2e75 726c 5f66 6f72 2820 636f 6e74  {h.url_for( cont
+000004e0: 726f 6c6c 6572 3d27 7573 6572 7327 2c20  roller='users', 
+000004f0: 6163 7469 6f6e 3d27 7370 6563 6966 6965  action='specifie
+00000500: 645f 6461 7465 272c 2073 7065 6369 6669  d_date', specifi
+00000510: 6564 5f64 6174 653d 7573 6572 5b30 5d20  ed_date=user[0] 
+00000520: 297d 223e 0a20 2020 2020 2020 2020 2020  )}">.           
 00000530: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000540: 2020 2020 203c 7468 2062 6763 6f6c 6f72       <th bgcolor
-00000550: 3d22 2344 3844 3844 3822 3e4e 616d 653c  ="#D8D8D8">Name<
-00000560: 2f74 683e 0a20 2020 2020 2020 2020 2020  /th>.           
-00000570: 2020 2020 2020 2020 2020 2020 203c 7468               <th
-00000580: 2062 6763 6f6c 6f72 3d22 2344 3844 3844   bgcolor="#D8D8D
-00000590: 3822 3e4f 776e 6572 3c2f 7468 3e0a 2020  8">Owner</th>.  
+00000540: 2020 2020 2024 7b75 7365 725b 335d 7d2c       ${user[3]},
+00000550: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000560: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000570: 2026 6e62 7370 3b24 7b6d 6f6e 7468 5f6c   &nbsp;${month_l
+00000580: 6162 656c 7d26 6e62 7370 3b24 7b75 7365  abel}&nbsp;${use
+00000590: 725b 315d 7d2c 0a20 2020 2020 2020 2020  r[1]},.         
 000005a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000005b0: 2020 2020 2020 3c74 6820 6267 636f 6c6f        <th bgcolo
-000005c0: 723d 2223 4438 4438 4438 223e 5479 7065  r="#D8D8D8">Type
-000005d0: 3c2f 7468 3e0a 2020 2020 2020 2020 2020  </th>.          
-000005e0: 2020 2020 2020 2020 2020 3c2f 7472 3e0a            </tr>.
+000005b0: 2020 2020 2020 2026 6e62 7370 3b24 7b79         &nbsp;${y
+000005c0: 6561 725f 6c61 6265 6c7d 0a20 2020 2020  ear_label}.     
+000005d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000005e0: 2020 2020 2020 203c 2f61 3e0a 2020 2020         </a>.    
 000005f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000600: 2020 2020 2566 6f72 2072 6570 6f73 6974      %for reposit
-00000610: 6f72 795f 7475 7020 696e 2072 6570 6f73  ory_tup in repos
-00000620: 6974 6f72 795f 7475 7073 3a0a 2020 2020  itory_tups:.    
-00000630: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000640: 2020 2020 3c25 206e 616d 652c 206f 776e      <% name, own
-00000650: 6572 2c20 7479 7065 203d 2072 6570 6f73  er, type = repos
-00000660: 6974 6f72 795f 7475 7020 253e 0a20 2020  itory_tup %>.   
-00000670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000680: 2020 2020 203c 7472 3e0a 2020 2020 2020       <tr>.      
-00000690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000006a0: 2020 2020 2020 3c74 643e 247b 206e 616d        <td>${ nam
-000006b0: 6520 7c20 6820 7d3c 2f74 643e 0a20 2020  e | h }</td>.   
-000006c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000006d0: 2020 2020 2020 2020 203c 7464 3e24 7b20           <td>${ 
-000006e0: 6f77 6e65 7220 7c20 6820 7d3c 2f74 643e  owner | h }</td>
-000006f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000700: 2020 2020 2020 2020 2020 2020 203c 7464               <td
-00000710: 3e24 7b20 7479 7065 207c 2068 207d 3c2f  >${ type | h }</
-00000720: 7464 3e0a 2020 2020 2020 2020 2020 2020  td>.            
-00000730: 2020 2020 2020 2020 2020 2020 3c2f 7472              </tr
-00000740: 3e0a 2020 2020 2020 2020 2020 2020 2020  >.              
-00000750: 2020 2020 2020 2565 6e64 666f 720a 2020        %endfor.  
-00000760: 2020 2020 2020 2020 2020 2020 2020 3c2f                </
-00000770: 7461 626c 653e 0a20 2020 2020 2020 2020  table>.         
-00000780: 2020 203c 2f64 6976 3e0a 2020 2020 2020     </div>.      
-00000790: 2020 2020 2020 3c64 6976 2073 7479 6c65        <div style
-000007a0: 3d22 636c 6561 723a 2062 6f74 6822 3e3c  ="clear: both"><
-000007b0: 2f64 6976 3e0a 2020 2020 2020 2020 2020  /div>.          
-000007c0: 2020 3c64 6976 2063 6c61 7373 3d22 666f    <div class="fo
-000007d0: 726d 2d72 6f77 223e 0a20 2020 2020 2020  rm-row">.       
-000007e0: 2020 2020 2020 2020 203c 696e 7075 7420           <input 
-000007f0: 7479 7065 3d22 7375 626d 6974 2220 636c  type="submit" cl
-00000800: 6173 733d 2270 7269 6d61 7279 2d62 7574  ass="primary-but
-00000810: 746f 6e22 206e 616d 653d 2263 7265 6174  ton" name="creat
-00000820: 655f 646f 636b 6572 5f69 6d61 6765 5f62  e_docker_image_b
-00000830: 7574 746f 6e22 2076 616c 7565 3d22 4372  utton" value="Cr
-00000840: 6561 7465 2044 6f63 6b65 7220 496d 6167  eate Docker Imag
-00000850: 6522 3e0a 2020 2020 2020 2020 2020 2020  e">.            
-00000860: 3c2f 6469 763e 0a20 2020 2020 2020 203c  </div>.        <
-00000870: 2f66 6f72 6d3e 0a20 2020 203c 2f64 6976  /form>.    </div
-00000880: 3e0a 3c2f 6469 763e 0a                   >.</div>.
+00000600: 2020 2020 3c2f 7464 3e0a 2020 2020 2020      </td>.      
+00000610: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000620: 2020 3c74 643e 247b 7573 6572 5b32 5d7d    <td>${user[2]}
+00000630: 3c2f 7464 3e0a 2020 2020 2020 2020 2020  </td>.          
+00000640: 2020 2020 2020 2020 2020 3c2f 7472 3e0a            </tr>.
+00000650: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000660: 2020 2020 3c25 2063 7472 202b 3d20 3120      <% ctr += 1 
+00000670: 253e 0a20 2020 2020 2020 2020 2020 2020  %>.             
+00000680: 2020 2025 656e 6466 6f72 0a20 2020 2020     %endfor.     
+00000690: 2020 2020 2020 2025 656e 6469 660a 2020         %endif.  
+000006a0: 2020 2020 2020 3c2f 7461 626c 653e 0a20        </table>. 
+000006b0: 2020 203c 2f64 6976 3e0a 3c2f 6469 763e     </div>.</div>
+000006c0: 0a                                       .
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/export_repository.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/find_tools.mako`

 * *Files 27% similar despite different names*

```diff
@@ -1,47 +1,62 @@
 <%inherit file="/base.mako"/>
 <%namespace file="/message.mako" import="render_msg" />
-<%namespace file="/webapps/tool_shed/repository/common.mako" import="*" />
-<%namespace file="/admin/tool_shed_repository/common.mako" import="*" />
-<%namespace file="/webapps/tool_shed/common/repository_actions_menu.mako" import="render_tool_shed_repository_actions" />
+<%namespace file="/webapps/tool_shed/common/common.mako" import="*" />
+<%namespace file="/webapps/tool_shed/common/repository_actions_menu.mako" import="render_galaxy_repository_actions" />
 
 <%!
    def inherit(context):
        if context.get('use_panels'):
            return '/webapps/tool_shed/base_panels.mako'
        else:
            return '/base.mako'
 %>
 <%inherit file="${inherit(context)}"/>
 
-<%def name="stylesheets()">
-    ${parent.stylesheets()}
-    ${h.css( "library" )}
-</%def>
-
-<%def name="javascripts()">
-    ${parent.javascripts()}
-    ${container_javascripts()}
-</%def>
-
-${render_tool_shed_repository_actions( repository, metadata=metadata, changeset_revision=changeset_revision )}
+%if trans.webapp.name == 'galaxy':
+    ${render_galaxy_repository_actions( repository=None )}
+%endif
 
 %if message:
     ${render_msg( message, status )}
 %endif
 
 <div class="toolForm">
-    <div class="toolFormTitle">Repository '${repository.name | h}'</div>
+    <div class="toolFormTitle">Search repositories for valid tools</div>
     <div class="toolFormBody">
-        <form name="export_repository" id="export_repository" action="${h.url_for( controller='repository', action='export', repository_id=trans.security.encode_id( repository.id ), changeset_revision=changeset_revision )}" method="post" >
-            %if containers_dict is not None and export_repository_dependencies_check_box is not None:
-                ${render_dependencies_section( None, export_repository_dependencies_check_box, None, containers_dict, revision_label=revision_label, export=True )}
-                <div style="clear: both"></div>
-            %else:
-                No repository dependencies are defined for revision <b>${revision_label}</b> of this repository, so click <b>Export</b> to export the selected revision.
-            %endif
+        <div class="form-row">
+            Valid tools are those that properly load in Galaxy.  Enter any combination of the following tool attributes to find repositories that contain 
+            valid tools matching the search criteria.<br/><br/>
+            Comma-separated strings may be entered in each field to expand search criteria.  Each field must contain the same number of comma-separated
+            strings if these types of search strings are entered in more than one field.
+        </div>
+        <div style="clear: both"></div>
+        <form name="find_tools" id="find_tools" action="${h.url_for( controller='repository', action='find_tools' )}" method="post" >
+            <div class="form-row">
+                <label>Tool id:</label>
+                <input name="tool_id" type="textfield" value="${tool_id | h}" size="40"/>
+            </div>
+            <div style="clear: both"></div>
+            <div class="form-row">
+                <label>Tool name:</label>
+                <input name="tool_name" type="textfield" value="${tool_name | h}" size="40"/>
+            </div>
+            <div style="clear: both"></div>
+            <div class="form-row">
+                <label>Tool version:</label>
+                <input name="tool_version" type="textfield" value="${tool_version | h}" size="40"/>
+            </div>
+            <div style="clear: both"></div>
+            <div class="form-row">
+                <label>Exact matches only:</label>
+                ${render_checkbox(exact_matches_check_box)}
+                <div class="toolParamHelp" style="clear: both;">
+                    Check the box to match text exactly (text case doesn't matter as all strings are forced to lower case).
+                </div>
+            </div>
+            <div style="clear: both"></div>
             <div class="form-row">
-                <input type="submit" name="export_repository_button" value="Export"/>
+                <input type="submit" value="Search repositories"/>
             </div>
         </form>
     </div>
 </div>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/find_tools.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/manage_email_alerts.mako`

 * *Files 26% similar despite different names*

```diff
@@ -1,171 +1,126 @@
 00000000: 3c25 696e 6865 7269 7420 6669 6c65 3d22  <%inherit file="
 00000010: 2f62 6173 652e 6d61 6b6f 222f 3e0a 3c25  /base.mako"/>.<%
 00000020: 6e61 6d65 7370 6163 6520 6669 6c65 3d22  namespace file="
 00000030: 2f6d 6573 7361 6765 2e6d 616b 6f22 2069  /message.mako" i
 00000040: 6d70 6f72 743d 2272 656e 6465 725f 6d73  mport="render_ms
 00000050: 6722 202f 3e0a 3c25 6e61 6d65 7370 6163  g" />.<%namespac
-00000060: 6520 6669 6c65 3d22 2f77 6562 6170 7073  e file="/webapps
-00000070: 2f74 6f6f 6c5f 7368 6564 2f63 6f6d 6d6f  /tool_shed/commo
-00000080: 6e2f 636f 6d6d 6f6e 2e6d 616b 6f22 2069  n/common.mako" i
-00000090: 6d70 6f72 743d 222a 2220 2f3e 0a3c 256e  mport="*" />.<%n
-000000a0: 616d 6573 7061 6365 2066 696c 653d 222f  amespace file="/
-000000b0: 7765 6261 7070 732f 746f 6f6c 5f73 6865  webapps/tool_she
-000000c0: 642f 636f 6d6d 6f6e 2f72 6570 6f73 6974  d/common/reposit
-000000d0: 6f72 795f 6163 7469 6f6e 735f 6d65 6e75  ory_actions_menu
-000000e0: 2e6d 616b 6f22 2069 6d70 6f72 743d 2272  .mako" import="r
-000000f0: 656e 6465 725f 6761 6c61 7879 5f72 6570  ender_galaxy_rep
-00000100: 6f73 6974 6f72 795f 6163 7469 6f6e 7322  ository_actions"
-00000110: 202f 3e0a 0a3c 2521 0a20 2020 6465 6620   />..<%!.   def 
-00000120: 696e 6865 7269 7428 636f 6e74 6578 7429  inherit(context)
-00000130: 3a0a 2020 2020 2020 2069 6620 636f 6e74  :.       if cont
-00000140: 6578 742e 6765 7428 2775 7365 5f70 616e  ext.get('use_pan
-00000150: 656c 7327 293a 0a20 2020 2020 2020 2020  els'):.         
-00000160: 2020 7265 7475 726e 2027 2f77 6562 6170    return '/webap
-00000170: 7073 2f74 6f6f 6c5f 7368 6564 2f62 6173  ps/tool_shed/bas
-00000180: 655f 7061 6e65 6c73 2e6d 616b 6f27 0a20  e_panels.mako'. 
-00000190: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
-000001a0: 2020 2020 2020 2072 6574 7572 6e20 272f         return '/
-000001b0: 6261 7365 2e6d 616b 6f27 0a25 3e0a 3c25  base.mako'.%>.<%
-000001c0: 696e 6865 7269 7420 6669 6c65 3d22 247b  inherit file="${
-000001d0: 696e 6865 7269 7428 636f 6e74 6578 7429  inherit(context)
-000001e0: 7d22 2f3e 0a0a 2569 6620 7472 616e 732e  }"/>..%if trans.
-000001f0: 7765 6261 7070 2e6e 616d 6520 3d3d 2027  webapp.name == '
-00000200: 6761 6c61 7879 273a 0a20 2020 2024 7b72  galaxy':.    ${r
-00000210: 656e 6465 725f 6761 6c61 7879 5f72 6570  ender_galaxy_rep
-00000220: 6f73 6974 6f72 795f 6163 7469 6f6e 7328  ository_actions(
-00000230: 2072 6570 6f73 6974 6f72 793d 4e6f 6e65   repository=None
-00000240: 2029 7d0a 2565 6e64 6966 0a0a 2569 6620   )}.%endif..%if 
-00000250: 6d65 7373 6167 653a 0a20 2020 2024 7b72  message:.    ${r
-00000260: 656e 6465 725f 6d73 6728 206d 6573 7361  ender_msg( messa
-00000270: 6765 2c20 7374 6174 7573 2029 7d0a 2565  ge, status )}.%e
-00000280: 6e64 6966 0a0a 3c64 6976 2063 6c61 7373  ndif..<div class
-00000290: 3d22 746f 6f6c 466f 726d 223e 0a20 2020  ="toolForm">.   
-000002a0: 203c 6469 7620 636c 6173 733d 2274 6f6f   <div class="too
-000002b0: 6c46 6f72 6d54 6974 6c65 223e 5365 6172  lFormTitle">Sear
-000002c0: 6368 2072 6570 6f73 6974 6f72 6965 7320  ch repositories 
-000002d0: 666f 7220 7661 6c69 6420 746f 6f6c 733c  for valid tools<
-000002e0: 2f64 6976 3e0a 2020 2020 3c64 6976 2063  /div>.    <div c
-000002f0: 6c61 7373 3d22 746f 6f6c 466f 726d 426f  lass="toolFormBo
-00000300: 6479 223e 0a20 2020 2020 2020 203c 6469  dy">.        <di
-00000310: 7620 636c 6173 733d 2266 6f72 6d2d 726f  v class="form-ro
-00000320: 7722 3e0a 2020 2020 2020 2020 2020 2020  w">.            
-00000330: 5661 6c69 6420 746f 6f6c 7320 6172 6520  Valid tools are 
-00000340: 7468 6f73 6520 7468 6174 2070 726f 7065  those that prope
-00000350: 726c 7920 6c6f 6164 2069 6e20 4761 6c61  rly load in Gala
-00000360: 7879 2e20 2045 6e74 6572 2061 6e79 2063  xy.  Enter any c
-00000370: 6f6d 6269 6e61 7469 6f6e 206f 6620 7468  ombination of th
-00000380: 6520 666f 6c6c 6f77 696e 6720 746f 6f6c  e following tool
-00000390: 2061 7474 7269 6275 7465 7320 746f 2066   attributes to f
-000003a0: 696e 6420 7265 706f 7369 746f 7269 6573  ind repositories
-000003b0: 2074 6861 7420 636f 6e74 6169 6e20 0a20   that contain . 
-000003c0: 2020 2020 2020 2020 2020 2076 616c 6964             valid
-000003d0: 2074 6f6f 6c73 206d 6174 6368 696e 6720   tools matching 
-000003e0: 7468 6520 7365 6172 6368 2063 7269 7465  the search crite
-000003f0: 7269 612e 3c62 722f 3e3c 6272 2f3e 0a20  ria.<br/><br/>. 
-00000400: 2020 2020 2020 2020 2020 2043 6f6d 6d61             Comma
-00000410: 2d73 6570 6172 6174 6564 2073 7472 696e  -separated strin
-00000420: 6773 206d 6179 2062 6520 656e 7465 7265  gs may be entere
-00000430: 6420 696e 2065 6163 6820 6669 656c 6420  d in each field 
-00000440: 746f 2065 7870 616e 6420 7365 6172 6368  to expand search
-00000450: 2063 7269 7465 7269 612e 2020 4561 6368   criteria.  Each
-00000460: 2066 6965 6c64 206d 7573 7420 636f 6e74   field must cont
-00000470: 6169 6e20 7468 6520 7361 6d65 206e 756d  ain the same num
-00000480: 6265 7220 6f66 2063 6f6d 6d61 2d73 6570  ber of comma-sep
-00000490: 6172 6174 6564 0a20 2020 2020 2020 2020  arated.         
-000004a0: 2020 2073 7472 696e 6773 2069 6620 7468     strings if th
-000004b0: 6573 6520 7479 7065 7320 6f66 2073 6561  ese types of sea
-000004c0: 7263 6820 7374 7269 6e67 7320 6172 6520  rch strings are 
-000004d0: 656e 7465 7265 6420 696e 206d 6f72 6520  entered in more 
-000004e0: 7468 616e 206f 6e65 2066 6965 6c64 2e0a  than one field..
-000004f0: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
-00000500: 2020 2020 2020 203c 6469 7620 7374 796c         <div styl
-00000510: 653d 2263 6c65 6172 3a20 626f 7468 223e  e="clear: both">
-00000520: 3c2f 6469 763e 0a20 2020 2020 2020 203c  </div>.        <
-00000530: 666f 726d 206e 616d 653d 2266 696e 645f  form name="find_
-00000540: 746f 6f6c 7322 2069 643d 2266 696e 645f  tools" id="find_
-00000550: 746f 6f6c 7322 2061 6374 696f 6e3d 2224  tools" action="$
-00000560: 7b68 2e75 726c 5f66 6f72 2820 636f 6e74  {h.url_for( cont
-00000570: 726f 6c6c 6572 3d27 7265 706f 7369 746f  roller='reposito
-00000580: 7279 272c 2061 6374 696f 6e3d 2766 696e  ry', action='fin
-00000590: 645f 746f 6f6c 7327 2029 7d22 206d 6574  d_tools' )}" met
-000005a0: 686f 643d 2270 6f73 7422 203e 0a20 2020  hod="post" >.   
-000005b0: 2020 2020 2020 2020 203c 6469 7620 636c           <div cl
-000005c0: 6173 733d 2266 6f72 6d2d 726f 7722 3e0a  ass="form-row">.
-000005d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000005e0: 3c6c 6162 656c 3e54 6f6f 6c20 6964 3a3c  <label>Tool id:<
-000005f0: 2f6c 6162 656c 3e0a 2020 2020 2020 2020  /label>.        
-00000600: 2020 2020 2020 2020 3c69 6e70 7574 206e          <input n
-00000610: 616d 653d 2274 6f6f 6c5f 6964 2220 7479  ame="tool_id" ty
-00000620: 7065 3d22 7465 7874 6669 656c 6422 2076  pe="textfield" v
-00000630: 616c 7565 3d22 247b 746f 6f6c 5f69 6420  alue="${tool_id 
-00000640: 7c20 687d 2220 7369 7a65 3d22 3430 222f  | h}" size="40"/
-00000650: 3e0a 2020 2020 2020 2020 2020 2020 3c2f  >.            </
-00000660: 6469 763e 0a20 2020 2020 2020 2020 2020  div>.           
-00000670: 203c 6469 7620 7374 796c 653d 2263 6c65   <div style="cle
-00000680: 6172 3a20 626f 7468 223e 3c2f 6469 763e  ar: both"></div>
-00000690: 0a20 2020 2020 2020 2020 2020 203c 6469  .            <di
-000006a0: 7620 636c 6173 733d 2266 6f72 6d2d 726f  v class="form-ro
-000006b0: 7722 3e0a 2020 2020 2020 2020 2020 2020  w">.            
-000006c0: 2020 2020 3c6c 6162 656c 3e54 6f6f 6c20      <label>Tool 
-000006d0: 6e61 6d65 3a3c 2f6c 6162 656c 3e0a 2020  name:</label>.  
-000006e0: 2020 2020 2020 2020 2020 2020 2020 3c69                <i
-000006f0: 6e70 7574 206e 616d 653d 2274 6f6f 6c5f  nput name="tool_
-00000700: 6e61 6d65 2220 7479 7065 3d22 7465 7874  name" type="text
-00000710: 6669 656c 6422 2076 616c 7565 3d22 247b  field" value="${
-00000720: 746f 6f6c 5f6e 616d 6520 7c20 687d 2220  tool_name | h}" 
-00000730: 7369 7a65 3d22 3430 222f 3e0a 2020 2020  size="40"/>.    
-00000740: 2020 2020 2020 2020 3c2f 6469 763e 0a20          </div>. 
-00000750: 2020 2020 2020 2020 2020 203c 6469 7620             <div 
-00000760: 7374 796c 653d 2263 6c65 6172 3a20 626f  style="clear: bo
-00000770: 7468 223e 3c2f 6469 763e 0a20 2020 2020  th"></div>.     
-00000780: 2020 2020 2020 203c 6469 7620 636c 6173         <div clas
-00000790: 733d 2266 6f72 6d2d 726f 7722 3e0a 2020  s="form-row">.  
-000007a0: 2020 2020 2020 2020 2020 2020 2020 3c6c                <l
-000007b0: 6162 656c 3e54 6f6f 6c20 7665 7273 696f  abel>Tool versio
-000007c0: 6e3a 3c2f 6c61 6265 6c3e 0a20 2020 2020  n:</label>.     
-000007d0: 2020 2020 2020 2020 2020 203c 696e 7075             <inpu
-000007e0: 7420 6e61 6d65 3d22 746f 6f6c 5f76 6572  t name="tool_ver
-000007f0: 7369 6f6e 2220 7479 7065 3d22 7465 7874  sion" type="text
-00000800: 6669 656c 6422 2076 616c 7565 3d22 247b  field" value="${
-00000810: 746f 6f6c 5f76 6572 7369 6f6e 207c 2068  tool_version | h
-00000820: 7d22 2073 697a 653d 2234 3022 2f3e 0a20  }" size="40"/>. 
-00000830: 2020 2020 2020 2020 2020 203c 2f64 6976             </div
-00000840: 3e0a 2020 2020 2020 2020 2020 2020 3c64  >.            <d
-00000850: 6976 2073 7479 6c65 3d22 636c 6561 723a  iv style="clear:
-00000860: 2062 6f74 6822 3e3c 2f64 6976 3e0a 2020   both"></div>.  
-00000870: 2020 2020 2020 2020 2020 3c64 6976 2063            <div c
-00000880: 6c61 7373 3d22 666f 726d 2d72 6f77 223e  lass="form-row">
-00000890: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000008a0: 203c 6c61 6265 6c3e 4578 6163 7420 6d61   <label>Exact ma
-000008b0: 7463 6865 7320 6f6e 6c79 3a3c 2f6c 6162  tches only:</lab
-000008c0: 656c 3e0a 2020 2020 2020 2020 2020 2020  el>.            
-000008d0: 2020 2020 247b 7265 6e64 6572 5f63 6865      ${render_che
-000008e0: 636b 626f 7828 6578 6163 745f 6d61 7463  ckbox(exact_matc
-000008f0: 6865 735f 6368 6563 6b5f 626f 7829 7d0a  hes_check_box)}.
-00000900: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000910: 3c64 6976 2063 6c61 7373 3d22 746f 6f6c  <div class="tool
-00000920: 5061 7261 6d48 656c 7022 2073 7479 6c65  ParamHelp" style
-00000930: 3d22 636c 6561 723a 2062 6f74 683b 223e  ="clear: both;">
-00000940: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000950: 2020 2020 2043 6865 636b 2074 6865 2062       Check the b
-00000960: 6f78 2074 6f20 6d61 7463 6820 7465 7874  ox to match text
-00000970: 2065 7861 6374 6c79 2028 7465 7874 2063   exactly (text c
-00000980: 6173 6520 646f 6573 6e27 7420 6d61 7474  ase doesn't matt
-00000990: 6572 2061 7320 616c 6c20 7374 7269 6e67  er as all string
-000009a0: 7320 6172 6520 666f 7263 6564 2074 6f20  s are forced to 
-000009b0: 6c6f 7765 7220 6361 7365 292e 0a20 2020  lower case)..   
-000009c0: 2020 2020 2020 2020 2020 2020 203c 2f64               </d
-000009d0: 6976 3e0a 2020 2020 2020 2020 2020 2020  iv>.            
-000009e0: 3c2f 6469 763e 0a20 2020 2020 2020 2020  </div>.         
-000009f0: 2020 203c 6469 7620 7374 796c 653d 2263     <div style="c
-00000a00: 6c65 6172 3a20 626f 7468 223e 3c2f 6469  lear: both"></di
-00000a10: 763e 0a20 2020 2020 2020 2020 2020 203c  v>.            <
-00000a20: 6469 7620 636c 6173 733d 2266 6f72 6d2d  div class="form-
-00000a30: 726f 7722 3e0a 2020 2020 2020 2020 2020  row">.          
-00000a40: 2020 2020 2020 3c69 6e70 7574 2074 7970        <input typ
-00000a50: 653d 2273 7562 6d69 7422 2076 616c 7565  e="submit" value
-00000a60: 3d22 5365 6172 6368 2072 6570 6f73 6974  ="Search reposit
-00000a70: 6f72 6965 7322 2f3e 0a20 2020 2020 2020  ories"/>.       
-00000a80: 2020 2020 203c 2f64 6976 3e0a 2020 2020       </div>.    
-00000a90: 2020 2020 3c2f 666f 726d 3e0a 2020 2020      </form>.    
-00000aa0: 3c2f 6469 763e 0a3c 2f64 6976 3e0a       </div>.</div>.
+00000060: 6520 6669 6c65 3d22 2f63 6f6d 6d6f 6e2f  e file="/common/
+00000070: 636f 6d6d 6f6e 2e6d 616b 6f22 2069 6d70  common.mako" imp
+00000080: 6f72 743d 2272 656e 6465 725f 6368 6563  ort="render_chec
+00000090: 6b62 6f78 2220 2f3e 0a0a 3c62 722f 3e3c  kbox" />..<br/><
+000000a0: 6272 2f3e 0a3c 756c 2063 6c61 7373 3d22  br/>.<ul class="
+000000b0: 6d61 6e61 6765 2d74 6162 6c65 2d61 6374  manage-table-act
+000000c0: 696f 6e73 223e 0a20 2020 203c 6c69 3e0a  ions">.    <li>.
+000000d0: 2020 2020 2020 2020 3c61 2063 6c61 7373          <a class
+000000e0: 3d22 6163 7469 6f6e 2d62 7574 746f 6e22  ="action-button"
+000000f0: 2020 6872 6566 3d22 247b 682e 7572 6c5f    href="${h.url_
+00000100: 666f 7228 2063 6f6e 7472 6f6c 6c65 723d  for( controller=
+00000110: 2772 6570 6f73 6974 6f72 7927 2c20 6163  'repository', ac
+00000120: 7469 6f6e 3d27 6d75 6c74 695f 7365 6c65  tion='multi_sele
+00000130: 6374 5f65 6d61 696c 5f61 6c65 7274 7327  ct_email_alerts'
+00000140: 2029 7d22 3e4d 616e 6167 6520 7265 706f   )}">Manage repo
+00000150: 7369 746f 7279 2061 6c65 7274 733c 2f61  sitory alerts</a
+00000160: 3e0a 2020 2020 3c2f 6c69 3e0a 2020 2020  >.    </li>.    
+00000170: 3c6c 693e 0a20 2020 2020 2020 203c 6120  <li>.        <a 
+00000180: 636c 6173 733d 2261 6374 696f 6e2d 6275  class="action-bu
+00000190: 7474 6f6e 2220 2068 7265 663d 2224 7b68  tton"  href="${h
+000001a0: 2e75 726c 5f66 6f72 2820 636f 6e74 726f  .url_for( contro
+000001b0: 6c6c 6572 3d27 7573 6572 272c 2061 6374  ller='user', act
+000001c0: 696f 6e3d 2769 6e64 6578 272c 2063 6e74  ion='index', cnt
+000001d0: 726c 6c65 723d 2772 6570 6f73 6974 6f72  rller='repositor
+000001e0: 7927 2029 7d22 3e55 7365 7220 7072 6566  y' )}">User pref
+000001f0: 6572 656e 6365 733c 2f61 3e0a 2020 2020  erences</a>.    
+00000200: 3c2f 6c69 3e0a 3c2f 756c 3e0a 0a25 6966  </li>.</ul>..%if
+00000210: 206d 6573 7361 6765 3a0a 2020 2020 247b   message:.    ${
+00000220: 7265 6e64 6572 5f6d 7367 2820 6d65 7373  render_msg( mess
+00000230: 6167 652c 2073 7461 7475 7320 297d 0a25  age, status )}.%
+00000240: 656e 6469 660a 0a3c 6469 7620 636c 6173  endif..<div clas
+00000250: 733d 2274 6f6f 6c46 6f72 6d22 3e0a 2020  s="toolForm">.  
+00000260: 2020 3c64 6976 2063 6c61 7373 3d22 746f    <div class="to
+00000270: 6f6c 466f 726d 5469 746c 6522 3e45 6d61  olFormTitle">Ema
+00000280: 696c 2061 6c65 7274 7320 666f 7220 6e65  il alerts for ne
+00000290: 7720 7265 706f 7369 746f 7269 6573 3c2f  w repositories</
+000002a0: 6469 763e 0a20 2020 203c 666f 726d 206e  div>.    <form n
+000002b0: 616d 653d 226e 6577 5f72 6570 6f5f 616c  ame="new_repo_al
+000002c0: 6572 7422 2069 643d 226e 6577 5f72 6570  ert" id="new_rep
+000002d0: 6f5f 616c 6572 7422 2061 6374 696f 6e3d  o_alert" action=
+000002e0: 2224 7b68 2e75 726c 5f66 6f72 2820 636f  "${h.url_for( co
+000002f0: 6e74 726f 6c6c 6572 3d27 7265 706f 7369  ntroller='reposi
+00000300: 746f 7279 272c 2061 6374 696f 6e3d 276d  tory', action='m
+00000310: 616e 6167 655f 656d 6169 6c5f 616c 6572  anage_email_aler
+00000320: 7473 2720 297d 2220 6d65 7468 6f64 3d22  ts' )}" method="
+00000330: 706f 7374 2220 3e0a 2020 2020 2020 2020  post" >.        
+00000340: 3c64 6976 2063 6c61 7373 3d22 666f 726d  <div class="form
+00000350: 2d72 6f77 223e 0a20 2020 2020 2020 2020  -row">.         
+00000360: 2020 203c 6c61 6265 6c3e 4e65 7720 7265     <label>New re
+00000370: 706f 7369 746f 7279 2061 6c65 7274 3a3c  pository alert:<
+00000380: 2f6c 6162 656c 3e0a 2020 2020 2020 2020  /label>.        
+00000390: 2020 2020 247b 7265 6e64 6572 5f63 6865      ${render_che
+000003a0: 636b 626f 7828 6e65 775f 7265 706f 5f61  ckbox(new_repo_a
+000003b0: 6c65 7274 5f63 6865 636b 5f62 6f78 297d  lert_check_box)}
+000003c0: 0a20 2020 2020 2020 2020 2020 203c 6469  .            <di
+000003d0: 7620 636c 6173 733d 2274 6f6f 6c50 6172  v class="toolPar
+000003e0: 616d 4865 6c70 2220 7374 796c 653d 2263  amHelp" style="c
+000003f0: 6c65 6172 3a20 626f 7468 3b22 3e0a 2020  lear: both;">.  
+00000400: 2020 2020 2020 2020 2020 2020 2020 4368                Ch
+00000410: 6563 6b20 7468 6520 626f 7820 616e 6420  eck the box and 
+00000420: 636c 6963 6b20 3c62 3e53 6176 653c 2f62  click <b>Save</b
+00000430: 3e20 746f 2072 6563 6569 7665 2065 6d61  > to receive ema
+00000440: 696c 2077 6865 6e20 7468 6520 6669 7273  il when the firs
+00000450: 7420 6368 616e 6765 2073 6574 2069 7320  t change set is 
+00000460: 6372 6561 7465 6420 666f 7220 6120 6e65  created for a ne
+00000470: 7720 7265 706f 7369 746f 7279 2e0a 2020  w repository..  
+00000480: 2020 2020 2020 2020 2020 3c2f 6469 763e            </div>
+00000490: 0a20 2020 2020 2020 203c 2f64 6976 3e0a  .        </div>.
+000004a0: 2020 2020 2020 2020 3c64 6976 2063 6c61          <div cla
+000004b0: 7373 3d22 666f 726d 2d72 6f77 223e 0a20  ss="form-row">. 
+000004c0: 2020 2020 2020 2020 2020 203c 696e 7075             <inpu
+000004d0: 7420 7479 7065 3d22 7375 626d 6974 2220  t type="submit" 
+000004e0: 6e61 6d65 3d22 6e65 775f 7265 706f 5f61  name="new_repo_a
+000004f0: 6c65 7274 5f62 7574 746f 6e22 2076 616c  lert_button" val
+00000500: 7565 3d22 5361 7665 222f 3e0a 2020 2020  ue="Save"/>.    
+00000510: 2020 2020 3c2f 6469 763e 0a20 2020 203c      </div>.    <
+00000520: 2f66 6f72 6d3e 0a3c 2f64 6976 3e0a 3c70  /form>.</div>.<p
+00000530: 2f3e 0a25 6966 2065 6d61 696c 5f61 6c65  />.%if email_ale
+00000540: 7274 5f72 6570 6f73 6974 6f72 6965 733a  rt_repositories:
+00000550: 0a20 2020 203c 6469 7620 636c 6173 733d  .    <div class=
+00000560: 2274 6f6f 6c46 6f72 6d22 3e0a 2020 2020  "toolForm">.    
+00000570: 2020 2020 3c64 6976 2063 6c61 7373 3d22      <div class="
+00000580: 746f 6f6c 466f 726d 5469 746c 6522 3e59  toolFormTitle">Y
+00000590: 6f75 2061 7265 2072 6567 6973 7465 7265  ou are registere
+000005a0: 6420 746f 2072 6563 6569 7665 2065 6d61  d to receive ema
+000005b0: 696c 2061 6c65 7274 7320 666f 7220 6368  il alerts for ch
+000005c0: 616e 6765 7320 746f 2074 6865 2066 6f6c  anges to the fol
+000005d0: 6c6f 7769 6e67 2072 6570 6f73 6974 6f72  lowing repositor
+000005e0: 6965 733c 2f64 6976 3e0a 2020 2020 2020  ies</div>.      
+000005f0: 2020 3c64 6976 2063 6c61 7373 3d22 666f    <div class="fo
+00000600: 726d 2d72 6f77 223e 0a20 2020 2020 2020  rm-row">.       
+00000610: 2020 2020 203c 7461 626c 6520 636c 6173       <table clas
+00000620: 733d 2267 7269 6422 3e0a 2020 2020 2020  s="grid">.      
+00000630: 2020 2020 2020 2020 2020 3c74 723e 0a20            <tr>. 
+00000640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000650: 2020 203c 7468 3e4e 616d 653c 2f74 683e     <th>Name</th>
+00000660: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00000670: 2020 2020 203c 7468 3e44 6573 6372 6970       <th>Descrip
+00000680: 7469 6f6e 3c2f 7468 3e0a 2020 2020 2020  tion</th>.      
+00000690: 2020 2020 2020 2020 2020 3c2f 7472 3e0a            </tr>.
+000006a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000006b0: 2566 6f72 2072 6570 6f73 6974 6f72 7920  %for repository 
+000006c0: 696e 2065 6d61 696c 5f61 6c65 7274 5f72  in email_alert_r
+000006d0: 6570 6f73 6974 6f72 6965 733a 0a20 2020  epositories:.   
+000006e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000006f0: 203c 7472 3e0a 2020 2020 2020 2020 2020   <tr>.          
+00000700: 2020 2020 2020 2020 2020 2020 2020 3c74                <t
+00000710: 643e 247b 7265 706f 7369 746f 7279 2e6e  d>${repository.n
+00000720: 616d 6520 7c20 687d 3c2f 7464 3e0a 2020  ame | h}</td>.  
+00000730: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000740: 2020 2020 2020 3c74 643e 247b 7265 706f        <td>${repo
+00000750: 7369 746f 7279 2e64 6573 6372 6970 7469  sitory.descripti
+00000760: 6f6e 207c 2068 7d3c 2f74 643e 0a20 2020  on | h}</td>.   
+00000770: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000780: 203c 2f74 723e 0a20 2020 2020 2020 2020   </tr>.         
+00000790: 2020 2020 2020 2025 656e 6466 6f72 0a20         %endfor. 
+000007a0: 2020 2020 2020 2020 2020 203c 2f74 6162             </tab
+000007b0: 6c65 3e0a 2020 2020 2020 2020 3c2f 6469  le>.        </di
+000007c0: 763e 0a20 2020 203c 2f64 6976 3e0a 2020  v>.    </div>.  
+000007d0: 2020 3c70 2f3e 0a25 656e 6469 660a         <p/>.%endif.
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/manage_repository.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/manage_repository.mako`

 * *Files 16% similar despite different names*

```diff
@@ -34,19 +34,14 @@
         can_push = False
 
     if not is_deprecated and not is_new and ( not changeset_is_malicious or can_push ):
         can_download = True
     else:
         can_download = False
 
-    if has_metadata and not is_deprecated and trans.app.security_agent.user_can_review_repositories( trans.user ):
-        can_review_repository = True
-    else:
-        can_review_repository = False
-
     if not is_new and not is_deprecated:
         can_set_metadata = True
     else:
         can_set_metadata = False
 
     if changeset_revision == repository.tip():
         changeset_revision_is_repository_tip = True
@@ -128,19 +123,15 @@
     <div class="toolForm">
         <div class="toolFormTitle">Repository revision</div>
         <div class="toolFormBody">
             <form name="change_revision" id="change_revision" action="${h.url_for( controller='repository', action='manage_repository', id=trans.security.encode_id( repository.id ) )}" method="post" >
                 <div class="form-row">
                     ${render_select(changeset_revision_select_field)} <i>${tip_str}</i>
                     <div class="toolParamHelp" style="clear: both;">
-                        %if can_review_repository:
-                            Select a revision to inspect for adding or managing a review or for download or installation.
-                        %else:
-                            Select a revision to inspect for download or installation.
-                        %endif
+                        Select a revision to inspect for download or installation.
                     </div>
                 </div>
             </form>
         </div>
     </div>
     <p/>
 %endif
@@ -339,55 +330,14 @@
                 <label>Average Rating:</label>
                 ${render_star_rating( 'avg_rating', avg_rating, disabled=True )}
                 <div style="clear: both"></div>
             </div>
         </div>
     </div>
     <p/>
-    <div class="toolForm">
-        <div class="toolFormBody">
-            %if display_reviews:
-                <div class="form-row">
-                    <a href="${h.url_for( controller='repository', action='view_repository', id=trans.security.encode_id( repository.id ), display_reviews=False )}"><label>Hide Reviews</label></a>
-                </div>
-                <div style="clear: both"></div>
-                <div class="form-row">
-                    <table class="grid">
-                        <thead>
-                            <tr>
-                                <th>Rating</th>
-                                <th>Comments</th>
-                                <th>Reviewed</th>
-                                <th>User</th>
-                            </tr>
-                        </thead>
-                        <% count = 0 %>
-                        %for review in repository.ratings:
-                            <%
-                                count += 1
-                                name = 'rating%d' % count
-                            %>
-                            <tr>
-                                <td>${render_star_rating( name, review.rating, disabled=True )}</td>
-                                <td>${render_review_comment( to_html_string( review.comment ) )}</td>
-                                <td>${time_ago( review.update_time )}</td>
-                                <td>${review.user.username | h}</td>
-                            </tr>
-                        %endfor
-                    </table>
-                </div>
-                <div style="clear: both"></div>
-            %else:
-                <div class="form-row">
-                    <a href="${h.url_for( controller='repository', action='view_repository', id=trans.security.encode_id( repository.id ), display_reviews=True )}"><label>Display Reviews</label></a>
-                </div>
-                <div style="clear: both"></div>
-            %endif
-        </div>
-    </div>
 %endif
 <p/>
 %if can_set_malicious:
     <p/>
     <div class="toolForm">
         <div class="toolFormTitle">Malicious repository tip</div>
         <div class="toolFormBody">
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/preview_tools_in_changeset.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/preview_tools_in_changeset.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/tool_form.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/tool_form.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/upload.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/upload.mako`

 * *Files 4% similar despite different names*

```diff
@@ -19,15 +19,14 @@
 <%def name="stylesheets()">
     ${parent.stylesheets()}
     ${h.css( "dynatree_skin/ui.dynatree" )}
 </%def>
 
 <%def name="javascripts()">
     ${parent.javascripts()}
-    ## ${h.js( "libs/jquery/jquery-ui", "libs/jquery/jquery.cookie", "libs/jquery/jquery.dynatree" )}
     ${common_javascripts(repository)}
     <script type="text/javascript">
     $( function() {
         $( "select[refresh_on_change='true']").change( function() {
             $( "#upload_form" ).submit();
         });
     });
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/view_changelog.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/view_changelog.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository/view_changeset.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/repository/view_changeset.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/repository_review/edit_component.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/username.mako`

 * *Files 26% similar despite different names*

```diff
@@ -1,37 +1,27 @@
 <%inherit file="/base.mako"/>
 <%namespace file="/message.mako" import="render_msg" />
 
 %if message:
     ${render_msg( message, status )}
 %endif
 
+<% is_admin = cntrller == 'admin' and trans.user_is_admin %>
+
+<h2>Manage Public Name</h2>
 <div class="toolForm">
-    <div class="toolFormTitle">Change component description</div>
-    <div class="toolFormBody">
-        <form name="edit_component" action="${h.url_for( controller='repository_review', action='edit_component' )}" method="post" >
-            <div class="form-row">
-                <label>Name:</label>
-                <div style="float: left; width: 250px; margin-right: 10px;">
-                    ${component.name | h}
-                </div>
-                <div style="clear: both"></div>
-            </div>
-            <div class="form-row">
-                <label>Description:</label>
-                <div style="float: left; width: 250px; margin-right: 10px;">
-                    <input  name="description" type="textfield" value="${component.description | h}" size=40"/>
-                </div>
-                <div style="clear: both"></div>
-            </div>
-            <div class="form-row">
-                <div style="float: left; width: 250px; margin-right: 10px;">
-                    <input type="hidden" name="id" value="${trans.security.encode_id( component.id )}"/>
-                </div>
-                <div style="clear: both"></div>
-            </div>
-            <div class="form-row">
-                <input type="submit" name="edit_component_button" value="Save"/>
+    <form name="username" id="username" action="${h.url_for( controller='user', action='edit_username', cntrller=cntrller, user_id=trans.security.encode_id( user.id ) )}" method="post" >
+        <div class="toolFormTitle">Login Information</div>
+        <div class="form-row">
+            <label>Public name:</label>
+            <input type="text" name="username" size="40" value="${username}"/>
+            <div class="toolParamHelp" style="clear: both;">
+                Your public name is an identifier that will be used to generate addresses for information
+                you share publicly. Public names must be at least four characters in length and contain only lower-case
+                letters, numbers, dots, underscores, and dashes ('.', '_', '-').
             </div>
-        </form>
-    </div>
+        </div>
+        <div class="form-row">
+            <input type="submit" name="change_username_button" value="Save"/>
+        </div>
+    </form>
 </div>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/role/role.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/role/role.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/api_keys.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/api_keys.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/index.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/index.mako`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 <%inherit file="/base.mako"/>
 
 %if trans.user:
-    <h2>${_('User preferences')}</h2>
+    <h2>User preferences</h2>
     <p>You are currently logged in as ${trans.user.email|h}.</p>
     <ul>
-        <li><a href="${h.url_for( controller='user', action='manage_user_info', cntrller=cntrller )}">${_('Manage your information')}</a></li>
-        <li><a href="${h.url_for( controller='user', action='change_password', id=trans.app.security.encode_id(trans.user.id) )}">${_('Change your password')}</a></li>
-        <li><a href="${h.url_for( controller='user', action='api_keys', cntrller=cntrller )}">${_('Manage your API keys')}</a></li>
-        <li><a href="${h.url_for( controller='repository', action='manage_email_alerts', cntrller=cntrller )}">${_('Manage your email alerts')}</a></li>
-        <li><a href="${h.url_for( controller='user', action='logout', logout_all=True )}" target="_top">${_('Logout')}</a> ${_('of all user sessions')}</li>
+        <li><a href="${h.url_for( controller='user', action='manage_user_info', cntrller=cntrller )}">Manage your information</a></li>
+        <li><a href="${h.url_for( controller='user', action='change_password', id=trans.app.security.encode_id(trans.user.id) )}">Change your password</a></li>
+        <li><a href="${h.url_for( controller='user', action='api_keys', cntrller=cntrller )}">Manage your API keys</a></li>
+        <li><a href="${h.url_for( controller='repository', action='manage_email_alerts', cntrller=cntrller )}">Manage your email alerts</a></li>
+        <li><a href="${h.url_for( controller='user', action='logout', logout_all=True )}" target="_top">Logout</a> of all user sessions</li>
     </ul>
 %else:
     %if not message:
-        <p>${n_('You are currently not logged in.')}</p>
+        <p>You are currently not logged in.</p>
     %endif
     <ul>
-        <li><a href="${h.url_for( controller='user', action='login' )}">${_('Login')}</li>
-        <li><a href="${h.url_for( controller='user', action='create', cntrller='user' )}">${_('Register')}</a></li>
+        <li><a href="${h.url_for( controller='user', action='login' )}">Login</li>
+        <li><a href="${h.url_for( controller='user', action='create', cntrller='user' )}">Register</a></li>
     </ul>
 %endif
```

#### html2text {}

```diff
@@ -1,14 +1,14 @@
 <%inherit file="/base.mako"/> %if trans.user:
-***** ${_('User preferences')} *****
+***** User preferences *****
 You are currently logged in as ${trans.user.email|h}.
-    * ${_('Manage_your_information')}
-    * ${_('Change_your_password')}
-    * ${_('Manage_your_API_keys')}
-    * ${_('Manage_your_email_alerts')}
-    * ${_('Logout')} ${_('of all user sessions')}
+    * Manage_your_information
+    * Change_your_password
+    * Manage_your_API_keys
+    * Manage_your_email_alerts
+    * Logout of all user sessions
 %else: %if not message:
-${n_('You are currently not logged in.')}
+You are currently not logged in.
 %endif
-    * ${_('Login')}
-    * ${_('Register')}
+    * Login
+    * Register
 %endif
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/login.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/login.mako`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,11 @@
 <%!
 #This is a hack, we should restructure templates to avoid this.
 def inherit(context):
-    if context.get('trans').webapp.name == 'galaxy' and context.get( 'use_panels', True ):
-        return '/webapps/galaxy/base_panels.mako'
-    elif context.get('trans').webapp.name == 'tool_shed' and context.get( 'use_panels', True ):
+    if context.get('trans').webapp.name == 'tool_shed' and context.get( 'use_panels', True ):
         return '/webapps/tool_shed/base_panels.mako'
     else:
         return '/base.mako'
 %>
 
 <%inherit file="${inherit(context)}"/>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/logout.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/logout.mako`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,11 @@
 <%!
 #This is a hack, we should restructure templates to avoid this.
 def inherit(context):
-    if context.get('trans').webapp.name == 'galaxy':
-        return '/webapps/galaxy/base_panels.mako'
-    elif context.get('trans').webapp.name == 'tool_shed':
+    if context.get('trans').webapp.name == 'tool_shed':
         return '/webapps/tool_shed/base_panels.mako'
     else:
         return '/base.mako'
 %>
 
 <%inherit file="${inherit(context)}"/>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/manage_info.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/manage_info.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/register.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/register.mako`

 * *Files 6% similar despite different names*

```diff
@@ -73,22 +73,14 @@
                     $('<div/>').addClass('errormessage').insertBefore('#registrationForm');
                 }
                 console.debug( $( '#registrationForm' ) );
                 console.debug( '.errormessage:', $( '.errormessage' ) );
                 $(".errormessage").html(message);
             }
 
-            $("[name='password']").complexify({'minimumChars':6}, function(valid, complexity){
-                var progressBar = $('.progress-bar');
-                var color = valid ? 'lightgreen' : 'red';
-
-                progressBar.css('background-color', color);
-                progressBar.css({'width': complexity + '%'});
-            });
-
             $('#registration').bind('submit', function(e) {
                 $('#send').attr('disabled', 'disabled');
 
                 // we need this value to detect submitting at backend
                 var hidden_input = '<input type="hidden" id="create_user_button" name="create_user_button" value="Submit"/>';
                 $("#email_input").before(hidden_input);
 
@@ -126,19 +118,14 @@
                 <input id="email_input" type="text" name="email" value="${email | h}" size="40"/>
                 <input type="hidden" name="redirect" value="${redirect | h}" size="40"/>
             </div>
             <div class="form-row">
                 <label>Password:</label>
                 <input id="password_input" type="password" name="password" value="" size="40"/>
             </div>
-            <div class="progress">
-                <div id="complexity-bar" class="progress-bar" role="progressbar">
-                    Strength
-                </div>
-            </div>
             <div class="form-row">
                 <label>Confirm password:</label>
                 <input id="password_check_input" type="password" name="confirm" value="" size="40"/>
             </div>
             <div class="form-row">
                 <label>Public name:</label>
                 <input id="name_input" type="text" name="username" size="40" value="${username |h}"/>
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/templates/webapps/tool_shed/user/reset_password.mako` & `galaxy-web-apps-23.0.2/tool_shed/webapp/templates/webapps/tool_shed/user/reset_password.mako`

 * *Files identical despite different names*

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/util/ratings_util.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/util/ratings_util.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,16 +4,16 @@
 
 log = logging.getLogger(__name__)
 
 
 class ItemRatings(UsesItemRatings):
     """Overrides rate_item method since we also allow for comments"""
 
-    def rate_item(self, trans, user, item, rating, comment=''):
-        """ Rate an item. Return type is <item_class>RatingAssociation. """
+    def rate_item(self, trans, user, item, rating, comment=""):
+        """Rate an item. Return type is <item_class>RatingAssociation."""
         item_rating = self.get_user_item_rating(trans.sa_session, user, item, webapp_model=trans.model)
         if not item_rating:
             # User has not yet rated item; create rating.
             item_rating_assoc_class = self._get_item_rating_assoc_class(item, webapp_model=trans.model)
             item_rating = item_rating_assoc_class()
             item_rating.user = trans.user
             item_rating.set_item(item)
```

### Comparing `galaxy-web-apps-20.5.0/tool_shed/webapp/util/shed_statistics.py` & `galaxy-web-apps-23.0.2/tool_shed/webapp/util/shed_statistics.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-from time import gmtime
-from time import strftime
+from time import (
+    gmtime,
+    strftime,
+)
 
 
-class ShedCounter(object):
+class ShedCounter:
     def __init__(self, model):
         # TODO: Enhance the ShedCounter to retrieve information from the db instead of displaying what's currently in memory.
         self.model = model
         self.custom_datatypes = 0
         self.generation_time = strftime("%b %d, %Y", gmtime())
         self.deleted_repositories = 0
         self.deprecated_repositories = 0
@@ -53,42 +55,42 @@
                 processed_invalid_tool_configs = []
                 processed_relative_workflow_paths = []
                 processed_tool_ids = []
                 # A repository's metadata_revisions are those that ignore the value of the
                 # repository_metadata.downloadable column.
                 for metadata_revision in repository.metadata_revisions:
                     metadata = metadata_revision.metadata
-                    if 'tools' in metadata:
-                        tool_dicts = metadata['tools']
+                    if "tools" in metadata:
+                        tool_dicts = metadata["tools"]
                         for tool_dict in tool_dicts:
-                            if 'guid' in tool_dict:
-                                guid = tool_dict['guid']
+                            if "guid" in tool_dict:
+                                guid = tool_dict["guid"]
                                 if guid not in processed_guids:
                                     self.valid_versions_of_tools += 1
                                     processed_guids.append(guid)
-                            if 'id' in tool_dict:
-                                tool_id = tool_dict['id']
+                            if "id" in tool_dict:
+                                tool_id = tool_dict["id"]
                                 if tool_id not in processed_tool_ids:
                                     self.unique_valid_tools += 1
                                     processed_tool_ids.append(tool_id)
-                    if 'invalid_tools' in metadata:
-                        invalid_tool_configs = metadata['invalid_tools']
+                    if "invalid_tools" in metadata:
+                        invalid_tool_configs = metadata["invalid_tools"]
                         for invalid_tool_config in invalid_tool_configs:
                             if invalid_tool_config not in processed_invalid_tool_configs:
                                 self.invalid_versions_of_tools += 1
                                 processed_invalid_tool_configs.append(invalid_tool_config)
-                    if 'datatypes' in metadata:
-                        datatypes = metadata['datatypes']
+                    if "datatypes" in metadata:
+                        datatypes = metadata["datatypes"]
                         for datatypes_dict in datatypes:
-                            if 'extension' in datatypes_dict:
-                                extension = datatypes_dict['extension']
+                            if "extension" in datatypes_dict:
+                                extension = datatypes_dict["extension"]
                                 if extension not in processed_datatypes:
                                     self.custom_datatypes += 1
                                     processed_datatypes.append(extension)
-                    if 'workflows' in metadata:
-                        workflows = metadata['workflows']
+                    if "workflows" in metadata:
+                        workflows = metadata["workflows"]
                         for workflow_tup in workflows:
                             relative_path, exported_workflow_dict = workflow_tup
                             if relative_path not in processed_relative_workflow_paths:
                                 self.workflows += 1
                                 processed_relative_workflow_paths.append(relative_path)
         self.generation_time = strftime("%b %d, %Y", gmtime())
```

